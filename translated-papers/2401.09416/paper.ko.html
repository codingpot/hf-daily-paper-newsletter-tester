<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '이미지 인식 세확산 차단제: 이미지 유도 패턴 합성\n' +
      '\n' +
      '유징 예하\\({}^{13}\\,{}^{23}\\) 창이 김\\({}^{3}\\,{}^{3}\\) 루 응우옌-Phuoc\\({}^{3}\\).\n' +
      '\n' +
      '정장\\({}^{3}\\)는 카를 S 마셜\\({}^{3}\\,{}^{3}\\) 자하닌 Li\\({}^{3}\\)\n' +
      '\n' +
      '캘리포니아 대학교의 샌디에이고\\({}^{1}\\,{}^{2}\\,{}^{3}\\)입니다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '소수의 입력 이미지(3~5)에서 임의의 카테고리에 걸쳐 3D 형상을 표적으로 하기 위해 재발 가능한 질감을 전달하기 위한 새로운 이미지 유도 텍스처 합성 방법인 TextureDreamer를 제시한다. 텍스트 생성은 비전과 그래픽의 중추적인 도전입니다. 산업 기업들은 경험 있는 아티스트를 고용하여 3D 자산을 위한 텍스처를 수동으로 제작합니다. 고전적인 방법은 조밀하게 샘플링된 뷰와 정확하게 정렬된 기하학을 필요로 하는 반면, 학습 기반 방법은 데이터세트 내의 카테고리 특이적 형상에 국한된다. 대조적으로, TextureDreamer는 매우 상세하고 복잡한 텍스처를 실제 환경에서 소수의 카지노 캡처된 이미지만으로 임의의 객체들로 전달할 수 있으며, 잠재적으로 유의하게 민주화된 텍스처 생성을 촉진할 수 있다. 우리의 핵심 아이디어, 개인화된 기하학 인식 점수 증류(PGSD)은 질감 정보 추출을 위한 개인화된 모델링, 세부 외관 합성을 위한 가변 점수 증류, 제어넷을 사용한 명시적인 기하학 안내 등 확산 모델의 최근 발전에서 영감을 얻는다. 우리의 통합과 몇 가지 필수 변형은 질감의 품질을 실질적으로 향상시킵니다. 상이한 카테고리에 걸쳐 있는 실제 이미지에 대한 실험은 TextureDreamer가 매우 현실적이고 의미 있는 의미 있는 질감을 임의의 물체에 성공적으로 전달할 수 있어 이전 최첨단 기술의 시각적 품질을 능가할 수 있음을 보여준다. 프로젝트 페이지: [https://://edreamer.githubio](https://textreamer.githubio) (https://://ulsereamer.githubio)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '고품질 3D 콘텐츠는 AR/VR, 로봇 공학, 필름 및 게임을 포함한 광범위한 중요한 응용 분야에 필수불가결한 것이다. 최근 3D 재구성 [40, 42] 및 생성 모델[18, 59]의 발전으로 촉진된 3D 콘텐츠 생성 파이프라인을 민주화하는 데 괄목할만한 진전이 있었다. NRF[40]과 같은 _ge측정 성분_[8, 12, 64] 및 신경 암묵적 표현[44]을 탐색하는 데 상당한 관심이 집중되었지만 고품질 _텍스트_의 생성은 상대적으로 과소 설명된다. 텍스트는 현실적인, 매우 상세한 모습을 만드는 데 중추적이며 전통적으로 산업이 전문적이고 경험 있는 아티스트들이 텍스처를 만들기 위해 텍스처를 만들기 위해 다양한 그래픽 파이프라인에 필수적입니다. 이 과정은 일반적으로 수동으로 절차 그래프[1]와 UV 지도를 저작하여 비싸고 비효율적인 것을 포함한다. 따라서 주변 대상의 다양한 시각적 외관을 목표 기하학의 질감으로 자동으로 이전하는 것은 매우 유익할 것이다.\n' +
      '\n' +
      '우리는 희박한 이미지로부터 고품질의 재발 가능한 텍스처를 생성하기 위한 새로운 프레임워크인 _TextureDreamer_를 제시한다. 객체에 대한 무작위로 샘플링된 3~5개의 뷰를 감안할 때, 우리는 그 텍스처를 다른 카테고리에서 발생할 수 있는 표적 기하학으로 전달할 수 있다. 이전 질감 생성 방법은 보통 정렬된 기하학[3, 32, 68]으로 조밀하게 샘플링된 뷰가 필요하거나 카테고리 특이적 형상[4, 21, 46, 58]에서만 작동할 수 있기 때문에 매우 어려운 문제이다. 우리의 프레임워크는 확산 기반 생성 모델[23, 59, 60]의 최근 발전에서 영감을 얻습니다. 수십억 개의 텍스트 이미지 쌍으로 훈련된 이러한 확산 모델은 특별한 시각적 품질과 다양성[52]로 텍스트 유도 이미지 생성을 가능하게 한다. 이 사전 훈련된 2D 확산 모델을 텍스트 유도 3D 콘텐츠 생성 [34, 47, 63]에 적용한 작품들이다. 그러나 이러한 방법들 사이의 공통 한계는 그림 2에서 보여지는 바와 같이, _텍스트 전용 입력_가 복잡한 세부 패턴을 설명하는 데 충분히 발현되지 않을 수 있다는 것이다. 텍스트 유도 방법과 대조적으로, 고유한 텍스트 토큰[16, 54]으로 미리 학습된 확산 모델을 미세 조정함으로써 입력 이미지들의 작은 세트로부터 텍스처 정보를 효과적으로 추출한다. 따라서 우리의 프레임워크는 복잡한 질감을 정확하게 설명하는 문제를 다룬다.\n' +
      '\n' +
      'The Score Distillation Sampling (SDS) [47, 62] is one core element that bridges pre-trained 2D diffusion models with 3D content creation. It is widely used to generate and edit 3D contents by minimizing the discrepancy between the distribution of rendered images and the distribution defined by the pre-trained diffusion models [34, 37]. Despite its popularity, two well-known limitations impede its ability to generate high-quality textures. First, it tends to create over-smoothed and saturated appearances due to the unusually high classifier-free guidance necessary for the method to converge. Second, it lacks the knowledge to generate a 3D-consistent appearance, often resulting in multi-face artifacts and mismatches between textures and geometry.\n' +
      '\n' +
      'We propose two key design choices to tackle these challenges. Instead of using SDS, we build upon Variational Score Distillation (VSD) in our optimization approach, which can generate much more photorealistic and diverse textures. Initially introduced in ProlificDreamer [63], VSD treats the whole 3D representation as a random variable and aligns its distribution with the pre-trained diffusion model. It does not need a large classifier-free guidance weight to converge, which is essential to create a realistic and diverse appearance. However, naively applying VSD update does not suffice for generating high-quality textures in our application. We identify a simple modification that can improve texture quality while slightly reducing the computational cost. Additionally, VSD loss alone cannot fully solve the 3D consistency issue. Fine-tuning on sparse inputs makes converging harder, as observed by previous work [51]. We, therefore, explicitly condition our texture generation process on geometry information extracted from the given mesh by injecting rendered normal maps into the fine-tuned diffusion model through the ControlNet [67] architecture. Our framework, designated as personalized geometry aware score distillation (PGSD), can effectively transfer highly detailed textures to diverse geometry in a semantically meaningful and visually appealing manner. Extensive qualitative and quantitative experiments demonstrate that our framework substantially outperforms state-of-the-art texture-transfer methods.\n' +
      '\n' +
      '2번으로 작업했습니다.\n' +
      '\n' +
      '**exture 합성 및 재구성** 클래식 텍스처 생성 방법은 동네 [13, 28], 타일링 반복 패턴[29]에서 파생된 분포로부터 샘플링하거나 대상체에 멀티뷰 이미지를 융합하는 것을 포함한다.\n' +
      '\n' +
      '그림 2: ** 텍스트 유도 텍스트링의 제한***는 이미지의 모든 세부 사항을 표현할 수 없는 텍스트 프롬프트를 생성하기 위해 자막 방법을 필요로 하는 텍스트 유도 텍스트링 방법에 따라 이미지 기반 가이드 텍스트링은 더 효과적이고 표현적일 수 있다. BLIP[33]에 의해 이미지 캡션이 예측되며, TEXTure[53]을 통해 텍스트 유도 텍스트링이 생성되며 이미지 유도 결과는 우리의 방법에서 비롯된다.\n' +
      '\n' +
      '표면 [3, 32, 68]. 전자는 의미론적 의미 있는 질감을 만드는 데 짧지만 후자는 매우 정확한 기하학 재구성이 필요하다. 대규모 3D 데이터세트[4, 11, 21, 46, 58]에서 텍스처 생성을 학습하되 데이터세트 내에서 특정 카테고리에 국한되도록 수많은 학습 기반 방법을 제안하였다. 최근의 작품들은 또한 임의의 객체들의 텍스트 유도 텍스처 생성(31, 36, 39, 41)에 CLIP 모델[50]을 사용하지만, 그 질감 특성은 보통 낮다. 대조적으로, Texture-Dreamer는 미관련 희소 이미지를 사용하여 임의의 객체에 대한 의미 있는 고품질 텍스처를 생성할 수 있다. 전통적으로 질감은 2D 이미지로 표현되며 UV 매핑을 통해 표면을 객체화할 것으로 예상된다. 신경 암묵적 표현의 최근 진행 상황을 추적해보면, 우리의 방법은 역 렌더링 [5, 7, 17, 61] 및 3D 생성[7, 17]의 최근 발전과 함께 신경 암묵적 텍스처 필드로 질감을 나타낸다.\n' +
      '\n' +
      '**Diffusion models** Diffusion models have emerged as the state-of-the-art generative models [23, 60], demonstrating exceptional visual quality [52]. Its training and inference involve iteratively adding noise with different variances and denoise the data. Trained on internet-scale image-text pair datasets [52], these pre-trained models exhibit unprecedented capability in text-guided image synthesis and have proven successful in various image editing tasks. Recent works also manage to fine-tune pre-trained diffusion models on much smaller datasets or even a few images to facilitate customized/personalized image synthesis [54] and image generation conditioned on multi-modal data [67], such as normal and semantic maps. Building upon this progress, TextureDreamer can effectively extract texture information from sparse views and transfer it to a novel target object in a geometry-aware manner.\n' +
      '\n' +
      '**3D generation with 2D diffusion priors** Diffusion-based 3D content creation has very recently gained substantial interest. Several methods directly train 3D diffusion models to generate 3D content in various representations, including point cloud [35], neural radiance filed [26], hypernetwork [14] and texture [66]. Others utilize pre-trained 2D diffusion models by either progressively fusing generated images from different views [2, 6, 9, 53] or optimizing the 3D representation through score distillation sampling [34, 37, 47] and its improved variations [27, 63]. While many methods concentrate on text-guided 3D generation, fewer attempt to leverage diffusion models to generate 3D content from images. A number of concurrent works fine-tune 2D diffusion models on large-scale 3D datasets for sparse view reconstruction [48, 57], primarily focusing on whole 3D object reconstruction. In contrast, Texture-Dreamer targets transferring textures from a small number of images to a target 3D shape with unmatched geometry. Dreambooth3D [51] and TEXTure [53] extract information from sparse views into a new text token and fine-tuned diffusion model weights, which can be used to generate personalized 3D object or texture unseen objects. Texture-Dreamer employs a similar method to extract information from sparse images. However, it differs from prior works on utilizing the extracted information for texture generation, leading to improvements in consistency and photorealism.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '우리는 물체의 3-5 입력 이미지와 유사한 외관을 가진 주어진 메쉬에 대해 기하학 인식 질감을 합성하는 프레임워크인 TextureDreamer를 제안한다. 3.1절에서 우리는 먼저 드림보트[54], 컨트롤넷[67]에 대한 예선을 소개하고 점수 증류 샘플링[47, 62, 63]을 소개한다. 3.2절에서는 희박한 이미지에서 임의의 기하학으로의 고품질의 이미지 유도 질감 전달을 가능하게 하는 핵심 기술 기여인 개인화된 기하학 인식 점수 증류(PGSD)을 제안한다.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '**Dreambooth***[54]는 개인화된 텍스트 유도 이미지 생성을 위한 소수의 입력 이미지 상에서 미리 학습된 텍스트 대 이미지 확산 모델을 미세 조정하기 위한 간단하고 효과적인 방법이다. 특정 텍스트-토큰 _"[V]"_로 확산 모델 가중치에 피험자의 모습을 저장한다. 드림보드는 두 개의 손실 기능으로 미세 조정됩니다. 재건축 손실은 입력 이미지에 대한 표준 확산 변성 감독이다. 언어 드리프트와 미세 조정으로 인한 다양성 손실을 피하기 위해 수업별 사전 보존 손실이 제안된다. 자체 생성 사례가 많은 사전 학습 모델을 더 감독한다. 텍스트링드림어는 드림보드를 사용하여 입력 이미지로부터 텍스처 정보를 증류합니다. 이미지 합성 대신 기하학이 다른 3D 객체에 증류 정보를 적용한다.\n' +
      '\n' +
      '**ControlNet**[67] proposes a novel architecture that adds spatial conditioning control to pre-trained diffusion models. The key insight is to reuse the large number of diffusion model parameters trained on billions of images and insert small convolution networks into the model with window size 1 and zero-initialized weights. It enables robust fine-tuning performance on small datasets with different types of 2D conditions, such as depth, normal, and edge maps. We utilize ControlNet models to ensure that our created textures are aligned with the given geometry.\n' +
      '\n' +
      '키인시그 히티 스톰 아르젠섬베 로 리프레쉬는 모델 파라에서 지질이 있는 nb, 100만소f 이미지를 충족했으며 혈청마 란콘볼 유키오 nne 트위스토르 카스 인테테모 델 wi는 1a nd 0-in 이디디렉트웨이를 알디워딩한다. 그것은 니메트로비스핀-튜닝프 소거파 nc 에네팔달다타제 tswalldatase tswithdif ferent 이온 s, 깊이 있는, 없음 rma l, 가닥이 있는 이온 사이, 루티 리 리 제크 온톨로넷 델스트렌스 요도 생성 질감 누게 분석법이다.\n' +
      '\n' +
      '}(\\mathbb{t)\\ll}(\\mathbb{E})\\epsilon}(\\mathbf{x}_{t},y,\\med)\\\n' +
      '\n' +
      '\\(c\\)는 문자 입력,\\(c\\)는 시간 계수로,\\(c\\)는 카메라 포즈, \\(g(\\cdot)는 서로 다른 렌더링자, \\(\\mathbf{x}_{t}\\)는 시간 \\(t\\)에 의존하는 분산을 갖는 렌더링된 이미지(\\mathbf{x}=g(\\mathbf{x})에 노이즈를 추가하여 계산된 시끄러운 이미지)인 경우,\\(I\\)는 시간(t\\)인 경우,\\(Hf{x}_{t)에 노이즈를 추가하여 계산된 시끄러운 이미지)인 경우,\\(\\)는 시간 \\(\\)에 대한 분산을 사용하여 계산되는 이미지 \\(\\)에 노이즈를 추가하여 계산된 시끄러운 이미지)인 경우,\\(\\)는 시간 \\)인 경우,\\(\\)에 노이즈를 추가하여 계산된 시끄러운 이미지이다. 광범위한 사용에도 불구하고 SDS는 정상 분류기가 없는 지침[22]보다 훨씬 더 높은 중량을 요구하여 수렴, 과잉 흡입 및 과포화 외관을 필요로 한다. 이 문제를 극복하기 위해 왕 등은 표준 분류기가 없는 지침으로 수렴할 수 있는 가변 점수 증류(VSD)이라는 개선된 버전을 제안한다. VSD는 전체 3D 표현 \\(\\theta\\)를 무작위 변수로 취급하고 \\(\\theta\\)와 사전 훈련된 확산 모델에 의해 정의된 분포 사이의 KL 분산을 최소화한다. 그것은 3D 표현 \\(\\)에서 생성된 시끄러운 이미지를 변성시키기 위해 카메라를 임베딩하는 LoRA[24] 네트워크 \\(\\epsilon_{\\phi}\\)와 카메라 인코더 \\(\\rho\\)를 포함한다.\n' +
      '\n' +
      '\\[\\min_{\\phi}\\mathbb{E}_{t,\\epsilon,c}\\big{[}||\\epsilon_{\\phi}(\\mathbf{x}_{t},y,t,c)-\\epsilon||_{2}^{2}\\big{]} \\tag{1}\\]\n' +
      '\n' +
      '그런 다음 3D 표현 \\(\\theta\\)에 대한 기울기는 3D 표현 \\(\\theta\\)에 대한 기울기로 계산된다.\n' +
      '\n' +
      '종종[\\mathbb{E}{t,\\mathbb{\\psi}\\]{mathbf{x}\\left[t)(\\mathbf{x}_{t}_{t},y,t \\right)-\\epsilon_{\\phi}(\\mathbf{x}_{t)-\\epsilon_{t)(\\mathbf{x}_{t,y,t \\right)-\\left)(\\mathbf{x}_{t,y,t)\\left(\\mathbf{x}_{t,y,t)\\left,y,t)\\left(\\mathbf{t)\\left,y,t-{t)-\\compon_{t)-\\compon_{t)-\\compon_{t)-\\compon_{t-{t)-\\compon_{t－{t-{t-{t)}(\\:\\left,y,y,t-{t)-\\compon_{t)-\n' +
      '\n' +
      'VSD는 생성된 3D 콘텐츠의 시각적 품질과 다양성을 모두 유의하게 향상시키지만 3D 지식의 고유한 부족으로 인해 3D 일관성 문제를 해결할 수 없어 기하학과 질감의 다면 오류 및 불일치로 이어진다. 우리는 확산 모델 기하학을 인식하도록 기하학 정보를 명시적으로 주입함으로써 이 문제를 해결한다.\n' +
      '\n' +
      '## 개인화 지질 측정-인식 점수 교차(PGSD)이다.\n' +
      '\n' +
      '목표 3 Dmes h\\(\\mathc al{M} \\) 및 목표 3 Dmathc al{M} \\에 있는 4가지 램의 근친교배(\\ {I\\_{k = \\. \\)를 모방한 제2절(\\ {I\\                                               � 우리의 여백은 파라메드 리프(BDF) 트라이즈 리디알(RDF) 모델[20, 42,\\-r\\) 엑시어스 타르메(스피라 타르에 있는 리슈만 3, 타르베스(팔라) 아티메(스피라 트라, 엠비네(Hd) 파미타, 폴리티메, 이끼의 테트루-다.\n' +
      '\n' +
      'Figure 3: **Overview of TextureDreamer**, a framework which synthesizes texture for a given mesh with appearance similar to 3-5 input images of an object. We first obtain personalized diffusion model \\(\\psi\\) with Dreambooth [54] finetuning on input images. The spatially-varying bidirectional reflectance distribution (BRDF) field \\(f_{\\theta}\\) for the 3D mesh \\(\\mathcal{M}\\) is then optimized through personalized geometric-aware score distillation (PGSD) (detailed in Section 3.2). After optimization finished, high-resolution texture maps corresponding to albedo, metallic, and roughness can be extracted from the optimized BRDF field.\n' +
      '\n' +
      '및 \\(f_{\\theta}\\)는 다중 규모의 해시 인코딩과 작은 MLP로 구성된다. 이러한 암묵적 표현은 최적화 과정을 더 잘 규칙화할 수 있어 보다 원활한 질감을 이끌어낼 수 있다는 것을 알게 된다. 그러나 \\(\\mathcal{M}\\)의 UV 매핑을 감안할 때, 우리의 표현은 그림 3의 오른쪽 측면에서 볼 수 있듯이 각 텍셀에 해당하는 3D 포인트마다 질의함으로써 표준 그래픽 파이프라인과 호환되는 표준 2D 텍스처 맵으로 전환될 수도 있다.\n' +
      '\n' +
      '**Personalized texture information extraction.** We follow Dreambooth [54] to extract texture information from sparse images. To be specific, we fine-tune a personalized diffusion model on input images with a text prompt \\(y\\), _"A photo of [V] object"_, where _"[V]"_ is a unique identifier to describe the input object. Compared to the alternative textual inversion method [16], we observe that Dreambooth converges faster and can better preserve intricate texture patterns, possibly due to its larger capacity. We first mask out the background of the target object with a white color. For the reconstruction loss, we resize the shorter edge of input images to 512 and randomly crop 512x512 patches for training. We do not apply class-specific prior preservation loss, as we hope our Dreambooth finetuning model can generalize to other categories. We also experiment with different variations, including jointly fine-tuning the text encoder and replacing the diffusion denoising network with a pre-trained ControlNet, but do not observe any improvements.\n' +
      '\n' +
      '**Geometry-aware score distillation** Once we finish extracting texture information with Dreambooth, we transfer the information to mesh \\(\\mathcal{M}\\) by adopting the fine-tuned Dreambooth model as the denoising network \\(\\epsilon_{\\psi}\\) for score distillation sampling. Specifically, we choose VSD instead of the original SDS because of its superior ability to generate highly realistic and diverse appearances. To render images \\(\\mathbf{x}\\) for VSD gradient computation, we follow Fantasia3D [10] to pre-select a fixed HDR environment map \\(E\\) as illumination and use Nvdiffrast [30] as our differentiable renderer. We set the object background to be a constant white color to match the input images for Dreambooth training. We observe this can help achieve better color fidelity compared to random color or neutral background.\n' +
      '\n' +
      '그러나 단순히 SDS를 VSD로 대체하면 2D 확산 모델에서 3D 지식이 부족한 한계를 해결할 수 없다. 따라서 우리는 메쉬 \\(\\mathcal{M}\\)에서 추출한 기하학 정보를 \\(\\mathcal{M}\\)에서 렌더링된 정상 맵(k\\)에 미리 학습된 제어넷을 통해 개인화된 확산 모델 \\(\\epsilon_{\\psi}\\)에 주입하는 기하학 인식 점수 증류를 제안한다. 이 증강은 생성된 질감의 3D 일관성을 크게 증가시킨다(그림 10 참조). 제어넷 컨디셔닝으로, 형상 불일치에도 불구하고 입력 이미지로부터의 베개 텍스처가 목표 형상에 정확하게 매칭될 수 있다. 우리는 다른 제어넷 조건을 실험하고 정상적인 조건이 질감 측정 불일치를 가장 잘 예방할 수 있음을 보여준다.\n' +
      '\n' +
      'Hg 방사선 fp a\\m\\bf{x}－\\m\\athbf{x(PGSD)to otimet fDFf(\\theta,c\\)b 에이터티(PGSD) ptimet fB LPp ar\\)b 에 헤딩된 mageu ndera f versee ndera nustaffppppppffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffs: S.\n' +
      '\n' +
      '\\[\\nabla_{\\theta}\\mathcal{L}_{\\text{PGSD}}(\\theta)\\] \\[\\quad\\triangleq\\mathbb{E}_{t,c,c}[w(t)(\\epsilon_{\\psi}(\\mathbf{x }_{t};y,k,t)-\\epsilon_{\\phi}(\\mathbf{x}_{t};y,k,t,c_{\\rho}))\\frac{\\partial \\mathbf{x}}{\\partial\\theta}],\\]\n' +
      '\n' +
      '시간 \\(t\\), \\(c_{\\rho}\\)에서 카메라 외재성 \\(c\\bf{I}(\\math{f{I})의 임베딩(\\math{f{I})은 학습 가능한 카메라 인코더 \\(\\rho\\)에 의해 암호화된 카메라 외재성 \\(c\\)의 임베딩(\\math{I}(\\math{f{I}(\\b\\,\\b\\(\\b\\)의 임베딩)이고,\\(c\\(\\b\\(\\b\\,\\b\\(\\b\\)은 각각\\(\\b\\(\\b\\(\\b\\(\\b\\(\\b\\)\\(\\b\\(\\b\\:\\(\\b\\)의 임베딩)이고,\\(c\\(\\b\\(\\b\\(\\b\\,\\b\\) 입니다.{t{t{t{t{t{t{t{f{f{f{f{f{f{f{f{f{f{f{f{f{f{I} 두 모델 모두 그림 3의 확산 모델 아래의 노란색 부분에 나타난 바와 같이 정상 지도 \\(k\\)에 조절망을 조건화하여 증강된다.\n' +
      '\n' +
      '우리는 이 방법이 분류기가 없는 안내(CFG) [22]에서 혜택을 받지 않는다는 것을 발견했는데, 이는 개인화된 모델 \\(에피실론_{\\psi}\\)가 소수의 이미지에 미세 조정되었기 때문일 것이다. 우리의 목표는 입력 외관을 목표 형태로 충실히 전달하는 것이기 때문에 다양성을 높이기 위해 CFG를 가질 필요는 없다. 최근의 문헌[55]에서도 유사한 관찰이 발견될 수 있다.\n' +
      '\n' +
      '우리는 또한 광범위한 실험을 통해 몇 가지 중요한 디자인 선택을 식별한다. 먼저 Eq에서 \\(\\epsilon_{\\파이}\\)를 초기화하는 것이 중요하다. 원래 사전 훈련된 확산 모델 가중치가 있는 1과 드림보드의 무게는 질감 디테일을 제거할 것입니다. 이는 드림보드의 미세 조정 과정이 이전 작업[51]에서 지적한 바와 같이 확산 모델이 작은 훈련 세트에 과적합하게 만들기 때문일 것이다. 또한 LoRA 가중치를 제거하는 것이 질감 충실도를 실질적으로 향상시킬 수 있다는 것을 발견했다. LoRA 훈련의 유사한 어려움도 [56]에서 보고되었다. 따라서 우리는 개인화된 기하학 인식 점수 증류 손실 \\(\\mathcal{L}_{PGSD}\\)를 구현하여 \\(\\epsilon_{\\파이}\\)에서 LoRA 구조를 제거하고 카메라 임베딩만 유지하면서 최고의 품질을 달성했다. 우리는 그림 10에서 더 많은 비교를 보여준다.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '### Experimental setup\n' +
      '\n' +
      '우리는 소파, 침대, 머그/보울 및 플러시 장난감 4가지 범주에서 실험을 수행한다. 각 카테고리에 대해 8개의 객체 인스턴스를 선택하고, 객체를 둘러싼 3~5개의 뷰를 자유로이 샘플링하여 작은 이미지를 생성하여 총 32개의 이미지 세트를 생성한다. 32개의 이미지 세트 내의 모든 이미지에 대해 U2-Net[49]을 적용하여 전경 마스크를 자동으로 획득하거나 반자동 배경 제거 애플리케이션1을 사용하여 보다 정확한 마스크를 얻는다. 우리는 동일한 카테고리 형상, 상이한 카테고리 형상 또는 다른 속 수를 갖는 기하학을 포함하지만 제한되지 않는 다양한 메쉬로 설정된 각 이미지 세트에 대한 텍스처 전달을 수행한다. 질감 전달 프레임워크를 테스트하기 위해 캡처된 이미지 세트와 다른 4개 카테고리 각각에 대해 3개의 메쉬를 선택합니다. 우리는 3D-FUTURE [15] 및 온라인 레포스터.23에서 이러한 3D 메서를 획득한다. 4가지 범주의 모든 객체에 대해 클래스 내 질감 전달을 실행하고 침대와 의자 사이의 클래스 간 질감 전달을 실행하여 방법의 일반화 능력을 테스트합니다.\n' +
      '\n' +
      '발주 2: [주행://www.cgtrader.com/] (https://www.cgtrader.com/) (https://www.cgtrader.com/)\n' +
      '\n' +
      '부츠 3: [https://sketchfah.com/] (https://sketchfah.com/)\n' +
      '\n' +
      '** 구현 세부 정보** 우리는 PyTorch [45] 및 트라리오 [19]를 기반으로 프레임워크를 구현한다. 잠재 확산 및 제어넷 v1.1을 각각 사전 훈련된 확산 모델 및 제어넷으로 사용한다. 모든 실험에서 우리는 \\(\\mathcal{L}_{PGSD}\\)의 분류기가 없는 지침 중량을 원래 CFG 제형에서 \\(\\omega=0\\) 설정과 동등하다고 설정했다. 드림퓨전[47]에 이어 입력 텍스트 프롬프트에도 뷰 의존적 컨디셔닝을 적용합니다. BRDF 필드는 사전 작업 [10, 63]에 따라 해시-그리드 위치 인코딩[42]을 사용하여 MLP로 매개변수가 된다. 당사의 카메라 인코더는 카메라 외재성을 U-Net에서 시간과 텍스트 임베딩과 융합될 \\(1,280\\) 치수의 잠재 벡터에 투영하는 두 개의 선형 층으로 구성된다. 모든 실험에 대한 카메라 인코더용 \\(0.001\\) 및 \\(0.0001\\)를 인코딩하기 위한 학습률을 \\(0.01\\)로 실증적으로 설정하였다.\n' +
      '\n' +
      '### Baseline methods\n' +
      '\n' +
      'Latent-paint [37]과 TEXTure[53]은 이전에 2D 확산을 가진 최근 두 가지 텍스트 유도 텍스트링 방법이다. 그들은 또한 이미지로부터 메서를 텍스트화하는 능력을 보여준다. 표현식 [37]은 이미지 정보를 텍스트 임베딩으로 추출하고 텍스쳐를 SDS로 증류하기 위해 텍스트 변환 [16]을 기록한다. TEXTure[53] 첫 번째 핀셋은 텍스트 전환과 드림보스[54]를 결합하여 사전 훈련된 확산 모델을 융합하고 이 미세 조정 모델을 사용하여 반복적인 메쉬 페인팅 알고리즘으로 텍스쳐를 합성한다. 이전 방법[53]에 의해 선호되는 바와 같이, 우리는 랜덤 컬러 바탕으로 입력 이미지를 증가시킨다. 우리는 실험을 실행하기 위해 기준선 방법의 원래 구현을 밀접하게 따른다.\n' +
      '\n' +
      '임기 유도 질감 전달.\n' +
      '\n' +
      '*** 정성적 평가** 우리 방법은 _same_ 범주의 기하학 또는 다양한 범주에서 기하학을 포함하여 다양한 객체 기하학으로의 질감 전달을 수행할 수 있다. 그림 4는 4가지 범주의 객체에 대한 우리의 질감 전달 결과를 보여준다. 우리의 방법은 입력과 유사한 패턴과 스타일을 갖는 기하학 인식 및 심리스 질감을 합성할 수 있다. 우리는 또한 우리의 방법이 텍스처 _가닥 다른 카테고리_를 전달할 수 있음을 보여준다. 그림 1에서 소파 이미지에서 침상 형상으로 질감 전달 결과를 보여주고 그 반대의 경우도 마찬가지이다. 우리의 방법은 또한 광범위한 다양한 범주에서 질감 전달을 수행할 수 있다. 그림 5와 같이 의자와 머그, 플러시 장난감 범주에 걸쳐 고품질 및 실감형 질감을 합성할 수 있다. 합성된 질감은 알베도, 금속성 및 거칠기 지도를 포함하므로 그림 6과 같이 합성된 외관이 있는 표적 물체를 재배치할 수 있으며, 다른 무작위 종자를 사용하면 그림 8과 같이 다양한 질감을 생성할 수 있다.\n' +
      '\n' +
      'In Figure 7, we qualitatively compare our method with baseline methods. Two views are shown in each example. Latent-Paint tends to generate textures with colors and patterns that are different from input images. TEXTure can preserve the color and texture better than Latent-Paint, but the texture contains visible seams (possibly due to the iterative painting). Our results method can reason the semantics of the geometry (_e.g_. the positions of eyes) and demonstrate higher quality, seamless, and geometry-aware texturing results with higher fidelity from the input images.\n' +
      '\n' +
      '스페이트 야코토토어 알리즘, 브라스포어 스콜레디 오크 설화 s : 1) W 히코 신은 테큐어 스미코 트레이커리 스미코 리메랄로 이미지 s.2) 그리고 그 중 하나는 테큐어 스미코 이클레이즈 스미코 라이포어 스미말레이즈 s.3을 가지고 있는가? 수은은 이끼를 먹었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\multicolumn{2}{c}{Ours preferred over} \\\\ \\cline{2-3}  & Latent-Paint & TEXTure \\\\ \\hline Image Fidelity & 71.82\\% & 69.43\\% \\\\ Texture Photorealism & 77.03\\% & 85.52\\% \\\\ Shape-Texture Consistency & 78.49\\% & 85.16\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '영상 유도 텍스처 전달에 대한 표 1: ** 사용자 연구***.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline  & CLIP similarity \\(\\uparrow\\) \\\\ \\hline Latent-Paint [37] & 0.7969 \\\\ TEXTure [53] & 0.7988 \\\\ Ours & **0.8296** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '영상 유도 텍스트화에 대한 표 2: ** 정량적 평가***.\n' +
      '\n' +
      '이미지 충실도, 텍스처 광자주의, 형상-텍스트 일관성 측면에서 사용자가 선호하는 것이다.\n' +
      '\n' +
      '우리는 또한 참조와 렌더링된 사이 이미지 기반 CLIP 특징 [41]을 통한 유사성을 평가할 것을 제안한다.\n' +
      '\n' +
      '그림 4: ** 이미지 유도 전달 결과**는 이미지 세트의 4개 범주(베드, 소파, 플러시 장난감 및 머그컵)에서 다양한 객체에 이르기까지 다양하다. 우리의 방법은 광범위한 객체 유형에 적용될 수 있고 텍스처를 다양한 객체 형태로 전달할 수 있다.\n' +
      '\n' +
      '합성된 질감의 장점. CLIP 유사성은 재료 매칭 [65] 및 양식화 [38]에 적용되었다. 좋은 전달은 이미지로부터 질감만을 전달해야 하며 목표 형상 기하학을 고려하여 정적으로 질감을 전달해야 한다. 예를 들어, 이체는 형상의 각 부분에 대해 도색되어야 한다. 우리는 비교를 계산하기 위해 평가 세트를 사용합니다. 각 이미지 세트 및 타겟 3D 메쉬 쌍에 대해 각 기준 이미지 중 메트릭의 평균과 샘플링된 \\(4\\)의 렌더링된 이미지 각각을 계산(좌측 전면, 우전, 좌뒤 및 우후면)한다. 우리는 모든(이미지 세트, 메쉬) 쌍에 걸쳐 CLIP 유사성을 평균화한다. 표 2는 우리의 방법이 CLIP 유사도가 가장 높다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '우리는 먼저 기하학 인식 제어망의 중요성에 대한 절제 연구를 질적으로 수행한다. 그림 10에서 보는 바와 같이, 그 결과는 제어넷이나 깊이 기반 제어넷이 없는 기하학-텍스트 오정렬에 시달린다. 정상 기반 제어넷만이 합성된 텍스처를 입력 메시 기하학과 일치하도록 정확하게 제어할 수 있다. 다음으로 점수 증류 손실의 중요성을 검증한다. 우리의 프레임워크에서 SDS 손실을 사용하는 것만이 충분한 입력 충실도를 달성할 수 없으며 결과는 더 흐릿해지는 경향이 있다. LoRA가 제거되지 않고(일반적으로 바닐라 VSD 손실로 최적화되어 있는 경우, 최적화는 드림보트-피네이션 분포에서 분포를 발산시키는 경향이 있다. 이것은 원래 질감이 적지만 입력과는 더 무관한 패턴을 포함하는 출력을 초래한다. 이는 희귀 식별자가 포함된 텍스트 조건으로 LoRA 가중치를 최적화하는 것이 희소한 외관을 가지도록 렌더링된 이미지의 분포를 유도하는 경향이 있기 때문이라고 가정한다.\n' +
      '\n' +
      '일반 확산 모델 \\(\\epsilon_{\\파이}\\)을 개인화된 확산 모델 \\(\\epsilon_{\\psi}\\)로 대체하거나 분류기 자유 안내 가중치 \\(7.5\\)를 적용하면, 그 결과는 입력 이미지에 존재하지 않는 랜덤 패턴을 도입하는 경향이 있다. 카메라 인코더 가중치(\\rho\\)를 동결하는 것을 선택하면 결과는 전체 방법보다 더 심하거나 시끄러워진다.\n' +
      '\n' +
      '우리는 또한 시스템에서 각 구성요소의 중요성을 정량적으로 평가한다. 우리는 참조 이미지 간의 유사성을 측정하기 위해 이미지 기반 CLIP 특징을 사용한다.\n' +
      '\n' +
      'Figure 5: **Example of cross-category texture transfer results.** In the first row, we transfer appearances from plush toys to cups and chairs. In the second row, special patterns from mugs are transferred to bears and chairs. In the third row, textures from input sofa are transferred to cups and bears.\n' +
      '\n' +
      '그림 6: ** 반복 결과의 예.* 질감은 원래 HDR 환경 맵 (로우)a ndth eno 벨 맵 ( seco ndandth ird 행)에 의해 재발한다.\n' +
      '\n' +
      '그랑 렌더링된 이미지. 공정한 평가를 보장하기 위해 참조 이미지와 렌더링된 이미지의 배경은 흰색 색상으로 마스킹된다.\n' +
      '\n' +
      '표 3에서 보는 바와 같이, 우리의 전체 방법은 _w/o ControlNet_ 및 _w/ ControlNet (Depth)_을 제외한 일반 기저부 중에서 가장 높은 유사성 점수를 달성한다. 그림 10에서 보는 바와 같이, 이 두 방법은 목표 형상을 무시하고 기하학에 적응하지 않고 질감을 직접 칠하는 경향이 있다. 따라서 모양에 관계없이 원래 질감을 그려 더 높은 점수에 도달할 수 있습니다. 또한 SDS 결과가 포화되거나 흐릿한 경향이 있으며 입력에서 질감을 회복할 수 없다는 것을 관찰했다. 일반 확산 모델(\\epsilon_{\\파이}\\)에서 LoRA를 잡는 것은 합성된 질감에 무작위 패턴을 도입할 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline  & CLIP similarity \\(\\uparrow\\) \\\\ \\hline w/o ControlNet & _0.8394_ \\\\ w/ ControlNet (Depth) & _0.8320_ \\\\ SDS, w/o CFG & 0.8101 \\\\ SDS, CFG \\(100\\) & 0.7983 \\\\ w/o LoRA removed & 0.8110 \\\\ Personalized model as \\(\\phi\\) & 0.8218 \\\\ CFG weight as \\(7.5\\) & 0.8218 \\\\ w/o camera encoder \\(\\rho\\) updated & 0.8267 \\\\ Ours & **0.8296** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '이미지 기반 텍스트링 w.r.t. CLIP 이미지 기반 특징에 대한 표 3: **Ablation 연구**. 비록 _w/o ControlNet_ 및 _w/ ControlNet (Depth)__는 더 높은 유사성 점수를 달성하지만, 전달 결과는 목표 형상을 무시하고 기하학을 추론하지 않고 질감을 직접 페인트리는 경향이 있다. 나머지 절제 방법 중 우리의 전체 방법은 가장 높은 CLIP 유사성 w.r.t 참조 이미지를 달성한다.\n' +
      '\n' +
      '합성된 질감의 그림 8: **다양성****.\n' +
      '\n' +
      'Figure 7: **Comparison between baseline methods.** Compared with Latent-Paint [37] and TEXTure [53], our method can synthesize seamless and geometry-aware textures which are compatible with the target mesh geometry.\n' +
      '\n' +
      'Figure 9: **Limitations.** Our method may bake-in lighting into texture, have Janus problem when lacking enough input viewpoints, and ignore special and non-repeated patterns from the input.\n' +
      '\n' +
      '## 5 Discussions\n' +
      '\n' +
      'We proposed a framework to transfer texture from input images to an arbitrary shape. While our method can transfer high-quality texture in most cases, there are some limitations. Figure 9 shows that our method may not be able to transfer special and non-repeated texture to the target shapes. In addition, our method tends to bake in lighting to texture when there are strong specular highlights in the input images. Janus problem might appear when the viewpoints of input images do not cover the entire object. Nevertheless, we believe that our method can be the first step to tackling this challenging problem and will make an impact in the 3D content creation community.\n' +
      '\n' +
      'Figure 10: **Ablation study.** (First row) With ControlNet conditioned on normal maps, the result has the best texture-geometry consistency. Without ControlNet or with depth-based ControlNet, the results suffer from texture-geometry misalignment. Using SDS loss leads to blurry textures. Without the LoRA module removed, the results tend to remove the existing texture from the personalized diffusion model. Our full method can synthesize accurate texture which is similar to input appearances. (Second row) If replacing generic diffusion model \\(\\phi\\) with personalized model or applying classifier guidance scale \\(7.5\\), some random patterns might appear in the synthesized texture. If we freeze the camera encoder \\(\\rho\\), the result might be worse or more noisy than our full method.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Adobe substance 3d. [https://docs.substance3d.com/sat](https://docs.substance3d.com/sat).\n' +
      '* [2] Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, and Jia-Bin Huang. Single-image 3d human digitization with shape-guided diffusion. In _SIGGRAPH Asia_, 2023.\n' +
      '* [3] Sai Bi, Nima Khademi Kalantari, and Ravi Ramamoorthi. Patch-based optimization for image-based texture mapping. _ACM Trans. Graph._, 36(4):106-1, 2017.\n' +
      '* [4] Alexey Bokhovkin, Shubham Tulsiani, and Angela Dai. Mesh2tex: Generating mesh textures from image queries. _arXiv preprint arXiv:2304.05868_, 2023.\n' +
      '* [5] G. Cai, K. Yan, Z. Dong, I. Gkioulekas, and S. Zhao. Physics-based inverse rendering using combined implicit and explicit geometries. _Computer Graphics Forum_, 41(4):129-138, 2022.\n' +
      '* [6] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp, and Kangxue Yin. Textfusion: Synthesizing 3d textures with text-guided image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4169-4181, 2023.\n' +
      '* [7] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16123-16133, 2022.\n' +
      '* [8] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. _arXiv preprint arXiv:1512.03012_, 2015.\n' +
      '* [9] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Niessner. Text2tex: Text-driven texture synthesis via diffusion models. _arXiv preprint arXiv:2303.11396_, 2023.\n' +
      '* [10] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. _arXiv preprint arXiv:2303.13873_, 2023.\n' +
      '* [11] Zhiqin Chen, Kangxue Yin, and Sanja Fidler. Auv-net: Learning aligned uv maps for texture transfer and synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1465-1474, 2022.\n' +
      '* [12] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-?2n2: A unified approach for single and multi-view 3d object reconstruction. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14_, pages 628-644. Springer, 2016.\n' +
      '* [13] Alexei A Efros and Thomas K Leung. Texture synthesis by non-parametric sampling. In _Proceedings of the seventh IEEE international conference on computer vision_, pages 1033-1038. IEEE, 1999.\n' +
      '* [14] Ziya Erkoc, Fangchang Ma, Qi Shan, Matthias Niessner, and Angela Dai. Hyperdiffusion: Generating implicit neural fields with weight-space diffusion. _arXiv preprint arXiv:2303.17015_, 2023.\n' +
      '* [15] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furniture shape with texture. _International Journal of Computer Vision_, 129:3313-3337, 2021.\n' +
      '* [16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* [17] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. Get3d: A generative model of high quality 3d textured shapes learned from images. _Advances In Neural Information Processing Systems_, 35:31841-31854, 2022.\n' +
      '* [18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. _Communications of the ACM_, 63(11):139-144, 2020.\n' +
      '* [19] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zixin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: A unified framework for 3d content generation. [https://github.com/threestudio-project/threestudio](https://github.com/threestudio-project/threestudio), 2023.\n' +
      '* [20] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg. Shape, light, and material decomposition from images using monte carlo rendering and denoising. _Advances in Neural Information Processing Systems_, 35:22856-22869, 2022.\n' +
      '* [21] Paul Henderson, Vagia Tsiminaki, and Christoph H Lampert. Leveraging 2d data to learn textured 3d mesh generation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 7498-7507, 2020.\n' +
      '* [22] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* [24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [25] Brian Karis and Epic Games. Real shading in unreal engine 4. _Proc. Physically Based Shading Theory Practice_, 4(3):1, 2013.\n' +
      '* [26] Animesh Karnewar, Andrea Vedaldi, David Novotny, and Niloy J Mitra. Holodiffusion: Training a 3d diffusion model using 2d images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18423-18433, 2023.\n' +
      '* [27] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani Lischinski. Noise-free score distillation. _arXiv preprint arXiv:2310.17590_, 2023.\n' +
      '* [28] Johannes Kopf, Chi-Wing Fu, Daniel Cohen-Or, Oliver Deussen, Dani Lischinski, and Tien-Tsin Wong. Solid texture synthesis from 2d exemplars. In _ACM SIGGRAPH 2007 papers_, pages 2-es. 2007.\n' +
      '* [29] Vivek Kwatra, Arno Schodl, Irfan Essa, Greg Turk, and Aaron Bobick. Graphcut textures: Image and video synthesis using graph cuts. _Acm transactions on graphics (tog)_, 22(3):277-286, 2003.\n' +
      '* [30] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. Modular primitives for high-performance differentiable rendering. _ACM Transactions on Graphics_, 39(6), 2020.\n' +
      '* [31] Jiabao Lei, Yabin Zhang, Kui Jia, et al. Tango: Text-driven photorealistic and robust 3d stylization via lighting decomposition. _Advances in Neural Information Processing Systems_, 35:30923-30936, 2022.\n' +
      '* [32] Marc Levoy, Kari Pulli, Brian Curless, Szymon Rusinkiewicz, David Koller, Lucas Pereira, Matt Ginzton, Sean Anderson, James Davis, Jeremy Ginsberg, et al. The digital michelangelo project: 3d scanning of large statutes. In _Proceedings of the 27th annual conference on Computer graphics and interactive techniques_, pages 131-144, 2000.\n' +
      '* [33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, 2022.\n' +
      '* [34] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 300-309, 2023.\n' +
      '* [35] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2837-2845, 2021.\n' +
      '* [36] Yiwei Ma, Xiaoqing Zhang, Xiaoshuai Sun, Jiayi Ji, Haowei Wang, Guannan Jiang, Weilin Zhuang, and Rongrong Ji. X-mesh: Towards fast and accurate text-driven 3d stylization via dynamic textual guidance. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2749-2760, 2023.\n' +
      '* [37] Gal Metzler, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12663-12673, 2023.\n' +
      '* [38] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-driven neural stylization for meshes. _arXiv preprint arXiv:2112.03221_, 2021.\n' +
      '* [39] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-driven neural stylization for meshes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13492-13502, 2022.\n' +
      '* [40] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. _Communications of the ACM_, 65(1):99-106, 2021.\n' +
      '* [41] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. Clip-mesh: Generating textured meshes from text using pretrained image-text models. In _SIGGRAPH Asia 2022 conference papers_, pages 1-8, 2022.\n' +
      '* [42] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Transactions on Graphics (ToG)_, 41(4):1-15, 2022.\n' +
      '* [43] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Muller, and Sanja Fidler. Extracting triangular 3d models, materials, and lighting from images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8280-8290, 2022.\n' +
      '* [44] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 165-174, 2019.\n' +
      '* [45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Braddury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.\n' +
      '* [46] Dario Pavllo, Jonas Kohler, Thomas Hofmann, and Aurelien Lucchi. Learning generative models of textured 3d meshes from real-world images. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 13879-13889, 2021.\n' +
      '* [47] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. _arXiv preprint arXiv:2209.14988_, 2022.\n' +
      '* [48] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandar Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. _arXiv preprint arXiv:2306.17843_, 2023.\n' +
      '* [49] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar Zaiane, and Martin Jagersand. U2-net: Going deeper with nested u-structure for salient object detection. page 107404, 2020.\n' +
      '* [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [51] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: Subject-driven text-to-3d generation. _arXiv preprint arXiv:2303.13508_, 2023.\n' +
      '* [52] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1 (2):3, 2022.\n' +
      '\n' +
      '* [53] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. In _ACM SIGGRAPH 2023 Conference Proceedings_, New York, NY, USA, 2023. Association for Computing Machinery.\n' +
      '* [54] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.\n' +
      '* [55] Prafull Sharma, Varun Jampani, Yuanzhen Li, Xuhui Jia, Dmitry Lagun, Fredo Durand, William T Freeman, and Mark Matthews. Alchemist: Parametric control of material properties with diffusion models. _arXiv preprint arXiv:2312.02970_, 2023.\n' +
      '* [56] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: a single image to consistent multi-view diffusion base model, 2023.\n' +
      '* [57] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. _arXiv preprint arXiv:2308.16512_, 2023.\n' +
      '* [58] Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan, Matthias Niessner, and Angela Dai. Texturify: Generating textures on 3d shape surfaces. In _European Conference on Computer Vision_, pages 72-88. Springer, 2022.\n' +
      '* [59] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International conference on machine learning_, pages 2256-2265. PMLR, 2015.\n' +
      '* [60] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.\n' +
      '* [61] Cheng Sun, Guangyan Cai, Zhengqin Li, Kai Yan, Cheng Zhang, Carl Marshall, Jia-Bin Huang, Shuang Zhao, and Zhao Dong. Neural-pbir reconstruction of shape, material, and illumination. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [62] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12619-12629, 2023.\n' +
      '* [63] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. _arXiv preprint arXiv:2305.16213_, 2023.\n' +
      '* [64] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. _Advances in neural information processing systems_, 29, 2016.\n' +
      '* [65] K. Yan, F. Luan, M. Hasan, T. Groueix, V. Deschaintre, and S. Zhao. Psdr-room: Single photo to scene using differentiable rendering. In _ACM SIGGRAPH Asia 2023 Conference Proceedings_, 2023.\n' +
      '* [66] Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Zhengzhe Liu, and Xiaojuan Qi. Texture generation on 3d meshes with point-uv diffusion. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4206-4216, 2023.\n' +
      '* [67] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3836-3847, 2023.\n' +
      '* [68] Qian-Yi Zhou and Vladlen Koltun. Color map optimization for 3d reconstruction with consumer depth cameras. _ACM Transactions on Graphics (ToG)_, 33(4):1-10, 2014.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>