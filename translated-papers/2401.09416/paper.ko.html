<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '이미지 인식 세확산 차단제: 이미지 유도 패턴 합성\n' +
      '\n' +
      '유징 예하\\({}^{13}\\,{}^{23}\\) 창이 김\\({}^{3}\\,{}^{3}\\) 루 응우옌-Phuoc\\({}^{3}\\).\n' +
      '\n' +
      '정장\\({}^{3}\\)는 카를 S 마셜\\({}^{3}\\,{}^{3}\\) 자하닌 Li\\({}^{3}\\)\n' +
      '\n' +
      '캘리포니아 대학교의 샌디에이고\\({}^{1}\\,{}^{2}\\,{}^{3}\\)입니다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '소수의 입력 이미지(3~5)에서 임의의 카테고리에 걸쳐 3D 형상을 표적으로 하기 위해 재발 가능한 질감을 전달하기 위한 새로운 이미지 유도 텍스처 합성 방법인 TextureDreamer를 제시한다. 텍스트 생성은 비전과 그래픽의 중추적인 도전입니다. 산업 기업들은 경험 있는 아티스트를 고용하여 3D 자산을 위한 텍스처를 수동으로 제작합니다. 고전적인 방법은 조밀하게 샘플링된 뷰와 정확하게 정렬된 기하학을 필요로 하는 반면, 학습 기반 방법은 데이터세트 내의 카테고리 특이적 형상에 국한된다. 대조적으로, TextureDreamer는 매우 상세하고 복잡한 텍스처를 실제 환경에서 소수의 카지노 캡처된 이미지만으로 임의의 객체들로 전달할 수 있으며, 잠재적으로 유의하게 민주화된 텍스처 생성을 촉진할 수 있다. 우리의 핵심 아이디어, 개인화된 기하학 인식 점수 증류(PGSD)은 질감 정보 추출을 위한 개인화된 모델링, 세부 외관 합성을 위한 가변 점수 증류, 제어넷을 사용한 명시적인 기하학 안내 등 확산 모델의 최근 발전에서 영감을 얻는다. 우리의 통합과 몇 가지 필수 변형은 질감의 품질을 실질적으로 향상시킵니다. 상이한 카테고리에 걸쳐 있는 실제 이미지에 대한 실험은 TextureDreamer가 매우 현실적이고 의미 있는 의미 있는 질감을 임의의 물체에 성공적으로 전달할 수 있어 이전 최첨단 기술의 시각적 품질을 능가할 수 있음을 보여준다. 프로젝트 페이지: [https://://edreamer.githubio](https://textreamer.githubio) (https://://ulsereamer.githubio)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'High-quality 3D content is indispensable for a wide range of critical applications, including AR/VR, robotics, film, and gaming. In recent years, remarkable progress has been made in democratizing 3D content creation pipelines, facilitated by advancements in 3D reconstruction [40, 42] and generative models [18, 59]. While substantial attention has been devoted to exploring the _geometry component_[8, 12, 64] and neural implicit representations [44], such as NeRF [40], creation of high-quality _textures_ is relatively under-explored. Textures are pivotal in creating realistic, highly detailed appearances and are integral to various graphics pipelines, where industry has traditionally relied on professional, experienced artists to craft textures. This process usually involves manually authoring procedural graphs [1] and UV maps, making it expensive and inefficient. Automatically transferring the diverse visual appearance of objects around us to the texture of any target geometry would thus be highly beneficial.\n' +
      '\n' +
      'We present _TextureDreamer_, a novel framework to create high-quality relightable textures from sparse images. Given 3 to 5 randomly sampled views of an object, we can transfer its texture to an target geometry that may come from a different category. This is an extremely challenging problem, as previous texture creation methods usually either require densely sampled views with aligned geometry [3, 32, 68], or can only work for category-specific shapes [4, 21, 46, 58]. Our framework draws inspiration from recent advancements in diffusion-based generative models [23, 59, 60]. Trained on billions of text-image pairs, these diffusion models enable text-guided image generation with extraordinary visual quality and diversity [52]. Pioneering works have applied these pre-trained 2D diffusion models to text-guided 3D content creation [34, 47, 63]. However, a common limitation among those methods is that _text-only input_ may not be sufficiently expressive to describe complex, detailed patterns, as demonstrated in Figure 2. In contrast to text-guided methods, we effectively extract texture information from a small set of input images by fine-tuning the pre-trained diffusion model with a unique text token [16, 54]. Our framework, therefore, addresses the challenge of accurately describing complex textures.\n' +
      '\n' +
      'The Score Distillation Sampling (SDS) [47, 62] is one core element that bridges pre-trained 2D diffusion models with 3D content creation. It is widely used to generate and edit 3D contents by minimizing the discrepancy between the distribution of rendered images and the distribution defined by the pre-trained diffusion models [34, 37]. Despite its popularity, two well-known limitations impede its ability to generate high-quality textures. First, it tends to create over-smoothed and saturated appearances due to the unusually high classifier-free guidance necessary for the method to converge. Second, it lacks the knowledge to generate a 3D-consistent appearance, often resulting in multi-face artifacts and mismatches between textures and geometry.\n' +
      '\n' +
      'We propose two key design choices to tackle these challenges. Instead of using SDS, we build upon Variational Score Distillation (VSD) in our optimization approach, which can generate much more photorealistic and diverse textures. Initially introduced in ProlificDreamer [63], VSD treats the whole 3D representation as a random variable and aligns its distribution with the pre-trained diffusion model. It does not need a large classifier-free guidance weight to converge, which is essential to create a realistic and diverse appearance. However, naively applying VSD update does not suffice for generating high-quality textures in our application. We identify a simple modification that can improve texture quality while slightly reducing the computational cost. Additionally, VSD loss alone cannot fully solve the 3D consistency issue. Fine-tuning on sparse inputs makes converging harder, as observed by previous work [51]. We, therefore, explicitly condition our texture generation process on geometry information extracted from the given mesh by injecting rendered normal maps into the fine-tuned diffusion model through the ControlNet [67] architecture. Our framework, designated as personalized geometry aware score distillation (PGSD), can effectively transfer highly detailed textures to diverse geometry in a semantically meaningful and visually appealing manner. Extensive qualitative and quantitative experiments demonstrate that our framework substantially outperforms state-of-the-art texture-transfer methods.\n' +
      '\n' +
      '2번으로 작업했습니다.\n' +
      '\n' +
      '**exture 합성 및 재구성** 클래식 텍스처 생성 방법은 동네 [13, 28], 타일링 반복 패턴[29]에서 파생된 분포로부터 샘플링하거나 대상체에 멀티뷰 이미지를 융합하는 것을 포함한다.\n' +
      '\n' +
      'Figure 2: **Limitation of text-guided texturing.** Compared to text-guided texturing method which requires a captioning method to generate a text prompt which might not express all the details of the image, image-based guided texturing can be more effective and more expressive. Image captioning is predicted by BLIP [33], text-guided texturing is generated via TEXTure [53], and image-guided result is from our method.\n' +
      '\n' +
      'faces [3, 32, 68]. The former two fall short in creating semantic meaningful textures while the latter one requires highly accurate geometry reconstruction. Numerous learning-based methods were proposed to learn texture creation from large-scale 3D datasets [4, 11, 21, 46, 58] but are confined to specific categories within the dataset. Recent works also use CLIP model [50] for text-guided texture generation of arbitrary objects [31, 36, 39, 41], but their texture qualities are usually low. In contrast, Texture-Dreamer can create semantically meaningful, high-quality textures for arbitrary objects using uncorrelated sparse images. Traditionally, textures are represented as a 2D image and projected to object surfaces through UV mapping. Leveraging the recent progress in neural implicit representation, our method, along with recent developments in inverse rendering [5, 7, 17, 61] and 3D generation [7, 17], represents texture as a neural implicit texture field.\n' +
      '\n' +
      '**Diffusion models** Diffusion models have emerged as the state-of-the-art generative models [23, 60], demonstrating exceptional visual quality [52]. Its training and inference involve iteratively adding noise with different variances and denoise the data. Trained on internet-scale image-text pair datasets [52], these pre-trained models exhibit unprecedented capability in text-guided image synthesis and have proven successful in various image editing tasks. Recent works also manage to fine-tune pre-trained diffusion models on much smaller datasets or even a few images to facilitate customized/personalized image synthesis [54] and image generation conditioned on multi-modal data [67], such as normal and semantic maps. Building upon this progress, TextureDreamer can effectively extract texture information from sparse views and transfer it to a novel target object in a geometry-aware manner.\n' +
      '\n' +
      '**3D generation with 2D diffusion priors** Diffusion-based 3D content creation has very recently gained substantial interest. Several methods directly train 3D diffusion models to generate 3D content in various representations, including point cloud [35], neural radiance filed [26], hypernetwork [14] and texture [66]. Others utilize pre-trained 2D diffusion models by either progressively fusing generated images from different views [2, 6, 9, 53] or optimizing the 3D representation through score distillation sampling [34, 37, 47] and its improved variations [27, 63]. While many methods concentrate on text-guided 3D generation, fewer attempt to leverage diffusion models to generate 3D content from images. A number of concurrent works fine-tune 2D diffusion models on large-scale 3D datasets for sparse view reconstruction [48, 57], primarily focusing on whole 3D object reconstruction. In contrast, Texture-Dreamer targets transferring textures from a small number of images to a target 3D shape with unmatched geometry. Dreambooth3D [51] and TEXTure [53] extract information from sparse views into a new text token and fine-tuned diffusion model weights, which can be used to generate personalized 3D object or texture unseen objects. Texture-Dreamer employs a similar method to extract information from sparse images. However, it differs from prior works on utilizing the extracted information for texture generation, leading to improvements in consistency and photorealism.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      'We propose TextureDreamer, a framework which synthesizes geometry-aware texture for a given mesh with appearance similar to 3-5 input images of an object. In Section 3.1, we first introduce preliminaries on Dreambooth [54], ControlNet [67] and score distillation sampling [47, 62, 63]. In Section 3.2, we propose personalized geometry-aware score distillation (PGSD), which is our core technical contribution that enables high-quality image-guided texture transfer from sparse images to arbitrary geometries.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '518552924.518552924.5.5ttimim anappi에 대한 파노브스 디프토메스 디프레이즈-아비스트 레디네이션에 대한 리프레시 디프터스, 스캅스 디프터스 디프레이즈 디프레이즈 디프테스티어스(Syyyyyyyyyyyyyyy)의 유제트-패스트레디페스-다크 트라베스 디프론-오페스 디프레이즈 디프스-리셉터스 애프터스 디프터스 디프터스 디프터스 디프터스 디프터스 디프터스 디프터스 디프터스 디프터스 디프터스-인 디프터스 디프터스 디프터스 디프터스 디프터스 디프터스 디프터스 디프터스 디프터스 디프터스 디프런스, 에프터스 디프터스 디프터스 디프터스 디프터스스스스스스스스스스스스-\n' +
      '\n' +
      '**ControlNet**[67] proposes a novel architecture that adds spatial conditioning control to pre-trained diffusion models. The key insight is to reuse the large number of diffusion model parameters trained on billions of images and insert small convolution networks into the model with window size 1 and zero-initialized weights. It enables robust fine-tuning performance on small datasets with different types of 2D conditions, such as depth, normal, and edge maps. We utilize ControlNet models to ensure that our created textures are aligned with the given geometry.\n' +
      '\n' +
      '**Score Distillation Sampling**[47, 62] is the core component of numerous methods that use pre-trained 2D diffusion models for 3D content creation [10, 34, 47]. It optimizes the 3D representation by pushing its rendered images to a high-dimensional manifold modeled by the pre-trained diffusion model. Let \\(\\theta\\) be the 3D representation and \\(\\epsilon_{\\psi}\\) be the pre-trained diffusion model. The gradient back-propagatedto the parameter \\(\\theta\\) is\n' +
      '\n' +
      '}(\\mathbb{t)\\ll}(\\mathbb{E})\\epsilon}(\\mathbf{x}_{t},y,\\med)\\\n' +
      '\n' +
      '\\(c\\)는 문자 입력,\\(c\\)는 시간 계수로,\\(c\\)는 카메라 포즈, \\(g(\\cdot)는 서로 다른 렌더링자, \\(\\mathbf{x}_{t}\\)는 시간 \\(t\\)에 의존하는 분산을 갖는 렌더링된 이미지(\\mathbf{x}=g(\\mathbf{x})에 노이즈를 추가하여 계산된 시끄러운 이미지)인 경우,\\(I\\)는 시간(t\\)인 경우,\\(Hf{x}_{t)에 노이즈를 추가하여 계산된 시끄러운 이미지)인 경우,\\(\\)는 시간 \\(\\)에 대한 분산을 사용하여 계산되는 이미지 \\(\\)에 노이즈를 추가하여 계산된 시끄러운 이미지)인 경우,\\(\\)는 시간 \\)인 경우,\\(\\)에 노이즈를 추가하여 계산된 시끄러운 이미지이다. 광범위한 사용에도 불구하고 SDS는 정상 분류기가 없는 지침[22]보다 훨씬 더 높은 중량을 요구하여 수렴, 과잉 흡입 및 과포화 외관을 필요로 한다. 이 문제를 극복하기 위해 왕 등은 표준 분류기가 없는 지침으로 수렴할 수 있는 가변 점수 증류(VSD)이라는 개선된 버전을 제안한다. VSD는 전체 3D 표현 \\(\\theta\\)를 무작위 변수로 취급하고 \\(\\theta\\)와 사전 훈련된 확산 모델에 의해 정의된 분포 사이의 KL 분산을 최소화한다. 그것은 3D 표현 \\(\\)에서 생성된 시끄러운 이미지를 변성시키기 위해 카메라를 임베딩하는 LoRA[24] 네트워크 \\(\\epsilon_{\\phi}\\)와 카메라 인코더 \\(\\rho\\)를 포함한다.\n' +
      '\n' +
      '\\[\\min_{\\phi}\\mathbb{E}_{t,\\epsilon,c}\\big{[}||\\epsilon_{\\phi}(\\mathbf{x}_{t},y,t,c)-\\epsilon||_{2}^{2}\\big{]} \\tag{1}\\]\n' +
      '\n' +
      '그런 다음 3D 표현 \\(\\theta\\)에 대한 기울기는 3D 표현 \\(\\theta\\)에 대한 기울기로 계산된다.\n' +
      '\n' +
      '종종[\\mathbb{E}{t,\\mathbb{\\psi}\\]{mathbf{x}\\left[t)(\\mathbf{x}_{t}_{t},y,t \\right)-\\epsilon_{\\phi}(\\mathbf{x}_{t)-\\epsilon_{t)(\\mathbf{x}_{t,y,t \\right)-\\left)(\\mathbf{x}_{t,y,t)\\left(\\mathbf{x}_{t,y,t)\\left,y,t)\\left(\\mathbf{t)\\left,y,t-{t)-\\compon_{t)-\\compon_{t)-\\compon_{t)-\\compon_{t-{t)-\\compon_{t－{t-{t-{t)}(\\:\\left,y,y,t-{t)-\\compon_{t)-\n' +
      '\n' +
      'VSD는 생성된 3D 콘텐츠의 시각적 품질과 다양성을 모두 유의하게 향상시키지만 3D 지식의 고유한 부족으로 인해 3D 일관성 문제를 해결할 수 없어 기하학과 질감의 다면 오류 및 불일치로 이어진다. 우리는 확산 모델 기하학을 인식하도록 기하학 정보를 명시적으로 주입함으로써 이 문제를 해결한다.\n' +
      '\n' +
      '## 개인화 지질 측정-인식 점수 교차(PGSD)이다.\n' +
      '\n' +
      '**Problem setup.** We illustrate our method in Figure 3. The inputs to our framework include a small set of images (3 to 5) casually captured from different views \\(\\{I\\}_{k=1}^{K}\\) and a target 3D mesh \\(\\mathcal{M}\\). The outputs of our framework are relightable textures transferred from image set \\(\\{I\\}_{k=1}^{K}\\) to \\(\\mathcal{M}\\) in a semantically meaningful and visually pleasing manner. Our relightable textures are parameterized as standard microfacet bidirectional reflectance distribution (BRDF) model [25], which consists of 3 parameters, diffuse albedo \\(a\\), roughness \\(r\\), and metallic \\(m\\). We deliberately _do not_ optimize normal maps as it encourages the pipeline to fake details that are inconsistent with mesh \\(\\mathcal{M}\\). Following the recent trend of neural implicit representation [20, 42, 43], during optimization, we represent our texture as a neural BRDF field \\(f_{\\theta}(v):v\\in\\mathbb{R}^{3}\\rightarrow,a,r,m\\in\\mathbb{R}^{5}\\), where \\(v\\) is an arbitrary point sampled on the surface of \\(\\mathcal{M}\\)\n' +
      '\n' +
      'Figure 3: **Overview of TextureDreamer**, a framework which synthesizes texture for a given mesh with appearance similar to 3-5 input images of an object. We first obtain personalized diffusion model \\(\\psi\\) with Dreambooth [54] finetuning on input images. The spatially-varying bidirectional reflectance distribution (BRDF) field \\(f_{\\theta}\\) for the 3D mesh \\(\\mathcal{M}\\) is then optimized through personalized geometric-aware score distillation (PGSD) (detailed in Section 3.2). After optimization finished, high-resolution texture maps corresponding to albedo, metallic, and roughness can be extracted from the optimized BRDF field.\n' +
      '\n' +
      '및 \\(f_{\\theta}\\)는 다중 규모의 해시 인코딩과 작은 MLP로 구성된다. 이러한 암묵적 표현은 최적화 과정을 더 잘 규칙화할 수 있어 보다 원활한 질감을 이끌어낼 수 있다는 것을 알게 된다. 그러나 \\(\\mathcal{M}\\)의 UV 매핑을 감안할 때, 우리의 표현은 그림 3의 오른쪽 측면에서 볼 수 있듯이 각 텍셀에 해당하는 3D 포인트마다 질의함으로써 표준 그래픽 파이프라인과 호환되는 표준 2D 텍스처 맵으로 전환될 수도 있다.\n' +
      '\n' +
      '** 개인화된 텍스처 정보 추출*** 우리는 희박한 이미지로부터 텍스처 정보를 추출하기 위해 드림보스[54]를 따른다. 구체적으로, 우리는 텍스트 프롬프트 \\(y\\), [V] 객체"___\'A 사진이 있는 입력 영상에 대한 개인화된 확산 모델을 미세 조정했으며, 여기서 _"[V]"_은 입력 객체를 설명하기 위한 고유 식별자이다. 대체 텍스트 반전 방법[16]에 비해 드림보드가 더 빠르게 수렴하여 더 큰 용량 때문에 복잡한 질감 패턴을 더 잘 보존할 수 있음을 관찰한다. 우리는 먼저 흰색으로 타겟 오브젝트의 배경을 가린다. 재건축 손실을 위해 입력 이미지의 더 짧은 가장자리를 512로 재구성하고 훈련용 무작위로 작물 512x512 패치를 재구성한다. 우리의 꿈보트 핀셋링 모델이 다른 범주에 일반화할 수 있기를 바라며, 우리는 계급별 사전 보존 손실을 적용하지 않는다. 또한 텍스트 인코더를 공동으로 미세 조정하고 확산 변성 네트워크를 사전 훈련된 제어넷으로 대체하는 것을 포함하여 다양한 변화를 실험하지만 개선점을 관찰하지 못한다.\n' +
      '\n' +
      '**Geometry-aware score distillation** Once we finish extracting texture information with Dreambooth, we transfer the information to mesh \\(\\mathcal{M}\\) by adopting the fine-tuned Dreambooth model as the denoising network \\(\\epsilon_{\\psi}\\) for score distillation sampling. Specifically, we choose VSD instead of the original SDS because of its superior ability to generate highly realistic and diverse appearances. To render images \\(\\mathbf{x}\\) for VSD gradient computation, we follow Fantasia3D [10] to pre-select a fixed HDR environment map \\(E\\) as illumination and use Nvdiffrast [30] as our differentiable renderer. We set the object background to be a constant white color to match the input images for Dreambooth training. We observe this can help achieve better color fidelity compared to random color or neutral background.\n' +
      '\n' +
      'However, simply replacing SDS with VSD cannot address the limitation of lacking 3D knowledge in 2D diffusion models. We thus propose geometry-aware score distillation, where we inject geometry information extracted from mesh \\(\\mathcal{M}\\) into our personalized diffusion model \\(\\epsilon_{\\psi}\\) through a pre-trained ControlNet conditioned on normal maps \\(k\\) rendered from \\(\\mathcal{M}\\). This augmentation significantly boosts 3D consistency of generated textures (see Figure 10). With the ControlNet conditioning, the pillow texture from the input images can be accurately matched to the target shape, despite the shape mismatch. We experiment with different ControlNet conditions and show that normal conditions can best prevent texture-geometry mismatch.\n' +
      '\n' +
      'Let \\(\\mathbf{x}=g(\\theta,c)\\) be the rendered image under a fixed environment map from camera pose \\(c\\) with the extracted BRDF maps \\(a_{\\theta},r_{\\theta},m_{\\theta}\\). The gradient of proposed Personalized Geometry-aware Score Distillation (PGSD) to optimize the MLP parameter \\(\\theta\\) of BRDF field is:\n' +
      '\n' +
      '\\[\\nabla_{\\theta}\\mathcal{L}_{\\text{PGSD}}(\\theta)\\] \\[\\quad\\triangleq\\mathbb{E}_{t,c,c}[w(t)(\\epsilon_{\\psi}(\\mathbf{x }_{t};y,k,t)-\\epsilon_{\\phi}(\\mathbf{x}_{t};y,k,t,c_{\\rho}))\\frac{\\partial \\mathbf{x}}{\\partial\\theta}],\\]\n' +
      '\n' +
      '시간 \\(t\\), \\(c_{\\rho}\\)에서 카메라 외재성 \\(c\\bf{I}(\\math{f{I})의 임베딩(\\math{f{I})은 학습 가능한 카메라 인코더 \\(\\rho\\)에 의해 암호화된 카메라 외재성 \\(c\\)의 임베딩(\\math{I}(\\math{f{I}(\\b\\,\\b\\(\\b\\)의 임베딩)이고,\\(c\\(\\b\\(\\b\\,\\b\\(\\b\\)은 각각\\(\\b\\(\\b\\(\\b\\(\\b\\(\\b\\)\\(\\b\\(\\b\\:\\(\\b\\)의 임베딩)이고,\\(c\\(\\b\\(\\b\\(\\b\\,\\b\\) 입니다.{t{t{t{t{t{t{t{f{f{f{f{f{f{f{f{f{f{f{f{f{f{I} 두 모델 모두 그림 3의 확산 모델 아래의 노란색 부분에 나타난 바와 같이 정상 지도 \\(k\\)에 조절망을 조건화하여 증강된다.\n' +
      '\n' +
      '그람은 이 딕티보더위 에쿠레데 아쿨라, 스니커트 오믈리, 스누시 알라 트릴레진(51])에 있는 스포트(tati)를 보는데, 이 세프-테니 알테니 알테니 스릴레인(lltrainin g)와 함께 항-티티 레미어(Eq.1)를 유인한다.\n' +
      '\n' +
      'We additionally identify several important design choices through extensive experiments. First, it is important to initialize the \\(\\epsilon_{\\phi}\\) in Eq. 1 with original pre-trained diffusion model weights while the Dreambooth weight will remove texture details. This is probably because the Dreambooth fine-tuning process makes the diffusion model overfit to a small training set, as pointed out by previous work [51]. Moreover, we find that removing the LoRA weights can substantially improve texture fidelity. Similar difficulties in training LoRA were also reported in [56]. We therefore implement our personalized geometry-aware score distillation loss \\(\\mathcal{L}_{PGSD}\\) by removing the LoRA structure in \\(\\epsilon_{\\phi}\\) and only keeping the camera embedding, achieving the best quality. We show more comparisons in Figure 10.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '### Experimental setup\n' +
      '\n' +
      '우리는 소파, 침대, 머그/보울 및 플러시 장난감 4가지 범주에서 실험을 수행한다. 각 카테고리에 대해 8개의 객체 인스턴스를 선택하고, 객체를 둘러싼 3~5개의 뷰를 자유로이 샘플링하여 작은 이미지를 생성하여 총 32개의 이미지 세트를 생성한다. 32개의 이미지 세트 내의 모든 이미지에 대해 U2-Net[49]을 적용하여 전경 마스크를 자동으로 획득하거나 반자동 배경 제거 애플리케이션1을 사용하여 보다 정확한 마스크를 얻는다. 우리는 동일한 카테고리 형상, 상이한 카테고리 형상 또는 다른 속 수를 갖는 기하학을 포함하지만 제한되지 않는 다양한 메쉬로 설정된 각 이미지 세트에 대한 텍스처 전달을 수행한다. 질감 전달 프레임워크를 테스트하기 위해 캡처된 이미지 세트와 다른 4개 카테고리 각각에 대해 3개의 메쉬를 선택합니다. 우리는 3D-FUTURE [15] 및 온라인 레포스터.23에서 이러한 3D 메서를 획득한다. 4가지 범주의 모든 객체에 대해 클래스 내 질감 전달을 실행하고 침대와 의자 사이의 클래스 간 질감 전달을 실행하여 방법의 일반화 능력을 테스트합니다.\n' +
      '\n' +
      '발주 2: [주행://www.cgtrader.com/] (https://www.cgtrader.com/) (https://www.cgtrader.com/)\n' +
      '\n' +
      '부츠 3: [https://sketchfah.com/] (https://sketchfah.com/)\n' +
      '\n' +
      '** 구현 세부 정보** 우리는 PyTorch [45] 및 트라리오 [19]를 기반으로 프레임워크를 구현한다. 잠재 확산 및 제어넷 v1.1을 각각 사전 훈련된 확산 모델 및 제어넷으로 사용한다. 모든 실험에서 우리는 \\(\\mathcal{L}_{PGSD}\\)의 분류기가 없는 지침 중량을 원래 CFG 제형에서 \\(\\omega=0\\) 설정과 동등하다고 설정했다. 드림퓨전[47]에 이어 입력 텍스트 프롬프트에도 뷰 의존적 컨디셔닝을 적용합니다. BRDF 필드는 사전 작업 [10, 63]에 따라 해시-그리드 위치 인코딩[42]을 사용하여 MLP로 매개변수가 된다. 당사의 카메라 인코더는 카메라 외재성을 U-Net에서 시간과 텍스트 임베딩과 융합될 \\(1,280\\) 치수의 잠재 벡터에 투영하는 두 개의 선형 층으로 구성된다. 모든 실험에 대한 카메라 인코더용 \\(0.001\\) 및 \\(0.0001\\)를 인코딩하기 위한 학습률을 \\(0.01\\)로 실증적으로 설정하였다.\n' +
      '\n' +
      '### Baseline methods\n' +
      '\n' +
      'Latent-paint [37] and TEXTure [53] are two recent text-guided texturing methods with 2D diffusion prior. They also demonstrate the capability of texturing meshes from images. Latent-paint [37] leverages the Texture Inversion [16] to extract image information into text embedding and distills the texture with SDS. TEXTure [53] first finetunes the pre-trained diffusion model by combining Texture Inversion and Dreambooth [54] and use this fine-tuned model to synthesize texture with an iterative mesh painting algorithm. As preferred by the previous method [53], we augment the input images with a random color background. We closely follow the original implementation of baseline methods to run the experiments.\n' +
      '\n' +
      '임기 유도 질감 전달.\n' +
      '\n' +
      '**Qualitative evaluation** Our method can perform texture transfer to diverse object geometry, including geometry in the _same_ category or across different categories. Figure 4 demonstrates our texture transferring results on 4 categories of objects. Our method can synthesize geometry-aware and seamless textures that has similar patterns and styles as the input. We also demonstrate that our method can transfer textures _across different categories_. In Figure 1, we show texture transfer results from images of sofa to bed shapes, and vice versa. Our method is also capable of performing texture transfers across a broader range of different categories. As shown in Figure 5, high-quality and realistic textures can be synthesized across chair, mug, and plush toy categories. Since our synthesized texture contains albedo, metallic, and roughness maps, the target objects with the synthesized appearance can be relit, as shown in Figure 6. By using different random seeds, our framework can generate diverse textures, as shown in Figure 8.\n' +
      '\n' +
      '수컷에 있는 이스트 우레트 니트 에프토크, 이메트로프 에메트릭스 이메트 에메스 스레트-비크 에메트릭스 에메스 이메트 에메틱스(M._e. 테오디렉트 에메스 에메스 이메트 에메스 이메트 에메스 이메트로프 에메스 이메트로프 에메틱스 이메트 에메틱스 이메틱스 이메틱스 이메틱스 이메틱스 이메틱스 이메틱스 이메트 에메틱스 이메트 에메틱스 이메틱스 이메틱스 이메틱스 이메트 에메틱스 이메틱스 이메트 에메틱스 이메트 에메틱스 이메트 에메틱스 이메트 에메틱스 이메트 에메틱스, 우레트 에메틱스 아레트 에메트인트 에메트 에메틱스 이메트 에메틱스 이메트 에메틱스 이메트 에메틱스, 우레트 에메트 에메틱스, 우레트\n' +
      '\n' +
      '**Quantitative evaluation** It is non-trivial to perform quantitative comparisons for texture transfer due to the shape difference between geometry and photos. We perform a user study to evaluate transfer fidelity, texture photorealism, and texture-geometry compatibility across baselines by asking users the following questions: 1) Which one has the texture that looks more similar to input images? 2) Which one has a texture which looks more like a real object? 3) Which one has the texture which is more compatible with the meshes? (Which texture painted more fitted to the geometry?) We conduct a user study with Amazon Turk with three separate tasks. For each task, we ask each user 24 questions. Each question is a forced single-choice selection with two options among our and one baseline result with the rendered images from the same \\(4\\) sampled views and is evaluated by 20 different users. We only show input photos for the first similarity question, and hide the input photos for the other two questions to make the user focus on texture quality. We summarize the results in Table 1. Our results show signifi\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\multicolumn{2}{c}{Ours preferred over} \\\\ \\cline{2-3}  & Latent-Paint & TEXTure \\\\ \\hline Image Fidelity & 71.82\\% & 69.43\\% \\\\ Texture Photorealism & 77.03\\% & 85.52\\% \\\\ Shape-Texture Consistency & 78.49\\% & 85.16\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '영상 유도 텍스처 전달에 대한 표 1: ** 사용자 연구***.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline  & CLIP similarity \\(\\uparrow\\) \\\\ \\hline Latent-Paint [37] & 0.7969 \\\\ TEXTure [53] & 0.7988 \\\\ Ours & **0.8296** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '영상 유도 텍스트화에 대한 표 2: ** 정량적 평가***.\n' +
      '\n' +
      '이미지 충실도, 텍스처 광자주의, 형상-텍스트 일관성 측면에서 사용자가 선호하는 것이다.\n' +
      '\n' +
      '우리는 또한 참조와 렌더링된 사이 이미지 기반 CLIP 특징 [41]을 통한 유사성을 평가할 것을 제안한다.\n' +
      '\n' +
      '그림 4: ** 이미지 유도 전달 결과**는 이미지 세트의 4개 범주(베드, 소파, 플러시 장난감 및 머그컵)에서 다양한 객체에 이르기까지 다양하다. 우리의 방법은 광범위한 객체 유형에 적용될 수 있고 텍스처를 다양한 객체 형태로 전달할 수 있다.\n' +
      '\n' +
      '합성된 질감의 장점. CLIP 유사성은 재료 매칭 [65] 및 양식화 [38]에 적용되었다. 좋은 전달은 이미지로부터 질감만을 전달해야 하며 목표 형상 기하학을 고려하여 정적으로 질감을 전달해야 한다. 예를 들어, 이체는 형상의 각 부분에 대해 도색되어야 한다. 우리는 비교를 계산하기 위해 평가 세트를 사용합니다. 각 이미지 세트 및 타겟 3D 메쉬 쌍에 대해 각 기준 이미지 중 메트릭의 평균과 샘플링된 \\(4\\)의 렌더링된 이미지 각각을 계산(좌측 전면, 우전, 좌뒤 및 우후면)한다. 우리는 모든(이미지 세트, 메쉬) 쌍에 걸쳐 CLIP 유사성을 평균화한다. 표 2는 우리의 방법이 CLIP 유사도가 가장 높다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '우리는 먼저 기하학 인식 제어망의 중요성에 대한 절제 연구를 질적으로 수행한다. 그림 10에서 보는 바와 같이, 그 결과는 제어넷이나 깊이 기반 제어넷이 없는 기하학-텍스트 오정렬에 시달린다. 정상 기반 제어넷만이 합성된 텍스처를 입력 메시 기하학과 일치하도록 정확하게 제어할 수 있다. 다음으로 점수 증류 손실의 중요성을 검증한다. 우리의 프레임워크에서 SDS 손실을 사용하는 것만이 충분한 입력 충실도를 달성할 수 없으며 결과는 더 흐릿해지는 경향이 있다. LoRA가 제거되지 않고(일반적으로 바닐라 VSD 손실로 최적화되어 있는 경우, 최적화는 드림보트-피네이션 분포에서 분포를 발산시키는 경향이 있다. 이것은 원래 질감이 적지만 입력과는 더 무관한 패턴을 포함하는 출력을 초래한다. 이는 희귀 식별자가 포함된 텍스트 조건으로 LoRA 가중치를 최적화하는 것이 희소한 외관을 가지도록 렌더링된 이미지의 분포를 유도하는 경향이 있기 때문이라고 가정한다.\n' +
      '\n' +
      '일반 확산 모델 \\(\\epsilon_{\\파이}\\)을 개인화된 확산 모델 \\(\\epsilon_{\\psi}\\)로 대체하거나 분류기 자유 안내 가중치 \\(7.5\\)를 적용하면, 그 결과는 입력 이미지에 존재하지 않는 랜덤 패턴을 도입하는 경향이 있다. 카메라 인코더 가중치(\\rho\\)를 동결하는 것을 선택하면 결과는 전체 방법보다 더 심하거나 시끄러워진다.\n' +
      '\n' +
      '우리는 또한 시스템에서 각 구성요소의 중요성을 정량적으로 평가한다. 우리는 참조 이미지 간의 유사성을 측정하기 위해 이미지 기반 CLIP 특징을 사용한다.\n' +
      '\n' +
      'Figure 5: **Example of cross-category texture transfer results.** In the first row, we transfer appearances from plush toys to cups and chairs. In the second row, special patterns from mugs are transferred to bears and chairs. In the third row, textures from input sofa are transferred to cups and bears.\n' +
      '\n' +
      'Figure 6: **Example of relighting results.** The textures are relit by the original HDR environment maps (first row) and the novel maps (second and third rows).\n' +
      '\n' +
      '그랑 렌더링된 이미지. 공정하게 평가를 보장하기 위해 와렌더 에디 마지어의 배경은 재치 흰색의 콜라 r을 가리고 있다.\n' +
      '\n' +
      '따라서, 그림 1 0에서 이 트라메스 아그레의 노린트 스물트 나루(오네프) 헤르페에 있는 오시노 스캅스 에우트 리슈(오베크 트란디) 레슬레, 이 스터브의 이끼는 더 크실레, 스윕의 질감.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline  & CLIP similarity \\(\\uparrow\\) \\\\ \\hline w/o ControlNet & _0.8394_ \\\\ w/ ControlNet (Depth) & _0.8320_ \\\\ SDS, w/o CFG & 0.8101 \\\\ SDS, CFG \\(100\\) & 0.7983 \\\\ w/o LoRA removed & 0.8110 \\\\ Personalized model as \\(\\phi\\) & 0.8218 \\\\ CFG weight as \\(7.5\\) & 0.8218 \\\\ w/o camera encoder \\(\\rho\\) updated & 0.8267 \\\\ Ours & **0.8296** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: **Ablation study** on image-based texturing w.r.t. CLIP image-based feature similarity. Although _w/o ControlNet_ and _w/ ControlNet (Depth)_ achieve higher similarity score, the transfer results tend to ignore target shape and directly paint the texture without reasoning the geometry. Among the remaining ablative methods, our full method achieves the highest CLIP similarity w.r.t. reference images.\n' +
      '\n' +
      '합성된 질감의 그림 8: **다양성****.\n' +
      '\n' +
      'Figure 7: **Comparison between baseline methods.** Compared with Latent-Paint [37] and TEXTure [53], our method can synthesize seamless and geometry-aware textures which are compatible with the target mesh geometry.\n' +
      '\n' +
      'Figure 9: **Limitations.** Our method may bake-in lighting into texture, have Janus problem when lacking enough input viewpoints, and ignore special and non-repeated patterns from the input.\n' +
      '\n' +
      '## 5 Discussions\n' +
      '\n' +
      'We proposed a framework to transfer texture from input images to an arbitrary shape. While our method can transfer high-quality texture in most cases, there are some limitations. Figure 9 shows that our method may not be able to transfer special and non-repeated texture to the target shapes. In addition, our method tends to bake in lighting to texture when there are strong specular highlights in the input images. Janus problem might appear when the viewpoints of input images do not cover the entire object. Nevertheless, we believe that our method can be the first step to tackling this challenging problem and will make an impact in the 3D content creation community.\n' +
      '\n' +
      'Figure 10: **Ablation study.** (First row) With ControlNet conditioned on normal maps, the result has the best texture-geometry consistency. Without ControlNet or with depth-based ControlNet, the results suffer from texture-geometry misalignment. Using SDS loss leads to blurry textures. Without the LoRA module removed, the results tend to remove the existing texture from the personalized diffusion model. Our full method can synthesize accurate texture which is similar to input appearances. (Second row) If replacing generic diffusion model \\(\\phi\\) with personalized model or applying classifier guidance scale \\(7.5\\), some random patterns might appear in the synthesized texture. If we freeze the camera encoder \\(\\rho\\), the result might be worse or more noisy than our full method.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Adobe substance 3d. [https://docs.substance3d.com/sat](https://docs.substance3d.com/sat).\n' +
      '* [2] Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, and Jia-Bin Huang. Single-image 3d human digitization with shape-guided diffusion. In _SIGGRAPH Asia_, 2023.\n' +
      '* [3] Sai Bi, Nima Khademi Kalantari, and Ravi Ramamoorthi. Patch-based optimization for image-based texture mapping. _ACM Trans. Graph._, 36(4):106-1, 2017.\n' +
      '* [4] Alexey Bokhovkin, Shubham Tulsiani, and Angela Dai. Mesh2tex: Generating mesh textures from image queries. _arXiv preprint arXiv:2304.05868_, 2023.\n' +
      '* [5] G. Cai, K. Yan, Z. Dong, I. Gkioulekas, and S. Zhao. Physics-based inverse rendering using combined implicit and explicit geometries. _Computer Graphics Forum_, 41(4):129-138, 2022.\n' +
      '* [6] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp, and Kangxue Yin. Textfusion: Synthesizing 3d textures with text-guided image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4169-4181, 2023.\n' +
      '* [7] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16123-16133, 2022.\n' +
      '* [8] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. _arXiv preprint arXiv:1512.03012_, 2015.\n' +
      '* [9] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Niessner. Text2tex: Text-driven texture synthesis via diffusion models. _arXiv preprint arXiv:2303.11396_, 2023.\n' +
      '* [10] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. _arXiv preprint arXiv:2303.13873_, 2023.\n' +
      '* [11] Zhiqin Chen, Kangxue Yin, and Sanja Fidler. Auv-net: Learning aligned uv maps for texture transfer and synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1465-1474, 2022.\n' +
      '* [12] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-?2n2: A unified approach for single and multi-view 3d object reconstruction. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14_, pages 628-644. Springer, 2016.\n' +
      '* [13] Alexei A Efros and Thomas K Leung. Texture synthesis by non-parametric sampling. In _Proceedings of the seventh IEEE international conference on computer vision_, pages 1033-1038. IEEE, 1999.\n' +
      '* [14] Ziya Erkoc, Fangchang Ma, Qi Shan, Matthias Niessner, and Angela Dai. Hyperdiffusion: Generating implicit neural fields with weight-space diffusion. _arXiv preprint arXiv:2303.17015_, 2023.\n' +
      '* [15] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furniture shape with texture. _International Journal of Computer Vision_, 129:3313-3337, 2021.\n' +
      '* [16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* [17] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. Get3d: A generative model of high quality 3d textured shapes learned from images. _Advances In Neural Information Processing Systems_, 35:31841-31854, 2022.\n' +
      '* [18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. _Communications of the ACM_, 63(11):139-144, 2020.\n' +
      '* [19] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zixin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: A unified framework for 3d content generation. [https://github.com/threestudio-project/threestudio](https://github.com/threestudio-project/threestudio), 2023.\n' +
      '* [20] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg. Shape, light, and material decomposition from images using monte carlo rendering and denoising. _Advances in Neural Information Processing Systems_, 35:22856-22869, 2022.\n' +
      '* [21] Paul Henderson, Vagia Tsiminaki, and Christoph H Lampert. Leveraging 2d data to learn textured 3d mesh generation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 7498-7507, 2020.\n' +
      '* [22] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* [24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [25] Brian Karis and Epic Games. Real shading in unreal engine 4. _Proc. Physically Based Shading Theory Practice_, 4(3):1, 2013.\n' +
      '* [26] Animesh Karnewar, Andrea Vedaldi, David Novotny, and Niloy J Mitra. Holodiffusion: Training a 3d diffusion model using 2d images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18423-18433, 2023.\n' +
      '* [27] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani Lischinski. Noise-free score distillation. _arXiv preprint arXiv:2310.17590_, 2023.\n' +
      '* [28] Johannes Kopf, Chi-Wing Fu, Daniel Cohen-Or, Oliver Deussen, Dani Lischinski, and Tien-Tsin Wong. Solid texture synthesis from 2d exemplars. In _ACM SIGGRAPH 2007 papers_, pages 2-es. 2007.\n' +
      '* [29] Vivek Kwatra, Arno Schodl, Irfan Essa, Greg Turk, and Aaron Bobick. Graphcut textures: Image and video synthesis using graph cuts. _Acm transactions on graphics (tog)_, 22(3):277-286, 2003.\n' +
      '* [30] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. Modular primitives for high-performance differentiable rendering. _ACM Transactions on Graphics_, 39(6), 2020.\n' +
      '* [31] Jiabao Lei, Yabin Zhang, Kui Jia, et al. Tango: Text-driven photorealistic and robust 3d stylization via lighting decomposition. _Advances in Neural Information Processing Systems_, 35:30923-30936, 2022.\n' +
      '* [32] Marc Levoy, Kari Pulli, Brian Curless, Szymon Rusinkiewicz, David Koller, Lucas Pereira, Matt Ginzton, Sean Anderson, James Davis, Jeremy Ginsberg, et al. The digital michelangelo project: 3d scanning of large statutes. In _Proceedings of the 27th annual conference on Computer graphics and interactive techniques_, pages 131-144, 2000.\n' +
      '* [33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, 2022.\n' +
      '* [34] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 300-309, 2023.\n' +
      '* [35] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2837-2845, 2021.\n' +
      '* [36] Yiwei Ma, Xiaoqing Zhang, Xiaoshuai Sun, Jiayi Ji, Haowei Wang, Guannan Jiang, Weilin Zhuang, and Rongrong Ji. X-mesh: Towards fast and accurate text-driven 3d stylization via dynamic textual guidance. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2749-2760, 2023.\n' +
      '* [37] Gal Metzler, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12663-12673, 2023.\n' +
      '* [38] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-driven neural stylization for meshes. _arXiv preprint arXiv:2112.03221_, 2021.\n' +
      '* [39] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-driven neural stylization for meshes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13492-13502, 2022.\n' +
      '* [40] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. _Communications of the ACM_, 65(1):99-106, 2021.\n' +
      '* [41] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. Clip-mesh: Generating textured meshes from text using pretrained image-text models. In _SIGGRAPH Asia 2022 conference papers_, pages 1-8, 2022.\n' +
      '* [42] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Transactions on Graphics (ToG)_, 41(4):1-15, 2022.\n' +
      '* [43] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Muller, and Sanja Fidler. Extracting triangular 3d models, materials, and lighting from images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8280-8290, 2022.\n' +
      '* [44] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 165-174, 2019.\n' +
      '* [45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Braddury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.\n' +
      '* [46] Dario Pavllo, Jonas Kohler, Thomas Hofmann, and Aurelien Lucchi. Learning generative models of textured 3d meshes from real-world images. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 13879-13889, 2021.\n' +
      '* [47] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. _arXiv preprint arXiv:2209.14988_, 2022.\n' +
      '* [48] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandar Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. _arXiv preprint arXiv:2306.17843_, 2023.\n' +
      '* [49] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar Zaiane, and Martin Jagersand. U2-net: Going deeper with nested u-structure for salient object detection. page 107404, 2020.\n' +
      '* [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [51] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: Subject-driven text-to-3d generation. _arXiv preprint arXiv:2303.13508_, 2023.\n' +
      '* [52] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1 (2):3, 2022.\n' +
      '\n' +
      '* [53] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. In _ACM SIGGRAPH 2023 Conference Proceedings_, New York, NY, USA, 2023. Association for Computing Machinery.\n' +
      '* [54] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.\n' +
      '* [55] Prafull Sharma, Varun Jampani, Yuanzhen Li, Xuhui Jia, Dmitry Lagun, Fredo Durand, William T Freeman, and Mark Matthews. Alchemist: Parametric control of material properties with diffusion models. _arXiv preprint arXiv:2312.02970_, 2023.\n' +
      '* [56] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: a single image to consistent multi-view diffusion base model, 2023.\n' +
      '* [57] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. _arXiv preprint arXiv:2308.16512_, 2023.\n' +
      '* [58] Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan, Matthias Niessner, and Angela Dai. Texturify: Generating textures on 3d shape surfaces. In _European Conference on Computer Vision_, pages 72-88. Springer, 2022.\n' +
      '* [59] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International conference on machine learning_, pages 2256-2265. PMLR, 2015.\n' +
      '* [60] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.\n' +
      '* [61] Cheng Sun, Guangyan Cai, Zhengqin Li, Kai Yan, Cheng Zhang, Carl Marshall, Jia-Bin Huang, Shuang Zhao, and Zhao Dong. Neural-pbir reconstruction of shape, material, and illumination. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [62] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12619-12629, 2023.\n' +
      '* [63] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. _arXiv preprint arXiv:2305.16213_, 2023.\n' +
      '* [64] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. _Advances in neural information processing systems_, 29, 2016.\n' +
      '* [65] K. Yan, F. Luan, M. Hasan, T. Groueix, V. Deschaintre, and S. Zhao. Psdr-room: Single photo to scene using differentiable rendering. In _ACM SIGGRAPH Asia 2023 Conference Proceedings_, 2023.\n' +
      '* [66] Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Zhengzhe Liu, and Xiaojuan Qi. Texture generation on 3d meshes with point-uv diffusion. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4206-4216, 2023.\n' +
      '* [67] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3836-3847, 2023.\n' +
      '* [68] Qian-Yi Zhou and Vladlen Koltun. Color map optimization for 3d reconstruction with consumer depth cameras. _ACM Transactions on Graphics (ToG)_, 33(4):1-10, 2014.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>