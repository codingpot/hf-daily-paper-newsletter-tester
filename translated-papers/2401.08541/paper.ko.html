<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '대규모 계량 계량 이미지 모델 제품 산출물 제공.\n' +
      '\n' +
      'Alaaeldin El-Nouby\n' +
      '\n' +
      'Alexander Toshev\n' +
      '\n' +
      'Michal Klein\n' +
      '\n' +
      'Vaishaal Shankar\n' +
      '\n' +
      'Shuangfei Zhai\n' +
      '\n' +
      '누누나.\n' +
      '\n' +
      '조슈아 M.\n' +
      '\n' +
      'Armand Joulin\\({}^{*}\\)\n' +
      '\n' +
      'Apple\n' +
      '\n' +
      '[https://github.com/apple/ml-aim](https://github.com/apple/ml-aim)\n' +
      '\n' +
      '애플에서 일을 했어요. 이제 구글 딥민드에서.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문은 자기회귀적 목표와 함께 사전 훈련된 비전 모델의 집합체인 Aim을 소개한다. 이 모델은 텍스트 대응물, 즉 대형 언어 모델(LLM)에서 영감을 받아 유사한 스케일링 특성을 나타낸다. 구체적으로, 우리는 (1) 모델 용량과 데이터의 양, (2) 목적 함수의 값은 다운스트림 태스크에 대한 모델의 성능과 상관관계가 있는 시각적 특징 척도의 성능이라는 두 가지 주요 결과를 강조한다. 우리는 냉동 트렁크를 사용하여 이미지넷-1k에서 84.0%를 달성하는 20억 개의 이미지에 대해 70억 매개변수 \\(텍스트sc{Aim}\\)를 사전 훈련함으로써 이러한 발견의 실질적인 시사점을 보여준다. 흥미로운 사실은 이 규모에서도 성능에서 포화 징후를 관찰하지 못하며, 이는 Aim이 잠재적으로 대규모 비전 모델을 훈련하기 위한 새로운 프론티어를 나타냄을 시사한다. Aim의 사전 훈련은 LLM의 사전 훈련과 유사하며, 스케일에서의 훈련을 안정화하기 위한 어떠한 이미지 특정 전략이 필요하지 않다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '사전 훈련 과제 탐색 모델은 최근 대형 언어 모델(LLM)[13, 64, 75]의 혁명으로 자연 언어 처리에 표준이 되었다. 이러한 모델은 몇 가지 예[13]에서 복잡한 추론 작업을 해결할 수 있으며, 지침[59]를 따를 수 있으며 현재는 ChatGPT와 같은 널리 사용되는 AI 보조자의 엔진 역할을 한다. 그들의 성공에 기여하는 핵심 요인은 용량(매개변수 수)이나 사전 학습 데이터(64])의 양이 증가함에 따라 일관되게 개선될 수 있는 능력이다.\n' +
      '\n' +
      '이 모델의 스케일링 행동은 두 가지 주요 이유로 주목할 만하다. 첫째, 이러한 모델들은 단순한 객관성으로 훈련되었음에도 불구하고 과거에 주어진 문장에서 다음 단어를 예측하는 것은 긴 맥락에 걸쳐 복잡한 패턴을 학습할 수 있다. 둘째, 이 자기회귀 목표의 확장성은 특정 아키텍처와 함께 사용될 때 대부분 관찰되며, 특히 트랜스퍼러[79]는 자기회귀 사전 훈련과 이 아키텍처 사이의 잠재적인 시너지 효과를 강조한다.\n' +
      '\n' +
      '이러한 관찰은 자연스럽게 자기회귀적 목적을 가진 스케일링 변환기의 성공이 텍스트에 배타적인지에 대한 후속 질문을 제기한다. 이는 앞서 언급한 요소 중 어느 것도 본질적으로 언어 모델링에 특이적이지 않다는 점을 고려할 때 특히 중요하다. 진보적 목표는 데이터 압축 문헌[69]에서 뿌리를 취하며 오디오[57] 및 이미지[18, 76]에서 유사한 접근법이 조사되었다. 트랜스포머 아키텍처는 특히 비전트랜스포머(ViT) [29]의 성공을 거두며 컴퓨터 비전에서도 성공적으로 사용되었다. 따라서 LLM의 결과를 일반화하기 위한 첫 번째 단계로, 우리는 자기회귀적 목적을 가진 ViT 모델을 훈련하는 것이 LLM과 동일한 스케일링 능력을 가진 학습 표현 측면에서 경쟁 성과로 이어졌는지 탐구한다.\n' +
      '\n' +
      '본 논문에서는 시각적 특징에 대한 대규모 사전 학습을 위한 자기회귀적 접근 방식인 자기회귀적 이미지 모델(**Aim***)을 소개한다. 시력 변압기, 대규모 웹 데이터 수집(32, 33]) 및 최근 LLM 사전 학습(43, 75])의 발전을 포함하는 현대적인 도구 세트를 사용하여 iGPT[18]와 같은 자기회귀 표현 학습에서 사전 작업을 재방문한다. 또한, 시각적 특징에 대한 자기회귀 사전 학습을 적응시키기 위해 두 가지 건축 변형을 소개합니다. 첫째, LLM의 경우와 마찬가지로 자기 의사를 완전히 인과적이라고 제한하는 대신 T5 [66]에서와 같이 프리픽스 관심을 채택한다. 이러한 선택은 하류 작업 동안 완전한 양방향 주의로 이동할 수 있게 한다. 둘째, 우리는 대조 학습 [19]에 사용된 머리에서 영감을 받은 크게 매개변수화된 토큰 수준 예측 헤드를 사용한다. 우리는 이러한 변형이 훈련 중 오버헤드가 거의 없는 후속 특징의 품질을 크게 향상시킨다는 것을 관찰한다. 전반적으로 Aim의 훈련은 최근 LLM의 훈련과 유사하며 [24, 74] 또는 자기 지도 [5, 58] 방법이 필요한 안정성 유발 기술[24, 45, 74]에 의존하지 않는다.\n' +
      '\n' +
      '우리는 허용 라이선스가 있는 2B 미확인 이미지를 사용하여 미리 훈련된 600M에서 7B 매개변수 범위의 일련의 모델에 대한 연구를 제공한다. 우리의 Aim 모델은 15개의 이미지 인식 벤치마크보다 평균 정확도로 측정된 더 높은 용량 모델이 더 나은 다운스트림 성능을 달성하는 그림 1과 같은 강력한 스케일링 행동 w.r.t를 나타낸다. 더 중요한 것은 검증 세트에 대한 목표 함수의 값과 후속 냉동 특징의 품질 사이에 상관관계가 있다는 것이다. 이 관찰은 자기회귀적 목표가 시각적 특징을 훈련하기에 적절하다는 것을 확인시켜준다. 또한, 우리는 포화 징후 없이 더 많은 이미지를 훈련시키기 때문에 하류 성능의 일관된 개선을 관찰한다. 전반적으로, 이러한 관찰은 스케일링 대형 언어 모델에 대한 이전 연구와 정렬된다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '** 자동회귀 모델** 자기회귀 모델에 대한 대부분의 문헌은 언어 모델링[9, 53, 64] 또는 음성[56, 57]에서 비롯되며, 이미지 [18, 49, 61, 68, 76]에 대한 이 접근의 잠재력을 탐구한 작품은 거의 없다. 특히 관심 있는 반 데니 오드 등[76]은 이미지에 적응된 아키텍처를 사용하여 더 일반적인 아키텍처[77]인 순환 네트워크[31]로 구축된 자기회귀 모델보다 컨볼루션 네트워크가 크게 향상되었음을 보여준다. 파마 등[61]은 변압기 아키텍처[79]를 채택하여 이러한 자기회귀 모델의 품질을 더욱 향상시킨다. 보다 최근에, Chen et al. [18]은 더 많은 계산으로 스케일링이 지속적인 개선으로 이어진다는 것을 보여주었다. 우리의 작업은 이러한 연구 라인을 따르며 훨씬 더 많은 데이터에 대한 훈련과 아키텍처 설계 [29], 훈련 [73, 75] 및 스케일링 법 [43]에 대한 이해의 추가 개선을 통해 혜택을 받는다. 우리의 작업과 함께 바이 등은 텍스트 내 픽셀 예측 과제(시맨틱 세분화, 깊이 추정)에 대한 대규모 자기회귀 비전 모델의 효과를 보여준다.\n' +
      '\n' +
      '** 자기 지도 사전 훈련** 감독 없이 이미지의 데이터 세트에 대한 사전 훈련 비전 모델은 최근 몇 년 동안 [10, 27, 28, 34, 54, 87, 88]의 과일 연구 영역이다. 특징 학습을 위한 다양한 대리 작업을 중심으로 다양한 접근법이 사용되었다. 예를 들어 노루지와 파바로[55]는 반짝이는 이미지 패치의 순서를 재연하는 법을 배우게 된다. 일부 다른 작품은 [7, 14, 17, 83] 군집링에 의존했다. 또 다른 인기 있는 접근법은 예측 코딩과 유사한 대조적 목적을 사용하는데, 여기서 목적은 각 이미지 [19, 40]를 식별하는 것이다. 가장 최근의 대조적 접근법에는 DINO[58], BYOL[38] 또는 iBot[88]이 포함된다. 유사한 맥락에서 일부 작업은 예측 접근법[2, 6] 또는 특징 미백[85]을 제안했다. 우리의 접근법에 대한 패치는 패치가 마스킹되고 개별 [5] 또는 픽셀 [41] 형태로 오토인코더로 예측된 BERT[26]에서 영감을 얻은 작품이다.\n' +
      '\n' +
      '** 기타 생성 사전 훈련** 자기 진보 모델링은 생성 모델링의 형태이며, 시각적 특징을 배우는 다른 생성 접근법이 거의 고려되지 않았다. 상기 제1 카테고리는 상기 프레이팅 태스크가 일부 디오징 태스크에 해당하는 자동차 인코딩의 일부 형태를 유지한다. 예를 들어, 소음은 소금 및 페퍼[81]이거나 마스킹[5, 62]일 수 있다. 또 다른 작업 라인은 제네릭 루버러네트웍스(GAN) [35]를 차지한다. 가장 중요한 것은 빅GAN[12]은 큰 GAN을 훈련시키고 이미지 판별기를 재설정하여 이미지 특징을 생성한다. 보다 최근에 DiffMAE[82]는 확산 모델을 사용하여 이미지 특징을 학습했다.\n' +
      '\n' +
      '** 사전 규모의 훈련*****는 감독 없이 시각적 특징을 사전 훈련하는 작품[15, 36, 37, 58, 70, 72]으로 스케일링하는 작품이 많다. 이 영역에서 가장 두드러진 작업은 DINOv2이며, 142M 이미지 및 460M 파라미터 모델의 개인 데이터 세트에 iBot 방법[88]을 스케일링하여 최고의 셀프 편집 기능을 생성한다. 이 작업의 결론은 신중하게 조정되는 대조적 방법이 합리적으로 잘 스케일링되지만 언어 모델링을 통해 관찰한 스케일링 법칙을 나타내지 않는다는 것이다. 그들은 또한 첸 et al. [20]에 의해 기술된 함정을 피하기 위해 대조 학습의 복잡한 구현에 의존한다. 동시에 싱 등은 마스킹 오토인코더(MAE) [39]의 스케일링을 연구한다. 연구는 약하게 구성된 설정에 초점을 맞추고 있지만, 수십억 개의 이미지로 데이터를 확장함에 따라 자체 학습 사전 학습에 대한 강력한 개선을 나타내지 않는다. 대조적으로, 우리는 수십억 개의 매개변수와 수십억 개의 이미지 규모에서도 특징 품질에 대한 규모의 명확한 이점을 관찰한다.\n' +
      '\n' +
      '프리트레이닝 Datasetet 3개의 Datasetet을 제공합니다.\n' +
      '\n' +
      '우리는 Fang et al.[32]에 의해 도입된 DFN 데이터 세트에 대한 모델을 사전 균주화한다. 이 데이터 세트는 공통 크로슬에서 필터링된 12.8B 이미지-텍스트 쌍[33]의 더 큰 컬렉션으로 구성된다. 데이터는 NSFW 함량을 제거하고 얼굴을 흐리고 평가 세트에 대해 중복하여 오염을 줄이기 위해 사전 처리되었다. 데이터 필터링 네트워크[32]는 이미지와 해당 캡션 간의 정렬 점수에 따라 12.8B 수집에서 샘플을 순위화한다. DFN-2B라고 하는 2B 이미지의 하위 집합은 상위 15% 샘플을 유지함으로써 데이터 화합물 12.8B 데이터세트[33]에서 추출되었다. 프라이버시 및 안전 필터 이외의 다른 프로세스에는 이미지 콘텐츠에 기초한 추가 큐레이션이 포함되어 있지 않다. 우리의 사전 훈련은 텍스트를 필요로 하지 않기 때문에, 우리의 방법은 캡션과 페어링되지 않거나 나머지 데이터 화합물 12.8B와 같은 이미지-텍스트 정렬이 낮은 더 큰 이미지 컬렉션을 사용하여 미리 트레이닝될 수 있다.\n' +
      '\n' +
      '프리트레이닝 동안 위키피디아와 북과 같은 양질의 데이터 소스의 LLM 사전 학습 [75]에서 공통 관행에 의해 재구성된 우리는 사전 훈련 동안 \\(p=0.8\\) 확률로 DFN-2B의 이미지 및 \\(p=0.2\\) 확률로 이미지넷-\\(1\\)k의 샘플 이미지를 샘플링한다. 우리는 DFN-2B+와 같은 데이터 세트를 말합니다.\n' +
      '\n' +
      '## 4 Approach\n' +
      '\n' +
      '### Training Objective\n' +
      '\n' +
      '우리의 훈련 목표는 이미지 패치 시퀀스에 적용된 표준 자기회귀 모델의 학습 목적을 따른다. 보다 정확히 말하면, 이미지 \\(x\\)는 집합적으로 토큰의 시퀀스를 형성하는 \\(K\\) 비오버레이핑 패치 \\(x_{k}\\), \\(k\\in[1,K]\\)의 격자로 분할된다. 우리는 시퀀스 순서가 모든 이미지에 걸쳐 고정되어 있다고 가정하며, 달리 명시되지 않는 한 기본으로 주문하는 래스터(로우-메이저)를 사용한다. 이상의 순서를 감안할 때, 영상의 확률은 패치 조건부 확률의 곱으로 요인화될 수 있다.\n' +
      '\n' +
      '1}\\[P(x)=1}^{k = 1}^{K}P(x_{k} H(x_{k}\\mid x_{<k})), 즉\\tag{1}\\\\]]]==\\prod_{k]]]]]\n' +
      '\n' +
      'Hf(x_{<k}\\)가 첫 번째 \\(k-1\\) 패치의 세트를 나타내는 곳이며, \\(k\\)번째 패치를 예측하는 데 사용되는 맥락이다. 언어 모델링과 대조적으로, 우리의 서열은 메모리에 맞는 \\(K\\)의 고정된 길이를 가지고 있으므로 컨텍스트 길이를 절단할 필요가 없다. 그런 다음 이미지의 집합 \\(\\mathcal{X}\\)에 대한 훈련 손실은 음의 로그 가능성(NLL)으로 정의된다.\n' +
      '\n' +
      '\\[\\sum_{x\\in\\mathcal{X}}\\sum_{k=1}^{K}-\\log P(x_{k}\\mid x_{<k})]]].\n' +
      '\n' +
      '더 이상의 가정이 없는 무한량의 이미지보다 이러한 목적을 최소화하면 이론적으로 진정한 기본 이미지 분포를 배우는 것과 동일하다.\n' +
      '\n' +
      '** 예측 손실** 우리 훈련 목표는 자연적으로 특정 손실 변이체를 생성하며, 각각은 분포 \\(P(x_{k}\\mid x_{<k})\\에 해당한다. 채무불이행에 의해 H et al. [41]과 유사한 정규화된 픽셀 수준 회귀 손실을 채택한다. 이 손실은 일정한 분산을 갖는 가우시안 분포로서 \\(P(x_{k}\\mid x_{<k})\\를 설정하는 것에 해당한다. \\(\\-ta\\)로 매개변화된 네트워크로부터의\\(k\\) 패치의 예측으로서,\\(\\hat{x}_{k}})는 해당 지상-진실값으로,\\(x_{k}\\)는 합 \\(\\_{k}(\\theta)을 계산하는 것이다.\n' +
      '\n' +
      '\\[\\min_{\\theta}\\frac{1}{K}\\sum_{k=1}^{K}\\|\\hat{x}_{k}(\\theta)-x_{k}\\|_{2}^{2}. \\tag{2}\\]\n' +
      '\n' +
      '우리는 또한 오프라인 토큰라이저를 사용하여 이산 토큰으로 변환된 패치와의 교차 엔트로피 손실을 고려한다. 우리의 절제 연구는 이러한 설계가 픽셀별 손실만큼 강력한 기능을 생성하지 않지만 작동한다는 것을 보여준다.\n' +
      '\n' +
      '그림 2: **Aim 사전 학습 개요** 입력 이미지는 비오버레이핑 패치로 분할되고 도소비츠키 등(29])에 따라 선형적으로 임베딩된다. 패치 특징은 이전 위치에 참석하는 것을 방지하기 위해 자기 의도 작동이 인과적으로 마스킹되는 변압기에 공급된다. 이후, 크게 파라미터화된 MLP는 패치 특징들 각각을 독립적으로 처리하여 최종적으로 픽셀 공간에 투사한다. 대상자들은 하나의 위치를 왼쪽으로 이동시킨 입력 시퀀스에 해당하여 모델이 래스터 순서대로 다음 패치를 예측하도록 요구한다.\n' +
      '\n' +
      '### Architecture\n' +
      '\n' +
      '백본으로서 비전 트랜스포머 아키텍처(ViT) [28]를 채택한다. 모델 용량의 스케일링을 위해 언어 모델링에서 일반적인 관행을 따르며 깊이[64, 75]보다는 폭을 확대하는 것을 우선한다. 표 1에서 우리는 각 모델 용량에 대한 데이터 및 최적화 방안의 양뿐만 아니라 그 깊이와 폭을 포함한 Aim의 설계 파라미터에 대한 개요를 제공한다. 전체 모델은 그림 2에 나와 있다.\n' +
      '\n' +
      '사전 훈련하는 동안 우리는 이전 패치가 주어진 패치의 확률을 모델링하기 위해 자기 의도 계층에 인과적 마스크를 적용한다. 보다 정확하게는 자기 의도 계층을 고려할 때, 패치 \\(i\\)에 대한 임베딩이 계산된다.\n' +
      '\n' +
      '\\[y_{i}=\\sum_{k=1}^{K}a_{ik}v_{i}, \\tag{3}\\]\n' +
      '\n' +
      'i\\(a_{ik}\\)는 주의 중량이고 \\(v_{k}\\) 값은 임베딩이다. 원하는 제약을 시행하기 위해 \\(a_{ik}=0\\)와 \\(k>i\\), \\(\\sum_{k=1}^{K}a_{ik}=1\\)에 대한 인과적 마스크를 사용한다. 이 접근법을 통해 추가적인 계산 오버헤드를 일으키지 않고 훈련 중 단일 정방향 패스로 이미지를 처리할 수 있습니다.\n' +
      '\n' +
      '** 프리픽스 트랜스포머*** 사전 훈련의 자기회귀 목표는 자기 의도 작동에서 인과적 마스크를 필요로 한다. 그러나 이는 양방향 자기 의사를 사용하는 하류 작업에서 ViT 모델의 표준 사용량과 다르다. 이러한 불일치는 다운스트림 적응 동안 인과 마스크가 유지되는지 여부에 관계없이 성능 감소로 이어진다(표 3에 제시된 연마에서 확인할 수 있다). 이 문제를 해결하기 위해 Raffel et al. [65]의 프레픽스LM 제형에 따른 나머지 패치를 예측하기 위한 맥락에서 프리픽스라고 하는 서열의 초기 패치를 고려할 것을 제안한다. 전구 패치는 자기회귀 예측에서 제외되므로 인과 관계에 제약을 받지 않는다. 보다 정확하게는 크기 \\(S\\in[1,K-1]\\)의 프리픽스 길이를 선택하고, \\(k<S\\)에 대한 인과 마스크 _i.e_, \\(a_{i,k}>0\\)를 제거한다. 이 수정은 모델이 인과 마스킹이 없는 상태에서 작동하도록 도와 하류 적응 동안 제거할 수 있도록 한다. 이 접근법은 다운스트림 작업에서 모델의 성능을 개선하고 ViT에 대한 건축 변화의 필요성을 제거한다. 그림 3은 인과관계와 프리픽스 주의도의 차이를 보여준다.\n' +
      '\n' +
      '**MLP 예측 헤드*** 하류 과제[16, 17, 19, 20, 38]로 이전할 때 폐기되는 사전 훈련 중 특정 예측 헤드를 채택하는 일반적인 관행이다. 이 머리의 목적은 트렁크 기능이 사전 훈련 목적에 너무 특화된 것을 방지하여 하류 전달에 대한 적합성을 향상시키는 것이다. 최종 변압기 층 위에 MLP의 \\(N\\) 블록을 사용하여 각 패치를 독립적으로 처리하는 간단한 디자인을 선택합니다. 이 디자인은 사전 훈련 중에 발생하는 성능과 추가 비용 사이의 좋은 균형을 불어넣는 것을 관찰했다.\n' +
      '\n' +
      '**Straight seekers 구현*** Aim은 LayerScale[74], 확률 깊이[45], QK 정상 [24] 또는 패치 프로젝터[20] 동결과 같은 특정 최적화 안정성 유발 메커니즘이 필요하지 않다는 점에 주목할 필요가 있다. 이러한 메커니즘은 감독 또는 자가 지도된 다른 방법의 성공에 중요했다. 반대로, 우리는 더 이상의 튜닝이 없는 모델 크기에 걸쳐 동일한 최적화 하이퍼파라미터 세트를 사용하는 Aim 스케일이 관찰한다(표 1 참조).\n' +
      '\n' +
      '우리는 변압기 전과 MLP 헤드 전에 입력 패치에 정현 위치 임베딩[79]을 추가한다. 우리는 트렁크 및 머리의 모든 MLP 블록에 대해 4의 표준 확장 비율을 사용한다. 단순성을 위한 편향항을 떨어뜨리고, 원래의 ViT와 달리 입력에 분류 토큰을 부리지 않는다. 기본으로 모든 모델 능력에 대해 MLP 헤드에 12개의 블록을 사용합니다. 화소 표적은 헤 등의 손실 계산 전에 패치당 정규화되어 있다[41] 우리는 bfloat16 정밀도를 사용하여 모델을 훈련합니다. 우리는 선형 평가 일정과 코사인 붕괴 일정을 가진 AdamW[52] 최적화기를 사용한다. 부록 D에서 사전 훈련 및 하류 적응에 사용되는 하이퍼모수들을 자세히 설명한다.\n' +
      '\n' +
      '대형 모델을 탐사하는** 사전 훈련하는 것은 자원 집약적 과정이며, 자원 집약적 과정이며, 이를 미세 조정하기라도***다운스트림 적응이라 할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l}\n' +
      '**Model** & **\\#Params** & **Hidden size** & **Layers** & **LR** & **\\#Patches** & **Batch size** \\\\ \\hline Aim-0.6B & 0.6B & 1536 & 24 & \\(1e^{-3}\\) & 0.5T & 4096 \\\\ Aim-1B & 1.2B & 2048 & 24 & \\(1e^{-3}\\) & 1.2T & 4096 \\\\ Aim-3B & 2.7B & 3072 & 24 & \\(1e^{-3}\\) & 1.2T & 4096 \\\\ Aim-7B & 6.5B & 4096 & 32 & \\(1e^{-3}\\) & 1.2T & 4096 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: ** 모델 사양** 우리는 모든 Aim 변이체에 대한 임베딩 차원, 레이어 수 및 파라미터 카운트를 제공한다. 또한 사전 훈련 중 학습 속도와 배치 크기를 제공합니다. 1B 매개변수가 1B 이상인 AIM의 경우, 사전 훈련 과정은 사전 훈련 중에 볼 수 있는 1.2M 반복, 또는 5B 이미지에 해당하는 1.2M 반복을 포함한다.\n' +
      '\n' +
      '그림 3: ** 프리픽스 인과적 주의를 기울였다.** 우리는 일률적으로 프리픽스 길이 \\(S\\)를 샘플링한다. 첫 번째 \\(S\\) 패치에 대한 주의는 양방향으로 설정되고 손실은 이미지 내의 나머지 패치에 대해서만 계산된다. 하류 작업에 적응하는 동안 주의력 인과 마스크를 떨어뜨려 하류 성능을 향상시킬 수 있습니다.\n' +
      '\n' +
      '요구하세요. 결과적으로, 우리는 모든 모델 가중치가 하류 작업에 대해 고정된 시나리오에 초점을 맞추고 있다. 이러한 맥락에서 우리는 작은 하류 데이터 세트에 과적합 위험을 완화시키고 적응 비용을 크게 감소시키는 분류 헤드만을 훈련시킨다.\n' +
      '\n' +
      '대조적으로, 우리의 손실은 대조적 학습과 달리 각 패치마다 독립적으로 계산된다. 이는 우리의 사전 훈련이 글로벌 이미지 서술자의 개념을 통합하지 않는다는 것을 의미하며, 따라서 우리는 어떤 이미지 레벨 토큰도 가지고 있지 않다. 일부 방법은 패치 특징으로부터 글로벌 특징을 구축하기 위해 글로벌 평균 풀링에 의존하지만 MAE와 같은 다른 생성 접근법과 함께 선형 분류기 전에 배치된 주의 풀링 동작[50]에서 더 많은 이점을 얻는다는 것을 발견했다. 다른 작품[1, 74, 84]은 매개변수 또는 FLOP의 최소 증가로 성능을 향상시키기 위해 이 주의 풀링을 채택했다.\n' +
      '\n' +
      '구체적으로, 패치 특징 \\(P=\\{p_{i}\\mid 1\\leq i\\leq K\\}\\) 세트를 감안할 때 패치 특징에 대한 멀티 헤드 주의 풀이를 통해 글로벌 디스크립터 \\(\\hat{p}\\)를 계산한다.\n' +
      '\n' +
      '힌트{K}(q_{h}^{v}W_{h}^{k})}}.\n' +
      '\n' +
      '각 주의 머리(h=\\{1,...,H\\}\\), \\(W_{h}^{k},W_{h}^{k},W_{h}^{v}\\in R^{d_{h}\\ d}\\)는 각각 키 및 값 가중치 매트릭스에 해당하며,\\(q_{h}\\)은 학습 가능한 쿼리 벡터이다. 그리고 우리는 선형 분류기에 대한 입력 역할을 하는 \\(\\hat{p}=[p_{1},.....,p_{H}],\\hat{p}\\in R^{d}\\\\)로서 모집 기능을 얻는다. 기본적으로, 우리는 주요 모델 크기에 비해 무시할 수 있는 비용인 총 학습 가능 매개변수 \\(2d^{2}+d\\)를 만드는 머리(H=\\frac{d}{d_{h}}\\)의 수를 설정했다. 이러한 주의 풀링을 포함하면 전체 작동이 엄격하게 선형적이지 않기 때문에 우리는 그것을 "존경적 프로베"라고 한다. 그럼에도 불구하고, 낮은 추가 매개변수 수와 과적합 위험 감소와 같은 선형 프로빙의 장점은 이 프로브로 보존된다.\n' +
      '\n' +
      '## 5 Results\n' +
      '\n' +
      '스케일링에 영향을 미칩니다.\n' +
      '\n' +
      '우리는 매개변수 및 훈련 데이터 측면에서 접근 방식을 스케일링할 때 영향을 측정한다. 특히 벤치마크 전반에 걸쳐 사전 훈련 목표와 하류 성능 사이에 상관관계가 있는지 여부를 조사한다. 우리는 또한 스케일링이 손실 함수의 값에 미치는 영향을 살펴본다. 이 모든 실험에 대해 IN-1k의 검증 세트에 대한 손실 함수의 값을 보고한다.\n' +
      '\n' +
      '교육 중***Loss와 성능*** <그림 4>에서는 훈련 반복 횟수의 함수로서 사전 훈련 손실과 유효 집합에 대한 분류 정확도의 값을 모형별로 측정한다. 우리는 두 프로브가 전체 훈련 동안 그에 따라 개선된다는 것을 관찰하여 목표를 최적화하는 것이 더 나은 하류 성능을 가져온다는 것을 보여준다.\n' +
      '\n' +
      '매개변수***** 우리는 모델의 용량을 스케일링함에 따라 손실값과 하류 태스크의 정확도가 향상됨을 관찰한다. 이 관찰은 LLM에서 관찰된 경향과 일치하며 우리의 목적 함수의 최적화에 직접적인 영향을 미칠 수 있으며, 이는 차례로 더 강한 표상의 학습으로 이어진다.\n' +
      '\n' +
      '**** 그림 5에서 우리는 1M 이미지, _i.e_, IN-1k 또는 더 큰 2B 이미지, _i.e_ 세트의 작은 큐레이트 데이터 세트에 대해 미리 균주화함에 따라 검증 손실의 진행을 보여준다. DFN-2B+입니다. IN-1k에 대한 훈련이 동일한 분포에서 측정된 바와 같이 낮은 검증 손실로 빠르게 이어지는 것은 놀라운 일이 아니다. 다만, 이러한 손실은 훈련 종료 시 악화되어 학습 데이터에 과적합함을 나타낸다. 미결정된 DFN-2B 데이터셋에 대한 트레이닝 시, 모델은 더 높은 검증 손실에서 시작하지만 과적합 징후 없이 손실은 계속 감소한다. SS 3에 자세히 설명된 바와 같이 동일한 데이터셋이 소량의 IN-1k 데이터로 증강되면 추가 개선을 관찰한다.\n' +
      '\n' +
      '그림 4: **Aim 사전 학습 모델 크기**** 우리는 Aim의 용량이 증가함에 따라 사전 훈련 목표의 성능의 명확한 개선을 관찰한다. 더욱이, 하류 성능(IN-1k 탑-1)은 더 높은 용량 모델과 더 긴 사전 트레이닝을 위해 단조적으로 개선된다. 500k 반복에 대한 훈련 후에도 사전 훈련 동안 안정될 뚜렷한 징후를 관찰하지 못하며, 이는 Aim이 더 긴 사전 훈련 일정으로부터 이익을 얻을 수 있음을 나타낸다. 훈련 종료 시 손실 포화도는 학습률이 효과적으로 0인 코사인 붕괴 일정에 의해 발생한다.\n' +
      '\n' +
      '결국 IN-1k에 대한 사전 교육을 능가하는 공연. 우리는 결과 모델이 표 2에서 더 나은 하류 성능으로 이어짐을 확인한다.\n' +
      '\n' +
      '***Com 분쟁 최적 사전 훈련***은 DFN-2B+ 데이터셋을 사용하여 훈련할 때 과적합 징후를 관찰하지 못하기 때문에 사전 훈련 일정의 길이를 연장하는 영향을 조사하기 위해 진행한다. 그림 6에서 사전 훈련 일정의 길이를 사전 훈련 중에 보이는 500k에서 1.2M 반복, _i.e_, 2B에서 5B 이미지로 증가시키는 영향을 연구하였다. 우리는 더 긴 일정으로 사전 훈련된 모델이 상당히 낮은 검증 손실을 달성한다는 것을 관찰했다. 이는 모델 용량을 높이거나 더 긴 일정을 위한 사전 학습을 통해 Aim의 성능을 향상시킬 수 있음을 시사한다. 흥미로운 사실은 더 긴 일정으로 훈련된 저용량 모델이 유사한 양의 FLOP를 사용하면서 더 짧은 일정에 대해 훈련된 고용량 모델에 대해 유사한 검증 손실을 달성한다는 것을 발견했다. 이 발견은 호프만 등의 결과와 일치하며 Aim이 유사한 스케일링 법칙을 따를 수 있음을 의미한다. 그러나 우리는 향후 작업을 위해 이 측면에서 추가 조사를 유예한다.\n' +
      '\n' +
      '건축물 및 디자인\n' +
      '\n' +
      '이 절에서는 모델 및 훈련 목적에서 일부 변화의 영향을 조사한다. 이러한 연마물은 IN-1k 데이터셋에서 사전 학습 및 평가를 받은 Aim-0.6B 모델을 사용하여 수행된다. 이러한 남용의 결과는 표 3에 나와 있다.\n' +
      '\n' +
      '** 목표 및 목표(a.***)는 타겟 패치에 대한 다양한 잠재적 표현을 탐색한다. 한 가지 접근법은 원시 픽셀 값을 활용하고 평균 제곱 오차(MSE) 회귀 손실로 모델을 훈련시키는 것이다. H et al.[41]에 의해 제안된 두 번째 옵션은 동일한 MSE 손실로 원시 신호 대신 배치당 정규화된 픽셀 값을 사용하는 것을 포함한다. 마지막으로, 다른 옵션은 k-평균 또는 이산 VAE[67, 78]를 사용하여 패치의 폐기된 표현을 사용하는 것이다. 이 경우 언어 모델링과 유사한 교차-엔트로피 목적을 사용하여 모델을 학습한다. 우리의 실험은 정규화된 픽셀 값으로 MSE 목표를 사용할 때 Aim이 가장 잘 수행한다는 것을 보여준다.\n' +
      '\n' +
      '**오토 회귀 패턴(b.** 오토회귀 사전 훈련)은 일반적으로 다음 토큰의 예측을 용이하게 하기 위해 특정 추적 순서를 따른다. 언어의 경우, 텍스트가 순차적 방식으로 한 번에 한 마디(_e.g_, 영어에 대해 좌우)로 읽혀지면서 횡단 패턴이 명확하다. 그러나 이미지에 대해서는 횡단 패턴을 결정하는 것이 덜 명확하다. 우리는 래스터, 스피킹 아웃, 체크어보드 및 무작위로 미리 샘플링된 패턴을 포함한 다양한 결정론적 패턴을 탐구한다. 각 패턴의 자세한 예는 부록 B에서 발견되지만 우리의 모델은 각 패턴과 합리적으로 잘 수행되지만 래스터 패턴이 훨씬 더 높은 성능으로 이어진다는 것을 관찰했다.\n' +
      '\n' +
      '이 결과에 대한 더 깊은 통찰력을 얻기 위해 각 패턴에 대한 서열을 따라 패치를 예측하는 데 어려움을 조사한다. 이는 그림 7과 같이 서열을 따라 진행됨에 따라 패치당 손실 값을 측정하여 수행할 수 있으며, 우리의 관찰은 시퀀스가 전개됨에 따라 예측이 점진적으로 쉬워지는 패턴과 비교하여 패치 전체에 걸쳐 더 균일한 난이도의 분포를 나타내는 패턴이 우수한 모델을 초래한다는 것이다. 우리는 이것에 대해 모델에 대한 더 많은 정보를 유지하기 위해 모델을 힘쓰는 시퀀스 전체에 패치를 예측하는 어려움으로 귀속한다. 이것은 더 나은 패치 기능으로 이어지며 결과적으로 전체로서 더 나은 이미지 표현으로 이어진다.\n' +
      '\n' +
      '**Cropping 스케일(c.***) 우리는 크로핑 스케일의 하한을 조절하여 각 패치의 정보 콘텐츠의 영향을 탐구한다. 하나 위에.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} pre-training dataset & IN-1k & DFN-2B & DFN-2B+ \\\\ \\hline _attentive_ & 73.5 & 74.5 & **75.6** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **Dataset 다운스트림 성능(15 벤치마크**)의 거동은 DFN-2B와 IN-1k의 데이터 혼합물을 사용하여 최상의 성능을 초래한다는 것을 관찰한 하류 성능과 일치한다.\n' +
      '\n' +
      '그림 5: **Dataset의 사전 훈련 성과에 미치는 영향***, 한 손에서 IN-1k를 이용한 사전 훈련은 Aim-0.6B 모델에서도 과적합으로 이어진다. 한편, 미처리 DFN-2B 데이터셋을 이용한 사전 훈련은 과적합을 방지하지만 분배 이동으로 인해 유사한 지점으로 수렴한다. DFN-2B+에 대한 사전 훈련은 IN-1k 샘플의 존재가 작은 DFN-2B로 주로 구성된 데이터 혼합물이 최상의 성능을 초래한다.\n' +
      '\n' +
      '그림 6: FLOP에서 **Scaling.** 훈련 중 FLOP의 총 수는 최종 검증 손실과 상관관계가 있으며, 이는 호프만 및 [43]와 유사한 계산 구동 스케일링 법칙을 시사한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '훈련 안정성을 향상시킵니다. 이 관찰은 훈련 LLM에서 얻은 통찰력 중 일부가 다른 도메인에 유사하게 적용될 수 있다는 개념을 뒷받침한다.\n' +
      '\n' +
      '** 부착 _vs_. 선형 프로브** 모든 절제에 대해 선형 및 적응성 조사 결과를 보고한다. 우리는 모든 실험에 걸쳐 일관되게 적응적 풀링이 생성 사전 훈련의 주요 약점 중 하나인 이미지 수준 글로벌 기술자의 부재를 우회하는 지역 기능의 보다 미묘한 응집을 허용하기 때문에 성능에 상당한 증가를 제공한다는 것을 관찰했다.\n' +
      '\n' +
      'MLP****의 구조 MLP는 표 3e에서 절제된 중요한 역할을 한다. 표 4에서 MLP 헤드의 능력과 하류 성능에 어떤 영향을 미치는지 추가로 조사했다. 우리는 MLP 블록의 수나 폭을 변경하여 머리의 용량을 달리한다. 기본적으로 12개의 블록의 머리와 2048의 임베딩 차원을 사용하여 먼저 깊이 또는 폭을 통해 MLP의 용량을 증가시키는 것이 하류 성능의 일관된 개선으로 이어진다는 것을 관찰했다. 둘째, 고정된 폭을 갖는 MLP 블록의 수를 증가시키면 고정 깊이의 폭을 증가시키는 것에 비해 더 큰 개선이 발생한다는 것을 발견했다. 흥미롭게도 MLP 용량을 늘리는 것이 추가 개선을 얻지 못한 지점을 찾을 수 없었다. 우리는 불균형적인 머리와 몸통 용량을 가진 모델로 이어지기 때문에 표 4에 보고된 것을 넘어 더 높은 능력을 탐구하지 못했다.\n' +
      '\n' +
      '### Pre-training objective\n' +
      '\n' +
      '** 자기회귀 _vs_. 태스킹** 우리는 자기회귀 객관으로 훈련된 건축물과 언어를 위해 BERT[26]에 의해 대중화된 마스킹 대물렌즈와 비전을 위해 BEiT와 MAE에 의해 비교된다. 아임과 동일한 설정에서 마스킹 목적을 적용하여 Aim과 다른 접근법 간에 다른 다른 설계 선택과 사전 훈련 목표의 성능에 미치는 영향을 분리하는 점에 유의하는 것이 중요하다. 마스킹 베이스라인에서 마스크를 무작위로 샘플링하고 마스크 패치를 학습할 수 있는 마스크 토큰으로 교체합니다.\n' +
      '\n' +
      '표 5에서 우리는 Aim이 마스킹 목표보다 자기회귀 대물렌즈로 더 잘 수행됨을 보여준다. 이것은 Chen et al. [18]에 의해 보고된 결과와 일치하며, 우리의 개선이 자가회귀 목표의 활용에서 비롯된다는 추가 증거를 제공한다.\n' +
      '\n' +
      '다른 방법들과 비교합니다.\n' +
      '\n' +
      '표 6에서 우리는 부록 A에 자세히 설명된 15개의 다양한 벤치마크 세트에 걸쳐 Aim의 적응적 조사 성능을 다른 최첨단 방법과 비교한다.\n' +
      '\n' +
      '** 생성 방법** Aim은 생성 대응물에 비해 강한 성능을 제공합니다. Aim 아웃퍼포먼스는 BEiT[5]가 큰 폭으로 나타난다. 또한 Aim-0.6B는 동등한 용량을 갖는 MAE-H [41]에 비해 모든 벤치마크에서 평균화된 더 나은 성능을 제공한다. 더욱이 인스타그램에서 30억 개의 이미지의 개인 데이터세트인 IG-3B에서 사전 훈련된 MAE-2B [70] 모델과 비교된다. 우리는 Aim-3B와 Aim-7B가 모두 MAE-2B를 능가한다는 것을 발견하며, Aim-7B는 특히 큰 개선을 나타낸다. Aim과 유사하게 두 가지 다른 생성 접근법인 BEiT와 MAE가 적절한 탐구의 이점을 얻음으로써 생성 및 공동 임베딩 방법 간의 격차를 좁히는 점에 주목할 필요가 있다.\n' +
      '\n' +
      '**J 공동 임베딩 방법** Aim은 DINO[17], iBOT[88], DINOV2[58] 등의 공동 임베딩 방법으로 경쟁 성능을 제공한다. 용어 측면에서 보면\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l l l l l l l l l l} \\multicolumn{1}{l}{**Model**} & Arch. & Data & \\multicolumn{1}{c}{\\multirow{2}{*}{\\(\\uparrow\\)}} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\(\\uparrow\\)}} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\(\\uparrow\\)}} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\(\\uparrow\\)}} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\(\\uparrow\\)}} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\(\\uparrow\\)}} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\(\\uparrow\\)}} & \\multicolumn{1}{c}{\\multirow{2}{*}{Avg}} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\(\\uparrow\\)}} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\(\\uparrow\\)}} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\(\\uparrow\\)}} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\(\\uparrow\\)}} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\(\\uparrow\\)}} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\(\\uparrow\\)}} & \\multicolumn{1}{c}{\\multirow{2}{*}{Avg}} \\\\ \\hline DINO [17] & ViT-B/8 & IN-1k & 80.1 & 66.0 & 97.8 & 87.3 & 89.5 & 78.4 & 92.3 & 89.2 & 58.5 & 93.7 & 90.2 & 6.1 & 98.2 & 57.0 & 41.1 & 75.0 \\\\ iBOT [88] & ViT-L/16 & IN-21k & 83.5 & 70.5 & 99.2 & 93.3 & 93.5 & 81.6 & 92.8 & 90.8 & 61.8 & 94.5 & 90.0 & 5.9 & 98.0 & 60.3 & 47.7 & 77.6 \\\\ DINOv2 [58] & ViT-g/14\\({}_{516}\\) & LVD & 86.4 & 84.5 & 99.6 & 95.2 & 96.3 & 86.3 & 96.4 & 95.6 & 68.2 & 96.5 & 90.7 & 8.0 & 98.6 & 66.7 & 58.8 & 81.9 \\\\ \\hline BEiT [5] & ViT-L/14 & IN-21k & 62.2 & 44.4 & 94.4 & 78.7 & 79.0 & 64.0 & 80.9 & 69.5 & 52.0 & 92.8 & 88.2 & 4.2 & 97.5 & 47.7 & 25.9 & 65.4 \\\\ MAE [41, 70] & ViT-H/14 & IN-1k & 80.9 & 64.6 & 97.1 & 85.8 & 90.2 & 78.1 & 95.0 & 93.7 & 58.1 & 94.2 & 89.8 & 5.4 & 98.1 & 56.9 & 42.2 & 75.3 \\\\  & ViT-2B/14 & IG-3B & 82.2 & 70.8 & 97.5 & 87.3 & 93.4 & 81.2 & 95.1 & 94.9 & 57.8 & 94.4 & 90.3 & 7.3 & 98.2 & 60.1 & 50.2 & 77.4 \\\\ \\hline Aim-0.6B & ViT-H/14 & \\multirow{4}{*}{\\(\\uparrow\\)} & 78.5 & 64.0 & 97.2 & 86.8 & 90.1 & 80.1 & 93.0 & 93.0 & 57.9 & 94.3 & 90.0 & 7.8 & 98.4 & 58.3 & 45.2 & 75.6 \\\\ Aim-1B & ViT-1B/14 & \\multirow{4}{*}{\\(\\uparrow\\)} & 80.6 & 67.2 & 98.2 & 88.3 & 91.6 & 81.8 & 93.4 & 93.9 & 58.6 & 94.5 & 90.0 & 9.0 & 98.6 & 59.8 & 47.5 & 76.9 \\\\ Aim-3B & ViT-3B/14 & \\multirow{4}{*}{\\(\\uparrow\\)} & 82.2 & 69.7 & 98.4 & 89.9 & 92.7 & 81.9 & 94.1 & 93.8 & 58.8 & 94.3 & 90.4 & 9.7 & 98.5 & 60.9 & 48.9 & 77.6 \\\\ Aim-7B & ViT-7B/14 & \\multirow{4}{*}{\\(\\uparrow\\)} & 82.4 & 70.9 & 98.6 & 90.0 & 93.1 & 82.3 & 93.8 & 92.1 & 59.5 & 93.6 & 90.7 & 10.1 & 98.6 & 61.7 & 49.6 & 77.8 \\\\ \\hline Aim-7B\\(\\dagger\\) & ViT-7B/14 & DFN-2B+ & 84.0 & 75.5 & 98.9 & 91.8 & 94.1 & 85.6 & 95.4 & 95.0 & 61.4 & 94.2 & 90.5 & 8.4 & 98.5 & 63.5 & 57.7 & 79.6 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: ** 냉동 트렁크를 사용한 다운스트림 평가*** 다양한 15개의 이미지 인식 벤치마크 세트에 대해 평가하여 Aim 특징의 품질을 평가한다. 심 및 기준 방법은 냉동 트렁크와 함께 유리한 탐구를 사용하여 평가된다. 임 모델은 모든 벤치마크, 특히 Aim-7B에 걸쳐 강한 성능을 나타낸다. 예를 들어 이미지넷에서 일반적으로 1-1.5% 개선을 초래하는 고해상도 이미지를 사용하는 DINOV2를 제외하고 관절 조립 또는 생성 접근법을 사용하는 다른 모든 방법을 능가한다. (\\dagger\\): 마지막(32\\({}^{\\text{th}}\\) 대신 20\\({}^{\\text{th}}\\) 층에서 특징을 추출하면 보다 자세한 내용은 표 7과 같다.\n' +
      '\n' +
      '모든 벤치마크에 걸친 평균 정확도는 Aim이 DINO와 iBOT를 능가한다. 그러나 고해상도 입력으로 평가하여 결과를 달성하는 DINOv2에 빠졌다. 아임은 더 높은 용량 트렁크를 사용하여 이러한 경쟁 성과를 발휘한다. 그럼에도 불구하고, Aim의 사전 훈련은 상당히 간단하며 매개변수 및 데이터 측면에서 사소한 규모로 확장되어 일관된 개선을 얻을 수 있다. 반대로 DINOv2와 같은 최첨단 공동 임베딩 방법은 강한 성과를 달성하기 위해 다중작물 증강, 고리스로 정규화, 플레이어스카일, 스토코스틱 예금, 교사 운동량과 체중 붕괴 일정, 고해상도 미세 조정과 같은 많은 트릭에 크게 의존한다.\n' +
      '\n' +
      '*** 더 강한 특징을 추출*** 마지막 층의 특징에 비해 더 낮은 층에서 고품질 기능을 추출할 수 있음을 관찰합니다. 이는 차별적 하류 작업과 본질적으로 다른 사전 훈련 목표의 생성적 특성 때문에 의미 내용이 가장 높은 특징이 반드시 마지막 층 주위에 집중되는 것은 아니다. 표 7에서 우리는 성능이 가장 높은 계층에 비해 마지막 층에서 추출한 특징에 대한 IN-1k 탑-1 정확도를 보고한다. 이 현상에 대한 보다 자세한 분석은 부록 D에 제공된다.\n' +
      '\n' +
      '### Low-Rank Adaptation\n' +
      '\n' +
      '냉동 정크 평가 외에도 대중적이고 효율적인 미세 조정 방식인 저탄소 적응(LoRA) [44]에 대해 살펴본다. 표 8에서 Aim의 LoRA 융합 결과를 보고한다. 우리는 LoRA가 Aim과 호환되어 냉동 실크 평가에 비해 성능이 크게 증가함을 관찰할 수 있다. 예를 들어, Aim-7B는 3.9%(마지막 층의 성능 대비) 개선하면서 트렁크 파라미터의 0.1%에 불과하다.\n' +
      '\n' +
      '## 6 Discussion\n' +
      '\n' +
      '본 논문에서는 감독 없이 규모별로 비전 모형을 사전 학습하기 위한 간단하고 확장 가능한 방법을 제시하였다. 사전 훈련 중 생성 자기회귀 목표를 사용하고 하류 전달에 더 잘 적응하기 위해 몇 가지 기술적 기여를 제안했다. 결과적으로, 우리는 우리의 Autoregressive Image Models에 대한 많은 바람직한 특성을 관찰했다. 먼저, 우리의 모델의 용량은 안정 유도 기술 또는 각 모델 척도에 대한 하이퍼모수 광범위한 조정에 의존하지 않고 바닐라 변압기 구현을 사용하여 70억 개의 매개변수로 쉽게 확장될 수 있다. 둘째, Aim의 사전 훈련 작업에 대한 성과는 하류의 성능과 강한 상관관계를 가지고 있다. 셋째, Aim은 15개의 인식 벤치마크에 걸쳐 강한 성능을 달성하고 MAE와 같은 최첨단 방법을 능가하고 생성 및 공동 임베딩 사전 훈련 접근법의 격차를 크게 감소시킨다. 마지막으로 매개변수 또는 데이터 측면에서 스케일링하기 때문에 명확한 포화 징후를 관찰하지 않았으며, 이는 더 긴 일정에 대해 훈련된 더 큰 모델을 사용하여 추가 성능 개선 가능성이 있음을 시사한다. Aim이 대상 중심 이미지에 대한 편향이나 캡션에 대한 강력한 의존 없이 _효과적으로 통합되지 않은 데이터세트_를 레버리지하는 확장 가능한 비전 모델에서 향후 연구의 종자 역할을 하기를 바랍니다.\n' +
      '\n' +
      '제한 Aim은 심리스 확장성과 대용량 미확인 이미지 데이터의 효과적인 활용에 탁월합니다. 그러나 대체 방법은 서로 다른 트레이딩을 제공할 수 있습니다. MAE[41]은 높은 표본 효율을 제공하고 소량의 사전 학습 데이터를 사용하여 좋은 표현을 학습할 수 있어 접근법과 대조적으로 과적합(30])의 위험을 줄일 수 있다. 비교 방법[17, 58, 88]은 현재 MAE 및 Aim과 같은 생성 접근법에 비해 주어진 모델 크기에 대한 더 강력한 표현을 초래하지만 목표의 복잡성으로 인해 확장성과 손실 가능성 측면에서 상당한 문제를 제기한다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '저자는 프로젝트 전반에 걸쳐 귀중한 피드백을 위해 브랜든 맥킨지, 사미라 아스페타르, 프레텀 나키란, 지아타오 구에게 감사드린다. 프로젝트 초기 단계에서 영감을 주는 토론에 에두아르 그레이브와 히즈 제구에게 감사드린다. 마코컷우리, 제임스 토렌튼, 피에르 아블린, 유진 나디예에게 지원 및 프로젝트 전반에 걸쳐 많은 유익한 논의에 감사드립니다. 마지막으로 애플 기계학습연구팀 전체가 인트라와 데이터에 대한 많은 도움과 도움을 주셔서 감사드린다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c}  & Aim-0.6B & Aim-1B & Aim-3B & Aim-7B \\\\ \\hline _last layer_ & 78.5 & 80.6 & 82.2 & 82.4 \\\\ _best layer_ & **79.4** & **82.3** & **83.3** & **84.0** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: **Feature 추출*** Aim 사전 훈련 후 최고 품질 기능은 보통 마지막보다 더 낮은 층에 존재한다. 이전 층에서 특징들을 추출하면 IN-1k에 대한 인식 성능으로 도달할 수 없는 부스트가 된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c}  & Aim-0.6B & Aim-1B & Aim-3B & Aim-7B \\\\ \\hline _attentive_ & 78.5 & 80.6 & 82.2 & 82.4 \\\\ _LoRA (rank=8)_ & 81.0 & 83.6 & 85.5 & 86.3 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: ** 낮은 순위 적응(IN-1k**Aim)은 냉동 정크 평가에 비해 큰 이득을 보이는 LoRA와 호환된다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Anonymous. V-JPEA: Latent video prediction for visual representation learning. In _Submitted to The Twelfth International Conference on Learning Representations_, 2023.\n' +
      '* [2] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive architecture. _arXiv preprint arXiv:2301.08243_, 2023.\n' +
      '* [3] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei A Efros. Sequential modeling enables scalable learning for large vision models. _arXiv preprint arXiv:2312.00785_, 2023.\n' +
      '* [4] Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. _IEEE Transactions on Medical Imaging_, 2018.\n' +
      '* [5] Hangbo Bao, Li Dong, and Furu Wei. BEiT: Bert pre-training of image transformers. In _ICLR_, 2022.\n' +
      '* [6] Adrien Bardes, Jean Ponce, and Yann LeCun. Vic-creg: Variance-invariance-covariance regularization for self-supervised learning. In _ICLR_, 2022.\n' +
      '* [7] Miguel A Bautista, Artsiom Sanakoyeu, Ekaterina Tikhoncheva, and Bjorn Ommer. Cliquecnn: Deep unsupervised exemplar learning. _Advances in Neural Information Processing Systems_, 29, 2016.\n' +
      '* [8] Sara Beery, Elijah Cole, and Arvi Gjoka. The iwildcam 2020 competition dataset. _arXiv preprint arXiv:2004.10340_, 2020.\n' +
      '* [9] Yoshua Bengio, Rejean Ducharme, and Pascal Vincent. A neural probabilistic language model. _Advances in neural information processing systems_, 13, 2000.\n' +
      '* [10] Piotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. In _International Conference on Machine Learning_, pages 517-526. PMLR, 2017.\n' +
      '* 랜덤 숲으로 판별 성분을 채굴합니다. 2014년 _ECCV_에서.\n' +
      '* [12] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. _arXiv preprint arXiv:1809.11096_, 2018.\n' +
      '* [13] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _preprint arXiv:2005.14165_, 2020.\n' +
      '* [14] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In _ECCV_, 2018.\n' +
      '* [15] Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of image features on non-curated data. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2959-2968, 2019.\n' +
      '* [16] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In _NeurIPS_, 2020.\n' +
      '* [17] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _ICCV_, 2021.\n' +
      '* [18] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pre-training from pixels. In _ICML_, 2020.\n' +
      '* [19] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _ICML_, 2020.\n' +
      '* [20] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In _ICCV_, 2021.\n' +
      '* [21] Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2018.\n' +
      '* [22] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In _CVPR_, 2014.\n' +
      '* [23] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 113-123, 2019.\n' +
      '* [24] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In _ICML_. PMLR, 2023.\n' +
      '* [25] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.\n' +
      '* [26] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _NAACL_, 2018.\n' +
      '* [27] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In _ICCV_, 2015.\n' +
      '* [28] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. _Advances in neural information processing systems_, 27, 2014.\n' +
      '* [29] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.\n' +
      '\n' +
      '* [30] Alaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herve Jegou, and Edouard Grave. Are large-scale datasets necessary for self-supervised pre-training? _arXiv preprint arXiv:2112.10740_, 2021.\n' +
      '* [31] Jeffrey L Elman. Finding structure in time. _Cognitive science_, 14(2):179-211, 1990.\n' +
      '* [32] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. _arXiv preprint arXiv:2309.17425_, 2023.\n' +
      '* [33] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. _arXiv preprint arXiv:2304.14108_, 2023.\n' +
      '* [34] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. _arXiv preprint arXiv:1803.07728_, 2018.\n' +
      '* [35] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. _Advances in neural information processing systems_, 27, 2014.\n' +
      '* [36] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised visual representation learning. In _ICCV_, 2019.\n' +
      '* [37] Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Mannat Singh, Ishan Misra, Levent Sagun, Armand Joulin, and Piotr Bojanowski. Vision models are more robust and fair when pretrained on uncurated images without supervision. _arXiv preprint arXiv:2202.08360_, 2022.\n' +
      '* [38] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _NeurIPS_, 2020.\n' +
      '* [39] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In _ICCV_, 2017.\n' +
      '* [40] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _CVPR_, 2020.\n' +
      '* [41] Kaiming He, Xinelei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _CVPR_, 2022.\n' +
      '* [42] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification, 2017.\n' +
      '* [43] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.\n' +
      '* [44] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [45] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In _ECCV_, 2016.\n' +
      '* [46] iNaturalist 2018 competition dataset. iNaturalist 2018 competition dataset. [https://github.com/visipedia/inat_comp/tree/master/2018](https://github.com/visipedia/inat_comp/tree/master/2018), 2018.\n' +
      '* [47] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In _4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13)_, Sydney, Australia, 2013.\n' +
      '* [48] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n' +
      '* [49] Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In _Proceedings of the fourteenth international conference on artificial intelligence and statistics_, pages 29-37. JMLR Workshop and Conference Proceedings, 2011.\n' +
      '* [50] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In _ICML_, 2019.\n' +
      '* [51] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In _ICLR_, 2017.\n' +
      '* [52] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* [53] Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent neural network based language model. In _Interspeech_, 2010.\n' +
      '* [54] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In _CVPR_, 2020.\n' +
      '* [55] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In _ECCV_, 2016.\n' +
      '* [56] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. _arXiv preprint arXiv:1609.03499_, 2016.\n' +
      '* [57] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. In _NeurIPS_, 2018.\n' +
      '* [58] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Poo-Yao Huang, Hu Xu, Vasu Sharma, Shanghai Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023.\n' +
      '* [59] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _NeurIPS_, 2022.\n' +
      '\n' +
      '* [60] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar. Cats and dogs. In _CVPR_, 2012.\n' +
      '* [61] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In _ICML_, 2018.\n' +
      '* [62] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In _CVPR_, 2016.\n' +
      '* [63] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In _ICCV_, 2019.\n' +
      '* [64] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 2019.\n' +
      '* [65] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.\n' +
      '* [66] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 2020.\n' +
      '* [67] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR, 2021.\n' +
      '* [68] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. _arXiv preprint arXiv:1701.05517_, 2017.\n' +
      '* [69] Claude E Shannon. Prediction and entropy of printed english. _Bell system technical journal_, 30(1):50-64, 1951.\n' +
      '* [70] Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron Adcock, Armand Joulin, Piotr Dollar, Christoph Feichtenhofer, Ross Girshick, et al. The effectiveness of mae pre-pretraining for billion-scale pretraining. _arXiv preprint arXiv:2303.13496_, 2023.\n' +
      '* [71] J. Taylor, B. Earnshaw, B. Mabey, M. Victors, and J. Yosinski. Rxrx1: An image set for cellular morphological variation across many experimental batches. In _ICLR_, 2019.\n' +
      '* [72] Yonglong Tian, Olivier J Henaff, and Aaron van den Oord. Divide and contrast: Self-supervised learning from uncurated data. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 10063-10074, 2021.\n' +
      '* [73] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _ICML_, 2021.\n' +
      '* [74] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herve Jegou. Going deeper with image transformers. _arXiv preprint arXiv:2103.17239_, 2021.\n' +
      '* [75] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [76] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. _Advances in neural information processing systems_, 29, 2016.\n' +
      '* [77] Aaron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In _International conference on machine learning_, pages 1747-1756. PMLR, 2016.\n' +
      '* [78] Aaron Van Den Oord, Oriol Vinyals, et al. Neurips. _Advances in neural information processing systems_, 2017.\n' +
      '* [79] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, 2017.\n' +
      '* [80] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology. In _Medical Image Computing and Computer Assisted Intervention-MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part II 11_, pages 210-218. Springer, 2018.\n' +
      '* [81] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and Leon Bottou. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. _Journal of machine learning research_, 11(12), 2010.\n' +
      '* [82] Chen Wei, Kartikkeya Mangalam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang Xie, Alan Yuille, and Christoph Feichtenhofer. Diffusion models as masked autoencoders. _arXiv preprint arXiv:2304.03283_, 2023.\n' +
      '* [83] Xueting Yan, Ishan Misra, Abhinav Gupta, Deepti Ghadiyaram, and Dhruv Mahajan. ClusterFit: Improving Generalization of Visual Representations. In _CVPR_, 2020.\n' +
      '* [84] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. _TMLR_, 2022.\n' +
      '* [85] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In _ICML_, 2021.\n' +
      '* [86] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In _ICLR_, 2018.\n' +
      '* [87] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In _ECCV_, 2016.\n' +
      '* [88] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. In _ICLR_, 2022.\n' +
      '\n' +
      'A Datasets\n' +
      '\n' +
      'Aim에 의한 학습된 표상의 효과와 일반적인 적용 가능성을 평가하기 위해 표 6의 다양한 15개의 벤치마크 수집에 대한 인식 정확도를 측정하는데, 각 벤치마크들의 구체화는 표 9에서 찾을 수 있으며, 이 벤치마크에는 미세한 인식, 의료 영상, 위성 영상, 자연 환경의 이미지 및 인포그래픽 이미지와 같은 작업에 대한 데이터셋이 포함된다.\n' +
      '\n' +
      '부록 B 자동 억제 패턴.\n' +
      '\n' +
      '표 3b의 사전 훈련 동안 이미지를 추적하기 위해 사용할 수 있는 다양한 패턴을 조사한다. 이 조사에 사용된 모든 패턴은 그림 8에 나와 있다.\n' +
      '\n' +
      '부록 C.\n' +
      '\n' +
      '라스터 패턴은 손실을 검증합니다.\n' +
      '\n' +
      '그림 7에서 우리는 청크 전체에 걸쳐 래스터 패턴의 검증 손실이 다시 증가하기 전에 두 번째 청크에 대해 놀랍게도 감소했음을 발견했다. 우리는 그림 9에서 이것을 추가로 조사하고 이 행동이 IN-1k 검증 세트를 사용하는 부작용임을 관찰했다. 특히, 우리는 첫 번째 행과 별도로, 일반적으로 상실이 일반 이미지 또는 그 수직 플립된 대응물에 대해 계산되는지 여부에 관계없이 더 낮은 손실을 갖는다는 것을 관찰했다.\n' +
      '\n' +
      '다운스트림 성능은 층 전체에 걸쳐 있습니다.\n' +
      '\n' +
      '표 7과 표 6에서 우리는 마지막보다는 모델에서 고층들을 조사함으로써 달성할 수 있는 하류 성능의 이득을 논의하였다. 이를 그림 10에서 보다 자세히 연구한다. 우리는 모든 Aim 변이체에 대해 모델 깊이로 가는 길의 대략 2/3 층에서 하류 전달과 관련하여 최고 품질의 특징을 추출한다는 것을 발견했다. 그러나 더 깊은 층의 성능이 가파른 감소를 경험하지 못하고 지속적인 강한 성과를 보이고 있다는 점에 유의할 필요가 있다.\n' +
      '\n' +
      '부록.\n' +
      '\n' +
      '모든 능력의**Aim 모델은 표 10에 보고된 동일한 하이퍼파라미터 세트를 사용하여 전처리되었으며, Aim-0.6 모델 그러나 Aim-0.6 모델은 동일한 세트의 하이퍼파라미터를 사용하여 전처리되었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r} Dataset & train & test & classes \\\\ \\hline Imagenet-1k [25] & 1,281,167 & 50,000 & 1000 \\\\ iNAT-18 [46] & 437,513 & 24,426 & 8142 \\\\ CIFAR-10 [48] & 50,000 & 10,000 & 10 \\\\ CIFAR-100 [48] & 50,000 & 10,000 & 100 \\\\ Food101 [11] & 75,750 & 25,250 & 101 \\\\ DTD [22] & 3,760 & 1,880 & 47 \\\\ Pets [60] & 3,680 & 3,669 & 37 \\\\ Cars [47] & 8,144 & 8,041 & 196 \\\\ iWildCam [8] & 129,809 & 14961 & 182 \\\\ Camelyon17 [4] & 302,436 & 34904 & 2 \\\\ PCAM [80] & 262,144 & 32768 & 2 \\\\ RxRx1 [71] & 40,612 & 9854 & 1139 \\\\ EuroSAT [42] & 16,200 & 5400 & 10 \\\\ fMoW [21] & 76,863 & 19915 & 62 \\\\ Infograph [63] & 36,023 & 15,582 & 345 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: **E 평가 벤치마크**. 참조, 기차 및 테스트 세트의 이미지 수 및 이 작업에 사용된 15개의 모든 인식 벤치마크 범주의 카테고리 수를 제공합니다.\n' +
      '\n' +
      '그림 8: ** 자동 회귀 패턴** 래스터, 나선형, 검문판 및 고정된 무작위 등 이 작업에서 연구된 다양한 자기 회귀 패턴을 보여준다.\n' +
      '\n' +
      '그림 9: ** 라스터 패턴** 패치 전체에 걸쳐 래스터 패턴을 사용하여 미리 훈련된 Aim-0.6B에 대한 16개의 패치(_i.e._ 행)의 청크당 IN-1k 검증 손실을 계산한다. 검증 세트의 수직 플립 이미지에 대해 동일한 손실을 측정한다. IN-1k 검증 세트의 경우 이미지 내 상단 행의 패치가 더 낮은 손실로 예측되기 쉽다는 것을 관찰하며, 이는 해당 영역의 배경 패치 농도 때문일 수 있다.\n' +
      '\n' +
      '500k 반복의 더 짧은 일정에 대해서만 훈련되었습니다. 우리는 모델의 용량을 스케일링하면서 불안정성을 관찰하지 않았으며, 따라서 최적화 하이퍼모수들의 추가 튜닝이 필요하지 않았다.\n' +
      '\n' +
      'Aim 및 바젤린에 대한*** 부착 프로빙** 다운스트림 평가는 주로 SS 4에서 설명한 대로 후속 조사를 통해 수행되었습니다. 우리는 표 11에서 모든 방법을 조사하는 데 사용된 하이퍼파라미터를 보고했으며, 다른 기저부와 공정한 비교를 위해 학습 속도에 대해 다른 값을 검색하고 [58]과 유사한 각 방법의 최상의 성능을 보고한다. Aim 및 기타 생성 기저부의 경우, 우리는 모델의 마지막 6개 레이어에 대한 특징을 평균하여 주목을 받는 머리에 먹여 성능이 약간 증가합니다. 설명자 치수는 평가장의 용량을 간접적으로 부풀리는 iGPT[18]와 유사한 특징을 연결하는 관행과 다른 그대로 유지된다는 점에 주목한다.\n' +
      '\n' +
      '*** 낮은 순위 적응*** LoRA 핀셋링의 경우 믹업 [86] (알파=0.8) 외에 표 11에 보고된 것과 동일한 하이퍼파라미터를 사용한다. 우리는 주목 블록의 파라미터에만 순위=8과 함께 LoRA 적응을 적용한다. 특히, 질의, 값 및 출력 투영에 대한 가중치 행렬을 포함한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l} config & value \\\\ \\hline Optimizer & AdamW [52] \\\\ Optimizer Momentum & \\(\\beta_{1}=0.9,\\beta_{2}=0.95\\) \\\\ Peak learning rate & \\(1e^{-3}\\) \\\\ Minimum Learning rate & 0.0 \\\\ Weight decay & 0.05 \\\\ Batch size & 4096 \\\\ Patch size & (14, 14) \\\\ Gradient clipping & 1.0 \\\\ Warmup iterations & 31,250 \\\\ Total iterations & 1,250,000 \\\\ Learning rate schedule & cosine decay [51] \\\\ Augmentations: & \\\\ RandomResizedCrop & \\\\ size & 224px \\\\ scale & [0.4, 1.0] \\\\ ratio & [0.75, 1.33] \\\\ interpolation & Bicubic \\\\ RandomHorizontalFlip & \\(p=0.5\\) \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: ** 사전 훈련 하이퍼파라미터** 다양한 용량의 모든 Aim 변이체는 위에서 설명한 동일한 하이퍼파라미터 세트를 사용하여 훈련되었다.\n' +
      '\n' +
      '그림 10: ** 다운스트림 성능 층 전체에 걸친 다운스트림 인식 작업으로의 이동 측면에서 최고 품질 특징은 마지막과 다른 층에서 추출될 수 있으며, 모델 깊이의 약 3분의 2에서 특징들을 추출하여 달성된 피크 성능이다. 더 짧은 층들은 여전히 강한 성능을 유지하고 있으며 급격한 감소는 관찰되지 않는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c} config & IN-1k & Others \\\\ \\hline Optimizer & AdamW [52] \\\\ Optimizer Momentum & \\(\\beta_{1}=0.9,\\beta_{2}=0.999\\) \\\\ Peak learning rate grid & [1, 3, 5, 10, 15, 20, 40] \\(\\times 1e^{-4}\\) \\\\ Minimum Learning rate & \\(1e^{-5}\\) \\\\ Weight decay & 0.1 \\\\ Batch size & 1024 & 512 \\\\ Gradient clipping & 3.0 \\\\ Warmup epochs & 5 & 0 \\\\ Epochs & 50 & 100 \\\\ Learning rate schedule & cosine decay [51] \\\\ Augmentations: & \\\\ RandomResizedCrop & \\\\ size & 224px \\\\ scale & [0.08, 1.0] \\\\ ratio & [0.75, 1.33] \\\\ interpolation & Bicubic \\\\ AutoAugment [23] & rand-m9-mstd0.5-incl \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 11: ** 부착 프로브 하이퍼파라미터** 우리는 기저부와 아임 프로빙에 사용되는 하이퍼파라미터를 자세히 설명한다. 모든 실험에 대해 다양한 학습률 값을 검색하고 Aim과 기저부 모두에 대해 가장 잘 보고한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>