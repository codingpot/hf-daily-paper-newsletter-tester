<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '이미지 세대를 위한 Better 평가 Metric: 제안 FID.\n' +
      '\n' +
      'Sadeep Jayasumana\n' +
      '\n' +
      'Srikumar Ramalingam\n' +
      '\n' +
      'Andreas Veit\n' +
      '\n' +
      'Daniel Glasner\n' +
      '\n' +
      'Ayan Chakrabarti\n' +
      '\n' +
      'Sanjiv Kumar\n' +
      '\n' +
      '구글 연구 뉴욕 뉴욕, 뉴욕, 뉴욕, 뉴욕, 뉴욕, 뉴욕, 뉴욕, 뉴욕, 뉴욕, 뉴욕.\n' +
      '\n' +
      '{sadeep, rsrikumar,veit,veit, dglasner,yhipakrab, sanjivk, 살균jivk}@google.com.com.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '많은 기계 학습 문제와 마찬가지로, 이미지 생성 방법의 진행은 좋은 평가 메트릭에 대해 부화한다. 가장 인기 있는 것 중 하나는 프로체트 인셉션 거리(FID)입니다. FID는 실제 이미지의 인셉션-v3 특징의 분포와 알고리즘에 의해 생성된 이미지의 분포 사이의 거리를 추정한다. 우리는 FID의 중요한 단점: 현대 텍스트 대 이미지 모델, 잘못된 정규성 가정 및 불량한 샘플 복잡성에 의해 생성된 풍부하고 다양한 콘텐츠에 대한 인셉트의 불량한 표현을 강조한다. 우리는 생성된 이미지에 대한 1차 품질 메트릭으로 FID의 사용에 대한 재평가를 요구한다. 우리는 FID가 인간 비준자와 모순된다는 것을 실증적으로 보여주며, 반복 텍스트 대 이미지 모델의 점진적인 개선을 반영하지 않으며 왜곡 수준을 포착하지 않으며 샘플 크기를 변경할 때 일관되지 않은 결과를 생성한다는 것을 보여준다. 또한 더 풍부한 CLIP 임베딩 및 가우시안 RBF 커널과의 최대 평균 불일치 거리를 기반으로 대체 새로운 메트릭인 CMMD를 제안한다. 이는 임베딩의 확률 분포에 대해 어떠한 가정도 하지 않고, 샘플 효율이 높은 편재적 추정기이다. 광범위한 실험과 분석을 통해 텍스트 대 이미지 모델에 대한 FID 기반 평가가 신뢰할 수 없으며 CMMD가 이미지 품질에 대한 보다 강력하고 신뢰할 수 있는 평가를 제공한다는 것을 보여준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '텍스트 대 이미지 모델은 이별 속도로 진행되고 있습니다. [21, 22, 23, 18, 28]과 같은 최근 모델은 텍스트 프롬프트에 충실한 사실적 이미지를 생성하는 데 매우 성공적이었다. 기계 학습의 많은 문제와 마찬가지로 신뢰할 수 있는 평가 메트릭은 진행 진행의 핵심이다. 불행히도 텍스트-이미지 모델, 프로체트 인셉션 거리(FID) [13]의 평가에서 사용되는 가장 인기 있는 메트릭이 일부 중요한 경우에 금 표준인 인간 비준자와 동의하지 않을 수 있으며, 따라서 이를 위해 불응한다는 것을 발견했다. 통계 테스트 및 경험적 평가를 통해 FID의 몇 가지 중요한 한계를 식별한다. 이러한 단점을 해결하기 위해 CLIP 임베딩 및 최대 Mean 차별적(MMD) 거리를 사용하는 CMMD의 대체 메트릭을 제안한다. 그림 1은 우리의 실험 중 하나로, FID가 이미지에 적용된 점진적인 왜곡을 반영하지 않는 반면 CMMD는 왜곡의 심각도에 따라 이미지 세트를 올바르게 순위화하는 섹션 6.2에서 논의되는 세부 사항을 보여준다.\n' +
      '\n' +
      '이미지 생성 모델을 평가하는 것은 독특한 도전 과제이다. 분류나 탐지와 같은 전통적인 비전 과제와 달리 텍스트 프롬프트에 대한 품질, 심미성, 충실성을 포함한 여러 차원의 성능을 평가해야 한다. 더욱이, 이는 인간의 인식에 의존하는 이해하기 어려운 개념들이다. 그 결과, 인간의 평가는 텍스트 대 이미지 연구의 금 기준으로 남아 있다. 인간의 평가는 규모가 잘 되지 않는 고가의 솔루션이기 때문에 연구자들은 자동화된 평가에 의존하는 경우가 많다. 구체적으로, 최근 작품은 이미지 품질과 신앙을 측정하기 위해 FID와 CLIP 거리를 사용하였다.\n' +
      '\n' +
      '그림 1: _비 행동 FID와 CMMD의 왜곡 아래. CMMD는 왜곡 수준에 따라 단조적으로 증가하여 왜곡이 증가함에 따라 화질 저하를 정확하게 식별한다. FID가 잘못되었습니다. 처음 몇 가지 왜곡 수준에 대해 개선(하향)하고, 이는 이러한 더 미묘한 왜곡이 적용될 때 품질이 향상됨을 시사한다. 세부 사항은_____ 세부 사항은 섹션 6.2입니다._ 세부 사항은 섹션 6.2입니다. 텍스트에 대한 감수성은 각각 프롬프트한다.\n' +
      '\n' +
      '이 작업에서 우리는 특히 이미지 품질의 척도로 FID를 사용하는 이 접근법에 대한 재평가를 요구한다. 다변량 정규 분포에서 나오는 바와 같이 이미지 세트의 인셉션 임베딩을 잘못 모델링하는 것과 샘플 크기([5]에서 언급된)를 변경할 때 일치하지 않는 결과와 같은 FID의 단점을 강조한다. 우리는 FID가 인간 비준자와 모순될 수 있으며 반복 텍스트 대 이미지 모델의 점진적인 개선을 반영하지 않으며 복잡한 이미지 왜곡을 포착하지 않는다는 것을 실증적으로 보여준다.\n' +
      '\n' +
      '제안된 메트릭은 CLIP 임베딩과 MMD 거리를 사용합니다. 약 100만 개의 이미지넷 이미지를 학습한 인셉션 임베딩과 달리 \\(1000\\) 클래스[25], CLIP는 해당 텍스트 설명[20]으로 4억 개의 이미지 상에서 학습되어 현대 이미지 생성 모델과 현대 텍스트 대 이미지 모델에 주어진 복잡한 텍스트 프롬프트에 훨씬 더 적합한 옵션이다.\n' +
      '\n' +
      'MMD는 Frechet 거리에 걸쳐 몇 가지 주목할 만한 이점을 제공하는 확률 분포 사이의 거리이다. 적절한 커널과 함께 사용할 때 MMD는 다변량 정규 분포를 가정하는 Frechet 거리와 달리 분포에 대한 가정을 하지 않는 메트릭이다. [5]에서 볼 수 있듯이 FID는 편향된 추정기이며, 여기서 편향은 평가되는 모델에 의존한다. 반면에 MMD는 편향되지 않은 추정기이며, 우리는 그것을 경험적으로 입증하기 때문에 Frechet 거리와 같은 샘플 크기에 대한 강력한 의존성을 나타내지 않는다. 마지막으로 간단한 병렬 실행을 인정합니다. 더 작은 샘플 크기 및 빠른 계산에서 추정할 수 있는 능력은 MMD를 빠르고 실용적인 애플리케이션에 유용하게 만든다. 두 가지 이미지 분포를 비교하기 위한 다양한 옵션을 표 1에 비교하며, 기존 FID 메트릭은 상좌 코너에 있으며 많은 불리한 속성을 가지고 있다. 제안된 메트릭인 CMMD는 오른쪽 아래 모서리에 있고 FID의 단점을 피합니다.\n' +
      '\n' +
      '우리는 아래 기여금을 요약합니다.\n' +
      '\n' +
      '* 우리는 현대 이미지 생성 및 텍스트 대 이미지 모델에 대한 평가 메트릭으로서 FID의 재평가를 요구한다. 우리는 그것이 반복 텍스트 대 이미지 모델의 점진적인 개선을 반영하지 않으며 명백한 이미지 왜곡을 포착하지 않는다는 일부 중요한 경우에 인간 비준자에게 동의하지 않는다는 것을 보여준다.\n' +
      '* 우리는 이미지 생성 모델의 평가 맥락에서 프로체트 거리와 인셉션 특징의 몇 가지 단점을 식별하고 분석한다.\n' +
      '* We는 MMD 거리를 갖는 CLIP 특징을 보다 신뢰할 수 있고 강력한 대안으로 사용하는 거리인 CMMD를 제안하며, FID의 주요 단점을 일부 완화시킨다는 것을 보여준다.\n' +
      '\n' +
      '2번으로 작업했습니다.\n' +
      '\n' +
      '로그 가능성 [9], 인셉션 스코어[1, 24], 케네넬 인셉션 거리[2, 27], 프리체트 인셉션 거리[13], 지각 경로 길이[14], 가우시안 파라젠 창[9], HYPE[29]를 포함한 다양한 메트릭을 사용하여 생성 이미지 품질을 평가했다.\n' +
      '\n' +
      'IS는 이미지넷에서 훈련된 인셉션-v3 모델[25]을 사용하여 생성된 이미지의 1000개의 클래스 확률을 레버리징하여 생성된 이미지의 다양성과 품질을 측정한다. IS는 원래의 실제 이미지를 필요로 하지 않지만, 실제 및 생성된 이미지의 분포 사이의 거리를 결정함으로써 KID와 FID를 계산한다. KID는 합리적인 2차 커널을 사용하여 제곱 MMD 거리를 사용한다. FID는 두 분포가 다변수 정규이라는 가정과 함께 바세르스타인-2 거리와 동일한 두 확률 분포 사이의 제곱된 Frechet 거리를 사용한다. FID와 KID는 모두 기본 인셉션 임베딩의 한계로 고통받고 있으며, 1000개의 클래스로 한정되는 100만 이미지에서만 훈련되었다. 직관적으로, 이것이 현대 생성 영상에서 볼 수 있는 풍부하고 복잡한 이미지 콘텐츠를 나타내는 능력을 제한할 수 있을 것으로 기대한다.\n' +
      '\n' +
      '이전 작업은 이미지 생성 [5, 19]에서 평가 메트릭의 신뢰할 수 없음을 지적했다. 총 등은 FID가 편향된 추정기이고 편향이 평가되는 모델에 의존한다는 것을 보여준다. 그들은 편견이 없는 추정기를 계산하기 위한 외삽 접근법을 제안한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l l|l}  & \\multicolumn{2}{c|}{Frechet distance} & \\multicolumn{1}{c}{MMD distance} \\\\ \\hline \\multirow{4}{*}{Inception embeddings} & \\(\\bigstar\\) & Weak image embeddings & \\(\\bigstar\\) & Weak image embeddings \\\\  & \\(\\bigstar\\) & Normality assumption & \\(\\bigstar\\) & Distribution-free \\\\  & \\(\\bigstar\\) & Sample inefficient & \\(\\bigstar\\) & Sample efficient \\\\  & \\(\\bigstar\\) & Biased estimator & \\(\\bigstar\\) & Unbiased estimator \\\\ \\hline \\multirow{4}{*}{CLIP embeddings} & \\(\\bigstar\\) & Rich image embeddings & \\(\\bigstar\\) & Rich image embeddings \\\\  & \\(\\bigstar\\) & Normality assumption & \\(\\bigstar\\) & Distribution-free \\\\ \\cline{1-1}  & \\(\\bigstar\\) & Sample inefficient & \\(\\bigstar\\) & Sample efficient \\\\ \\cline{1-1}  & \\(\\bigstar\\) & Biased estimator & \\(\\bigstar\\) & Unbiased estimator \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '두 가지 이미지 분포를 비교하기 위한 옵션의 비교는 표 1: _이다. 현재 텍스트 대 이미지 평가를 위한 사실상의 표준인 FID는 상위좌표 코너에 있다. 제안된 메트릭인 CMMD는 우하구석에 있으며 FID_ \\보다 바람직한 특성이 많다. (\\mathrm{FID}_{\\infty}\\) 파마(Parmar et al. [19]는 압축 및 리사이징과 같은 저수준의 이미지 처리 작업이 FID의 상당한 변화를 초래할 수 있음을 보여주며, 반계량적 리사이징 작업의 사용을 옹호한다. 이 연구에서 우리는 FID의 문제가 이전 작업에서 논의된 것을 넘어 잘 확장되고 \\(\\mathrm{FID}_{\\infty}\\) 및/또는 항계량적 재화가 이러한 문제를 해결하지 않는다는 것을 보여준다.\n' +
      '\n' +
      'FID 3은 FID에 대한 것이다.\n' +
      '\n' +
      '이 섹션에서는 FID의 몇 가지 주요 한계를 강조한다. 우리는 그 한계를 더 잘 이해하기 위해 메트릭에 대한 배경 논의에서 시작한다. Frechet 인셉션 거리(FID)는 \\(\\mathcal{I}\\)와 \\(\\mathcal{I}^{\\prime}\\)의 두 이미지 세트 간의 불일치를 측정하는 데 사용된다. 통상 하나의 이미지 세트는 실제(예를 들어, COCO 데이터세트로부터)이고, 다른 세트는 평가될 이미지 생성 모델을 사용하여 생성된다. FID를 계산하기 위해 이미지넷 분류 작업에 대해 학습된 인셉션-v3 모델을 사용하여 두 이미지 세트에 대해 인셉션-v31 임베딩들(25])을 먼저 추출한다. 그런 다음 \\(\\mathcal{I}\\)와 \\(\\mathcal{I}^{\\prime}\\) 사이의 FID는 이 두 세트의 인셉션 임베딩 사이의 Frechet 거리로 정의된다.\n' +
      '\n' +
      '주절 1: 논문을 통해 우리는 인셉션 및 인셉션-v3이라는 용어를 교환적으로 사용한다.\n' +
      '\n' +
      '프로체트 향수.\n' +
      '\n' +
      '1차 및 2차 순간을 유한한\\(\\mathbb{R}^{d}\\)보다 두 가지 확률 분포 \\(P\\) 및 \\(Q\\)의 경우, Frechet 거리는 [6, 17]로 정의된다.\n' +
      '\n' +
      '}\\mathbf{x}(P,Q)\\mathbf{x}\\mathbf{y}.\n' +
      '\n' +
      'HH(\\Gamma(P,Q)\\)가 P와 Q의 모든 결합 집합이다. 이것은 또한 \\(\\mathbb{R}^{d}\\)의 와서스타인-2 거리에 해당한다. 일반적으로 프로케트 거리에 대한 폐쇄형 솔루션을 얻는 것은 어렵다. 그러나 [6]의 저자는 형태의 다변량 정규 분포를 위한 폐쇄형 솔루션이 존재함을 보여주었다.\n' +
      '\n' +
      '>{Q}.\n' +
      '\n' +
      '\\(\\boldsymbol{\\mu}_{P},\\boldsymbol{\\mu}_{Q}\\)는 수단이고 \\(\\boldsymbol{\\Sigma}_{P},\\boldsymbol{\\Sigma}_{P},\\boldsymbol{\\Sigma}_{Q}\\)은 두 다변량 정규 분포 \\(P\\)와 \\(Q\\)의 공생의 수단이고 \\(Hap)이다. 이 단순화된 공식은 \\(P\\)와 \\(Q\\) 모두 다변량 정규 분포 [6]일 때만 엄격하게 유효하다.\n' +
      '\n' +
      'FID의 경우 두 개의 해당 샘플을 사용하여 인셉션 임베딩의 두 분포 사이의 Frechet 거리를 추정해야 한다. 이는 기망 임베딩의 높은 차원, \\(d=2048\\)로 인해 어렵다. 개념 임베딩이 정규 분포에서 도출된다고 가정하면 문제를 단순화하여 Eq를 사용할 수 있다. (I\\bolds{\\mu}_{P},\\boldsymbol{\\mu}_{Q}\\) 및 \\(\\boldsymbol{\\mu}_{Q}\\) 및 \\(\\boldsymymbol{\\Sigma}_{P}_{P},\\boldsymymbol{\\Sigma}_{P}_{P}_{Q})를 사용한 (2)는 두 샘플 \\(\\mathcal{I}<\\) 및 \\(\\boldsymymymbol{\\Sigma}_{Q}_{P}_{P}_{P}_{P}_{P}_{P}_{P}_{P}_{P}_{TP}_{P}_{P}_{P}_{P}_{P}_{P}:\\) 및 \\) 및 \\(\\) 및 \\(\\boldsymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymym 이 절차에는 두 가지 종류의 오류가 있습니다.\n' +
      '\n' +
      '1. 3.3절에서 알 수 있듯이 전형적인 이미지 세트에 대한 인셉션 임베딩은 정규 분포와는 거리가 멀다. 프로케트 거리를 계산할 때 부정확한 가정의 의미는 3.2절에서 논의된다.\n' +
      '2. 작은 샘플에서 추출된 \\(2048년 2048년)차원 공분산 매트릭스를 계산하는 것은 섹션 6.3에서 논의된 바와 같이 큰 오류를 초래할 수 있다.\n' +
      '\n' +
      '동생물의 수식.\n' +
      '\n' +
      '두 분포 사이의 Frechet 거리를 계산할 때 잘못된 정규성 가정을 만드는 것은 놀라운 결과를 초래할 수 있다. 우리는 출처에서 2D 동위원소 가우시안 분포를 기준 분포로 사용하고 아래에 설명된 대로 생성된 일련의 혼합 가우시안 분포 사이의 거리를 측정하여 이것을 설명한다. 결과는 표 2에 요약되어 있다.\n' +
      '\n' +
      '두 번째 분포의 시리즈를 생성하기 위해 각각 참조 가우시안와 동일한 평균과 공분산을 갖는 4개의 가우스 인디언의 혼합물에서 시작한다. 이 혼합물은 기준 분포와 동일한 분포를 가지기 때문에 0을 측정하는 합리적인 거리가 있을 것으로 기대한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline FD & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ \\(\\mathrm{FD}\\infty\\) & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ MMD & 0.0 & 0.5875 & 5.794 & 17.21 & 78.88 & 202.8 & 244.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '정규성 가정이 침해될 때 추정된 Fréchet 거리와 MMD의 행동 표 2: _Behavior. 좌측에서 우회전하여 최대 좌측 분포에서 확률 분포가 점점 더 변화하게 된다. 그러나 정규성 가정으로 계산된 가장 왼쪽 분포로의 Fréchet 거리는 잘못된 0으로 남아 있다. 반면에 MMD는 진보적 출발을 정확하게 포착할 수 있다.__MMD가 있다. 이 분포와 참조 분포 사이의 거리(표 2의 첫 번째 열). 그런 다음 두 번째 분포의 네 가지 구성 요소는 전체 평균과 공분산 고정(표 2의 첫 번째 행)을 유지하면서 서로 더 멀리 떨어져 있을 것이다. 이렇게 되면 두 번째 분포는 분명히 기준 분포에서 더 멀리 떨어져 있다. 그러나 정규성 가정으로 계산된 Frechet 거리(이는 _not_ 진정한 Frechet 거리, 쉽게 계산할 수 없는 것)는 잘못된 0으로 남아 있다. 이는 두 번째 분포가 시작 시에만 정상이므로 정규성 가정은 테이블의 첫 번째 열에 대해서만 합리적이기 때문에 발생한다. 그 후 두 번째 분포는 정상이 아니기 때문에 정규성 가정으로 계산된 Frechet 거리는 완전히 잘못된 결과를 제공한다. 표 2의 세 번째 행인 \\(\\mathrm{FID}_{\\infty}\\)에서 볼 수 있듯이 [5]에서 제안된 편향되지 않은 FID 버전도 정규성 가정에 의존하기 때문에 이러한 단점을 겪는다. 대조적으로, 제4절(표 2의 바닥 행)에 기술된 MMD 거리는 참조 분포로부터 제2 분포의 점진적인 이탈을 포착할 수 있다. 실험 설정에 대한 자세한 내용은 부록 B에 있다.\n' +
      '\n' +
      '### 생물적 소비량의 교정.\n' +
      '\n' +
      '프로체트 거리를 추정할 때, 각각의 이미지 세트(진짜 및 생성된)에 대한 인셉션 임베딩은 다변량 정규 분포로부터 나온다고 가정한다. 이 절에서는 이러한 가정이 잘못되었음을 보여준다. 3.2절에서 논의한 바와 같이 기저 분포에 대한 잘못된 정규성 가정을 만드는 것은 완전히 잘못된 결과를 초래할 수 있다.\n' +
      '\n' +
      '일반적인 이미지 세트에 대한 인셉션 임베딩이 단일 모드로 다변량 정규 분포를 갖지 않는다는 것은 놀라운 일이 아니다. 인셉션 임베딩들은 인셉션-v3 네트워크의 음영층에서 추출된 활성화들이다. 훈련 중 이러한 활성화는 _선형_ 분류기(인식-v3 네트워크의 마지막 완전 연결 층)를 사용하여 1000개 클래스 중 하나로 분류된다. 따라서 인셉션-v3 네트워크는 이미지넷 분류 작업에 대한 좋은 분류 결과를 얻기 때문에 인셉션 임베딩이 적어도 \\(1,000\\) 클러스터 또는 모드를 가질 것으로 예상할 수 있다. 이 경우 정상적으로 배포할 수 없습니다.\n' +
      '\n' +
      '그림 2는 텍스트 대 이미지 FID 벤치마크에서 설정된 기준(실시간) 이미지로 일반적으로 사용되는 COCO 30K 데이터셋의 인셉션 임베딩의 2차원 t-SNE [26] 시각화를 보여준다. 낮은 차원 시각화는 여러 모드를 갖는 것이 분명하므로 원래 2048차원 분포는 다변량 정규 분포에 가깝지 않다는 것도 분명하다.\n' +
      '\n' +
      '마지막으로, COCO 30K 데이터셋의 인셉션 임베딩 정규성을 테스트하기 위해 Mardia의 왜도 테스트, Mardia의 첨도 테스트, 헨제-Zirkler 테스트 등 세 가지 다른 널리 수용된 통계 테스트를 적용했다. 이들 모두 _강력한ly_는 인셉션 임베딩이 다변수 정규 분포에서 비롯된다는 가설을 반박하며, \\(p\\) 값은 사실상 0(정규성의 귀무가설을 거부하는 압도적인 신뢰를 나타낸다. 이러한 테스트의 세부 사항은 부록 A에서 찾을 수 있다.\n' +
      '\n' +
      '명확히 말해서, 우리는 CLIP 임베딩도 정상적으로 분배될 것으로 기대하지 않는다. 우리가 대상으로 하는 비정상적 인셉션 특징에 정규성 가정을 가지고 프로체트 거리를 적용하는 것은 FID의 것이다. 실제로 COCO 30K의 CLIP 임베딩은 또한 사실상 0 \\(p\\) 값으로 정규성 테스트를 실패하여 CLIP 임베딩에 대한 정규성을 가정하는 것이 타당하지 않음을 나타낸다.\n' +
      '\n' +
      'CMMD 4.\n' +
      '\n' +
      '이 섹션에서는 가우시안 RBF 커널과 함께 CLIP 임베딩 및 최대 Mean차별화(MMD) 거리를 사용하여 이미지 생성 모델을 평가하는 새로운 메트릭을 제안한다. CMMD(CLIP-MMD) 메트릭은 기준(실시간) 이미지 세트의 CLIP 임베딩과 생성된 이미지 세트 사이의 제곱 MMD 거리이다.\n' +
      '\n' +
      'CLIP 임베딩[20]은 우리가 공동 공간에서 학습함으로써 이미지와 텍스트 표현에 대해 생각하는 방식을 변화시켰다. CLIP는 복잡한 장면을 포함하는 4억 개의 이미지-텍스트 쌍을 사용하여 영상 인코더 및 텍스트 인코더를 공동으로 훈련시킨다. 대조적으로, 인츠-v3은 임에 대해 훈련된다.\n' +
      '\n' +
      '그림 2: _t-SNE 시각화는 COCO 30K 데이터셋의 인셉션 임베딩이다. 환원차원 2D 표현에서도 임베딩이 여러 모드를 갖고 다변량 정규 분포를 따르지 않는다는 것을 쉽게 식별할 수 있다.__\n' +
      '\n' +
      '1000개의 클래스로 제한되고 이미지당 하나의 두드러진 오브젝트만 있는 100만 개의 이미지 순서를 가지고 있는ageNet이다. 결과적으로 CLIP 임베딩은 현대 이미지 생성 알고리즘에 의해 생성된 이미지와 텍스트 대 이미지 모델에 주어진 거의 무한한 다양한 프롬프트에서 보는 다양하고 복잡한 콘텐츠를 나타내는 데 더 적합하다.\n' +
      '\n' +
      '두 분포 사이의 거리를 계산하기 위해 MMD 거리[10, 11]를 사용한다. MMD는 원래 두 샘플이 동일한 분포에서 나온지를 결정하기 위해 2표본 통계 시험의 일부로 개발되었다. 이 테스트에서 계산된 MMD 통계량은 또한 두 분포 사이의 불일치를 측정하는 데 사용될 수 있다. 두 가지 확률 분포 \\(P\\) 및 \\(Q\\)에 대해 \\(\\mathbb{R}^{d}\\)에 대한 MMD 거리는 양의 정적 커널 \\(k\\)에 대해 정의된다.\n' +
      '\n' +
      '>\\mathbf}}(\\mathbf}})\\math{f}(\\math{{x,\\math{{x}) +\\math{{f}.\n' +
      '\n' +
      'H\\(\\mathbf{x}\\) 및 \\(\\mathbf{x}^{\\f}^{\\ime}\\)는 \\(P\\) 및 \\(P\\)에 의해 독립적으로 분포되며, \\(\\mathbf{y}\\) 및 \\(\\mathbf{y}^{\\}^{\\)는 \\(Q\\)에 의해 독립적으로 분포한다. MMD는 특징적인 낟알 \\(k\\)[8, 11]에 대한 메트릭인 것으로 알려져 있다.\n' +
      '\n' +
      '\\bf}_{1},\\mathbf}_{2},\\mathbf}},\\mathbf}_{m}} 및\\(X=\\mathbf}_{math}})의 두 세트의\\(X=\\mathbf}_{n},\\mathbf}},\\mathbf}_{mathd\\bf} <\\bf}_{mathne}_{mathf})는 각각\\bf}_{mathbf}_{x},\\bf}_{mathd\\bf}_{mathbf}_{mathf}_{mathf}_{mraq,\\bf}_{mathf}_{math}_{math}_{f}_{math}_{mraq,\\) 및\\(P\\)에 대해 샘플링된 두 세트의\\bf},\\bf}},\\bf}_{math}_{f}},\\) 및\\(If}},\\\n' +
      '\n' +
      '}\\mathbf{i}}}<\\mathbf{i}}} <\\mf{n}}} <\\mf{i}}} <\\mf{n}}} <\\mf{m{i}}} <\\mf{m}}}} <\\mf{m{i}}}} <\\mf{i>}}} <\\mf{m{i}}} <\\mf{m{i}}}}} <\\mf{m{i}}}} <{m{m{m{m{m{m{m{i}}}}}}{m{m{m{m{m{i}}}}} <{m{m{m{m{m{m{m{i}}}}}}}}} <{m{m{m{m{m{m{m{i}}}}}}}}}}}}}}}} <{m{m{m{m{m}}}}}}}}}}}}}}}}}}\n' +
      '\n' +
      '프로체트 거리에 비해 MMD의 몇 가지 장점은 있다.\n' +
      '\n' +
      '1. MMD 메트릭은 특징적인 커널[8]과 함께 사용될 때 _분포가 없는_이다. 즉, 분포 \\(P\\)와 \\(Q\\)에 대해서는 어떠한 가정도 하지 않는다. 대조적으로, Eq의 Frechet 거리는. (2)는 정규성을 가정하고 이 가정이 침해될 때 잘못된 결과를 부여할 책임이 있다.\n' +
      '2. [5]에 나타난 바와 같이 유한 시료에서 추정된 FID는 평가되는 모델에 의존하는 편향을 가지므로 표본 크기가 평가되는 모델의 순위가 다를 수 있다. 이 편향을 제거하려면 다중 FID 추정치[5]의 계산을 포함하는 계산적으로 비싼 절차가 필요하다. 대조적으로, Eq의 MMD 추정기는. (4)는 _unbiased_이다.\n' +
      '3. 이미지 임베딩과 같은 고차원 벡터와 함께 작업할 때 MMD는 _샘플 효율_이다. 반면에 Frechet 거리는 \\(d\\tacill d\\) 공분산 행렬을 안정적으로 추정하기 위해 큰 샘플이 필요하다. 이것은 6.3절에서 가우시안 RBF 커널 \\(k(\\mathbf{x},\\mathbf{y})=\\|\\mathbf{x}-\\mathbf{y}\\|^{2}/\\sigma^{2})\\(\\sigma=10\\)으로 설정된 대역폭 매개변수와 함께 특징적인 커널인\\exp(\\|\\mathbf{x},\\mathbf{y}-\\mathbf{y}-\\mathbf{y}\\|^{2}/\\sigma^{2}/\\sigma^{2})\\)을 사용하여 MMD 계산에서 더 정교하게 사용할 것이다. 실증적으로 대역폭 파라미터가 메트릭의 전체 경향에 유의한 영향을 미치지 않는다는 것을 관찰했다. 그러나 메트릭에 대한 일관된 값을 얻기 위해 이를 \\(10\\)에 고정시킬 것을 제안한다. 가우시안 커널을 사용한 MMD 메트릭은 \\(2\\)에서 위에 결합되기 때문에(두 분포가 최대 상이할 때) 일반 분포에 대해 작은 값을 제공한다. 따라서 우리는 Eq의 값을 확장합니다. (4) x \\(1000\\)를 통해 보다 판독 가능한 값을 얻을 수 있다. CLIP 임베딩 모델의 경우 가장 크고 최고의 수행 CLIP 모델[20]인 공개적으로 사용할 수 있는 ViT-L/14@336px 모델을 사용한다. 또한 Eq에는 \\(m=n\\)가 있습니다. (4)는 동일한 캡션/프로플리션을 공유하는 실제 이미지에 대해 생성된 이미지를 평가하기 때문에 텍스트 대 이미지 평가를 위한 것이다. 컴퓨팅 CMMD에 대한 코드를 공개적으로 사용할 수 있습니다.\n' +
      '\n' +
      '5개의 인적 평가.\n' +
      '\n' +
      '우리는 이제 FID가 이미지 품질에 대한 인간의 인식에 동의하지 않는다는 것을 보여주기 위해 인간 평가를 제시한다. 이를 위해 24개의 염기 모델 반복과 8개의 초해상도 모델 반복이 있는 [3]에 설명된 대로 모델-A: 전체 Muse 모델 두 가지 모델을 선택했다. 모델-B: 20개의 염기 모델 반복과 3개의 초해상도 모델 반복만 있는 초기 재생 Muse 모델이다. 이것은 생산된 이미지의 품질을 줄이기 위해 의도적으로 수행되었다. 우리는 웹LI 데이터세트[4]에서 훈련된 Muse 모델을 사용하는데, Muse 저자가 아낌없이 이용할 수 있게 만들었다. 초기 재생 반복의 선택은 임의의데, 그림 4와 같이 FID는 전체 모델(모형-A)과 비교할 때 모든 초기 재생 모델에 대해 일관되게 더 나은(하중)이다.\n' +
      '\n' +
      '우리는 인간 쥐에게 모델-A에서 생성된 두 개의 이미지와 모델-B에서 생성된 다른 두 개의 이미지를 제시한 측면 평가를 수행했다. 우리는 프롬프트에 대한 이미지 함량과 정렬 정도가 동일하도록 동일한 랜덤 종자를 사용했다. 이를 통해 비준자들은 화질에 집중할 수 있었다. 어둠들은 어떤 이미지가 더 잘 보이는지를 평가하도록 요청받았다. 물들은 이미지 중 하나를 선택하거나 무관심하다는 옵션을 가지고 있었다. 모든 이미지 쌍은 3개의 독립 비준자에 의해 평가되었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r} \\hline \\hline \\multicolumn{1}{c}{Model} & Model-A & Model-B \\\\ \\hline FID & 21.40 & 18.42 \\\\ \\(\\mathrm{FID}_{\\infty}\\) & 20.16 & 17.19 \\\\ CMMD & 0.721 & 0.951 \\\\ Human rater preference & 92.5\\% & 6.9\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: _인간의 다양한 모델에 대한 평가는 다음과 같다. FID는 인간 평가와 모순되는 반면 CMMD는 일치하지만 FID는 인간 평가와 모순된다. 고품질의 크라우드 컴퓨팅 플랫폼을 통해 영감을 받았습니다. 청탁자는 이미지 세트의 세부 사항과 순수하게 시각적 품질을 기반으로 평가한 이미지에 민영화되지 않았다. 저자와 비준자는 서로 익명의 것이었다.\n' +
      '\n' +
      '우리는 텍스트 대 이미지 모델 평가를 위해 설계된 1633개의 프롬프트의 모음인 모든 파피 프로모션[28]을 사용했다. 이러한 프롬프트는 광범위한 범주(공간, 차량, 삽화, 미술, 세계 지식, 동물, 야외 장면 등)와 도전 수준(기본, 복잡한 미세 지능, 상상력 등)을 표 3에 포함하고 있으며, 각 비교 결과는 2명 이상의 비준자들이 해당 모델에 의해 생성된 이미지를 선호했다면 모델을 승자로 간주한다. 만약 비준자 간에 합의가 없거나 선정된 비준자 대부분이 무관심하다면 모델이 승리하지 않는다. 우리는 모델-A가 비교의 92.5%에서 선호되는 반면 모델-B는 시간의 6.9%만을 선호한다는 것을 관찰했다. 쥐는 시간의 무관심 0.6%였다. 따라서 인간 비준자가 모델-B에 대한 모델-A를 압도적으로 선호한다는 것은 분명하다. 그러나 COCO 30K FID와 그 편향되지 않은 변이체 \\(\\mathrm{FID}_{\\infty}\\)는 불행히도 그렇지 않다고 말한다. 반면에 제안된 CMMD 메트릭은 인간의 선호도와 올바르게 정렬된다.\n' +
      '\n' +
      '6개의 공연을 비교했습니다.\n' +
      '\n' +
      '우리는 이제 다양한 환경에서 제안된 CMMD 메트릭과 FID를 비교하여 CMMD의 이점을 부각하면서 FID의 한계를 지적한다. 모든 실험에서 COCO 30K 데이터 세트 [15]를 참조(실시간) 이미지 데이터 세트로 사용한다. 이 데이터셋에 대한 제로샷 평가는 현재 텍스트 대 이미지 생성 모델[3, 22, 23]에 대한 사실상의 평가 기준이다. 적용 가능한 실험 전반에 걸쳐 [19]에서 제안된 바와 같이 안티알레이싱과 함께 고품질 바이큐빅 리사이징을 사용한다. 이는 [19]에서 보고된 바와 같이 FID에 대한 부적절하게 구현된 저수준 이미지 처리 작업의 부작용을 방지한다.\n' +
      '\n' +
      '스테인리스 디퓨전 [22]의 경우 공개적으로 이용 가능한 Stable Diffusion 1.4 모델을 사용한다. 우리는 CLIP 분류와 같은 별도의 벨과 휘스티클 없이 모든 모델을 평가한다.\n' +
      '\n' +
      '진보 생업 모델.\n' +
      '\n' +
      '대부분의 현대 텍스트 대 이미지 생성 모델은 반복적이다. 예를 들어, 확산 모델[22, 23]은 최종 이미지를 생성하기 위해 다수의 변성 단계가 필요하며, 파피 모델[28] 오토-스트레스적으로 한 번에 이미지 토큰을 생성한다. Muse 모델[3]은 한 번에 여러 개의 토큰을 생성하지만, 그림 3과 같이 최종 이미지를 생성하기 위해서는 반복적 샘플링 단계가 여전히 필요하며, 이러한 방법은 각 단계에서 생성된 이미지의 품질을 향상시켰으며, 이는 불량한 품질 이미지 또는 순수한 노이즈 이미지로부터 전례 없는 사진-현실주의로 이동한다. 이러한 품질의 진행은 인간 관찰자에게 분명하며 이미지 생성의 반복을 통해 진행됨에 따라 합리적인 메트릭이 단조적으로 개선될 것으로 예상한다.\n' +
      '\n' +
      '그림 4는 FID, \\(\\mathrm{FID}_{\\infty}\\) 및 진행성 Muse 반복에 대한 CMMD 값을 보여준다. <그림 3>에 예시된 바와 같이 품질 개선이 명백한 경우, FID와 \\(\\mathrm{FID}_{\\infty}\\)는 화질이 저하되는 것을 잘못 시사한다.\n' +
      '\n' +
      '<그림 4> _Muse 단계에 대한 FID 및 CMMD의 행동. CMMD는 단조적으로 내려와 이미지에 대한 반복적 개선을 정확하게 식별한다(그림 3 참조). FID는 반복이 진행됨에 따라 화질 저하를 시사하는 것이 완전히 잘못되어 있다. 다만, FID는 반복이 진행됨에 따라 화질 저하를 시사하는 것은 완전히 잘못된다. (\\mathrm{FID}_{\\infty}\\)은 FID_와 동일한 행동을 가지고 있다.\n' +
      '\n' +
      '그림 3: _는 Muse의 정제 반복을 통해 진행됨에 따라 생성된 이미지의 품질이 단조적으로 향상된다. CMMD는 개선을 정확하게 식별합니다. 그러나 FID는 품질 저하를 잘못 나타낸다(그림 4 참조). "페테논".__ 페테논.\n' +
      '\n' +
      '대조적으로, CMMD는 Muse의 반복적 정제 동안 만들어진 품질 개선을 정확하게 식별한다. 그림 4에서 볼 수 있듯이, 우리는 FID와 \\(\\mathrm{FID}_{\\infty}\\)가 절대값이 다르지만 동일한 행동을 갖는다는 실험에서 일관되게 관찰한다. 이는 \\(\\mathrm{FID}_{\\infty}\\)가 FID에서 파생되고 많은 단점을 계승하기 때문에 놀라운 일이 아니다.\n' +
      '\n' +
      '그림 6은 100회기 안정화 Diffusion 모델의 마지막 5회 반복에 대한 평가를 보여준다. 우리의 제안된 CMMD 메트릭은 반복 진행과 함께 단조적으로 개선(감소)되는 반면, FID는 예상치 못한 행동을 가지고 있다. FID와 CMMD 모두 초기 반복에서 쉽게 감지할 수 있는 고소음 수준에서 단조성을 나타내기 때문에 Stable Diffusion의 최종 반복에서 보다 미묘한 차이에 초점을 맞추고 있다.\n' +
      '\n' +
      '### Image Distortions\n' +
      '\n' +
      '여기에서 우리는 FID가 복잡한 이미지 왜곡 하에서 화질을 정확하게 반영하지 않는다는 추가적인 증거를 제공한다. [13]에서 FID는 가우시안 노이즈, 가우시안 블러 등의 저수준 영상 처리 왜곡하에서 영상 왜곡을 정확하게 포착하는 것으로 나타났다. 인셉션 임베딩은 극단적인 데이터 증강 없이 이미지넷 이미지에서 트레이닝되기 때문에 FID가 이러한 왜곡을 식별할 수 있다는 것은 놀라운 일이 아니다. 그러나 이 절에서는 FID가 잠재 공간에 더 복잡한 소음을 식별할 수 없음을 보여준다.\n' +
      '\n' +
      '이를 위해 Muse에 의해 생성된 이미지 세트를 촬영하고 VQGAN 잠재 공간[7]에 노이즈를 추가하여 점진적으로 왜곡한다. 각 이미지에 대해 VQGAN 토큰을 획득하고, 확률 \\(p\\)로 랜덤 토큰으로 교체하고, 영상을 VQGAN 탈착기로 재구성한다. 예의 왜곡은 그림 5와 같으며, 그 이미지는 \\(p\\)가 증가함에 따라 점점 더 왜곡되고 \\(p\\)가 증가함에 따라 품질 손실이 눈에 띄게 명백하다. 그러나 그림 7과 같이 FID는 \\(p\\)의 값을 높이기 위한 화질 저하를 반영하지 못한다. 반면에, 우리의 CMMD 메트릭은 왜곡 수준 \\(p\\)으로 단조적으로 악화(증가)되어 품질 회귀를 정확하게 식별한다. 그림 1은 COCO 30K 데이터셋의 점진적으로 왜곡된 버전(동일한 절차를 사용)과 해당 데이터셋의 참조 클린 버전 사이의 거리를 측정할 때도 FID가 제대로 행동하지 않음을 보여준다.\n' +
      '\n' +
      '### Sample Efficiency\n' +
      '\n' +
      '제4절에서 언급한 바와 같이, FID를 계산하는 것은 \\(4\\) 백만 개의 엔트리들을 갖는 \\(2048\\시 2048\\) 공분산 행렬을 추정해야 한다. 이는 FID가 불량하게 하는 많은 수의 이미지를 필요로 한다.\n' +
      '\n' +
      '그림 5: 왜곡하에 FID 및 CMMD의 행동. 첫 번째 행(FID: 21.40, CMMD: 0.721)의 이미지는 일치하지 않는다. 두 번째(FID: 18.02, CMMD: 1.190)의 이미지는 각 VQGAN 토큰을 확률 \\(p=0.2\\)로 무작위로 대체함으로써 왜곡된다. 이미지 품질은 왜곡의 결과로 명확하게 분해되지만 FID는 그렇지 않으면 암시하는 반면 CMMD는 분해를 정확하게 식별한다.\n' +
      '\n' +
      '그림 6: StableD확산 단계에 대한 FID 및 CMMD의 행동이다. CMMD는 이미지의 개선을 반영하여 단조적으로 개선(목표 다운)한다. FID의 행동은 일관적이지 않으며, 이는 실수로 마지막 두 반복에서 품질의 감소를 시사한다.\n' +
      '\n' +
      '샘플 효율입니다. 이것은 [5]의 저자들에서도 언급되었다. 제안된 CMMD 메트릭은 Frechet 거리 대신 MMD 거리를 사용하는 덕분에 이 문제로 고통받지 않는다.\n' +
      '\n' +
      '그림 8에서 FID의 신뢰할 수 있는 추정은 많은 수의 이미지를 생성함으로써 FID 평가가 비용이 많이 들고 두 가지 이유로 FID 평가보다 빠르게 평가될 수 있으며, 대조적으로, FID 평가는 COCO 30K 데이터세트로부터 무작위로 샘플링된 다양한 수의 CMMD가 FID를 확실하게 추정하는 데 필요한 반면, CMMD는 작은 이미지 세트에서도 일관된 추정치를 제공하는 반면, CMMD는 빠른 온라인 평가, 예를 들어 빠른 온라인 평가가 필요하다. 소수의 이미지만 생성되어야 합니다. 2) 이미지를 생성하면 다음 섹션에서 논의된 바와 같이 CMMD의 계산이 FID 계산보다 빠르다.\n' +
      '\n' +
      '### Computational Cost\n' +
      '\n' +
      '(n\\)는 이미지의 개수이고, \\(d\\)는 임베딩 길이이다. 프로케트 거리(FD)를 컴퓨팅하는 비용은 비쌀 뿐만 아니라 쉽게 평행하지 않는 \\(d\\tacill d\\) 매트릭스에 대한 매트릭스 제곱근 조작에 의해 지배된다. 편향되지 않은 버전 FD\\({}_{\\infty}\\)를 컴퓨팅하는 비용은 샘플 크기가 다른 여러 번 컴퓨팅 FD가 필요하기 때문에 훨씬 더 높다. 컴퓨팅 MMD의 점근 복잡성은 \\(O(n^{2}d)\\이다. 그러나 실제로 MMD는 Tensorflow, PyTorch 및 JAX와 같은 모든 딥러닝 라이브러리에 3차적으로 평행하고 고도로 최적화된 매트릭스 다중화만을 포함하기 때문에 매우 효율적으로 계산할 수 있다.\n' +
      '\n' +
      '표 4는 JAX 구현과 함께 TPUv4 플랫폼에서 \\(d=2048\\) 차원 특징을 갖는 크기 \\(n=30,000\\) 세트에 대한 컴퓨팅 FD와 MMD의 경험적 런타임 비교를 보여준다. FD 계산을 위해 우리는 JAX 구현 및 [19] 및 [5]로부터의 공개적으로 이용 가능한 PyTorch/numpy 구현들을 사용하고 최고의 런타임을 보고한다. 같은 표에서 우리는 또한 32개의 이미지 배치에 대한 인셉션 및 CLIP 특징 추출을 위한 런타임을 보고한다.\n' +
      '\n' +
      '## 7 Discussion\n' +
      '\n' +
      '이미지 생성 연구자들이 이미지 품질에 대한 1차 평가 메트릭으로 FID 사용을 재조명하도록 권장한다. FID가 인간 비준자와 잘 상관되지 않는다는 우리의 연구 결과는 반복 텍스트 대 이미지 모델의 점진적인 개선을 반영하지 않으며 명백한 왜곡을 포착하지 못한다는 점과 지적의 증가하는 몸을 추가하지 않는다[5, 19]. 우리는 FID에 대한 의존도가 이미지 생성 방법 중 결함이 있는 순위가 될 수 있으며 좋은 아이디어가 조기 거부될 수 있다고 우려하고 있다. 이러한 우려를 해결하기 위해 우리는 현대 텍스트 대 이미지 모델의 평가에 적합한 보다 강력한 메트릭으로 CMMD를 제안한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Operation & Time \\\\ \\hline Fréchet distance & 7007.59 \\(\\pm\\) 231 ms \\\\ MMD distance & 71.42 \\(\\pm\\) 0.67 ms \\\\ Inception model inference & 2.076 \\(\\pm\\) 0.15 ms \\\\ CLIP model inference & 1.955 \\(\\pm\\) 0.14 ms \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: _는 Fréchet/MMD 거리 및 Inception/CLIP 특징 추출을 컴퓨팅하기 위한 런타임과 비교한다._\n' +
      '\n' +
      '그림 8: _행동 FID 및 CMMD는 다른 샘플 크기 하에서 행동한다. 표: 메트릭의 절대값. Bottom: \\(30k\\) 샘플 크기___\\(30k\\) 샘플 크기에서의 값에 대한 값이다.\n' +
      '\n' +
      '생성된 이미지에 추가된 잠재 공간 잡음 하에서 FID와 CMMD의 행동 7: _Behavior. CMMD는 단조적으로 올라가 이미지의 품질 저하를 반영한다. FID의 행동은 일관성이 없으며, 이는 잘못 품질 증가를 시사한다. 녹색과 노란색에서 강조된 이미지 세트는 각각 그림 5의 상행 및 하행에서 시각화된다.\n' +
      '\n' +
      '## Acknowledgment\n' +
      '\n' +
      '귀중한 논의를 위해 위타와트 지트크릿산에 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Shane Barratt and Rishi Sharma. A note on the inception score, 2018.\n' +
      '* [2] Mikolaj Bitkowski, Danica J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs, 2021.\n' +
      '* [3] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-to-image generation via masked generative transformers. _ICML_, 2023.\n' +
      '* [4] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thalpiyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: A jointly-scaled multilingual language-image model, 2022.\n' +
      '* [5] Min Jin Chong and David A. Forsyth. Effectively unbiased FID and inception score and where to find them. _CoRR_, abs/1911.07023, 2019.\n' +
      '* [6] D.C Dowson and B.V Landau. The Frechet distance between multivariate normal distributions. _Journal of Multivariate Analysis_, 12(3):450-455, 1982.\n' +
      '* [7] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In _CVPR_, 2021.\n' +
      '* [8] Kenji Fukumizu, Arthur Gretton, Bernhard Scholkopf, and Bharath K. Sriperumbudur. Characteristic kernels on groups and semigroups. In _NeurIPS_. Curran Associates, Inc., 2008.\n' +
      '* [9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In _NeurIPS_, 2014.\n' +
      '* [10] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Scholkopf, and Alex Smola. A kernel method for the two-sample-problem. In _NeurIPS_. MIT Press, 2006.\n' +
      '* [11] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel two-sample test. _J. Mach. Learn. Res._, 13(1):723-773, 2012.\n' +
      '* [12] Norbert Henze and Bernd Zirkler. A class of invariant consistent tests for multivariate normality. _Communications in statistics-Theory and Methods_, 1990.\n' +
      '* [13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium, 2018.\n' +
      '* [14] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. _CoRR_, abs/1812.04948, 2018.\n' +
      '* [15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In _ECCV_, pages 740-755. Springer, 2014.\n' +
      '* [16] K. V. Mardia. Measures of Multivariate Skewness and Kurotsis with Applications. _Biometrika_, 1970.\n' +
      '* [17] Maurice Frechet. Sur la distance de deux lois de probabilite. _Annales de l\'ISUP_, 1957.\n' +
      '* [18] Midjourney, 2022. [https://www.midjourney.com](https://www.midjourney.com).\n' +
      '* [19] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On Aliased Resizing and Surprising Subtleties in GAN Evaluation. In _CVPR_, 2022.\n' +
      '* [20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _ICML_, 2021.\n' +
      '* [21] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _preprint_, 2022.\n' +
      '* [22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022.\n' +
      '* [23] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. _preprint_, 2022. [arXiv:2205.11487].\n' +
      '* [24] Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alce Radford, and Xi Chen. Improved techniques for training gans. _CoRR_, abs/1606.03498, 2016.\n' +
      '* [25] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. _CoRR_, abs/1512.00567, 2015.\n' +
      '* [26] Laurens van der Maaten and Geoffrey E. Hinton. Visualizing high-dimensional data using t-sne. _Journal of Machine Learning Research_, 9:2579-2605, 2008.\n' +
      '* [27] Qiantong Xu, Gao Huang, Yang Yuan, Chuan Guo, Yu Sun, Felix Wu, and Kilian Q. Weinberger. An empirical study on evaluation metrics of generative adversarial networks. _CoRR_, abs/1806.07755, 2018.\n' +
      '* [28] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. In _ICML_, 2022.\n' +
      '* [29] Sharon Zhou, Mitchell L. Gordon, Ranjay Krishna, Austin Narcomey, Durim Morina, and Michael S. Bernstein. HYPE: human eye perceptual evaluation of generative models. _CoRR_, abs/1904.01121, 2019.\n' +
      '\n' +
      '부록.\n' +
      '\n' +
      'Frechet 인셉션 거리(FID)는 다변량 정규성 가정에 초점을 맞추고 있다. 표준 테스트가 없기 때문에 COCO 30K와 같은 전형적인 이미지 데이터 세트에 대한 인셉션 특징이 세 가지 다른 널리 수용된 통계 테스트를 사용하여 이 가정을 충족하지 못한다는 것을 보여주는데, Mardia의 왜도 테스트[16], Mardia의 첨도 테스트[16] 및 헨제-지룰러 테스트[12]이다.\n' +
      '\n' +
      '모든 테스트에 대한 귀무 가설은 샘플이 다변량 정규 분포에서 도출된다는 것이다. 다른 테스트는 아래에 설명된 대로 다른 통계를 사용한다.\n' +
      '\n' +
      '메벨라(Mardia, Skewness Test)는 Mardia의 Skewness Test.\n' +
      '\n' +
      '다변량 왜도의 측정은 \\(\\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots,\\mathbf{x}_{n}\\mathbf{d})의 무작위 샘플에 대한 것이다.\n' +
      '\n' +
      '}\\{n_{i=1}\\mathbf{n}{n}\\{n}}\\{n} <\\bf{n} <\\bf{n}_{i}_{i}-\\bar{i}-\\bar{mathbf{x}})^{T}\\hat{{T}(\\hat{bathbf{n}:^{n}.\n' +
      '\n' +
      '여기서 \\(\\hat{\\boldsymbol{\\Sigma}}\\)는 편향된 샘플 공분산 행렬이고, \\(\\bar{\\mathbf{x}}\\)는 샘플 평균이다.\n' +
      '\n' +
      'Mardia [16]은 \\(\\mathbf{x}_{i}\\)가 정상적으로 다변량이라는 귀무가설 하에서 통계량(A\\)은 \\(d+1)(d+2)/6\\로 분포된 점근적으로 카이 제곱될 것임을 보여주었다. 따라서, 계산된 \\(A\\)-statistic가 얼마나 극단적인지 확인함으로써 주어진 샘플의 정규성을 테스트할 수 있다. COCO 30K 데이터셋에 계산된 인셉션 임베딩의 경우, 이 테스트는 최대 기계 정밀도인 \\(p\\)-값 \\(0.0\\)으로 정규성 가정을 거부한다.\n' +
      '\n' +
      '멜라디아의 첨도검사에서 Mardia의 Kurtosis Test.\n' +
      '\n' +
      '다변량 첨도 측정은 \\(\\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots,\\mathbf{x}_{n}\\mathbf{d})의 무작위 샘플에 대한 것이다.\n' +
      '\n' +
      '(\\mathbf{x}_{i})^mathbathbf{x}}^{i}}\\\\bfath{n}}\\ \\math{n}}\\ \\hat{\\bath{d}}}\\ \\hat{\\bfbol{d}}} <\\a{bat{d{n}}\\g{d{n}}\\)\\g{n}}\\ha{bat{d{n}}\\g{d{n}}\\g{n}\\g{n}}\\g{n}}\\g{d{n}}\\g{d{d{n}}\\g{d{d{d{d{n}}\\{bat{bat{d{d{n}}\\g{d{d{d{d{d{d{d{d{n}}}\\g{d{d{d{d{d{d{d{d{d{d{d{d{d{d{n}}}\\a{fffffff\n' +
      '\n' +
      '[16]에서 \\(\\mathbf{x}_{i}\\)가 정상적으로 분포된 다변량이라는 귀무가설 하에서 통계량 \\(B\\)은 일반적으로 점근적으로 분포될 것으로 나타났다. COCO 30K 데이터셋에 계산된 인셉션 임베딩의 경우, 이 테스트는 또한 \\(p\\)-값 \\(0.0\\)으로 정규성 가정을 거부하며, 결과의 확신을 직관적으로 이해하기 위해, 이 마디아의 테스트는 테스트 통계량 \\(19,023\\) 표준 편차를 정규 분포에서 평균에서 멀어지게 한다. 이는 인셉션 임베딩의 정규성을 거부하는 테스트의 극단적인 자신감을 나타낸다.\n' +
      '\n' +
      '### Henze-Zirkler Test\n' +
      '\n' +
      '헨제-지러 검정[12]은 두 분포 사이의 거리를 측정하는 기능을 기반으로 하며 분포 중 하나가 표준 다변량 정규일 때 두 번째 분포도 표준 다변량 정상인 경우에만 0이라는 특성을 가지고 있다. 헨제-지러 테스트는 아핀 불변으로 대체 다변량 정상 테스트와 비교하여 더 나은 전력 성능을 갖는 것으로 나타났다.\n' +
      '\n' +
      'COCO 30K의 인셉션 임베딩에 대한 헨제-지러 테스트의 \\(p\\) 값은 기계 정밀도까지 다시 \\(0.0\\)이다. 따라서 헨제-지러 테스트는 또한 압도적으로 높은 자신감을 가진 인셉션 임베딩에 대한 정상적인 가정을 거부한다.\n' +
      '\n' +
      '부록 B 심적 실험\n' +
      '\n' +
      '이 섹션에서 3.2에 설명된 실험의 세부 사항에 대해 논의하며 참조 분포에서 우리는 공분산 매트릭스 \\(\\sigma^{2}\\mathbf{I}_{2}\\)를 사용하여 출처를 중심으로 동위원소 가우시안 분포를 사용하며, 여기서 \\(\\mathbf{I}_{2}\\)는 \\(2\\tcer 2\\) 동일성 매트릭스이다. 두 번째 분포는 좌표 \\((살람다,0), (0,\\lambda),(-\\lambda,0), (0,\\lambda)\\을 중심으로 4개의 서로 다른 동등하게 유사한 가우스 아시아인으로 구성되며, 각각 공분산 행렬 \\(\\tau_{\\lambda}^{2}\\mathbf{I}_{2}\\\\)로 구성된다. 표 2에서 우리는 분포 시각화(첫 번째 행)와 \\(\\lambda\\)의 값이 증가함에 따라 서로 다른 거리 메트릭(제거 행)의 거동을 보여준다. \\(\\lambda\\)가 증가함에 따라, \\(\\tau_{\\lambda}\\)는 아래에서 설명한 대로 조정되어 가우스 혼합물의 전체 공분산 행렬이 \\(\\sigma^{2}\\mathbf{I}_{2}\\)와 동일하도록 한다. 개인적으로, 가우스 혼합물의 평균은 출처이다. 따라서 \\(\\lambda\\)가 달라짐에 따라 염기군 분포의 평균과 공분산 행렬 모두 기준 분포와 동일하게 유지된다. 따라서 Eq를 사용하여 추정된 FD와 FD\\(\\infty\\) 모두이다. 2는 \\(\\lambda\\)가 증가함에 따라 0으로 남아 있다. 이는 동양인의 혼합물이 \\(\\lambda\\)가 증가함에 따라 기준에서 더 멀리 떨어져 있기 때문에 분명히 오해의 소지가 있다. 이 오류는 동양인의 혼합 분포에 대한 잘못된 정규성 가정의 직접적인 결과이다.\n' +
      '\n' +
      '(f_{1},f_{2}}\\) 전체 공분산 매트릭스를 \\(f_{1},f_{2},\\ldots,f_{n})와 같은 1-D PDF(f_{2},\\ldots,f_{n})와 가중치 \\(p_{1},p_{n},p_{n},\\_{n},\\_{n},\\_{n})로 구성된 혼합 분포를 고려하고(f_{n},\\_\\_f_{n},\\_\\_{n},\\_{n},\\_{n},\\_{n},\\_{n},f_{n},f_{n},f_{n},f_{n},f_{n},f_{n},f_{n},f_{n},f_{n},f_{n},f_{n},f_{n},f_{n},f_{n},f_{n}\\)와 그런 다음 혼합물 분포의 PDF는 \\(f(x)=\\sum_{i}p_{i}f_{i}(x)\\에 의해 제공된다. \\(\\mu{{(k)}=\\mu_{i}p_{i}\\mu_{i}\\mu_{i}^{{(k)}\\\\)과 \\(\\mu_{i}^{(k)}\\)가 각각\\(k^{\\{th}}\\) 및 \\(f\\,f_{i}\\)의 원시 순간이라는 예상값의 정의에서 따르고 있다. 또한 분산이 \\(\\mu^{(2)}-\\{\\mu^{{(1)}\\}^{2}\\\\)라는 것을 참조한다. 위 결과를\\(x\\)와 \\(y\\)에 개별적으로 적용하여 \\(\\lambda\\)에 의해 평균에서 떨어진 4개의 가우스 혼합물의 전체 공분산 매트릭스가 \\((\\tau_{\\lambda}^{2}^{2}/2)\\mathbf{I}_{2}\\)에 의해 주어진다는 것을 알 수 있다. I\\(\\tau_{\\lambda}^{2}=\\sigma^{2}-\\lambda^{2}/2\\)를 설정하므로 전체 공분산 매트릭스를 \\(\\sigma^{2}\\mathbf{I}_{2}_{2}\\)에서 유지하여\\(\\lambda\\)에 따라 다르다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>