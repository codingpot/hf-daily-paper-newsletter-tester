<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '공식 국가 공간 모델# VMamba.\n' +
      '\n' +
      ' 리야, 리야.\n' +
      '\n' +
      'UCAS\n' +
      '\n' +
      'liuyue171@mails.ucas.ac.cn\n' +
      '\n' +
      '&Yunjie Tian\n' +
      '\n' +
      'UCAS\n' +
      '\n' +
      'tianyunjie19@mails.ucas.ac.cn\n' +
      '\n' +
      '&Yuzhong Zhao\n' +
      '\n' +
      'UCAS\n' +
      '\n' +
      'zhaoyuzhong20@mails.ucas.ac.cn\n' +
      '\n' +
      '&Hongtian Yu\n' +
      '\n' +
      'UCAS\n' +
      '\n' +
      'yuhongtian17@mails.ucas.ac.cn\n' +
      '\n' +
      '&Lingxi Xie\n' +
      '\n' +
      'Huawei Inc.\n' +
      '\n' +
      '198808xc@gmail.com\n' +
      '\n' +
      '&Yaowei Wang\n' +
      '\n' +
      'Pengcheng Lab.\n' +
      '\n' +
      'wangyw@pcl.ac.cn\n' +
      '\n' +
      '&Qixiang Ye\n' +
      '\n' +
      'UCAS\n' +
      '\n' +
      'qxye@ucas.ac.cn\n' +
      '\n' +
      '&Yunfan Liu\n' +
      '\n' +
      'UCAS\n' +
      '\n' +
      'yunfan.liu@cripac.ia.ac.cn\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '컨볼루션 신경네트웍스(CNN)와 비전트랜스포머(ViT)는 시각적 표현 학습을 위한 가장 인기 있는 두 가지 기반 모델로 서 있다. CNN은 선형 복잡성 w.r.t 이미지 해상도로 놀라운 확장성을 나타내는 반면, ViT는 2차 복잡성과 일치함에도 불구하고 피팅 능력에서 능가한다. 더 가까운 검사는 ViT가 글로벌 수용 분야와 동적 가중치의 통합을 통해 우수한 시각적 모델링 성능을 달성한다는 것을 보여준다. 이 관찰은 계산 효율을 향상시키면서 이러한 구성 요소를 계승하는 새로운 아키텍처를 제안하도록 동기를 부여한다. 이를 위해 최근에 도입된 국가 공간 모델에서 영감을 도출하고 글로벌 수용 분야를 희생하지 않고 선형 복잡성을 달성하는 비주얼 국가 공간 모델(VMamba)을 제안한다. 대략적인 방향에 민감한 문제를 해결하기 위해 크로스-스캔 모듈(CSM)을 도입하여 공간 영역을 추적하고 비인과 시각적 이미지를 주문 패치 서열로 변환한다. 광범위한 실험 결과는 VMamba가 다양한 시각적 인식 과제에 걸쳐 유망한 역량을 보여줄 뿐만 아니라 이미지 해상도가 증가함에 따라 확립된 벤치마크보다 더 뚜렷한 이점을 나타낸다는 것을 입증한다. 소스 코드는 _[https://github.com/M제로Miko/VMamba_](https://github.com/MzeroMiko/VMamba__)에서 이용 가능했다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '시각적 표현 학습은 컴퓨터 비전에서 가장 근본적인 연구 주제 중 하나로 딥러닝 시대가 시작된 이후 상당한 돌파구를 경험했다. 심층 기반 모델인 _i._i._, Convolution Neural Net웍스[38; 19; 22; 29; 42] 및 비전 트랜스포머[10; 28; 45; 56]의 두 가지 주요 범주가 다양한 시각적 작업에 광범위하게 사용되었다. 두 가지 모두 컴퓨팅 표현 시각적 표현에서 놀라운 성공을 거두었지만 ViT는 보통 CNN에 비해 우수한 성능을 나타내는데, 이는 글로벌 수용 분야와 주의 메커니즘에 의해 촉진되는 동적 가중치에 기인할 수 있다.\n' +
      '\n' +
      '그러나 주의 메커니즘은 이미지 크기 측면에서 2차 복잡성을 요구하여 객체 검출, 의미 세분화, _etc._etc._와 같은 하류 조밀한 예측 작업을 다룰 때 값비싼 계산 오버헤드를 생성한다. 이 문제를 해결하기 위해 수용 분야 규모에 제한을 가하는 비용에도 불구하고 컴퓨팅 창문[43]의 크기나 보물을 제한함으로써 주의 효율성을 향상시키는 데 실질적인 노력을 기울였다. 이것은 우리가 선형 복잡성을 가진 새로운 시각적 기반 모델을 설계하는 동시에 여전히 글로벌 수용 분야와 동적 가중치의 이점을 보존하도록 동기를 부여한다.\n' +
      '\n' +
      '최근 제안된 상태 공간 모델[12; 34; 47]에서 영감을 얻은 우리는 효율적인 시각적 표현 학습을 위해 비주얼 국가 공간 모델(VAMuba로 표시)을 소개한다. VAMuba의 효과적인 주의 복잡성 감소 성공의 중추적 개념은 원래 자연 언어 처리(NLP) 과제를 해결하기 위해 고안된 선택적 우주 국가 예측 모델(S6) [12]에서 계승된다. 기존의 주의 연산 접근법과 대조적으로, S6은 1-D 어레이(_e.g._, 텍스트 서열) 내의 각 요소가 압축된 히든 상태를 통해 이전에 스캔된 샘플 중 임의의 것과 상호작용하여 이차 복잡성을 선형으로 효과적으로 감소시킬 수 있게 한다.\n' +
      '\n' +
      '그러나 시각적 데이터의 비인과적 특성으로 인해 이러한 전략을 패치되고 평탄화된 이미지에 직접 적용하는 것은 미스캔 패치에 대한 관계를 추정할 수 없기 때문에 제한된 수용 필드를 초래할 수밖에 없다. 이 문제를 \'방향에 민감한\' 문제로 하며 새로 도입된 크로스스캔 모듈(CSM)을 통해 해결할 것을 제안한다. 이미지 특징 맵의 공간 영역을 단방향 패턴(열별 또는 행별)으로 횡단하는 대신 CSM은 특징 맵에 걸쳐 있는 4개의 모서리에서 반대 위치(그림 2(b 참조)까지 4방향 스캐닝 전략 _i._)를 채택한다. 이 전략은 특징 맵 내의 각 요소가 다른 방향의 다른 모든 위치로부터의 정보를 통합하도록 보장하며, 이는 선형 계산 복잡도를 증가시키지 않으면서 글로벌 수용 필드를 만든다.\n' +
      '\n' +
      'VAMuba의 효과를 검증하기 위해 다양한 시각 과제에 대한 광범위한 실험이 수행된다. 그림 1에서 보는 바와 같이 VAMuba 모델은 Resnet[19], ViT[10], Swin[28]1을 포함한 벤치마크 비전 모델과 비교하여 이미지넷-1K에 대해 우수하거나 최소한 경쟁력 있는 성능을 보여주고 있다. 예를 들어, VAMuba-Tiny 소형/Base를 사용하여 ADE20K의 mIoU(\\(22/44/75\\) 입력과 \\(47.3\\%/49.5\\%/50.5\\%\\) M 매개변수를 사용하여 COCO의 mAP를 달성하여 강력한 기반 모델 역할을 할 가능성이 있음을 보여준다. 또한, 더 큰 이미지를 입력으로 사용할 때 ViT의 FLOP는 보통 여전히 우수한 성능을 나타내지만 CNN 모델보다 훨씬 더 빠르게 증가한다. 그러나 본질적으로 트랜스포머 아키텍처를 기반으로 하는 기반 모델인 VAMuba가 FLOP가 꾸준히 증가함에 따라 ViT와 유사한 성능을 얻을 수 있다는 점은 흥미롭다.\n' +
      '\n' +
      '부타주 1: VAMuba-B 훈련 중 버그를 접하게 되며, 가능한 한 빨리 최신 결과를 업데이트하겠습니다.\n' +
      '\n' +
      '아래의 기여금을 요약합니다.\n' +
      '\n' +
      '* 우리는 글로벌 수용 분야가 있는 시각적 상태 공간 모델인 VAMuba와 시각적 표현 학습을 위한 동적 가중치를 제안한다. VAMuba는 CNN과 ViT의 기존 선택을 넘어 확장되는 비전 기반 모델에 대한 새로운 옵션을 제시한다.\n' +
      '* 더 크로스-스캔 모듈(CSM)을 도입하여 1-D 어레이 스캐닝과 2-D 일반 트래버링 사이의 격차를 해소하여 수신 분야를 손상시키지 않으면서 S6의 시각적 데이터로의 확장을 촉진한다.\n' +
      '벨과 휘슬이 없는**, 우리는 VAMuba가 이미지 분류, 객체 검출 및 의미 세분화를 포함한 다양한 시각적 과제에 걸쳐 유망한 결과를 달성한다는 것을 보여준다. 이러한 발견은 VAMuba가 강력한 비전 기반 모델 역할을 할 가능성을 강조한다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '딥 뉴럴 네트워크는 기계 시각적 인식에서 실질적으로 연구를 발전시켰다. 시각 기반 모델, _i._, CNN[23; 38; 19; 42; 및 ViT[10; 48; 6; 56;]의 두 가지 널리 퍼진 유형이 있다. 최근 주우주모델(SSM)[12; 34; 47]의 성공은 NLP 및 CV 군집 모두에서 광범위한 관심을 끌었던 효율적인 긴 서열 모델링에서 효능을 강화했다. 우리의 연구는 이 작업 라인을 고수하고 비전 도메인에서 데이터 모델링을 위한 SSM 기반 아키텍처인 VMamba를 제안한다. VMamba는 CNN 및 ViT와 함께 커뮤니티에 대한 대체 기반 모델로 기여한다.\n' +
      '\n' +
      '**콘볼루션 신경네트웍스***(CNN)는 시각적 인식 역사상 랜드마크 모델 역할을 한다. 초기 CNN 기반 모델[25, 23]은 필기 자릿수[24]를 인식하고 문자 범주[55]를 분류하는 등 기본 과제를 위해 설계된다. CNN의 독특한 특성은 컨볼루션 커널에 캡슐화되어 있으며, 이는 이미지로부터 관심 있는 시각적 정보를 캡처하기 위해 수용 필드를 사용한다. 강력한 컴퓨팅 디바이스(_GPU_)와 대규모 데이터세트(7)의 도움으로 점점 더 깊이 있는 [38, 41, 19, 22] 및 효율적인 모델[20, 42, 52, 36]이 시각 작업의 스펙트럼을 가로질러 성능을 향상시키는 것으로 제시되었다. 이러한 노력 외에도 보다 발전된 컨볼루션 연산자[4, 21, 53, 5] 이상의 효율적인 네트워크 아키텍처[59, 3, 51, 20]를 제안하기 위한 진전이 있었다.\n' +
      '\n' +
      '**Vision Transformers***(ViT)는 NLP 커뮤니티에서 적응되어 시각적 작업에 대한 강력한 인식 모델을 보여주고 가장 유망한 시각적 기반 모델 중 하나로 빠르게 진화한다. 초기 ViT 기반 모델은 보통 대규모 데이터셋[10]을 필요로 하며 일반 구성[54, 58, 1, 31]에 나타난다. 이후 DeiT[45]는 최적화 과정에서 직면하는 과제를 해결하기 위해 훈련 기법을 사용하고, 후속 연구에서는 시각적 인식의 귀납적 편향을 네트워크 설계에 통합하는 경향이 있다. 예를 들어, 커뮤니티는 백본 전체에 걸쳐 특징 해상도를 점진적으로 감소시키기 위해 계층적 ViT[28, 9, 48, 31, 56, 44, 6, 8, 57]을 제안한다. 더욱이, 다른 연구는 CNN 및 ViT 모듈[6, 40, 31], _etc_를 결합하여 하이브리드 아키텍처를 설계함으로써 컨볼루션 연산[49, 6, 46]을 도입하는 것과 같은 CNN의 이점을 이용하는 것을 제안한다.\n' +
      '\n' +
      '** 스테이트 스페이스 모델(SSM)**는 최근 국가 공간 변형 [16, 15, 39]로 딥러닝에 도입되는 모델이 제안되고 있다. 제어 시스템에서 연속 상태 공간 모델에 의해 영감을 받아 HiPPO[13] 초기화와 결합하여 LSSL[16]은 긴 범위 의존 문제를 처리할 가능성을 보여준다. 그러나 상태 표현에 의해 유도된 금지 계산 및 기억 요구 사항으로 인해 LSSL은 실무에서 사용할 수 없다. 이 문제를 해결하기 위해 S4 [15]는 파라미터를 대각선 구조로 정규화할 것을 제안한다. 이후 구조화된 상태 공간 모델의 많은 풍미는 복잡한 범위의 구조 [17, 14], 다중 입력 다중 출력[39], 대각선과 낮은 순위 연산(18], 선택 메커니즘[12]과 같은 다양한 구조로 분사된다. 그런 다음 이러한 모델은 큰 표현 모델[34, 33, 11]에 통합된다.\n' +
      '\n' +
      '이러한 모델은 주로 언어 이해 [33, 34], 콘텐츠 기반 추론[12], 같은 언어 및 언어와 같은 장거리 및 캐주얼 데이터에 상태 공간 모델이 적용되는 방법에 초점을 맞추고 있다.\n' +
      '\n' +
      '그림 1: ** ImageNet-1K** VMamba 시리즈에 대한 성능 비교는 인기 있는 대응물에 비해 우수한 상위 1의 정확도를 달성한다. 제안된 VMamba는 선형 복잡성을 갖는 동적 가중치인 글로벌 유효 수신 필드(ERF)를 보여주는 능력이 있다는 점에 주목한다.\n' +
      '\n' +
      '픽셀 수준의 1D 이미지 분류[15]는 시각적 인식에 주목한 사람은 거의 없다. 우리와 가장 유사한 작업은 S4ND[35]입니다. S4ND는 상태 공간 메커니즘을 시각적 작업에 적용하고 그 성능이 ViT[10]와 경쟁할 수 있는 가능성을 보여주는 첫 번째 작업이다. 그러나 S4ND는 간단한 방식으로 S4 모델을 확장하며, 입력 의존적 방식으로 이미지 정보를 효율적으로 캡처하는 데 실패한다. 우리는 삼바[12]가 도입한 선택적 스캔 메커니즘으로 제안된 VAMaba가 ResNet[19], ViT[10], 스와인[27] 및 소집 [29]와 같은 기존의 인기 비전 기반 모델과 일치할 수 있음을 보여준다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '이 절에서는 국가 공간 모델, 폐기 과정 및 선택적 스캔 메커니즘을 포함하여 VAMaba와 관련된 예비 개념을 도입하여 시작한다. 그런 다음 VAMaba의 핵심 요소 역할을 하는 2D 상태 공간 모델의 상세한 사양을 제공합니다. 마지막으로, 우리는 전체 VAMaba 아키텍처에 대한 포괄적인 논의를 제시한다.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '** 스테이트 스페이스 모델** 스테이트 스페이스 모델(SSM)은 일반적으로 자극 \\(x(t)\\in\\mathbb{R}^{L}\\)을 매핑하는 선형 시간 불변 시스템으로 간주된다(y(t)\\in\\mathbb{R}^{L}\\). 수학적으로 이러한 모델은 일반적으로 선형 일반 미분 방정식(ODE)(Eq)으로 공식화된다. 파라미터가 \\(A\\in\\mathbb{C}^{N\\times N}\\), \\(B,C\\in\\mathbb{C}^{N}\\), 상태 크기 \\(N\\)에 대한 \\(D\\in\\mathbb{C}^{1}\\) 및 스킵 연결 \\(D\\in\\mathbb{C}^{C}^{C}^{C.\n' +
      '\n' +
      '(t)\\[\\begin{splime} (t)+Dx(t)\\{split})=Ah(t)+Dx(t)\\tag{split} (t)\\tag{split} (t)\n' +
      '\n' +
      '** 표준화** 스테이트 스페이스 모델** 스테이트 스페이스 모델(SSM)은 연속적인 모델로서 딥러닝 알고리즘에 통합될 때 큰 도전에 직면한다. 이러한 장애물을 극복하기 위해서는 폐기 과정이 절실해진다.\n' +
      '\n' +
      '불복화의 주요 목적은 ODE를 이산 함수로 변환하는 것이다. 이러한 변환은 입력 데이터에 구현된 기본 신호의 샘플 속도와 모델을 정렬하는 데 중요하여 계산적으로 효율적인 동작[16]을 가능하게 한다. 입력 \\(x_{k}\\in\\mathbb{R}^{L\\times D}\\)를 고려한 것이다.\n' +
      '\n' +
      '그림 2: ** 정보 흐름의 비교:의도 대 내용. 크로스캔 모듈(CSM** (a) 주의 메커니즘은 중심 픽셀에 대한 모든 픽셀을 균일하게 통합하여 \\(\\mathcal{O}(N^{2})\\의 복잡성을 생성한다. (b) CSM은 상좌, 하우, 우상 및 하단의 픽셀들을 \\(\\mathcal{O}(N)\\) 복잡성과 통합한다.\n' +
      '\n' +
      '[17], ODE(Eq. 1)에 이어 길이 \\(L\\)의 신호 흐름 내에서 샘플링된 벡터이다. 제너트 주문 홀드 룰을 사용하여 다음과 같이 폐기할 수 있습니다.\n' +
      '\n' +
      '"^{D}x_{k}]\\\\bar{D}.\n' +
      '\n' +
      '\\(B\\), \\(C\\in\\mathbb{R}^{D\\times N}\\) 및 \\(\\Delta\\in\\mathbb{R}^{D}\\)의 경우. 실제로 [12]에 이어 1차 테일러 시리즈를 사용하여 \\(\\bar{B}\\)의 근사치를 정제한다.\n' +
      '\n' +
      '>(^{Delta A}-I)^{{-1}\\Delta B=\\Delta B \\tag{3}]\n' +
      '\n' +
      '** 선택적인 스칸 메카니즘***는 주로 선형 시간 가변제(LTI) SSM에 초점을 맞추는 널리 퍼진 접근법에서 벗어나, 제안된 VMamba는 선택적 스캔 메커니즘(S6) [12]을 코어 SSM 연산자로 통합하여 스스로 이격한다. S6에서 매트릭스 \\(B\\in\\mathbb{L}\\in\\mathbb{R}\\)는 입력 데이터(x\\in\\mathbb{R}^mathbb{R}^{B\\times L\\times N}\\)에서 파생된다. 이는 S6이 입력에 내장된 상황 정보를 인지하여 이 메커니즘 내에서 가중치의 역동성을 보장한다는 것을 의미한다.\n' +
      '\n' +
      '2D 세이브 스칸.\n' +
      '\n' +
      '특이한 특성에도 불구하고 S6은 입력된 데이터를 인과적으로 처리하여 데이터의 스캔된 부분 내에서만 정보를 캡처할 수 있다. 이것은 자연적으로 시간 데이터를 포함하는 NLP 과제와 S6을 정렬하지만 이미지, 그래프, 세트, _etc._etc._와 같은 비인과 데이터에 적응할 때 상당한 문제를 제기한다. 이 문제에 대한 간단한 해결책은 두 가지 다른 방향(_i.e._, 전방 및 후방)을 따라 데이터를 스캔하여 계산 복잡도를 증가시키지 않으면서 서로의 수용 분야를 보상할 수 있도록 하는 것이다.\n' +
      '\n' +
      '비인과적 성격에도 불구하고, 이미지는 2D 공간 정보(_e.g._ 로컬 텍스처 및 글로벌 구조)를 포함하고 있다는 점에서 텍스트와 다르다. 이 문제를 해결하기 위해 S4ND[35]는 컨볼루션으로 SSM을 개혁하고 외부 제품을 통해 커널을 1-D에서 2-D로 간단하게 확장한다는 것을 제안한다. 그러나 이러한 변형은 가중치가 동적(즉, 입력 독립)인 것을 방지하여 컨텍스트 기반 데이터 모델링 능력의 손실을 초래한다. 따라서 우리는 선택적 스캔 접근 방식[12]에 달라붙어 동적 가중치를 보존하는 것을 선택하며, 안타깝게도 [35]를 따르고 컨볼루션 연산을 통합한다.\n' +
      '\n' +
      '이 문제를 해결하기 위해 그림 2와 같이 크로스-스캔 모듈(CSM)을 제안하며 행과 열을 따라 이미지 패치를 시퀀스(_scan 확장_)로 전개한 다음, 탑-좌측, 상단-우측, 하측-좌측-좌측-좌측-좌측-좌측-좌측-좌측-좌측으로 4가지 방향을 따라 스캔을 진행한다. 이와 같이, 임의의 픽셀(그림 2의 중심 픽셀 등)은 다른 모든 픽셀들의 정보를 서로 다른 방향으로 통합한다. 그런 다음 각 서열을 단일 이미지로 재구성하고 그림 3(_scan 병합_)에 도시된 바와 같이 모든 서열을 새로운 서열로부터 병합한다.\n' +
      '\n' +
      '그림 3: **-선택적-Scan의 이미지***. CSM(_scan 확장_)을 이용하여 영상을 스캔하여 시작한다. 이어서, 4개의 결과 특징을 S6 블록을 통해 개별적으로 처리하고, 4개의 출력 특징을 병합(_scan 병합_)하여 최종 2D 특징 맵을 구성한다.\n' +
      '\n' +
      'S6 블록을 S6 블록으로 지칭하는 CSM과 S6의 통합은 VMamba(다음으로 하위 섹션에서 자세히 자세히)의 기본 구축 블록을 구성하는 비주얼 스테이트 스페이스(VSS) 블록을 구성하는 핵심 요소 역할을 한다. 우리는 S6 블록이 글로벌 수용 분야를 유지하면서 선택적 스캔 메커니즘의 선형 복잡성을 계승한다는 것을 강조하며, 이는 이러한 비전 모델을 구성하려는 동기와 일치한다.\n' +
      '\n' +
      '### VMamba Model\n' +
      '\n' +
      '3.3.1 및 3.3.1 오버올건축건축물 건축물#########\n' +
      '\n' +
      'VMamba-Tiny의 아키텍처에 대한 개요는 그림 4(a)에 나와 있다. VMamba는 ViT와 유사하게 줄기 모듈을 사용하여 입력 이미지를 패치로 분할하여 프로세스를 시작하지만 패치를 1-D 시퀀스로 더 평탄화하지 않고 시작한다. 이 수정은 이미지의 2D 구조를 보존하여 \\(\\frac{H}{4}\\tep\\frac{W}{4}\\times C_{1}\\\\)의 치수를 갖는 특징 맵을 생성한다.\n' +
      '\n' +
      '그런 다음 VMamba는 특징 맵에 여러 VSS 블록을 스택하여 "Stage 1"을 구성하는 동일한 차원을 유지한다. VMamba에서의 계층적 표현은 패치 병합 동작[27]을 통해 "스테이지 1"에서 특징 맵을 다운 샘플링하여 구축된다. 그 후, 더 많은 VSS 블록들이 관여하여 \\(\\frac{H}{8}\\tep\\frac{W}{8}\\)의 출력 해상도를 생성하고 "Stage 2"를 형성한다. 이 절차는 \\(\\frac{H}{16}\\tep\\frac{W}{16}\\) 및 \\(\\frac{H}{92}\\CI\\frac{W}{23}\\\\)의 결의로 "Stage 3" 및 "Stage 4"를 생성하기 위해 반복된다. 이 모든 단계는 대중 CNN 모델[19; 22; 41; 29; 42; 일부 ViT[27; 48; 56]과 유사한 계층적 표현을 집단적으로 구성한다. 결과 아키텍처는 유사한 요구 사항을 가진 실제 애플리케이션에서 다른 비전 모델에 대한 다재다능한 대체 역할을 할 수 있다.\n' +
      '\n' +
      '우리는 VMamba를 3개의 별개의 척도인 _i._, VMamba-Tiny, VMamba-소규모 및 VMamba-Base(VMamba-T, VMamba-S 및 VMamba-B로 각각 참조)로 개발한다. 자세한 건축 사양이 표 1에 설명되어 있으며 모든 모델에 대한 FLOP는 \\(224\\times 224\\) 입력 크기를 사용하여 평가된다. 향후 업데이트에는 대규모 모델과 같은 추가 아키텍처가 도입된다.\n' +
      '\n' +
      '3.3.2 VSS 블록 블록 3.3.2 VSS 블록###### 3.3.2 VSS 블록 블록.\n' +
      '\n' +
      'VSS 블록의 구조는 그림 4(b)에 나와 있다. 입력은 초기 선형 임베딩 레이어를 거치며, 출력은 두 개의 정보로 분할된다. 하나의 흐름은 \\(3\\t 3\\) 깊이별 컨볼루션 레이어를 통과하고, 이어서 코어 SS2D 모듈에 들어가기 전에 실루 활성화 함수[37]를 통과한다. SS2D의 출력은 계층 정규화 레이어를 거친 다음 실루 활성화를 거친 다른 정보 흐름의 출력에 추가된다. 이 조합은 VSS 블록의 최종 출력을 생성한다.\n' +
      '\n' +
      '시력 변압기와는 달리 인과성으로 인해 VMamba에서 위치 임베딩 편향을 활용하는 것을 자제한다. 당사의 설계는 블록 내\\(이하 텍스트{Norm}\\rightarrow\\\\texttt{astic}\\rightarrow\\\\texttt{LP} \\Hyarrow\\\\texttt{MLP}\\)와 다음 작업 순서를 사용하는 전형적인 비전 변압기 구조에서 분기됩니다.\n' +
      '\n' +
      '그림 4:(a) VMamba 모델(VMamba-T)의 전체 아키텍처; (b) VMamba의 기본 건물 블록, 즉 VSS 블록의 기본 구축 블록.\n' +
      '\n' +
      'MLP 조작. 결과적으로 VSS 블록은 ViT 블록보다 낮아 전체 모델 깊이의 유사한 예산으로 더 많은 블록을 쌓을 수 있다.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '이 절에서는 CNN 및 비전 변압기를 포함한 인기 모델에 대해 VMamba를 평가하고 비교하기 위한 일련의 실험을 수행한다. 우리의 평가는 이미지넷-1K에 대한 이미지 분류, COCO에 대한 객체 검출 및 ADE20K의 의미 세분화 등 다양한 과제에 걸쳐 있다. 그 후, 우리는 VMamba의 아키텍처에 대한 더 깊은 통찰력을 얻기 위해 분석 실험에 설명되어 있다.\n' +
      '\n' +
      '이미지넷-1K에 대한 이미지 분류.\n' +
      '\n' +
      '** 설정** 우리는 이미지넷-1K[7]에 대한 VMamba의 분류 성능을 평가한다. [27]의 구성에 이어 VMamba-T/S/B는 \\(300\\) epoch에 대한 처음부터(처음 20 epoch와 함께 평가)를 통해 배치 크기의 \\(1024\\)를 사용하여 훈련을 받는다. 훈련 과정은 \\(0.9,0.999)\\로 설정된 베타로 AdamW 최적화기와 \\(0.9\\), 코사인 붕괴 학습률 스케줄러, \\(1\\ 10^{-3}\\)의 초기 학습률 및 \\(0.05\\)의 체중 붕괴를 통합한다. 라벨 스무딩(\\(0.1\\)) 및 지수 이동 평균(EMA)과 같은 추가 기술도 사용된다. 이 외에도 더 이상의 훈련 기술이 적용되지 않는다.\n' +
      '\n' +
      '** 결과***표 2는 VMamba와 인기 CNN 모델 및 비전 변압기를 비교한 이미지넷-1K에 대한 결과를 요약한 것이다. 비교 결과, 유사한 FLOP와 함께 VMamba-T가 \\(82.2\\%\\)의 성능을 달성하여 RegNetY-4G를 \\(2.2\\%\\), DeiT-S를 \\(2.4\\%\\), Swin-T를 \\(0.9\\%\\)만큼 능가한다는 것을 알 수 있다. 특히, VMamba의 성능 이점은 작고 베이스 스케일 모델에 걸쳐 지속된다. 예를 들어, 소규모에서 VMamba-S는 \\(83.5\\%\\)의 상위 1위 정확도를 가지며, \\(1.8\\%\\)에 의해 RegNetY-8G 및 Swin-S를 \\(0.5\\%\\)만큼 능가한다. 한편, VMamba-B는 상위 1의 정확도를 달성합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c} \\hline layer name & output size & Tiny & Small & Base \\\\ \\hline stem & 112\\(\\times\\)112 & conv 4\\(\\times\\)4, 96, stride 4 & conv 4\\(\\times\\)4, 96, stride 4 & conv 4\\(\\times\\)4, 128, stride 4 \\\\ \\hline stage 1 & 56\\(\\times\\)56 & \\(\\left[\\begin{array}{c}\\text{linear }96\\to 2\\times\\)96 \\\\ DWConv 3\\(\\times\\)3, 2\\(\\times\\)96 \\\\ SS2D, dim 2\\(\\times\\)96 \\\\ linear 2\\(\\times\\)96 \\(\\rightarrow\\) 96 \\\\ \\end{array}\\right]\\times 2\\) & \\(\\left[\\begin{array}{c}\\text{linear }96\\to 2\\times\\)96 \\\\ DWConv 3\\(\\times\\)3, 2\\(\\times\\)96 \\\\ SS2D, dim 2\\(\\times\\)96 \\\\ linear 2\\(\\times\\)96 \\(\\rightarrow\\) 96 \\\\ \\end{array}\\right]\\times 2\\) & \\(\\left[\\begin{array}{c}\\text{linear }128\\to 2\\times\\)128 \\\\ DWConv 3\\(\\times\\)3, 2\\(\\times\\)96 \\\\ SS2D, dim 2\\(\\times\\)96 \\\\ linear 2\\(\\times\\)96 \\(\\rightarrow\\) 96 \\\\ \\end{array}\\right]\\times 2\\) & \\(\\left[\\begin{array}{c}\\text{linear }128\\to 2\\times\\)128 \\\\ DWConv 3\\(\\times\\)3, 2\\(\\times\\)128 \\\\ SS2D, dim 2\\(\\times\\)128 \\\\ linear 2\\(\\times\\)128 \\(\\rightarrow\\) 128 \\\\ \\end{array}\\right]\\times 2\\) & \\(\\left[\\begin{array}{c}\\text{linear }128\\to 2\\times\\)128 \\\\ DWConv 3\\(\\times\\)3, 2\\(\\times\\)128 \\\\ SS2D, dim 2\\(\\times\\)128 \\\\ \\end{array}\\right]\\times 2\\) & \\(\\left[\\begin{array}{c}\\text{linear }128\\to 2\\times\\)128 \\\\ DWConv 3\\(\\times\\)3, 2\\(\\times\\)128 \\\\ linear 2\\(\\times\\)96 \\(\\rightarrow\\) 96 \\\\ \\end{array}\\right]\\times 2\\) & \\(\\left[\\begin{array}{c}\\text{linear }128\\to 2\\times\\)128 \\\\ DWConv 3\\(\\times\\)3, 2\\(\\times\\)128 \\\\ linear 2\\(\\times\\)128 \\(\\rightarrow\\) 128 \\\\ \\end{array}\\right]\\times 2\\) \\\\ \\hline stage 2 & 28\\(\\times\\)28 & \\(\\left[\\begin{array}{c}\\text{linear }192\\to 2\\times\\)192 \\\\ DWConv 3\\(\\times\\)3, 2\\(\\times\\)192 \\\\ SS2D, dim 2\\(\\times\\)192 \\\\ \\end{array}\\right]\\times 2\\) & \\(\\left[\\begin{array}{c}\\text{linear }192\\to 2\\times\\)192 \\\\ DWConv 3\\(\\times\\)3, 2\\(\\times\\)256 \\\\ SS2D, dim 2\\(\\times\\)192 \\\\ \\end{array}\\right]\\times 2\\) & \\(\\left[\\begin{array}{c}\\text{linear }256\\to 2\\times\\)256 \\\\ SWConv 3\\(\\times\\)3, 2\\(\\times\\)256 \\\\ S2D, dim 2\\(\\times\\)256 \\\\ \\end{array}\\right]\\times 2\\) \\\\ \\hline stage 3 & 14\\(\\times\\)14 & \\(\\left[\\begin{array}{c}\\text{linear }384\\to 2\\times\\)384 \\\\ DWConv 3\\(\\times\\)3, 2\\(\\times\\)384 \\\\ linear 2\\(\\times\\)384 \\(\\rightarrow\\) 384 \\\\ \\end{array}\\right]\\times 9\\) & \\(\\left[\\begin{array}{c}\\text{linear }384\\to 2\\times\\)384 \\\\ DWConv 3\\(\\times\\)3, 2\\(\\times\\)384 \\\\ linear 2\\(\\times\\)384 \\(\\rightarrow\\) 384 \\\\ \\end{array}\\right]\\times 27\\) & \\(\\left[\\begin{array}{c}\\text{linear }512\\to 2\\times\\)512 \\\\ DWConv 3\\(\\times\\)3, 2\\(\\times\\)512 \\\\ linear 2\\(\\times\\)384 \\(\\rightarrow\\) 384 \\\\ \\end{array}\\right]\\times 27\\) & \\(\\left[\\begin{array}{c}\\text{linear }512\\to 2\\times\\)512 \\\\ DWConv 3\\(\\times\\)3, 2\\(\\times\\)512 \\\\ linear 2\\(\\times\\)384 \\(\\rightarrow\\) 384 \\\\ \\end{array}\\right]\\times 27\\) \\\\ \\hline stage 4 & 7\\(\\times\\)7 & \\(\\left[\\begin{array}{c}\\text{linear }768\\to 2\\times\\)768 \\\\ DWConv 3\\(\\times\\)3, 2\\(\\times\\)768 \\\\ SS2D, dim 2\\(\\times\\)768 \\\\ linear 2\\(\\times\\)768 \\(\\rightarrow\\) 768 \\\\ \\end{array}\\right]\\times 2\\) & \\(\\left[\\begin{array}{c}\\text{linear }768\\to 2\\times\\)768 \\\\ SWConv 3\\(\\times\\)3, 2\\(\\times\\)768 \\\\ SS2D, dim 2\\(\\times\\)768 \\\\ linear 2\\(\\times\\)768 \\(\\rightarrow\\) 768 \\\\ \\end{array}\\right]\\times 2\\) & \\(\\left[\\begin{array}{c}\\text{linear }1024\\to 2\\times\\)1024 \\\\ SWConv 3\\(\\times\\)3, 2\\(\\times\\)1024 \\\\ SS2D, dim 2\\(\\times\\)1024 \\\\ \\end{array}\\right]\\times 2\\) \\\\ \\hline  & 1\\(\\times\\)1 & \\multicolumn{3}{c|}{average pool, 1000-4 fc, softmax} \\\\ \\hline Param. (M) & 22 & 44 & 75 \\\\ \\hline FLOPs & 4.5\\(\\times\\)10\\({}^{9}\\) & 9.1\\(\\times\\)10\\({}^{9}\\) & 15.2\\(\\times\\)10\\({}^{9}\\) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: ** VMamba 시리즈**의 건축 개요. 다운 샘플링은 1단계, 2단계, 3단계의 패치 병합 작업을 통해 실행되며, \'선형\'이라는 용어는 선형층을 의미한다. DWConv\'는 깊이별 컨볼루션 연산을 나타낸다. 제안된 2D-선택적-스캔은 "SS2D"로 표시된다.\n' +
      '\n' +
      '\\(83.2\\%\\)는 RegNetY-16G를 \\(0.3\\%\\) 및 DeiT-B를 \\(0.1\\%\\)만큼 능가한다. 이러한 유망한 결과는 기존의 CNN 모델과 비전 변압기를 넘어 우월성을 확장하면서 강력한 발견 모델로서 VMamba의 잠재력을 강조한다.\n' +
      '\n' +
      'CO에 대한 검출.\n' +
      '\n' +
      '** 설정** 이 섹션에서는 MSCOCO 2017 데이터세트[26]를 사용하여 객체 검출에 대해 제안된 VMamba의 성능을 평가한다. 우리의 훈련 프레임워크는 mm검출 라이브러리[2]에 구축되며 마스크-RCNN 검출기와 함께 Swin[27]의 하이퍼모수들을 부착한다. 구체적으로, 우리는 \\(12\\) 및 \\(36\\) epoch 모두에 대해 사전 훈련된 분류 모델(이미지Net-1K)을 AdamW 최적화기를 사용하고 미세 조정한다. 하락 경로율은 VMamba-T/S/B에 대해 각각 \\(0.2\\%/0.2\\%/0.2\\%\\)2로 설정된다. 학습률은 \\(1\\t 10^{-4}\\)에서 초기화되고, 9, 11번째 epoch에서 \\(10\\tot\\)의 요인으로 감소한다. 우리는 \\(16\\)의 배치 크기로 멀티 스케일 트레이닝 및 랜덤 플립을 구현한다. 이러한 선택은 객체 검출 평가를 위한 확립된 관행과 일치한다.\n' +
      '\n' +
      '2: 모두 0.2인 것은 감독 덕분이며 최신 실험을 업데이트하겠습니다.\n' +
      '\n' +
      '** 결과** COCO에 대한 결과는 표 3에 요약되어 있으며, COCO에 대한 상자/마스크 평균 정밀도(AP)는 사용된 훈련 일정(12\\) 또는 \\(36\\) epochs에 관계없이 COCO에 대한 상자/마스크 평균 정밀도(AP)에서 우열을 유지한다. 구체적으로, \\(12\\)-포크 미세 조정 일정, VMamba-T/S/B 모델은 \\(46.5\\%/48.2\\%/48.5\\%\\%) mAP 및 ConvNeXt-T/B에서 Swin-T/S/B를 초과하여 \\(2.3\\%/3.8\\%/3.6\\%/3.6\\%/1.6\\%/1.6\\%/1.5\\%/1.5\\%/1.5\\) mAP에 의한 Swin-T/B) mAP에 의한 Swin-T/2.5\\%/1.5\\%/1.5\\%/1.5\\%/1.5\\%/1.5\\%/1.5\\%/1.5\\%/1.6\\%/1.6\\%/1.6\\%/1.5\\%/1.5\\%/1.5\\%/1.5\\%/1.5\\%/1.5\\%/1.5\\%/B 같은 구성을 사용하여 VMamba-T/S/B는 \\(42.1\\%/43.0\\%/43.1\\%\\%) mIoU의 인스턴스 분할 mIU를 달성하며, \\(2.8\\%/2.1\\%/0.8\\%/0.7\\% mIoU) mIoU 및 ConvNeXt-T/S/B가 각각 mIoU(2.0\\%/0.2\\%/0.2\\%/0.2\\%/0.2\\%/0.2\\%/0.2\\%/0.2\\%/0.2\\%/0.2\\%/0.2\\%/0.7\\% mIoU) mIoU 및 ConvNeXt-T/0.2\\%/0.2\\%/0.2\\%/0.2\\%/0.7\\% mIoU) mIoU) mIoU) mIoU에 의해 Swin-T/S/B) mIoU에\n' +
      '\n' +
      '또한, 그림 3과 같이 Swin [28], ConvNeXt [29], PVTv2 [49] 및 ViT[10](개관 포함), VMamba-T/S를 포함한 상대와 비교하여 VMamba의 장점은\\(36\\)-포크 미세 조정 일정에서 지속된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c} \\hline \\hline \\multirow{2}{*}{method} & image & \\multirow{2}{*}{\\#param.} & \\multirow{2}{*}{FLOPs} & ImageNet \\\\  & size & & & top-1 acc. \\\\ \\hline RegNetY-4G [36] & \\(224^{2}\\) & 21M & 4.0G & 80.0 \\\\ RegNetY-8G [36] & \\(224^{2}\\) & 39M & 8.0G & 81.7 \\\\ RegNetY-16G [36] & \\(224^{2}\\) & 84M & 16.0G & 82.9 \\\\ \\hline EffNet-B3 [42] & \\(300^{2}\\) & 12M & 1.8G & 81.6 \\\\ EffNet-B4 [42] & \\(380^{2}\\) & 19M & 4.2G & 82.9 \\\\ EffNet-B5 [42] & \\(456^{2}\\) & 30M & 9.9G & 83.6 \\\\ EffNet-B6 [42] & \\(528^{2}\\) & 43M & 19.0G & 84.0 \\\\ \\hline ViT-B/16 [10] & \\(384^{2}\\) & 86M & 55.4G & 77.9 \\\\ ViT-L/16 [10] & \\(384^{2}\\) & 307M & 190.7G & 76.5 \\\\ \\hline DeiT-S [45] & \\(224^{2}\\) & 22M & 4.6G & 79.8 \\\\ DeiT-B [45] & \\(224^{2}\\) & 86M & 17.5G & 81.8 \\\\ DeiT-B [45] & \\(384^{2}\\) & 86M & 55.4G & 83.1 \\\\ \\hline Swin-T [28] & \\(224^{2}\\) & 29M & 4.5G & 81.3 \\\\ Swin-S [28] & \\(224^{2}\\) & 50M & 8.7G & 83.0 \\\\ Swin-B [28] & \\(224^{2}\\) & 88M & 15.4G & 83.5 \\\\ \\hline S4ND-ViT-B [35] & \\(224^{2}\\) & 89M & - & 80.4 \\\\ \\hline V Mamba-T & \\(224^{2}\\) & 22M & 4.5G & 82.2 \\\\ V Mamba-S & \\(224^{2}\\) & 44M & 9.1G & 83.5 \\\\ V Mamba-B & \\(224^{2}\\) & 75M & 15.2G & 83.2\\({}^{\\dagger}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: ** 정확성 비교는 이미지넷-1K의 다양한 모델에 걸쳐 비교된다. 심벌 \\({}^{\\dagger}\\)은 VMamba-B의 훈련 중에 버그가 접한다는 것을 나타내며, 대상 검출에서 \\(48.5\\%/49.7\\%\\) mAP를 달성하고 인스턴스 세분화에 대한 mIoU(43.2\\%/44.0\\%/44.0\\) mIoU를 달성하여 가까운 장래에 정확한 수를 업데이트할 것이다. 이러한 결과는 하류 조밀한 예측 작업에서 VMamba의 잠재력을 강조한다.\n' +
      '\n' +
      'ADE20K.\n' +
      '\n' +
      '** 설정** 플라이닝 스킨[28]은 미리 훈련된 모델 위에 우퍼헤드[50]를 구성합니다. 아담와 최적화기[30]를 사용하여 학습률을 \\(6\\t10^{-5}\\)로 설정하였다. 미세 조정 과정은 배치 크기의 \\(16\\)로 총 \\(160k\\) 반복에 걸쳐 있다. 채무불이행 입력 해상도는 \\(512\\시 512\\)이며, \\(640\\·640\\) 입력 및 다중 규모(MS) 테스트를 사용하여 실험 결과를 추가로 제시한다.\n' +
      '\n' +
      '** 결과*** 결과는 표 4에 나와 있으며, 특히 VMamba는 다중 규모(MS) 입력을 사용하여 \\(512\\tco 512\\) 및 \\(48.3\\%\\) mIoU의 해상도로 \\(47.3\\%\\) mIoU를 달성하는 VMamba-T 모델에서 우수한 정확도를 나타낸다. 이러한 점수는 ResNet[19], DeiT[45], Swin[28], ConvNeXt[29]를 포함한 모든 경쟁자를 능가한다. 특히, 장점이 \\(640\\times 640\\) 입력을 사용할 때에도 VMamba-S/B 모델로 확장된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c|c} \\hline \\hline \\multicolumn{8}{c}{**Mask R-CNN 1\\(\\times\\) schedule**} \\\\ \\hline Backbone & AP\\({}^{\\text{b}}\\) & AP\\({}^{\\text{b}}_{50}\\) & AP\\({}^{\\text{b}}_{75}\\) & AP\\({}^{\\text{m}}\\) & AP\\({}^{\\text{m}}_{50}\\) & AP\\({}^{\\text{m}}_{75}\\) & \\#param. & FLOPs \\\\ \\hline ResNet-50 & 38.2 & 58.8 & 41.4 & 34.7 & 55.7 & 37.2 & 44M & 260G \\\\ Swin-T & 42.7 & 65.2 & 46.8 & 39.3 & 62.2 & 42.2 & 48M & 267G \\\\ ConvNeXt-T & 44.2 & 66.6 & 48.3 & 40.1 & 63.3 & 42.8 & 48M & 262G \\\\ PVTv2-B2 & 45.3 & 67.1 & 49.6 & 41.2 & 64.2 & 44.4 & 45M & 309G \\\\ ViT-Adapter-S & 44.7 & 65.8 & 48.3 & 39.9 & 62.5 & 42.8 & 48M & 403G \\\\\n' +
      '**VMamba-T** & 46.5 & 68.5 & 50.7 & 42.1 & 65.5 & 45.3 & 42M & 262G \\\\ \\hline ResNet-101 & 38.2 & 58.8 & 41.4 & 34.7 & 55.7 & 37.2 & 63M & 336G \\\\ Swin-S & 44.8 & 66.6 & 48.9 & 40.9 & 63.2 & 44.2 & 69M & 354G \\\\ ConvNeXt-S & 45.4 & 67.9 & 50.0 & 41.8 & 65.2 & 45.1 & 70M & 348G \\\\ PVTv2-B3 & 47.0 & 68.1 & 51.7 & 42.5 & 65.7 & 45.7 & 65M & 397G \\\\\n' +
      '**VMamba-S** & 48.2 & 69.7 & 52.5 & 43.0 & 66.6 & 46.4 & 64M & 357G \\\\ \\hline Swin-B & 46.9 & - & - & 42.3 & - & - & 107M & 496G \\\\ ConvNeXt-B & 47.0 & 69.4 & 51.7 & 42.7 & 66.3 & 46.0 & 108M & 486G \\\\ PVTv2-B5 & 47.4 & 68.6 & 51.9 & 42.5 & 65.7 & 46.0 & 102M & 557G \\\\ ViT-Adapter-B & 47.0 & 68.2 & 51.4 & 41.8 & 65.1 & 44.9 & 102M & 557G \\\\\n' +
      '**VMamba-B** & 48.5 & 69.6 & 53.0 & 43.1 & 67.0 & 46.4 & 96M & 482G \\\\ \\hline \\multicolumn{8}{c}{**Mask R-CNN 3\\(\\times\\) MS schedule**} \\\\ \\hline Swin-T & 46.0 & 68.1 & 50.3 & 41.6 & 65.1 & 44.9 & 48M & 267G \\\\ ConvNeXt-T & 46.2 & 67.9 & 50.8 & 41.7 & 65.0 & 44.9 & 48M & 262G \\\\ PVTv2-B2 & 47.8 & 69.7 & 52.6 & 43.1 & 66.8 & 46.7 & 45M & 309G \\\\ ViT-Adapter-S & 48.2 & 69.7 & 52.5 & 42.8 & 66.4 & 45.9 & 48M & 403G \\\\ \\hline VMamba-T & 48.5 & 69.9 & 52.9 & 43.2 & 66.8 & 46.3 & 42M & 262G \\\\ \\hline Swin-S & 48.2 & 69.8 & 52.8 & 43.2 & 67.0 & 46.1 & 69M & 354G \\\\ ConvNeXt-S & 47.9 & 70.0 & 52.7 & 42.9 & 66.9 & 46.2 & 70M & 348G \\\\ PVTv2-B3 & 48.4 & 69.8 & 53.3 & 43.2 & 66.9 & 46.7 & 65M & 397G \\\\ \\hline VMamba-S & 49.7 & 70.4 & 54.2 & 44.0 & 67.6 & 47.3 & 64M & 357G \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: **객체 검출 및 COCO 데이터세트**에 대한 인스턴스 분할 결과는 표 3이다. FLOP는 크기 \\(1280\\10 800\\)의 입력을 사용하여 계산된다. 여기서 \\(AP^{b}\\)와 \\(AP^{m}\\)는 각각 박스 AP와 마스크 AP를 나타낸다. \'\\(1\\tents\\)\'는 12개의 epoch에 대해 미세 조정된 모델을 나타내는 반면, \'\\(3\\)MS\'는\\(36\\) epochs에 대한 다중 규모 훈련의 활용을 의미한다.\n' +
      '\n' +
      '### Analysis Experiments\n' +
      '\n' +
      '** 효과적인 수용 필드*** 다양한 모델에 걸쳐 효과적인 수용 필드(ERF) [32]를 평가하기 위해 그림 5의 비교 분석을 제시했으며 ERT는 출력에 관한 모델 입력의 중요성을 측정한다. 중앙 픽셀의 ERF를 입력 크기(1024\\ 시간 1024\\)로 시각화하여 VMamba와 4가지 두드러진 시각적 기반 모델인 ResNet50 [19], ConvNeXt-T[29], Swin-T [28], DeiT-S [45](ViT)를 훈련 전과 훈련 단계 모두에서 비교한다. 그림 5의 주요 관찰은 1) 온리 데이티(ViT)와 V 모바(VMamba)가 글로벌 ERF를 나타내는 반면, 다른 모델은 이론적 글로벌 잠재력에도 불구하고 지역 ERF를 보여준다. DiT(ViT) 모델은 2차 복잡성 비용(그림 6 참조)을 유발한다는 점에 유의하는 것이 중요하다. DiT와 대조적으로\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c|c c} \\hline \\hline method & crop size & mIoU (SS) & mIoU (MS) & \\#param. & FLOPs \\\\ \\hline ResNet-50 & \\(512^{2}\\) & 42.1 & 42.8 & 67M & 953G \\\\ DeiT-S + MLN & \\(512^{2}\\) & 43.8 & 45.1 & 58M & 1217G \\\\ Swin-T & \\(512^{2}\\) & 44.4 & 45.8 & 60M & 945G \\\\ ConvNeXt-T & \\(512^{2}\\) & 46.0 & 46.7 & 60M & 939G \\\\ \\hline VMamba-T & \\(512^{2}\\) & 47.3 & 48.3 & 55M & 939G \\\\ \\hline ResNet-101 & \\(512^{2}\\) & 42.9 & 44.0 & 85M & 1030G \\\\ DeiT-B + MLN & \\(512^{2}\\) & 45.5 & 47.2 & 144M & 2007G \\\\ Swin-S & \\(512^{2}\\) & 47.6 & 49.5 & 81M & 1039G \\\\ ConvNeXt-S & \\(512^{2}\\) & 48.7 & 49.6 & 82M & 1027G \\\\ \\hline VMamba-S & \\(512^{2}\\) & 49.5 & 50.5 & 76M & 1037G \\\\ \\hline Swin-B & \\(512^{2}\\) & 48.1 & 49.7 & 121M & 1188G \\\\ ConvNeXt-B & \\(512^{2}\\) & 49.1 & 49.9 & 122M & 1170G \\\\ \\hline VMamba-B & \\(512^{2}\\) & 50.0 & 51.3 & 110M & 1167G \\\\ \\hline \\hline Swin-S & \\(640^{2}\\) & 47.9 & 48.8 & 81M & 1614G \\\\ ConvNeXt-S & \\(640^{2}\\) & 48.8 & 48.9 & 82M & 1607G \\\\ \\hline VMamba-S & \\(640^{2}\\) & 50.8 & 50.8 & 76M & 1620G \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'UperNet [50]***를 사용하여 ADE20K에 대한 표 4: ** 의미 세분화 결과를 보여준다. UperNet[50]과 함께 ADE20K 데이터셋에 대한 의미 세분화의 성능을 평가한다. FLOP는 작물 크기에 따라 \\(512\\ 시간 2048\\) 또는 \\(640\\번호 2560\\)의 입력 크기로 계산된다. SS\'와 \'MS\'는 각각 단일 규모 및 다중 규모 테스트를 나타낸다.\n' +
      '\n' +
      '그림 5: ** 유효 기만 필드(ERF)**는 ResNet50 [19], ConvNeXt-T[29], Swin-T[28], DeiT-S[45](ViT) 및 제안된 VMamba-T에 대해 시각화된다. 더 큰 ERF는 보다 광범위하게 분포된 어두운 영역으로 표시된다. ***는 DeiT[45]와 제안된 VMamba만이 글로벌 ERF****를 나타낸다. 이러한 시각화에 대한 영감은 [32]에서 그려진다.\n' +
      '\n' +
      'VMamba는 주의 메커니즘을 사용하여 모든 픽셀을 고르게 활성화시키는 (ViT) 모든 픽셀을 활성화하고 교차 형태의 액티베이션을 특히 강조하고, 크로스-Scan Module의 스캐닝 메커니즘은 중심 픽셀이 십자가를 따라 픽셀의 영향을 가장 많이 받기 때문에 각 픽셀에 대한 로컬 정보에 대한 장거리 의존 컨텍스트를 우선시한다. 흥미롭게도 VMamba는 처음에 훈련 전에 지역 ERF만 나타낸다. 그러나 교육은 ERF를 전 세계로 변환한 후 모델의 글로벌 능력에서 적응 과정을 의미한다. 우리는 이러한 적응 과정이 모델의 이미지 인식 향상에 기여한다고 믿는다. 이것은 훈련 전 및 훈련 후 모두에서 거의 동일한 ERF를 유지하는 DeiT와 대조적이다.\n' +
      '\n' +
      '** 인풋 스칼링**는 그림 6과 같이 이미지넷-1K 및 FLOP에 대한 상위 1 정확도를 측정하는 입력 스케일링에 대한 실험을 진행하는데, 그림 6(a)에서 다양한 이미지 해상도(64\\(64\\, 1024\\)에서 \\(1024\\)로 변경)에 걸쳐 인기 모델(224\\(224\\)의 추론 성능을 평가한다. 상대방에 비해 VMamba는 다양한 입력 이미지 크기에 걸쳐 가장 안정적인 성능을 보여준다. 특히, 입력 크기가 \\(224\\·224\\)에서 \\(384\\지라도 384\\)로 증가함에 따라 VMamba만이 성능 상승 추세(VMamba-S 달성 \\(84\\%\\)를 나타내어 입력 이미지 크기의 변화에 대한 견고성을 강조한다. 그림 6(b)에서 우리는 서로 다른 이미지 해상도(64\\t 64\\)에서 \\(1024\\t1024\\)를 사용하여 FLOP를 평가한다. 예상대로 VMamba 시리즈는 CNN 모델과 정렬하여 복잡성의 선형 성장을 보고한다. VMamba의 복잡성은 Swin[28]과 같이 신중하게 설계된 비전 변압기와 일치한다. 그러나 VMamba만이 글로벌 효과적인 수용 분야(ERF)를 달성한다는 점에 유의할 필요가 있다. 글로벌 ERF 능력도 나타내는DeiT는 복잡성에서 2차 성장을 경험한다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '컨볼루션 신경네트웍스(CNN)와 비전트랜스포머(ViTs)는 시각적 표현 학습을 위한 주요 기반 모델을 나타낸다. CNN은 이미지 해상도와 관련하여 선형 복잡성을 나타내지만 ViT는 2차 복잡성에도 불구하고 피팅 능력에서 탁월하다. 우리의 조사는 ViT가 글로벌 수용 분야와 동적 가중치를 통해 우수한 시각적 모델링을 달성한다는 것을 보여준다. 이를 통해 글로벌 수용 분야를 희생하지 않고 선형 복잡성을 달성하기 위해 국가 공간 모델에서 영감을 얻은 비주얼 국가 공간 모델(VMamba)을 제안한다. 방향 민감도를 해결하기 위해 공간 횡단용 크로스-스캔 모듈(CSM)을 도입하여 비인과 시각적 이미지를 순서화된 패치 시퀀스로 변환한다. 광범위한 실험은 영상 해상도가 증가함에 따라 뚜렷한 장점이 있는 시각 과제에 걸쳐 VMamba의 유망한 성능을 보여주며 확립된 벤치마크를 능가한다.\n' +
      '\n' +
      '<그림 6> ** \\(224\\times 224\\) 입력**로 학습된 인기 모델에 대한 입력 스케일링 평가 비교. 우리는 \\(64\\시 64\\)에서 \\(1024\\시 1024\\)까지 다양한 입력 크기에 걸쳐 \\(224\\tco 224\\) 입력으로 훈련된 다양한 인기 모델의 성능(a) 및 FLOP(b)을 평가한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer for image classification. In _IEEE ICCV_, 2021.\n' +
      '* [2] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiraui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. Mmdetection: Open mmlab detection toolbox and benchmark. _arXiv preprint arXiv:1906.07155_, 2019.\n' +
      '* [3] Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, and Jiashi Feng. Dual path networks. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [4] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1251-1258, 2017.\n' +
      '* [5] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In _Proceedings of the IEEE international conference on computer vision_, pages 764-773, 2017.\n' +
      '* [6] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes. _NeurIPS_, 34:3965-3977, 2021.\n' +
      '* [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale hierarchical image database. In _IEEE CVPR_, pages 248-255, 2009.\n' +
      '* [8] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong Wang, and Lu Yuan. Davit: Dual attention vision transformers. In _European Conference on Computer Vision_, pages 74-92. Springer, 2022.\n' +
      '* [9] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In _IEEE CVPR_, pages 12124-12134, 2022.\n' +
      '* [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.\n' +
      '* [11] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry huppos: Towards language modeling with state space models. In _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* [12] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. _arXiv preprint arXiv:2312.00752_, 2023.\n' +
      '* [13] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with optimal polynomial projections. _Advances in neural information processing systems_, 33:1474-1487, 2020.\n' +
      '* [14] Albert Gu, Karan Goel, Ankit Gupta, and Christopher Re. On the parameterization and initialization of diagonal state space models. _Advances in Neural Information Processing Systems_, 35:35971-35983, 2022.\n' +
      '* [15] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In _International Conference on Learning Representations_, 2021.\n' +
      '* [16] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Re. Combining recurrent, convolutional, and continuous-time models with linear state space layers. _Advances in neural information processing systems_, 34:572-585, 2021.\n' +
      '* [17] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. _Advances in Neural Information Processing Systems_, 35:22982-22994, 2022.\n' +
      '* [18] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _IEEE CVPR_, pages 770-778, 2016.\n' +
      '\n' +
      '* [20] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. _arXiv preprint arXiv:1704.04861_, 2017.\n' +
      '* [21] Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Pointwise convolutional neural networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 984-993, 2018.\n' +
      '* [22] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4700-4708, 2017.\n' +
      '* [23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In _NeurIPS_, pages 1106-1114, 2012.\n' +
      '* [24] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. _Neural computation_, 1(4):541-551, 1989.\n' +
      '* [25] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.\n' +
      '* [26] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In _ECCV_, pages 740-755, 2014.\n' +
      '* [27] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12009-12019, 2022.\n' +
      '* [28] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _IEEE ICCV_, pages 10012-10022, 2021.\n' +
      '* [29] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11976-11986, 2022.\n' +
      '* [30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* [31] Jiasen Lu, Roozbeh Mottaghi, Aniruddha Kembhavi, et al. Container: Context aggregation networks. _NeurIPS_, 34:19160-19171, 2021.\n' +
      '* [32] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in deep convolutional neural networks. _Advances in neural information processing systems_, 29, 2016.\n' +
      '* [33] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. In _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* [34] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In _International Conference on Learning Representations_, 2023.\n' +
      '* [35] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher Re. S4nd: Modeling images and videos as multidimensional signals with state spaces. _Advances in neural information processing systems_, 35:2846-2861, 2022.\n' +
      '* [36] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollar. Designing network design spaces. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10428-10436, 2020.\n' +
      '* [37] Noam Shazeer. Glu variants improve transformer. _arXiv preprint arXiv:2002.05202_, 2020.\n' +
      '* [38] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.\n' +
      '* [39] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '\n' +
      '* Srinivas et al. [2021] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 16519-16529, 2021.\n' +
      '* Szegedy et al. [2015] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1-9, 2015.\n' +
      '* 탄과 Le[2019] 명화 탄과 Quoc V. 어. 효율적인 네트워크: 합성곱 신경망에 대한 레티션 모델 스케일링이다. _ICML_에서, 2019년 6105-6114쪽.\n' +
      '* Tay et al. [2022] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. volume 55, 2022.\n' +
      '* Tian et al. [2023] Yunjie Tian, Lingxi Xie, Zhaozhi Wang, Longhui Wei, Xiaopeng Zhang, Jianbin Jiao, Yaowei Wang, Qi Tian, and Qixiang Ye. Integrally pre-trained transformer pyramid networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18610-18620, 2023.\n' +
      '* Touvron et al. [2021] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _ICML_, pages 10347-10357, 2021.\n' +
      '* Vaswani et al. [2021] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon Shlens. Scaling local self-attention for parameter efficient visual backbones. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12894-12904, 2021.\n' +
      '* Wang et al. [2023] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. Selective structured state-spaces for long-form video understanding. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6387-6397, 2023.\n' +
      '* Wang et al. [2021] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In _ICCV_, pages 568-578, 2021.\n' +
      '* Wang et al. [2022] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt v2: Improved baselines with pyramid vision transformer. _Computational Visual Media_, 8(3):415-424, 2022.\n' +
      '* Xiao et al. [2018] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In _ECCV_, pages 418-434, 2018.\n' +
      '* Xie et al. [2017] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1492-1500, 2017.\n' +
      '* Yang et al. [2021] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers. _arXiv preprint arXiv:2107.00641_, 2021.\n' +
      '*유, 콜룬[2015] 피셔 유, 블라디렌콜튼. 확장된 컨볼루트에 의한 다중 규모의 컨텍스트 응집 __확장된 컨벌루션에 의한 다중 규모의 컨텍스트 집합이다. arXiv 프리프린트 arXiv:1511.07122_ 2015.\n' +
      '* Yuan et al. [2021] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In _ICCV_, pages 558-567, 2021.\n' +
      '* Zhang et al. [2015] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. _Advances in neural information processing systems_, 28, 2015.\n' +
      '* Zhang et al. [2023] Xiaosong Zhang, Yunjie Tian, Lingxi Xie, Wei Huang, Qi Dai, Qixiang Ye, and Qi Tian. Hivit: A simpler and more efficient design of hierarchical vision transformer. In _International Conference on Learning Representations_, 2023.\n' +
      '* Zhao et al. [2022] Weixi Zhao, Weiqiang Wang, and Yunjie Tian. Graformer: Graph-oriented transformer for 3d pose estimation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 20438-20447, 2022.\n' +
      '* Zhou et al. [2021] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer. _arXiv preprint arXiv:2103.11886_, 2021.\n' +
      '* 조프와 Le[2016] 바렛 조프 및 Quoc V Le입니다. 강화 학습으로 신경 아키텍처 검색 __신경 아키텍처 검색, __ 강화 학습으로 강화 아키텍처 검색. arXiv 프리프린트 arXiv:1611.01578_ 2016.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>