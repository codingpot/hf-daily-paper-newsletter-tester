<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'S4D[23] 및 S4D[23]은 특히 장거리 의존성을 모델링하는 데 광범위한 작업 및 양식에 걸쳐 시퀀스 데이터를 처리하도록 제안된다. 컨볼루션 계산과 근선형 계산 때문에 긴 시퀀스에서 효율적입니다. 2-D SSM[2], SGConvNeXt[37], ConvSSM[52]는 SSM과 CNN 또는 트랜스포머 구조를 결합하여 2-D 데이터를 처리한다. 최근 작품인 만바[20]는 SSM에 시간 가변 파라미터를 통합하고 효율적인 훈련과 추론을 가능하게 하기 위해 하드웨어 인식 알고리즘을 제안한다. 삼바의 우수한 스케일링 성능은 언어 모델링에서 트랜스퍼에 대한 유망한 대안임을 나타낸다. 그럼에도 불구하고, 비전 작업에 대한 일반적인 순수-SSM 기반 백본은 조사되지 않았다.\n' +
      '\n' +
      '비전트랜스포머(ViT)는 대규모 자기 지도 사전 훈련과 하류 활동에 대한 높은 성능 모두에서 우수한 시각적 표현 학습에서 큰 성공을 거두었다. 컨볼루션 신경망과 비교하여 핵심 이점은 ViT가 자기 의사를 통해 각각의 이미지 패치에 데이터/패치 의존적 글로벌 컨텍스트를 제공할 수 있다는 데 있다. 이것은 모든 위치에 대해 동일한 매개변수인 _i.e._ 콘볼루션 필터를 사용하는 컨볼루션 네트워크와 다르다. 또 다른 장점은 이미지를 2D 유도 편향 없이 패치 시퀀스로 처리하여 모달-진단 모델링이며, 이는 다중 모드 애플리케이션(3, 36, 40])에 대한 선호 아키텍처로 만든다. 동시에, 트랜스퍼의 자기 의도 메커니즘은 장거리 시각적 의존도, _e.g.__를 다룰 때 속도와 메모리 사용 측면에서 도전을 제기한다.\n' +
      '\n' +
      '바이올드 이스트레시, 아그레네 트라메스 오트니, 스캅스 아디노브 테네프, 스윕 스패티, 스터디 애프터스, 에비네트 퍼펙트, 스패스트 노린트, 스템포크, 스캅스 엠프티, 스캅스 아지네, 스캅스, 스패스트 리디엔티, 스캅스, 스패니즈 디프테미, 스캅스-마이코인, 스캅스, 스캅스, 스캅스, 스캅스, 스캅스, 스캅스(베드 이끼, 스패티, 스캅스, 스패티, 스캅스, 스패스트 인스페드 어시네, 스패스트 스패니즈 오브 에프터디 앤 스피어스, 스캅스, 스캅스, 스캅스, 스캅스, 스캅스, 스캅스, 스캅스, 스캅스, 스캅스, 스캅스, 스캅스텀스 Li ke Tran sforme rs, Vimc Sepretra - 스케일루 n슈퍼 바이 세비 수다 tafor는 tte rvisualrep resen tation입니다. 감사합니다.\n' +
      '\n' +
      'Compared with other SSM-based models for vision tasks, Vim is a pure-SSM-based method and models images in a sequence manner, which is more promising for a generic and efficient backbone. Thanks to the bidirectional compressing modeling with positional awareness, Vim is the first pure-SSM-based model to handle dense prediction tasks. Compared with the most convincing Transformer-based model, _i.e._, DeiT [60], Vim achieves superior performance on ImageNet classification. Furthermore, Vim is more efficient in terms of GPU memory and inference time for high-resolution images. The efficiency in terms of memory and speed empowers Vim to directly perform sequential visual representation learning without relying on 2D priors (such as the 2D local window in ViTDet [38]) for high-resolution visual understanding tasks while achieving higher accuracy than DeiT.\n' +
      '\n' +
      '우리의 주요 기여금은 다음과 같이 요약될 수 있다.\n' +
      '\n' +
      '* We propose Vision Mamba (Vim), which incorporates bidirectional SSM for data-dependent global visual context modeling and position embeddings for location-aware visual understanding.\n' +
      '* Without the need of attention, the proposed Vim has the same modeling power as ViT while it only has subquadratic-time computation and linear memory complexity. Specifically, our Vim is 2.8\\(\\times\\) faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images at the resolution of 1248\\(\\times\\)1248.\n' +
      '* We conduct extensive experiments on ImageNet classification and dense prediction downstream tasks. The results demonstrate that Vim achieves superior performance compared to the well-established and highly-optimized plain vision Transformer, _i.e._, DeiT.\n' +
      '* 삼바의 효율적인 하드웨어 인식 설계에서 요약하면 Vim은 고해상도 컴퓨터 비전 작업, _e._e._, 비디오 분할, 공중 이미지 분석, 의료 이미지 세분화, 계산 병리학에 대해 자기 의도 기반 DeiT[60]보다 훨씬 더 효율적이다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '일반 비전 백본을 위한 아키텍처 초기 귀에서 ConvNet[34]은 컴퓨터 비전을 위한 디-계면 표준 네트워크 설계 역할을 한다. 많은 합성곱 신경망[25, 26, 33, 50, 51, 56, 57, 58, 63, 72]이 다양한 시각적 응용 프로그램의 비전 백본으로 제안되었다. 개척작인 비전트랜스포머(ViT)[14]는 경관을 변화시킨다. 이미지를 평탄화 된 2D 패치의 시퀀스로 취급하고 순수한 변환 아키텍처를 직접 적용한다. 이미지 분류 및 스케일링 능력에 대한 ViT의 놀라운 결과는 많은 후속 작업[16, 59, 61, 62]을 장려한다. 한 줄의 작품은 2D 콘볼루션 사제들을 ViT[9, 13, 15, 69]에 도입하여 하이브리드 아키텍처 설계에 초점을 맞추고 있다. PVT[66]은 피라미드 구조 트랜스포머를 제안한다. 스윈 트랜스포머[42]는 시프트 윈도우 내에서 자기 의사를 적용한다. 또 다른 작품 라인은 보다 발전된 설정[41, 67]이 있는 전통적인 2D ConvNets 개선에 중점을 둔다. ConvNeXt[43]은 설계 공간을 검토하고 ViT 및 그 변이체로 확장 가능한 순수한 ConvNets를 제안한다. RepLKNet[12]는 개선을 가져오기 위해 기존 ConvNets의 커널 크기를 스케일링할 것을 제안한다.\n' +
      '\n' +
      'Though these dominant follow-up works demonstrate superior performance and better efficiency on ImageNet [10] and various downstream tasks [39, 74] by introducing 2D priors, with the surge of large-scale visual pretraining [1, 5, 17] and multi-modality applications [3, 29, 35, 36, 40, 49], vanilla Transformer-style model strikes back to the center stage of computer vision. The advantages of larger modeling capacity, unified multi-modality representation, being friendly to self-supervised learning, make it the preferred architecture. However, the number of visual tokens is limited due to the quadratic complexity of Transformer. There are plenty of works [7, 8, 11, 32, 48, 55, 65] to address this long-standing and prominent challenge, but few of them focus on visual applications. Recently, LongViT [68] built an efficient Transformer architecture for computational pathology applications via dilated attention. The linear computation complexity of LongViT allows it to encode the extremely long visual sequence. In this work, we draw inspiration from Mambo [20] and explore building a pure-SSM-based model as a generic vision backbone without using attention, while preserving the sequential, modality-agnostic modeling merit of ViT.\n' +
      '\n' +
      'State space models for long sequence modeling.[21] proposes a Structured State-Space Sequence (S4) model, a novel alternative to CNNs or Transformers, to model the long-range dependency. The promising property of linearly scaling in sequence length attracts further explorations. [53] proposes a new S5 layer by introducing MIMO SSM and efficient parallel scan into S4 layer. [18] designs a new SSM layer, H3, that nearly fills the performance gap between SSMs and Transformer attention in language modeling. [46] builds the Gated State Space layer on S4 by introducing more gating units to improve the expressivity. Recently, [20] proposes a data-dependent SSM layer and builds a generic language model backbone, Mamboa, which outperforms Transformers at various sizes on large-scale real data and enjoys linear scaling in sequence length. In this work, we explore transferring the success of Mamboa to vision,, building a generic vision backbone purely upon SSM without attention.\n' +
      '\n' +
      '시각적 응용을 위한 상태 공간 모델[27][27]] 시각 애플리케이션을 위한 상태 공간 모델. 영상 분류를 위한 장거리 시간적 의존성을 처리하기 위해 1D S4를 1D S4로 구성하여 비디오 분류를 위한 장거리 시간적 의존도를 처리한다[47][47]. 2D 이미지 및 3D 비디오를 포함하는 다차원 데이터를 처리하기 위해 1D S4를 추가 연장한다[28][28] S4의 강점과 트란S4mer 모델 구축을 위한 자구력을 결합시켜 영화 장면 검출을 위한 최첨단 성능을 달성하였다.[64] S4에 새로운 선택성 메커니즘을 도입하여 훨씬 낮은 메모리 발자국과 함께 긴 형태의 비디오 이해에 대한 S4의 성능을 크게 개선한다[73]]. 저렴한 계산 하에서 고해상도 이미지를 생성하고 미세 구성 표현을 처리하기 위해 보다 확장 가능한 SSM 기반 백본으로 공급자 주의 메커니즘.[45] 하이브리드 CNN-SSM 아키텍처인 U-람바다는 생물학적 이미지 세분화의 장거리 의존성을 다루기 위해 촉진된다. 상기 작업은 SSM을 특정 시각 애플리케이션에 적용하거나 SSM을 컨볼루션 또는 주의와 결합하여 하이브리드 아키텍처를 구축한다. 그들과 다른, 우리는 일반적인 비전 백본으로 채택될 수 있는 순수-SSM 기반 모델을 구축한다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '비전 마모바(Vim)의 목표는 첨단 상태 공간 모델(SSM), 마모보아[20]을 컴퓨터 비전에 도입하는 것이다. 이 섹션은 SSM의 예선에 대한 설명에서 시작된다. 그 다음은 Vim에 대한 개요가 뒤따른다. 그런 다음 Vim 블록이 입력 토큰 시퀀스를 처리하는 방법을 자세히 설명하고 Vim의 아키텍처 세부 사항을 설명하기 위해 진행한다. 섹션은 제안된 Vim의 효율성에 대한 분석으로 마무리된다.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      'SSM 기반 모델, 구조화된 상태 서열 모델(S4) 및 마모나는 연속 시스템에 의해 영감을 받아 1D 기능 또는 서열 \\(x(t)\\in\\mathbb{R}\\mapsto y(t)\\in\\mathbb{R}\\)을 숨겨진 상태 \\(h(t)\\in\\mathbb{R}^{\\bb{R}^{\\)를 통해 1-D 기능 또는 서열 \\(t)\\in\\mathbb{R}\\(t)\\in\\mathbb{R}\\(t)\\in\\in\\mathbb{R}\\in\\in\\mathbb{R}\\in\\in\\mathbb{R}\\)를 매핑하는 연속 시스템(t)\\in\\mathbb{R}^Mathbb{R}^Mathbb{R}^in\\in\\mathbb{R}^Mathb{R}^Mathb{R}^Mathb{R}^Mathb{R}^Mathb{R 이 시스템은 진화 매개변수로서\\(\\mathbbb{A}\\mathbb{R}\\mathbb{R}\\mathbb{N}\\mathbb{N}\\)를 사용하고 \\(\\mathbf{B}\\mathbb{B}\\in\\mathbb{B}\\in\\mathbb{B}\\mathb{B}\\mathbb{B}\\mathbb{B}\\mathbb{B}\\mathb{B}\\mathb{B}\\mathb{B}\\mathb{B}\\in\\mathb{B}\\in\\mathb{B}\\in\\mathb{B}\\in\\mathb{B}\\mathb{B}\\in\\mathb{B}\\mathb{B}\\mathb{B}\\mathb{B}\\mathb{B}\\in\\mathb{B}\\mathb{B}\\mathb{B}\\math\n' +
      '\n' +
      '\\[\\begin{split} h^{\\prime}(t)&=\\mathbf{A}h(t)+\\mathbf{A}h(t)+\\mathbf{B}}h(t),\\\\ y(t)&=\\mathbf{C}h(t)\n' +
      '\n' +
      'S4와 Mamboa는 연속 파라미터 \\(\\mathbf{\\Delta}\\)를 포함하여 연속 파라미터 \\(\\mathbf{A}\\), \\(\\mathbf{B}\\)를 이산 파라미터 \\(\\overline{\\mathbf{A}}\\), \\(\\overline{\\mathbf{B}\\), \\(\\overline{\\mathbf{B}\\), \\(\\overline{\\mathbf{B}\\), \\(\\overline{\\mathbf{B}\\)로 변환한다. 일반적으로 사용되는 변환 방법은 제로차 홀드(ZOH)로 다음과 같이 정의된다.\n' +
      '\n' +
      '\\\\\\mathbf{B}}}(\\mathline{\\mathbf{B}\\mathbf{B}}\\mathline{\\mathbf{A}\\mathbf{A}})^{-1}(\\math{{ 및\\mathbf{}\\mathbf{}\\mathbf{}\\mathbf{}\\mathbf{f}\\mathbf{f}\\mathbf{f}\\mathbf{f}\\mathbf{f}\\mathbf{f}\\mathbf{f}\\math{f}\\mathbf{f}\\mathbf{f}\\mathbf{f}\\mathbf{f}\\mathbf{f{f}\\math{f{f{f}\\mathbf{f{f}\\math{f{f{f}\\math{f{f{f{f{f{f{f{f{f{}\\ HPA(\\overline{\\mathbf{A}}\\), \\(\\overline{\\mathbf{B}}\\)의 폐기 후 Eq의 폐기된 버전이다. 단계 크기 \\(\\mathbf{\\Delta}\\)를 사용하는 (1)을 재작성할 수 있다.\n' +
      '\n' +
      '\\[\\begin{split} h_{t}=\\오버라인{\\mathbf{A}}}h_{t-1}+ \\overline{\\mathbf{B}}}}x_{t},\\\\ y_{t}－=\\mathbf{C}h_{t}.\n' +
      '\n' +
      '마지막으로 모델은 글로벌 컨벌루션을 통해 출력을 계산합니다.\n' +
      '\n' +
      '}\\math{f}\\math{{}\\math{f}\\math{{f}\n' +
      '\n' +
      'r\\(\\mathtt{M}\\)가 입력 서열 \\(\\mathbf{x}\\)의 길이이고, \\(\\overline{\\mathbf{K}}\\in\\mathbb{R}\\in\\mathtt{M}}\\)는 구조화된 컨볼루션 커널이다.\n' +
      '\n' +
      '### Vision Mamba\n' +
      '\n' +
      '제안된 Vim의 개요는 그림 2에 나와 있으며 표준 만바는 1D 서열을 위해 설계되었다. rmath{R}\\math{P}\\math{P}\\math{C}}\\math{P}\\math{C}}\\\\math{C}}\\로 전환되며, 여기서 우리는 먼저 2-D 패치(\\math{H}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{F}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\)의 크기,\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C} 다음으로 우리는\\(\\mathtt{D}}) 크기의 벡터에\\(\\mathbf{f}_{\\mathbf{p}}\\)를 선형적으로 프로젝트하고 위치 임베딩(\\mathbf{E}_{pos}\\mathb{{{{{(\\mathtt{I}+1)을 추가한다.\n' +
      '\n' +
      't}_{mathtt{t}} <\\bf{f}> <\\mathbf}> <\\mathbf}> <\\mathbf{f}>.\n' +
      '\n' +
      '\\(\\mathbf{t}_{p}^{\\tt{j}}\\)는 \\(\\mathtt{j}}\\), \\(\\mathbf{W}\\mathbb{R}\\in\\mathbb{R}^{{{{(\\mathtt{P}^{P}^{P}\\)의\\(\\mathtt{j}}\\)의\\(\\mathtt{j}}\\) 패치(\\mathbf{f{t}\\), \\(\\mathbf{W}\\mathbf{W}\\in\\in\\in\\in\\mathbbb{R}\\in\\in\\in\\in\\in\\mathbbb{R}\\in\\in\\in\\in\\mathbbb{R}\\in\\in\\in\\mathbbb{R}\\in\\in\\mathbbb{R}\\in\\in\\mathbbb{R}\\in\\mathbb{R}\\in\\ ViT[14] 및 BERT[31]에 의해 영감을 받아 그룹 토큰을 사용하여 \\(\\mathbf{t}_{cls}\\)로 표시되는 전체 패치 서열을 나타낸다. 그런 다음 토큰 서열(\\(\\mathbf{T}_{1-1}\\))을 Vim 인코더의 \\(\\mathtt{l}\\)-제 레이어로 보내고 출력 \\(\\mathbf{T}_{1}\\)를 얻는다. 마지막으로 출력 클래스 토큰 \\(\\mathbf{T}_{\\mathtt{L}}^{0}\\)를 정상화하고 멀티 레이어 퍼셉트론(MLP) 헤드에 공급하여 최종 예측 \\(\\hat{p}\\)를 얻으세요.\n' +
      '\n' +
      '}(\\mathbf{T})\\mathbf{f}(\\math{f}:\\math{f}:\\math{f})+\\mathbf{f}.\n' +
      '\n' +
      '\\(\\mathbf{Vim}\\)가 제안된 비전 만바 블록, \\(\\mathtt{L}\\)는 층의 수이고, \\(\\mathbf{Norm}\\)는 정규화 계층이다.\n' +
      '\n' +
      '```\n' +
      '입력: 토큰 서열 \\(\\mathbf{T}_{l-1}\\math{T}}:\\math{B}:\\math{B},\\mathtt{M},\\mathtt{D})을 정규화한다.\n' +
      '1\\(\\mathbf{T}_{l-1}^{\\prime}:(\\mathtt{B},\\mathtt{M},\\mathtt{D})\\leftarrow\\mathbf{ Norm}(\\mathbf{T}_{l-1})\\)\n' +
      '2\\(\\mathtt{x}:(\\mathtt{B},\\mathtt{M},\\mathtt{E})\\leftarrow\\mathbf{Linear}^{ \\mathbf{x}}(\\mathbf{T}_{l-1}^{\\prime})\\)\n' +
      '3\\(\\mathtt{B},\\mathtt{M},\\mathtt{E})는 서로 다른 방향 */f}^{ \\mathbf{x}}(\\mathbf{T}_{l-1}^{\\ime})로 처리한다.\n' +
      'P{ 포워드, 백워드}도 4foro입니다.\n' +
      '5\\(\\mathbf{x}_{o}^{\\prime}:(\\mathtt{B},\\mathtt{M},\\mathtt{E})\\leftarrow\\mathbf{ SiLU}(\\mathbf{Conv1d}_{o}(\\mathbf{x}))\\)\n' +
      '6\\(\\mathbf{B}_{o}:(\\mathtt{B},\\mathtt{M},\\mathtt{N})\\leftarrow\\mathbf{Linear}_{o}^{ \\mathtt{B}}(\\mathbf{x}_{o}^{\\prime})\\)\n' +
      '(\\mathbf{C},\\mathtt{M},\\mathtt{M},\\mathtt{N}_{o}d\\mathtt{C}}(\\mathbf{x}_{o})\n' +
      '\\log(1+\\log,\\math{B},\\math{{f}},\\math{{\\math{{f}}:\\math{{\\math{{f}})입니다.\n' +
      '9\\(\\overline{\\mathbf{\\Delta}}_{o}:(\\mathtt{B},\\mathtt{M},\\mathtt{E},\\mathtt{N}) \\leftarrow\\mathbf{\\Delta}_{o}\\bigotimes\\mathbf{Parameter}_{o}^{\\mathbf{A}}\\)\n' +
      '10\\(\\overline{\\mathbf{B}}_{o}:(\\mathtt{B},\\mathtt{M},\\mathtt{E},\\mathtt{N}) \\leftarrow\\mathbf{\\Delta}_{o}\\bigotimes\\mathbf{B}_{o}\\)\n' +
      '}(\\mathtt{M},\\math{f})\\mathbf{SSM}(I\\overline{\\mathbf{\\Delta}}_{o}}\\mathbf{f}_{o})(\\mathbf{x}_{o})\n' +
      '12\n' +
      '13 end for /* get gated \\(\\mathbf{y}_{o}\\) */\n' +
      '14\\(\\mathbf{y}_{forward}^{\\prime}:(\\mathtt{B},\\mathtt{M},\\mathtt{E})\\leftarrow \\mathbf{y}_{forward}\\bigodot\\mathbf{SiLU}(\\mathbf{z})\\)\n' +
      '15\\(\\mathbf{backward}^{\\prime): (\\{B},\\mathtt{M},\\mathtt{E})\n' +
      '}(\\math{T},\\math{M})}(\\mathbf{f}} <\\mathbf{f}_{backime})\n' +
      '```\n' +
      '\n' +
      '표 1*\n' +
      '\n' +
      '### Vim Block\n' +
      '\n' +
      '원래의 만바 블록은 1-D 시퀀스를 위해 설계되었으며, 이는 공간 인식 이해가 필요한 비전 작업에 적합하지 않다. 본 절에서는 비전 태스크에 대한 양방향 시퀀스 모델링을 포함하는 Vim 블록을 소개한다. 비임 블록은 그림 2에 나와 있다.\n' +
      '\n' +
      '구체적으로 알고 1에서 Vim 블록의 동작을 제시한다. 입력 토큰 서열 \\(\\mathbf{T}_{1-1}\\)는 정규화 계층에 의해 먼저 정규화된다. 다음으로, 우리는 정규화된 서열을 측정 크기 \\(E\\)를 갖는 \\(\\mathbf{x}\\) 및 \\(\\mathbf{z}\\)에 선형적으로 투영한다. 그런 다음, 우리는 전방 및 후방 방향에서 \\(\\mathbf{x}\\)를 처리한다. 각 방향에 대해 먼저 \\(\\mathbf{x}\\)에 1-D 컨볼루션을 적용하고 \\(\\mathbf{x}_{o}^{\\prime}\\)를 얻는다. 그런 다음\\(\\mathbf{x}_{o}^{\\prime}\\), \\(\\mathbf{B}_{o}_{o}\\), \\(\\mathbf{C}_{o}\\), \\(\\mathbf{\\Delta}_{o}_{o}\\)를 각각 선형적으로 프로젝트한다. 그런 다음 \\(\\mathbf{\\Delta}_{o}\\)를 사용하여 \\(\\overline{\\mathbf{A}}}_{o}\\), \\(\\overline{\\mathbf{B}}_{o}\\)를 각각 변환한다. 마지막으로 SSM을 통해 \\(\\mathbf{y}_{forward}\\) 및 \\(\\mathbf{y}_{backward}\\)를 계산한다. r\\(\\mathbf{y}_{forward}\\) 및 \\(\\mathbf{y}_{backward}\\)는 \\(\\mathbf{z}\\)에 의해 게이팅되고 함께 추가되어 출력 토큰 서열 \\(\\mathbf{T}_{1}\\)을 얻는다.\n' +
      '\n' +
      '### Architecture Details\n' +
      '\n' +
      '요약하면, 우리의 건축의 하이퍼 파라미터는 다음과 같이 나열된다.\n' +
      '\n' +
      'L: the number of blocks,\n' +
      '\n' +
      'D: 히든 상태 차원, D: 히든 상태 차원.\n' +
      '\n' +
      'E: 확장 상태 차원\n' +
      '\n' +
      'N: SSM 차원.\n' +
      '\n' +
      'VVT[14] 및 DeiT[61] 후, 우리는 먼저 16\\(표본)16 커널 크기 투영층을 사용하여 1D 시퀀스가 아닌 패치 임베딩을 얻는다. 이어서, 우리는 L Vim 블록을 직접 스택합니다. 기본적으로 블록 L의 수를 24, SSM 차원 N에서 16으로 설정하여 DeiT 시리즈의 모델 크기와 정렬하여 숨겨진 상태 차원 D를 192로 설정하고 작은 크기의 변이체에 대해 상태 차원 E를 384로 확장했다. 작은 크기의 변이체의 경우 D를 384 및 E에서 768으로 설정했다.\n' +
      '\n' +
      '### Efficiency Analysis\n' +
      '\n' +
      '그런 다음 Vi mg ai mfi mff}(\\text tt{B},\\ tf{B}},\\ tf{B}},\\math t{E}},\\mathb f{B}}},\\mathb f{B}}},\\mathn}})\\ S. SSR AM 및 리는 f\\((SS Mop PresSS MopypsSS)를 테스트한다.\n' +
      '\n' +
      'IO-Efficiency.The high bandwidth memory (HBM) and SRAM are two important components for GPUs. Among them, SRAM has a larger bandwidth and HBM has a bigger memory size. The standard implementation of Vim\'s SSM operation with HBM requires the number of memory IO on the order of \\(O(\\texttt{BME})\\). Inspired by Mamba, Vim first reads in \\(O(\\texttt{BME}+\\texttt{EN})\\) bytes of memory \\((\\mathbf{\\Delta_{o}},\\mathbf{\\Delta_{o}},\\mathbf{B_{o}},\\mathbf{C_{o}})\\) from slow HBM to fast SRAM. Then, Vim gets the discrete \\(\\overline{\\mathbf{A}_{o}}\\), \\(\\overline{\\mathbf{B}_{o}}\\) of a size of \\((\\texttt{B},\\texttt{H},\\texttt{E},\\texttt{N})\\) in SRAM. Last, Vim performs SSM operations in SRAM and writes the output of a size of \\((\\texttt{B},\\texttt{M},\\texttt{E})\\) back to HBM. This method can help to reduce IOs from \\(O(\\texttt{BME})\\) to \\(O(\\texttt{BME}+\\texttt{EN})\\).\n' +
      '\n' +
      '메모리 효율성은 메모리 외 문제를 피하고 긴 서열을 다룰 때 더 낮은 메모리 사용을 달성하기 위해 Vim은 Mamba와 동일한 재입력 방법을 선택한다. 크기 \\((\\texttt{B},\\texttt{M},\\texttt{E},\\texttt{N})\\)의 중간 상태에 대해 구배를 계산하기 위해 Vim은 네트워크 백워드 패스에서 그것들을 재입력한다. 활성화 함수 및 컨볼루션의 출력과 같은 중간 액티베이션에 대해, Vim은 또한 활성화 값이 많은 메모리를 취하지만 재선택을 위해 빠르기 때문에 GPU 메모리 요구 사항을 최적화하기 위해 이를 재입력한다.\n' +
      '\n' +
      '비임 블록에서의 컴퓨터-효율성(알고 1의 라인 11)과 트랜스포머에 대한 자기 의도는 모두 적응적으로 글로벌 컨텍스트를 제공하는 데 중요한 역할을 한다. 시각적 서열 \\(\\mathbf{T}\\in R^{1\\in\\texttt{M}\\tcer\\texttt{D}}\\\\)와 디폴트 설정 \\(\\texttt{E}=\\texttt{2D}\\)을 감안할 때, 글로벌 자기 의도 및 SSM의 계산 복잡성은 마찬가지이다.\n' +
      '\n' +
      'SF{M}.\n' +
      '\n' +
      '자기 의도가 2차에서 서열 길이 M이고, SSM은 서열 길이 M(N은 고정 파라미터이고 디폴트로 16으로 설정됨)에 선형이다. 계산 효율은 시퀀스 길이가 큰 기가픽셀 애플리케이션에 대해 Vim 스케일링을 가능하게 한다.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '### Image Classification\n' +
      '\n' +
      '세트 설정은 1,000개 범주의 1.28M 훈련 이미지와 50K 검증 이미지를 포함하는 이미지넷-1K 데이터세트[10]에 Vim을 벤치마킹한다. 모든 모델은 트레이닝 세트에 대해 훈련되며 검증 세트에 대한 상위 1의 정확도가 보고된다. 공정 비교를 위해 교육 설정은 주로 DeiT[61]를 따른다. 구체적으로, 우리는 데이터 증가에 따라 랜덤 크로핑, 랜덤 수평 플핑, 레이블-스모딩 정규화, 믹싱 및 랜덤 소거를 적용한다. I\\(224^{2}\\) 입력 이미지에 대해 훈련할 때, 우리는 모델 최적화를 위해 \\(0.9\\), 총 배치 크기 \\(1024\\) 및 중량 붕괴(0.05\\)의 운동량으로 AdamW[44]를 사용한다. 우리는 코사인 일정, \\(1\\, 10^{-3}\\) 초기 학습률 및 EMA를 사용하여 \\(300\\) epoch에 대한 Vim 모델을 훈련시킨다. 테스트하는 동안 우리는 작물 아웃(224^{2}\\) 이미지에 대한 검증 세트에 중심 작물을 적용한다. 실험은 8 A800 GPU에 대해 수행된다.\n' +
      '\n' +
      '그림 2: 제안된 Vim 모델의 개요. 먼저 입력 영상을 패치로 나눈 후 패치 토큰으로 투사합니다. 마지막으로 제안된 Vim 인코더에 토큰의 시퀀스를 보냅니다. 이미지넷 분류를 수행하기 위해 패치 토큰 시퀀스에 추가 학습 가능한 분류 토큰을 연결한다. 텍스트 서열 모델링을 위해 만바와는 다른 Vim 인코더는 토큰 시퀀스를 전후방향으로 처리한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '* 단방향 서킷. 훈련 중 시각적 순서를 무작위로 뒤집습니다. 이것은 데이터 증강과 같은 역할을 합니다.\n' +
      '* 비방향 블록. 우리는 적층된 블록을 페어링합니다. 각 쌍의 제1 블록은 전방 방향으로 시각적 시퀀스를 처리하고, 각 쌍의 제2 블록은 후방 방향으로 처리한다.\n' +
      '* 비방향 SSM. 우리는 시각 시퀀스를 후방 방향으로 처리하기 위해 각 블록에 대한 추가 SSM을 추가한다.\n' +
      '* 비방향 SSM + Conv1d. 비방향 SSM을 기반으로 후진 SSM 전에 후방 Conv1d를 추가한다(그림 2).\n' +
      '\n' +
      '타브에서 보는 바와 같이. 4, 만바 블록을 직접 채택하면 분류에서 좋은 성능을 얻을 수 있다. 그러나 부자연스러운 단방향 방식은 하류 조밀한 예측에서 문제를 야기한다. 구체적으로, 비방향블록을 사용하는 예비 양방향 전략은 분류에 대한 상위 1의 정확도가 낮은 7점을 달성한다. 그러나 의미 세분화에 대해 바닐라 단방향 만바 블록을 1.3 mIoU 능가한다. 추가 후방 SSM 및 Conv1d를 추가하여 유사한 분류 정확도(73.1 탑-1 acc _vs_ 73.2 탑-1 acc)를 달성했다. 특수 분할 우위와 예외적인 분할 우성(34.8 mIoU _vs_ 32.3 mIoU)이다. 우리는 바이방향 SSM + Conv1d의 전략을 Vim 블록에서 기본 설정으로 사용한다.\n' +
      '\n' +
      '5배제 및 미래 근무.\n' +
      '\n' +
      'We have proposed Vision Mamba (Vim) to explore the very recent efficient state space model, _i.e._, Mamba, as generic vision backbones. Unlike prior state space models for vision tasks which use hybrid architecture or equivalent global 2D convolutional kernel, Vim learns visual representation in the sequence modeling manner and does not introduce image-specific inductive biases. Thanks to the proposed bidirectional state space modeling, Vim achieves data-dependent global visual context and enjoys the same modeling power as Transformer, while having lower computation complexity. Benefiting from the hardware-aware designs of Mamba, the inference speed and memory usage of Vim are significantly better than ViTs when processing high-resolution images. Experiment results on standard computer vision benchmarks have verified the modeling power and high efficiency of Vim, showing that Vim has great potential to be the next-generation vision backbone.\n' +
      '\n' +
      '세르브 아메메스 디펜트 리네네, 우르멘타 알펜타 루트 헤르메드 이너너트 로즈 에크 애프터 엘-토노우드 인드 우드 우드 우드 우드 우드 우드 우드 우드 등 1초와 달리 아이솔러스의 아세모젠스 또는 비시스트 스캅스 스칼라 트라메스 스웨이트 리메메스 스레즈 오크인드 호르메스 아흐메스 스물스 오브 레즈 리네르메스 스웨스트 리네르메메스 스웨스트 리네르메스 스웨스트 리네르메드 라이스의 시들스, 베드 이스트 리네트 리메메드 이끼는 서클레스 리메드 리네르메스 스터스 스터스 스웨이트 리메드 리메메드 리네르메스 스웨이트 리메드 리네르메스 스웨이트 리메드 리네르메스 스터스 디 디엔트 리메드 리슈트 리네트 리네트 리네트 리네트 리네트 리네트 리네\n' +
      '\n' +
      '## Acknowledgement\n' +
      '\n' +
      '초안에 대한 도움이 되는 피드백을 위해 톈정청, 유신포랑, 신정양, 보장, 진펑야오 등을 인정하고자 합니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: BERT pre-training of image transformers. In _ICLR_, 2022.\n' +
      '* [2] Ethan Baron, Itamar Zimerman, and Lior Wolf. 2-d ssm: A general spatial layer for visual transformers. _arXiv preprint arXiv:2306.06635_, 2023.\n' +
      '* [3] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sagnak Tasirlar. Introducing our multimodal models, 2023.\n' +
      '* [4] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: High quality object detection and instance segmentation. _TPAMI_, 2019.\n' +
      '* [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _ICCV_, 2021.\n' +
      '* [6] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In _ECCV_, 2018.\n' +
      '\n' +
      '그림 5: Cascade Mask R-CNN [4] 프레임워크에서 DeiT-Ti[61]과 Vim-Ti의 표준화 비교는 그림 5이다. SSM의 장거리 맥락 학습 덕분에 DeiT-Ti 상대가 인지하지 못하는 영상에서 매우 큰 객체를 캡처할 수 있다.\n' +
      '\n' +
      '* [7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. _arXiv preprint arXiv:1904.10509_, 2019.\n' +
      '* [8] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In _ICLR_, 2021.\n' +
      '* [9] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes. _NeurIPS_, 34, 2021.\n' +
      '* [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, 2009.\n' +
      '* [11] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. _arXiv preprint arXiv:2307.02486_, 2023.\n' +
      '* [12] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In _CVPR_, 2022.\n' +
      '* [13] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In _CVPR_, 2022.\n' +
      '* [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2020.\n' +
      '* [15] Stephane d\'Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. In _ICML_, 2021.\n' +
      '* [16] Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu Liu, and Qi Tian. Msg-transformer: Exchanging local spatial information by manipulating messenger tokens. In _CVPR_, 2022.\n' +
      '* [17] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In _CVPR_, 2023.\n' +
      '* [18] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hupps: Towards language modeling with state space models. In _ICLR_, 2023.\n' +
      '* [19] Golanz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. In _CVPR_, 2021.\n' +
      '* [20] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. _arXiv preprint arXiv:2312.00752_, 2023.\n' +
      '* [21] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. _arXiv preprint arXiv:2111.00396_, 2021.\n' +
      '* [22] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Re. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In _NeurIPS_, 2021.\n' +
      '* [23] Albert Gu, Karan Goel, Ankit Gupta, and Christopher Re. On the parameterization and initialization of diagonal state space models. In _NeurIPS_, 2022.\n' +
      '* [24] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In _NeurIPS_, 2022.\n' +
      '* [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, 2016.\n' +
      '* [26] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _CVPR_, 2017.\n' +
      '* [27] Md Mohaimul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In _ECCV_, 2022.\n' +
      '* [28] Md Mohaimul Islam, Mahmudul Hasan, Kishan Shamsundar Athrey, Tony Braskich, and Gedas Bertasius. Efficient movie scene detection using state-space transformers. In _CVPR_, 2023.\n' +
      '* [29] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _ICML_, 2021.\n' +
      '* [30] Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. 1960.\n' +
      '* [31] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _NAACL-HLT_, 2019.\n' +
      '* [32] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In _ICLR_, 2020.\n' +
      '* [33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In _NeurIPS_, 2012.\n' +
      '* [34] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.\n' +
      '* [35] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _ICML_, 2022.\n' +
      '* [36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.\n' +
      '* [37] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? In _ICLR_, 2022.\n' +
      '* [38] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. In _ECCV_, 2022.\n' +
      '\n' +
      '* [39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _ECCV_, 2014.\n' +
      '* [40] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.\n' +
      '* [41] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Tommi Karkkainen, Mykola Pechenizkiy, Decebal Mocanu, and Zhangyang Wang. More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. _arXiv preprint arXiv:2207.03620_, 2022.\n' +
      '* [42] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _ICCV_, 2021.\n' +
      '* [43] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In _CVPR_, 2022.\n' +
      '* [44] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _ICLR_, 2019.\n' +
      '* [45] Jun Ma, Feifei Li, and Bo Wang. U-mamba: Enhancing long-range dependency for biomedical image segmentation. _arXiv preprint arXiv:2401.04722_, 2024.\n' +
      '* [46] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In _ICLR_, 2023.\n' +
      '* [47] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher Re. S4nd: Modeling images and videos as multidimensional signals with state spaces. In _NeurIPS_, 2022.\n' +
      '* [48] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. In _NeurIPS_, 2023.\n' +
      '* [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.\n' +
      '* [50] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollar. Designing network design spaces. In _CVPR_, 2020.\n' +
      '* [51] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.\n' +
      '* [52] Jimmy TH Smith, Shalini De Mello, Jan Kautz, Scott Linderman, and Wonmin Byeon. Convolutional state space models for long-range spatiotemporal modeling. In _NeurIPS_, 2023.\n' +
      '* [53] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In _ICLR_, 2023.\n' +
      '* [54] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In _ICCV_, 2021.\n' +
      '* [55] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. _arXiv preprint arXiv:2307.08621_, 2023.\n' +
      '* [56] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In _CVPR_, 2015.\n' +
      '* [57] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In _ICML_, 2019.\n' +
      '* [58] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In _ICML_, 2021.\n' +
      '* [59] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. In _NeurIPS_, 2021.\n' +
      '* [60] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _ICML_, 2021.\n' +
      '* [61] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _ICML_, 2021.\n' +
      '* [62] Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaedin El-Nouby, Edouard Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, et al. Resmlp: Feedforward networks for image classification with data-efficient training. _TPAMI_, 2022.\n' +
      '* [63] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. _TPAMI_, 2020.\n' +
      '* [64] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. Selective structured state-spaces for long-form video understanding. In _CVPR_, 2023.\n' +
      '* [65] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.\n' +
      '* [66] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In _ICCV_, 2021.\n' +
      '* [67] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Intermimage: Exploring large-scale vision foundation models with deformable convolutions. In _CVPR_, 2023.\n' +
      '* [68] Wenhui Wang, Shuming Ma, Hanwen Xu, Naoto Usuyama, Jiayu Ding, Hoifung Poon, and Furu Wei. When an image is worth 1,024 x 1,024 words: A case study in computational pathology. _arXiv preprint arXiv:2312.03558_, 2023.\n' +
      '* [69] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In _ICCV_, 2021.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '* [71] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In _ECCV_, 2018.\n' +
      '* [72] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In _CVPR_, 2017.\n' +
      '* [73] Jing Nathan Yan, Jiatao Gu, and Alexander M Rush. Diffusion models without attention. _arXiv preprint arXiv:2311.18257_, 2023.\n' +
      '* [74] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. _IJCV_, 2019.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>