<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'and S4D [23], are proposed to process sequence data across a wide range of tasks and modalities, particularly on modeling long-range dependencies. They are efficient on long sequences because of convolutional computation and near-linear computation. 2-D SSM [2], SGConvNeXt [37], and ConvSSM [52] combine SSM with CNN or Transformer architecture to process 2-D data. The recent work, Mamba [20], incorporates time-varying parameters into the SSM and proposes a hardware-aware algorithm to enable efficient training and inference. The superior scaling performance of Mamba indicates that it is a promising alternative to Transformer in language modeling. Nevertheless, a generic pure-SSM-based backbone has not been explored for vision tasks.\n' +
      '\n' +
      'Vision Transformers (ViTs) have achieved great success in visual representation learning, excelling in both large-scale self-supervised pre-training and high performance on downstream tasks. Compared with convolutional neural networks, the core advantage lies in that ViT can provide each image patch with data/patch-dependent global context through self-attention. This differs from convolutional networks that use the same parameters, _i.e._, the convolutional filters, for all positions. Another advantage is the modality-agnostic modeling by treating an image as a sequence of patches without 2D inductive bias, which makes it the preferred architecture for multimodal applications [3, 36, 40]. At the same time, the self-attention mechanism in Transformers poses challenges in terms of speed and memory usage when dealing with long-range visual dependencies, _e.g._, processing high-resolution images.\n' +
      '\n' +
      'Motivated by the success of Mamba in language modeling, it is appealing that we can also transfer this success from language to vision, _i.e._, to design a generic and efficient visual backbone with the advanced SSM method. However, there are two challenges for Mamba, _i.e._, unidirectional modeling and lack of positional awareness. To address these challenges, we propose the Vision Mamba (Vim) block, which incorporates the bidirectional SSMs for data-dependent global visual context modeling and position embeddings for location-aware visual recognition. We first split the input image into patches and linearly project them as vectors to Vim. Image patches are treated as the sequence data in Vim blocks, which efficiently compresses the visual representation with the proposed bidirectional selective state space. Furthermore, the position embedding in Vim block provides the awareness for spatial information, which enables Vim to be more robust in dense prediction tasks. In the current stage, we train the Vim model on the supervised image classification task using the ImageNet dataset and then use the pretrained Vim as the backbone to perform sequential visual representation learning for downstream dense prediction tasks, _i.e._, semantic segmentation, object detection, and instance segmentation. Like Transformers, Vim can be pretrained on large-scale unsupervised visual data for better visual representation. Thanks to the better efficiency of Mamba, the large-scale pretraining of Vim can be achieved with lower computational cost.\n' +
      '\n' +
      'Compared with other SSM-based models for vision tasks, Vim is a pure-SSM-based method and models images in a sequence manner, which is more promising for a generic and efficient backbone. Thanks to the bidirectional compressing modeling with positional awareness, Vim is the first pure-SSM-based model to handle dense prediction tasks. Compared with the most convincing Transformer-based model, _i.e._, DeiT [60], Vim achieves superior performance on ImageNet classification. Furthermore, Vim is more efficient in terms of GPU memory and inference time for high-resolution images. The efficiency in terms of memory and speed empowers Vim to directly perform sequential visual representation learning without relying on 2D priors (such as the 2D local window in ViTDet [38]) for high-resolution visual understanding tasks while achieving higher accuracy than DeiT.\n' +
      '\n' +
      '우리의 주요 기여금은 다음과 같이 요약될 수 있다.\n' +
      '\n' +
      '* We propose Vision Mamba (Vim), which incorporates bidirectional SSM for data-dependent global visual context modeling and position embeddings for location-aware visual understanding.\n' +
      '* Without the need of attention, the proposed Vim has the same modeling power as ViT while it only has subquadratic-time computation and linear memory complexity. Specifically, our Vim is 2.8\\(\\times\\) faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images at the resolution of 1248\\(\\times\\)1248.\n' +
      '* 우리는 이미지넷 분류 및 조밀한 예측 다운스트림 작업에 대한 광범위한 실험을 수행한다. 결과는 Vim이 잘 정립되고 고도로 최적화 된 일반 비전 트랜스포머인 _i.e._, DeiT에 비해 우수한 성능을 달성한다는 것을 보여준다.\n' +
      '* Benefiting from the efficient hardware-aware design of Mamba, Vim is much more efficient than the self-attention-based DeiT [60] for high-resolution computer vision tasks, _e.g._, video segmentation, aerial image analysis, medical image segmentation, computational pathology.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      'Architectures for generic vision backbone.In the early ears, ConvNet [34] serves as the de-facto standard network design for computer vision. Many convolutional neural architectures [25, 26, 33, 50, 51, 56, 57, 58, 63, 72] have been proposed as the vision backbone for various visual applications. The pioneering work, Vision Transformer (ViT) [14] changes the landscape. It treats an image as a sequence of flattened 2D patches and directly applies a pure Transformerarchitecture. The surprising results of ViT on image classification and its scaling ability encourage a lot of follow-up works [16, 59, 61, 62]. One line of works focuses on hybrid architecture designs by introducing 2D convolutional priors into ViT [9, 13, 15, 69]. PVT [66] proposes a pyramid structure Transformer. Swin Transformer [42] applies self-attention within shift windows. Another line of works focuses on improving traditional 2D ConvNets with more advanced settings [41, 67]. ConvNeXt [43] reviews the design space and proposes pure ConvNets, which can be scalable as ViT and its variants. RepLKNet [12] proposes to scale up the kernel size of existing ConvNets to bring improvements.\n' +
      '\n' +
      'Though these dominant follow-up works demonstrate superior performance and better efficiency on ImageNet [10] and various downstream tasks [39, 74] by introducing 2D priors, with the surge of large-scale visual pretraining [1, 5, 17] and multi-modality applications [3, 29, 35, 36, 40, 49], vanilla Transformer-style model strikes back to the center stage of computer vision. The advantages of larger modeling capacity, unified multi-modality representation, being friendly to self-supervised learning, make it the preferred architecture. However, the number of visual tokens is limited due to the quadratic complexity of Transformer. There are plenty of works [7, 8, 11, 32, 48, 55, 65] to address this long-standing and prominent challenge, but few of them focus on visual applications. Recently, LongViT [68] built an efficient Transformer architecture for computational pathology applications via dilated attention. The linear computation complexity of LongViT allows it to encode the extremely long visual sequence. In this work, we draw inspiration from Mambo [20] and explore building a pure-SSM-based model as a generic vision backbone without using attention, while preserving the sequential, modality-agnostic modeling merit of ViT.\n' +
      '\n' +
      '긴 서열 모델링을 위한 상태 공간 모델[21][21]. CNN 또는 트랜스퍼러에 대한 새로운 대안인 구조화된 국가-공간 서열(S4) 모델을 제공하여 장거리 의존성을 모델링한다. 서열 길이의 선형 스케일링의 유망한 특성은 추가 탐색을 끌어낸다.[53] MIMO SSM과 효율적인 병렬 스캔을 S4 레이어에 도입하여 새로운 S5 레이어를 활성화한다[18][18] 언어 모델링에서 SSM과 트랜스포머 주의력의 성능 격차를 거의 채워주는 새로운 SSM 레이어인 H3를 디자인한다[46][46] 표현성을 향상시키기 위해 더 많은 게이팅 유닛을 도입하여 S4에 Gated 스테이트 스페이스 레이어를 제작한다. 최근 [20]은 데이터 의존적 SSM 레이어를 제안하고, 대규모 실제 데이터에 다양한 크기의 트랜스포머를 능가하고 서열 길이의 선형 스케일링을 즐기는 일반 언어 모델 백본인 마모바를 구축한다. 이 작품에서 우리는 마모바의 성공을 비전으로 옮기고, 주의를 기울이지 않고 순수 SSM에서 일반 비전 백본을 구축하는 것을 탐구한다.\n' +
      '\n' +
      'State space models for visual applications.[27] uses 1D S4 to handle the long-range temporal dependencies for video classification. [47] further extends 1D S4 to handle multi-dimensional data including 2D images and 3D videos. [28] combines the strengths of S4 and self-attention to build TranS4mer model, achieving state-of-the-art performance for movie scene detection. [64] introduces a novel selectivity mechanism to S4, largely improving the performance of S4 on long-form video understanding with a much lower memory footprint. [73] supplants attention mechanisms with a more scalable SSM-based backbone to generate high-resolution images and process fine-grained representation under affordable computation. [45] proposes U-Lambda, a hybrid CNN-SSM architecture, to handle the long-range dependencies in biomedical image segmentation. The above works either apply SSM to specific visual applications or build a hybrid architecture by combining SSM with convolution or attention. Different from them, we build a pure-SSM-based model, which can be adopted as a generic vision backbone.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      'The goal of Vision Mamboa (Vim) is to introduce the advanced state space model (SSM),, Mamboa [20], to computer vision. This section begins with a description of the preliminaries of SSM. It is followed by an overview of Vim. We then detail how the Vim block processes input token sequences and proceed to illustrate the architecture details of Vim. The section concludes with an analysis of the efficiency of the proposed Vim.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      'SSM 기반 모델, 구조화된 상태 서열 모델(S4) 및 마모나는 연속 시스템에 의해 영감을 받아 1D 기능 또는 서열 \\(x(t)\\in\\mathbb{R}\\mapsto y(t)\\in\\mathbb{R}\\)을 숨겨진 상태 \\(h(t)\\in\\mathbb{R}^{\\bb{R}^{\\)를 통해 1-D 기능 또는 서열 \\(t)\\in\\mathbb{R}\\(t)\\in\\mathbb{R}\\(t)\\in\\in\\mathbb{R}\\in\\in\\mathbb{R}\\in\\in\\mathbb{R}\\)를 매핑하는 연속 시스템(t)\\in\\mathbb{R}^Mathbb{R}^Mathbb{R}^in\\in\\mathbb{R}^Mathb{R}^Mathb{R}^Mathb{R}^Mathb{R}^Mathb{R 이 시스템은 진화 매개변수로서\\(\\mathbbb{A}\\mathbb{R}\\mathbb{R}\\mathbb{N}\\mathbb{N}\\)를 사용하고 \\(\\mathbf{B}\\mathbb{B}\\in\\mathbb{B}\\in\\mathbb{B}\\mathb{B}\\mathbb{B}\\mathbb{B}\\mathbb{B}\\mathb{B}\\mathb{B}\\mathb{B}\\mathb{B}\\in\\mathb{B}\\in\\mathb{B}\\in\\mathb{B}\\in\\mathb{B}\\mathb{B}\\in\\mathb{B}\\mathb{B}\\mathb{B}\\mathb{B}\\mathb{B}\\in\\mathb{B}\\mathb{B}\\mathb{B}\\math\n' +
      '\n' +
      '\\[\\begin{split} h^{\\prime}(t)&=\\mathbf{A}h(t)+\\mathbf{A}h(t)+\\mathbf{B}}h(t),\\\\ y(t)&=\\mathbf{C}h(t)\n' +
      '\n' +
      'The S4 and Mamboa are the discrete versions of the continuous system, which include a timescale parameter \\(\\mathbf{\\Delta}\\) to transform the continuous parameters \\(\\mathbf{A}\\), \\(\\mathbf{B}\\) to discrete parameters \\(\\overline{\\mathbf{A}}\\), \\(\\overline{\\mathbf{B}}\\). The commonly used method for transformation is zero-order hold (ZOH), which is defined as follows:\n' +
      '\n' +
      '\\[\\begin{split}\\overline{\\mathbf{A}}&=\\exp{(\\mathbf{ \\Delta}\\mathbf{A})},\\\\ \\overline{\\mathbf{B}}&=(\\mathbf{\\Delta}\\mathbf{A})^{- 1}(\\exp{(\\mathbf{\\Delta}\\mathbf{A})}-\\mathbf{I})\\cdot\\mathbf{\\Delta}\\mathbf{B}.\\end{split} \\tag{2}\\]After the discretization of \\(\\overline{\\mathbf{A}}\\), \\(\\overline{\\mathbf{B}}\\), the discretized version of Eq. (1) using a step size \\(\\mathbf{\\Delta}\\) can be rewritten as:\n' +
      '\n' +
      '\\[\\begin{split} h_{t}=\\오버라인{\\mathbf{A}}}h_{t-1}+ \\overline{\\mathbf{B}}}}x_{t},\\\\ y_{t}－=\\mathbf{C}h_{t}.\n' +
      '\n' +
      'At last, the models compute output through a global convolution.\n' +
      '\n' +
      '}\\mathf{}\\math{{}\\math{{}\\math{{\n' +
      '\n' +
      'r\\(\\mathtt{M}\\)가 입력 서열 \\(\\mathbf{x}\\)의 길이이고, \\(\\overline{\\mathbf{K}}\\in\\mathbb{R}\\in\\mathtt{M}}\\)는 구조화된 컨볼루션 커널이다.\n' +
      '\n' +
      '### Vision Mamba\n' +
      '\n' +
      '제안된 Vim의 개요는 그림 2에 나와 있으며 표준 만바는 1D 서열을 위해 설계되었다. rmath{R}\\math{P}\\math{P}\\math{C}}\\math{P}\\math{C}}\\\\math{C}}\\로 전환되며, 여기서 우리는 먼저 2-D 패치(\\math{H}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{F}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\)의 크기,\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C} 다음으로 우리는\\(\\mathtt{D}}) 크기의 벡터에\\(\\mathbf{f}_{\\mathbf{p}}\\)를 선형적으로 프로젝트하고 위치 임베딩(\\mathbf{E}_{pos}\\mathb{{{{{(\\mathtt{I}+1)을 추가한다.\n' +
      '\n' +
      't}_{mathtt{t}} <\\bf{f}> <\\mathbf}> <\\mathbf}> <\\mathbf{f}>.\n' +
      '\n' +
      '\\(\\mathbf{t}_{p}^{\\tt{j}}\\)는 \\(\\mathtt{j}}\\), \\(\\mathbf{W}\\mathbb{R}\\in\\mathbb{R}^{{{{(\\mathtt{P}^{P}^{P}\\)의\\(\\mathtt{j}}\\)의\\(\\mathtt{j}}\\) 패치(\\mathbf{f{t}\\), \\(\\mathbf{W}\\mathbf{W}\\in\\in\\in\\in\\mathbbb{R}\\in\\in\\in\\in\\in\\mathbbb{R}\\in\\in\\in\\in\\mathbbb{R}\\in\\in\\in\\mathbbb{R}\\in\\in\\mathbbb{R}\\in\\in\\mathbbb{R}\\in\\mathbb{R}\\in\\ ViT[14] 및 BERT[31]에 의해 영감을 받아 그룹 토큰을 사용하여 \\(\\mathbf{t}_{cls}\\)로 표시되는 전체 패치 서열을 나타낸다. 그런 다음 토큰 서열(\\(\\mathbf{T}_{1-1}\\))을 Vim 인코더의 \\(\\mathtt{l}\\)-제 레이어로 보내고 출력 \\(\\mathbf{T}_{1}\\)를 얻는다. 마지막으로 출력 클래스 토큰 \\(\\mathbf{T}_{\\mathtt{L}}^{0}\\)를 정상화하고 멀티 레이어 퍼셉트론(MLP) 헤드에 공급하여 최종 예측 \\(\\hat{p}\\)를 얻으세요.\n' +
      '\n' +
      '}(\\mathbf{T})\\mathbf{f}(\\math{f}:\\math{f}:\\math{f})+\\mathbf{f}.\n' +
      '\n' +
      '\\(\\mathbf{Vim}\\)가 제안된 비전 만바 블록, \\(\\mathtt{L}\\)는 층의 수이고, \\(\\mathbf{Norm}\\)는 정규화 계층이다.\n' +
      '\n' +
      '```\n' +
      '입력: 토큰 서열 \\(\\mathbf{T}_{l-1}\\math{T}}:\\math{B}:\\math{B},\\mathtt{M},\\mathtt{D})을 정규화한다.\n' +
      '1\\(\\mathbf{T}_{l-1}^{\\prime}:(\\mathtt{B},\\mathtt{M},\\mathtt{D})\\leftarrow\\mathbf{ Norm}(\\mathbf{T}_{l-1})\\)\n' +
      '2\\(\\mathtt{x}:(\\mathtt{B},\\mathtt{M},\\mathtt{E})\\leftarrow\\mathbf{Linear}^{ \\mathbf{x}}(\\mathbf{T}_{l-1}^{\\prime})\\)\n' +
      '3\\(\\mathtt{B},\\mathtt{M},\\mathtt{E})는 서로 다른 방향 */f}^{ \\mathbf{x}}(\\mathbf{T}_{l-1}^{\\ime})로 처리한다.\n' +
      'P{ 포워드, 백워드}도 4foro입니다.\n' +
      '5\\(\\mathbf{x}_{o}^{\\prime}:(\\mathtt{B},\\mathtt{M},\\mathtt{E})\\leftarrow\\mathbf{ SiLU}(\\mathbf{Conv1d}_{o}(\\mathbf{x}))\\)\n' +
      '6\\(\\mathbf{B}_{o}:(\\mathtt{B},\\mathtt{M},\\mathtt{N})\\leftarrow\\mathbf{Linear}_{o}^{ \\mathtt{B}}(\\mathbf{x}_{o}^{\\prime})\\)\n' +
      '7\\(\\mathbf{C}_{o}:(\\mathtt{B},\\mathtt{M},\\mathtt{N})\\leftarrow\\mathbf{Linear}_{o}^{ \\mathtt{C}}(\\mathbf{x}_{o}^{\\prime})\\) /* softplus ensures positive \\(\\mathbf{\\Delta}_{o}\\) */\n' +
      '8\\(\\mathbf{\\Delta}_{o}:(\\mathtt{B},\\mathtt{M},\\mathtt{E})\\leftarrow\\)\\(\\log(1+\\exp(\\mathbf{Linear}_{o}^{\\mathbf{\\Delta}}(\\mathbf{x}_{o}^{\\prime})+\\mathbf{ Parameter}_{o}^{\\mathbf{\\Delta}}))\\) /* shape of \\(\\mathbf{Parameter}_{o}^{\\mathbf{A}}\\) is (E,N) */\n' +
      '9\\(\\overline{\\mathbf{\\Delta}}_{o}:(\\mathtt{B},\\mathtt{M},\\mathtt{E},\\mathtt{N}) \\leftarrow\\mathbf{\\Delta}_{o}\\bigotimes\\mathbf{Parameter}_{o}^{\\mathbf{A}}\\)\n' +
      '10\\(\\overline{\\mathbf{B}}_{o}:(\\mathtt{B},\\mathtt{M},\\mathtt{E},\\mathtt{N}) \\leftarrow\\mathbf{\\Delta}_{o}\\bigotimes\\mathbf{B}_{o}\\)\n' +
      '11\\(\\mathbf{y}_{o}:(\\mathtt{B},\\mathtt{M},\\mathtt{E})\\leftarrow\\mathbf{SSM}( \\overline{\\mathbf{\\Delta}}_{o},\\overline{\\mathbf{B}}_{o},\\mathbf{C}_{o})( \\mathbf{x}_{o}^{\\prime})\\)\n' +
      '12\n' +
      '② \\(\\mathbf{y}_{o}\\)\n' +
      '14\\(\\mathbf{y}_{forward}^{\\prime}:(\\mathtt{B},\\mathtt{M},\\mathtt{E})\\leftarrow \\mathbf{y}_{forward}\\bigodot\\mathbf{SiLU}(\\mathbf{z})\\)\n' +
      '15\\(\\mathbf{y}:\\mathtt{B},\\mathtt{M},\\mathtt{E}) /* 잔여 연결 */{backward\\mathbf{SiLU}(\\mathbf{{f})\n' +
      '}(\\math{T},\\math{M})}(\\mathbf{f}} <\\mathbf{f}_{backime})\n' +
      '```\n' +
      '\n' +
      '표 1*\n' +
      '\n' +
      '### Vim Block\n' +
      '\n' +
      '원래의 만바 블록은 1-D 시퀀스를 위해 설계되었으며, 이는 공간 인식 이해가 필요한 비전 작업에 적합하지 않다. 본 절에서는 비전 태스크에 대한 양방향 시퀀스 모델링을 포함하는 Vim 블록을 소개한다. 비임 블록은 그림 2에 나와 있다.\n' +
      '\n' +
      '구체적으로 알고 1에서 Vim 블록의 동작을 제시한다. 입력 토큰 서열 \\(\\mathbf{T}_{1-1}\\)는 정규화 계층에 의해 먼저 정규화된다. 다음으로, 우리는 정규화된 서열을 측정 크기 \\(E\\)를 갖는 \\(\\mathbf{x}\\) 및 \\(\\mathbf{z}\\)에 선형적으로 투영한다. 그런 다음, 우리는 전방 및 후방 방향에서 \\(\\mathbf{x}\\)를 처리한다. 각 방향에 대해 먼저 \\(\\mathbf{x}\\)에 1-D 컨볼루션을 적용하고 \\(\\mathbf{x}_{o}^{\\prime}\\)를 얻는다. 그런 다음\\(\\mathbf{x}_{o}^{\\prime}\\), \\(\\mathbf{B}_{o}_{o}\\), \\(\\mathbf{C}_{o}\\), \\(\\mathbf{\\Delta}_{o}_{o}\\)를 각각 선형적으로 프로젝트한다. 그런 다음 \\(\\mathbf{\\Delta}_{o}\\)를 사용하여 \\(\\overline{\\mathbf{A}}}_{o}\\), \\(\\overline{\\mathbf{B}}_{o}\\)를 각각 변환한다. 마지막으로 SSM을 통해 \\(\\mathbf{y}_{forward}\\) 및 \\(\\mathbf{y}_{backward}\\)를 계산한다. r\\(\\mathbf{y}_{forward}\\) 및 \\(\\mathbf{y}_{backward}\\)는 \\(\\mathbf{z}\\)에 의해 게이팅되고 함께 추가되어 출력 토큰 서열 \\(\\mathbf{T}_{1}\\)을 얻는다.\n' +
      '\n' +
      '### Architecture Details\n' +
      '\n' +
      'In summary, the hyper-parameters of our architecture are listed as follows:\n' +
      '\n' +
      '말하자면, 그는 무리를 합하면, 그는 하이퍼-오가 된다.\n' +
      '\n' +
      'D: 히든 상태 차원, D: 히든 상태 차원.\n' +
      '\n' +
      'E: 확장 상태 차원\n' +
      '\n' +
      'N: SSM dimension.\n' +
      '\n' +
      'VVT[14] 및 DeiT[61] 후, 우리는 먼저 16\\(표본)16 커널 크기 투영층을 사용하여 1D 시퀀스가 아닌 패치 임베딩을 얻는다. 이어서, 우리는 L Vim 블록을 직접 스택합니다. 기본적으로 블록 L의 수를 24, SSM 차원 N에서 16으로 설정하여 DeiT 시리즈의 모델 크기와 정렬하여 숨겨진 상태 차원 D를 192로 설정하고 작은 크기의 변이체에 대해 상태 차원 E를 384로 확장했다. 작은 크기의 변이체의 경우 D를 384 및 E에서 768으로 설정했다.\n' +
      '\n' +
      '### Efficiency Analysis\n' +
      '\n' +
      '전통적인 SSM 기반 방법은 Eq와 같이 컨볼루션 연산을 증가시키기 위해 빠른 푸리에 변환을 레버리지한다. (4) Mamba와 같은 데이터 의존적 방법의 경우 Algo 11호선 SSM 연산을 수행한다. 1은 더 이상 컨볼루션과 동등하지 않다. 이 문제를 해결하기 위해, 만바와 제안된 Vim은 효율성을 보장하기 위한 현대식 하드웨어 친화적인 방법을 선택한다. 이 최적화의 핵심 아이디어는 현대 하드웨어 가속기(GPU)의 IO-결합 및 메모리-결합을 피하기 위한 것이다.\n' +
      '\n' +
      'IO-효율성은 높은 대역폭 메모리(HBM)와 SRAM은 GPU의 두 가지 중요한 구성요소이다. 이 중 SRAM은 대역폭이 크고 HBM은 메모리 크기가 더 크다. HBM을 사용한 Vim의 SSM 동작의 표준 구현은 \\(O(\\texttt{BME})\\ 순서에 대한 메모리 IO의 수를 요구한다. 모바에서 영감을 받은 Vim은 느린 HBM에서 빠른 SRAM까지 메모리 \\(O(\\texttt{BME}+\\texttt{BME}+\\texttt{EN}}) 바이트((\\mathbf{\\Delta_{o}},\\mathbf{B_{o}},\\mathbf{C_{o}},\\mathbf{C_{o}})를 통해 먼저 판독한다. 그런 다음 Vim은 SRAM에서 이산 \\(\\overline{\\mathbf{A}_{o}}), \\(\\overline{\\mathbf{B}_{o}_{o}})를 얻는다. 마지막으로, Vim은 SRAM에서 SSM 연산을 수행하고 \\((텍스트tt{B},\\texttt{M},\\texttt{E}) 크기의 출력을 다시 HBM으로 기입한다. 이 방법은 IO를 \\(O(\\texttt{BME})\\에서 \\(O(\\texttt{BME})\\(O(\\texttt{BME}+\\texttt{EN})\\로 환원시키는 데 도움이 될 수 있다.\n' +
      '\n' +
      'Memory-Efficiency.To avoid out-of-memory problems and achieve lower memory usage when dealing with long sequences, Vim chooses the same recomputation method as Mamba. For the intermediate states of size \\((\\texttt{B},\\texttt{M},\\texttt{E},\\texttt{N})\\) to calculate the gradient, Vim recomputes them at the network backward pass. For intermediate activations such as the output of activation functions and convolution, Vim also recomputes them to optimize the GPU memory requirement, as the activation values take a lot of memory but are fast for recomputation.\n' +
      '\n' +
      '비임 블록에서의 컴퓨터-효율성(알고 1의 라인 11)과 트랜스포머에 대한 자기 의도는 모두 적응적으로 글로벌 컨텍스트를 제공하는 데 중요한 역할을 한다. 시각적 서열 \\(\\mathbf{T}\\in R^{1\\in\\texttt{M}\\tcer\\texttt{D}}\\\\)와 디폴트 설정 \\(\\texttt{E}=\\texttt{2D}\\)을 감안할 때, 글로벌 자기 의도 및 SSM의 계산 복잡성은 마찬가지이다.\n' +
      '\n' +
      '\\[\\Omega(\\text{self-attention})=4\\texttt{MD}^{2}+2\\texttt{M}^{2} \\texttt{D}, \\tag{7}\\] \\[\\Omega(\\text{SSM})=3\\texttt{M}(\\texttt{2D})\\texttt{N}+\\texttt{M}( \\texttt{2D})\\texttt{N}^{2}, \\tag{8}\\]\n' +
      '\n' +
      '자기 의도가 2차에서 서열 길이 M이고, SSM은 서열 길이 M(N은 고정 파라미터이고 디폴트로 16으로 설정됨)에 선형이다. 계산 효율은 시퀀스 길이가 큰 기가픽셀 애플리케이션에 대해 Vim 스케일링을 가능하게 한다.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '### Image Classification\n' +
      '\n' +
      '세트 설정은 1,000개 범주의 1.28M 훈련 이미지와 50K 검증 이미지를 포함하는 이미지넷-1K 데이터세트[10]에 Vim을 벤치마킹한다. 모든 모델은 트레이닝 세트에 대해 훈련되며 검증 세트에 대한 상위 1의 정확도가 보고된다. 공정 비교를 위해 교육 설정은 주로 DeiT[61]를 따른다. 구체적으로, 우리는 데이터 증가에 따라 랜덤 크로핑, 랜덤 수평 플핑, 레이블-스모딩 정규화, 믹싱 및 랜덤 소거를 적용한다. I\\(224^{2}\\) 입력 이미지에 대해 훈련할 때, 우리는 모델 최적화를 위해 \\(0.9\\), 총 배치 크기 \\(1024\\) 및 중량 붕괴(0.05\\)의 운동량으로 AdamW[44]를 사용한다. 우리는 코사인 일정, \\(1\\, 10^{-3}\\) 초기 학습률 및 EMA를 사용하여 \\(300\\) epoch에 대한 Vim 모델을 훈련시킨다. 테스트하는 동안 우리는 작물 아웃(224^{2}\\) 이미지에 대한 검증 세트에 중심 작물을 적용한다. 실험은 8 A800 GPU에 대해 수행된다.\n' +
      '\n' +
      '그림 2: 제안된 Vim 모델의 개요. 먼저 입력 영상을 패치로 나눈 후 패치 토큰으로 투사합니다. 마지막으로 제안된 Vim 인코더에 토큰의 시퀀스를 보냅니다. 이미지넷 분류를 수행하기 위해 패치 토큰 시퀀스에 추가 학습 가능한 분류 토큰을 연결한다. 텍스트 서열 모델링을 위해 만바와는 다른 Vim 인코더는 토큰 시퀀스를 전후방향으로 처리한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '* Bidirectional Sequence. During training, we randomly flip the visual sequence. This works like data augmentation.\n' +
      '* 비방향 블록. 우리는 적층된 블록을 페어링합니다. 각 쌍의 제1 블록은 전방 방향으로 시각적 시퀀스를 처리하고, 각 쌍의 제2 블록은 후방 방향으로 처리한다.\n' +
      '* 비방향 SSM. 우리는 시각 시퀀스를 후방 방향으로 처리하기 위해 각 블록에 대한 추가 SSM을 추가한다.\n' +
      '* 비방향 SSM + Conv1d. 비방향 SSM을 기반으로 후진 SSM 전에 후방 Conv1d를 추가한다(그림 2).\n' +
      '\n' +
      '타브에서 보는 바와 같이. 4, 만바 블록을 직접 채택하면 분류에서 좋은 성능을 얻을 수 있다. 그러나 부자연스러운 단방향 방식은 하류 조밀한 예측에서 문제를 야기한다. 구체적으로, 비방향블록을 사용하는 예비 양방향 전략은 분류에 대한 상위 1의 정확도가 낮은 7점을 달성한다. 그러나 의미 세분화에 대해 바닐라 단방향 만바 블록을 1.3 mIoU 능가한다. 추가 후방 SSM 및 Conv1d를 추가하여 유사한 분류 정확도(73.1 탑-1 acc _vs_ 73.2 탑-1 acc)를 달성했다. 특수 분할 우위와 예외적인 분할 우성(34.8 mIoU _vs_ 32.3 mIoU)이다. 우리는 바이방향 SSM + Conv1d의 전략을 Vim 블록에서 기본 설정으로 사용한다.\n' +
      '\n' +
      '5배제 및 미래 근무.\n' +
      '\n' +
      '비메스 아레시네, 아레시네, 아레시네아 에메메스 아라메스 이그노프, 아메메스 아메메스 아메메스 아메메스 아메메스 아메메스 아메메스 애프터, 아메메스 아메메스 아메메스 아레디브, 아메메스 아메스 스베크, 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스 아메스, 아메스 아메스, 아메스펠레스 아, 아메스, 아메스 아, 아메스 아, 아메스 아, 아메스 아, 아메스 아, 아메스 아, 아메스 아, 아메스 아, 아메스 아, 아메스 아, 아메스 아, 아메스 아, 아메스 아, 아메스\n' +
      '\n' +
      'In future works, Vim with the bidirectional SSM modeling with position embeddings is suitable for unsupervised tasks such as mask image modeling pretraining and the similar architecture with Mamba enables multimodal tasks such as CLIP-style pretraining. Based on the pretrained Vim weights, exploring the usefulness of Vim for analyzing high-resolution medical images, remote sensing images, and long videos, which can be regarded as downstream tasks, is very straightforward.\n' +
      '\n' +
      '## Acknowledgement\n' +
      '\n' +
      '초안에 대한 도움이 되는 피드백을 위해 톈정청, 유신포랑, 신정양, 보장, 진펑야오 등을 인정하고자 합니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: BERT pre-training of image transformers. In _ICLR_, 2022.\n' +
      '* [2] Ethan Baron, Itamar Zimerman, and Lior Wolf. 2-d ssm: A general spatial layer for visual transformers. _arXiv preprint arXiv:2306.06635_, 2023.\n' +
      '* [3] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sagnak Tasirlar. Introducing our multimodal models, 2023.\n' +
      '* [4] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: High quality object detection and instance segmentation. _TPAMI_, 2019.\n' +
      '* [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _ICCV_, 2021.\n' +
      '* [6] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In _ECCV_, 2018.\n' +
      '\n' +
      '그림 5: Cascade Mask R-CNN [4] 프레임워크에서 DeiT-Ti[61]과 Vim-Ti의 표준화 비교는 그림 5이다. SSM의 장거리 맥락 학습 덕분에 DeiT-Ti 상대가 인지하지 못하는 영상에서 매우 큰 객체를 캡처할 수 있다.\n' +
      '\n' +
      '* [7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. _arXiv preprint arXiv:1904.10509_, 2019.\n' +
      '* [8] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In _ICLR_, 2021.\n' +
      '* [9] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes. _NeurIPS_, 34, 2021.\n' +
      '* [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, 2009.\n' +
      '* [11] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. _arXiv preprint arXiv:2307.02486_, 2023.\n' +
      '* [12] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In _CVPR_, 2022.\n' +
      '* [13] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In _CVPR_, 2022.\n' +
      '* [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2020.\n' +
      '* [15] Stephane d\'Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. In _ICML_, 2021.\n' +
      '* [16] Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu Liu, and Qi Tian. Msg-transformer: Exchanging local spatial information by manipulating messenger tokens. In _CVPR_, 2022.\n' +
      '* [17] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In _CVPR_, 2023.\n' +
      '* [18] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hupps: Towards language modeling with state space models. In _ICLR_, 2023.\n' +
      '* [19] Golanz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. In _CVPR_, 2021.\n' +
      '* [20] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. _arXiv preprint arXiv:2312.00752_, 2023.\n' +
      '* [21] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. _arXiv preprint arXiv:2111.00396_, 2021.\n' +
      '* [22] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Re. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In _NeurIPS_, 2021.\n' +
      '* [23] Albert Gu, Karan Goel, Ankit Gupta, and Christopher Re. On the parameterization and initialization of diagonal state space models. In _NeurIPS_, 2022.\n' +
      '* [24] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In _NeurIPS_, 2022.\n' +
      '* [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, 2016.\n' +
      '* [26] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _CVPR_, 2017.\n' +
      '* [27] Md Mohaimul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In _ECCV_, 2022.\n' +
      '* [28] Md Mohaimul Islam, Mahmudul Hasan, Kishan Shamsundar Athrey, Tony Braskich, and Gedas Bertasius. Efficient movie scene detection using state-space transformers. In _CVPR_, 2023.\n' +
      '* [29] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _ICML_, 2021.\n' +
      '* [30] Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. 1960.\n' +
      '* [31] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _NAACL-HLT_, 2019.\n' +
      '* [32] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In _ICLR_, 2020.\n' +
      '* [33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In _NeurIPS_, 2012.\n' +
      '* [34] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.\n' +
      '* [35] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _ICML_, 2022.\n' +
      '* [36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.\n' +
      '* [37] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? In _ICLR_, 2022.\n' +
      '* [38] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. In _ECCV_, 2022.\n' +
      '\n' +
      '* [39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _ECCV_, 2014.\n' +
      '* [40] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.\n' +
      '* [41] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Tommi Karkkainen, Mykola Pechenizkiy, Decebal Mocanu, and Zhangyang Wang. More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. _arXiv preprint arXiv:2207.03620_, 2022.\n' +
      '* [42] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _ICCV_, 2021.\n' +
      '* [43] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In _CVPR_, 2022.\n' +
      '* [44] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _ICLR_, 2019.\n' +
      '* [45] Jun Ma, Feifei Li, and Bo Wang. U-mamba: Enhancing long-range dependency for biomedical image segmentation. _arXiv preprint arXiv:2401.04722_, 2024.\n' +
      '* [46] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In _ICLR_, 2023.\n' +
      '* [47] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher Re. S4nd: Modeling images and videos as multidimensional signals with state spaces. In _NeurIPS_, 2022.\n' +
      '* [48] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. In _NeurIPS_, 2023.\n' +
      '* [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.\n' +
      '* [50] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollar. Designing network design spaces. In _CVPR_, 2020.\n' +
      '* [51] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.\n' +
      '* [52] Jimmy TH Smith, Shalini De Mello, Jan Kautz, Scott Linderman, and Wonmin Byeon. Convolutional state space models for long-range spatiotemporal modeling. In _NeurIPS_, 2023.\n' +
      '* [53] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In _ICLR_, 2023.\n' +
      '* [54] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In _ICCV_, 2021.\n' +
      '* [55] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. _arXiv preprint arXiv:2307.08621_, 2023.\n' +
      '* [56] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In _CVPR_, 2015.\n' +
      '* [57] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In _ICML_, 2019.\n' +
      '* [58] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In _ICML_, 2021.\n' +
      '* [59] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. In _NeurIPS_, 2021.\n' +
      '* [60] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _ICML_, 2021.\n' +
      '* [61] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _ICML_, 2021.\n' +
      '* [62] Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaedin El-Nouby, Edouard Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, et al. Resmlp: Feedforward networks for image classification with data-efficient training. _TPAMI_, 2022.\n' +
      '* [63] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. _TPAMI_, 2020.\n' +
      '* [64] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. Selective structured state-spaces for long-form video understanding. In _CVPR_, 2023.\n' +
      '* [65] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.\n' +
      '* [66] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In _ICCV_, 2021.\n' +
      '* [67] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Intermimage: Exploring large-scale vision foundation models with deformable convolutions. In _CVPR_, 2023.\n' +
      '* [68] Wenhui Wang, Shuming Ma, Hanwen Xu, Naoto Usuyama, Jiayu Ding, Hoifung Poon, and Furu Wei. When an image is worth 1,024 x 1,024 words: A case study in computational pathology. _arXiv preprint arXiv:2312.03558_, 2023.\n' +
      '* [69] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In _ICCV_, 2021.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '* [71] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In _ECCV_, 2018.\n' +
      '* [72] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In _CVPR_, 2017.\n' +
      '* [73] Jing Nathan Yan, Jiatao Gu, and Alexander M Rush. Diffusion models without attention. _arXiv preprint arXiv:2311.18257_, 2023.\n' +
      '* [74] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. _IJCV_, 2019.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>