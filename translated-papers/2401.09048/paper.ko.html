<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '확산 기반 3D 주문형 이미지 합성\n' +
      '\n' +
      '종현 이\\({}^{1,2,}\\), 한삼초\\({}^{1,2}\\), 영준유\\({}^{2}\\), 서영범김\\({}^{1}\\), 용현정\\({}^{2}\\)\n' +
      '\n' +
      '한국대학교는\\({}^{1}\\,{}^{2}\\)\n' +
      '\n' +
      '카.krtomtomll103, 보삼95, shkim1}@korea.ac.krtomtomll103, chosam95, shkim1}@korea}@korea.\n' +
      '\n' +
      '{youngjoon.yoo,yonghyun.jeong}@navercorp.com\n' +
      '\n' +
      '1등. NAVER 클라우드 인턴십 기간 동안 수행된 작업. 응답 권한.\n' +
      '\n' +
      '카.krtomtomll103, 보삼95, shkim1}@korea.ac.krtomtomll103, chosam95, shkim1}@korea}@korea.\n' +
      '\n' +
      '{youngjoon.yoo,yonghyun.jeong}@navercorp.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '텍스트-조건 확산 모델에서 정확한 레이아웃 표현의 원천으로서 텍스트의 한계를 추가하는데, 많은 작품은 생성된 이미지 내에서 특정 속성을 조건하기 위해 추가적인 신호를 통합한다. 성공적이기는 하지만 이전 작품은 3차원 평면으로 확장된 해당 속성의 구체적인 위치를 설명하지 않는다. 이러한 맥락에서 우리는 3차원 객체 배치에 대한 제어를 여러 예시 이미지로부터 글로벌 양식적 의미론의 단절된 표현과 통합하는 조건부 확산 모델을 제시한다. 구체적으로, 우리는 먼저 _깊이 이젠트 학습_을 도입하여 객체들의 상대적 깊이를 추정기로 활용함으로써, 모델이 합성 이미지 트리플트의 사용을 통해 비세그먼트 오브젝트의 절대 위치를 식별할 수 있도록 한다. 또한 추가 국소화 신호를 사용하지 않고 글로벌 의미학을 표적 영역에 부과하는 방법인 _soft 안내_를 소개합니다. 우리의 통합 프레임워크인 목적 및 정복(CnC)은 이러한 기술을 통일하여 여러 조건을 불쾌하게 국소화한다. 우리는 우리의 접근법이 다양한 깊이에서 객체에 대한 인식을 허용하면서 다양한 글로벌 의미론으로 국소화된 객체를 구성하기 위한 다재다능한 프레임워크를 제공한다는 것을 보여준다.\n' +
      '\n' +
      '그림 1: 목적 및 정복은 3D 깊이 인식 방식으로 지역 조건과 글로벌 조건을 모두 현지화할 수 있다. 그림에 대한 자세한 내용은 1절에서 확인할 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Following the recent progress in text-conditional diffusion models (Rombach et al., 2022; Ramesh et al., 2022; Saharia et al., 2022; Nichol et al., 2021), many subsequent studies have emerged to address their inherent limitation in accurately representing the global layout of generated images. These follow-up works enrich the text-based conditioning capabilities of diffusion models by incorporating additional conditions such as segmentation maps (Zeng et al., 2023; Goel et al., 2023), depth maps (Zhang and Agrawala, 2023; Mou et al., 2023), bounding boxes (Li et al., 2023), and inpainting masks (Yang et al., 2023). These modifications effectively retain the extensive knowledge encapsulated in the pretrained priors.\n' +
      '\n' +
      'Despite these advancements, two primary challenges persist in the current literature. Firstly, while existing models are efficient in generating an object under locally constrained conditions like depth maps and bounding boxes, which inherently capture structural attributes, they confine the generative space to a two-dimensional plane. This limitation makes them less adept at handling object placement within a three-dimensional (3D) or z-axis (depth) perspective, and hence vulnerable to generating images without properly reflecting the depth-aware placement of multiple objects. Secondly, the issue of applying global conditions, such as style and semantics, from multiple image sources to specific regions of the target image in a controlled manner has yet to be resolved.\n' +
      '\n' +
      '현존하는 지역 및 글로벌 조건에 대한 한계를 해결하고 이미지 생성 모델의 역량 제고를 위해 구성 및 커밍(CnC)을 소개합니다. 우리의 제안된 CnC는 각 문제를 해결하기 위해 설계된 로컬 퓨저 및 글로벌 퓨저의 두 개의 빌딩 블록으로 구성된다. 먼저, 로컬 퓨저는 _깊이 디멘탕트 학습_(DDT)이라는 새로운 훈련 패러다임을 가지고 작동하여 3D 공간에서 여러 오브젝트가 서로 어떻게 관계되게 배치되어야 하는지 모델을 이해하도록 한다. DDT는 원래 이미지 구성 분야에서 도입된 합성 이미지 트리플트의 깊이 맵을 추출하여 로컬 퓨저에 대한 염기성 객체의 상대적 배치에 대한 정보를 증류한다. 둘째, 글로벌 퓨저는 명시적인 구조적 신호 없이 글로벌 조건을 현지화하는 데 우리의 모델을 보조하는 _soft 안내_라고 하는 방법을 사용한다. 부드러운 지침은 각 염기성 객체의 특정 영역에 참석하는 교차 의도 계층의 유사성 매트릭스의 영역을 선택적으로 마스크한다.\n' +
      '\n' +
      'Figure 1 demonstrates the main capabilities of our model trained on DDT and soft guidance. In Figure 1(a), DDT lets our model infer relative depth associations of multiple objects within one image, and generates objects that are placed in different depths of the z-axis with foreground objects effectively occluding other objects. In Figure 1(b), we show that by applying soft guidance, our model can localize global semantics in a disentangled manner. By utilizing the local and global fuser simultaneously as demonstrated in Figure 1(c), our model gives users the ability to compose multiple localized objects with different global semantics injected into each localized area, providing a vast degree of creative freedom.\n' +
      '\n' +
      'We quantitatively evaluate our model against other baseline models and gauge the fidelity of samples and robustness to multiple input conditions, and demonstrate that our model substantially outperforms other models on various metrics. We also evaluate our model in terms of reconstruction ability, and the ability to _order_ objects into different relative depths. We shed light onto the use of DDT, where we demonstrate that DDT dissipates the need to provide additional viewpoints of a scene to infer the relative depth placement of objects. Furthermore, we show that soft guidance not only enables our model to inject global semantics onto localized areas, but also prevents different semantics from bleeding into other regions.\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약된다.\n' +
      '\n' +
      '* We는 여러 물체의 3D 상대 포지셔닝에 대한 모델의 이해를 용이하게 하는 새로운 훈련 패러다임인 _깊이 디멘탕트 훈련_(DDT)을 제안한다.\n' +
      '* We introduce _soft guidance_, a technique that allows for the localization of global conditions without requiring explicit structural cues, thereby providing a unique mechanism for imposing global semantics onto specific image regions.\n' +
      '* By combining these two propositions, we present Compose and Conquer (CnC), a framework that augments text-conditional diffusion models with enhanced control over three-dimensional object placement and injection of global semantics onto localized regions.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      'Conditional Diffusion Models.Diffusion models (DMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020) are generative latent variable models that are trained to reverse a forward process that gradually transforms a target data distribution into a known prior. Proving highly effective in its ability to generate samples in an unconditional manner, many following works (Dhariwal and Nichol, 2021; Ho et al., 2022; Nichol et al., 2021; Rombach et al., 2022; Ramesh et al., 2022) formulate the diffusion process to take in a specific condition to generate corresponding images. Out of said models, Rombach et al. (2022) proposes LDM, a latent text-conditional DM that utilizes an autoencoder, effectively reducing the computational complexity of generation while achieving high-fidelity results. LDMs, more commonly known as Stable Diffusion, is one of the most potent diffusion models open to the research community. LDMs utilize a twofold approach, where an encoder maps \\(\\mathbf{x}\\) to its latent representation \\(\\mathbf{z}\\), and proceeds denoising \\(\\mathbf{z}\\) in a much lower, memory-friendly dimension. Once fully denoised, a decoder maps \\(\\mathbf{z}\\) to the original image dimension, effectively generating a sample.\n' +
      '\n' +
      'Beyond Text Conditions.While text-conditional DMs enable creatives to use free-form prompts, text as the sole condition has limitations. Namely, text-conditional DMs struggles with localizing objects and certain semantic concepts with text alone, because text prompts of large web-scale datasets used to train said models (Schuhmann et al., 2021) do not provide explicit localized descriptions and/or semantic information. Addressing this limitation, many works have introduced methods to incorporate additional conditional signals to the models while preserving its powerful prior, _e.g._ freezing the model while training an additional module. Among these models, ControlNet (Zhang and Agrawala, 2023) and T2I-Adapter (Mou et al., 2023) train additional modules that incorporate modalities such as depth maps and canny edge images to aid generation of localized objects. However, these models only support a single condition, lacking the ability to condition multiple signals or objects. Taking inspiration from ControlNet, Uni-ControlNet (Zhao et al., 2023) extends its framework to accept multiple local conditions and a single global condition at once. Whereas the works detailed above all leverage Stable Diffusion as a source of their priors, Composer (Huang et al., 2023) operates in the pixel-space. Although being able to process multiple conditions at once, both Composer and Uni-ControlNet struggle in processing incompatible conditions, or conditions that overlap with each other. They also do not provide methods to localize global semantics onto a localized region. In contrast, our approach directly addresses these challenges by proposing two novel methods, depth disentanglement training and soft guidance, which enables the composition of multiple local/global conditions onto localized regions.\n' +
      '\n' +
      '3가지 방법론.\n' +
      '\n' +
      '그림 2에 예시된 아키텍처는 제안된 방법의 전반적인 틀을 보여준다. CnC는 전처리된 텍스트-조건 DM의 로컬 퓨저, 글로벌 퓨저 및 구성 요소로 구성된다. 당사의 로컬 퓨저는 깊이 맵을 통해 이미지의 상대적 z축 위치를 캡처하고, 우리의 글로벌 퓨저는 지정된 영역에 CLIP 이미지 임베딩(라드포드 등 2021)으로부터 글로벌 의미학을 부과한다. 지역 및 글로벌 퓨저의 설명은 아래에 자세히 설명되어 있습니다.\n' +
      '\n' +
      '정확하게.\n' +
      '\n' +
      '추가 조건 신호(Li et al, 2023; Mou et al., 2023; Zhang 및 Agrawala, 2023; Zhao et al., 2023; Zhao et al., 2023)를 포함하는 이전 연구에 따르면 우리는 이전 출처로 Stable Diffusion(SD)로 알려져 있는 LDM 변이체를 사용한다. 구체적으로 SD는 시끄러운 잠재 기능이 12개의 공간 다운샘플링 블록, 1개의 센터 블록(C\\), 12개의 공간 업샘플링 블록을 연속적으로 통과하는 구조와 같은 UNet(론네버거 등 2015)을 사용한다. 각 블록은 ResNet(He et al, 2016) 블록 또는 트랜스포머(Vaswani et al, 2017) 블록으로 구성되며, 양조성은 각각 12개 블록의 각 그룹을 인코더 \\(E\\) 및 디코더 \\(D\\)로 지칭한다. 제어넷 장과 아크로알라라(2023) 및 유니 컨트롤넷 자오 등(2023)에 의해 영감을 받은 Stable Diffusion 아키텍처는 먼저 전체 모델을 동결하고 인코더 및 센터 블록의 훈련 가능한 사본을 복제하는 모델에서 2배 활용되며, 이는 \\(E^{\\prime}\\) 및 \\(C^{\\prime}\\)로 표시된다. 우리는 SD에서 전체 모델의 가중치와 Uni-대조군Net에서 \\(E^{\\prime}\\) 및 \\(C^{\\prime}\\)의 가중치를 초기화한다. 복제된 인코더 \\(E^{\\prime}\\) 및 센터 블록 \\(C^{\\prime}\\)은 우리 모델의 시작점으로 작용하는 로컬 퓨저로부터 국부적인 신호를 수신한다. 아래 섹션에서 모델 아키텍처, 두 건물 블록의 방법론 및 대응하는 훈련 패러다임을 자세히 설명합니다.\n' +
      '\n' +
      '### Local Fuser\n' +
      '\n' +
      '먼저 추출된 깊이 맵을 포함하는 로컬 퓨저에 대한 세부 정보를 제공하며, 이는 지역 조건으로 전처리된 단안 깊이 추정 네트워크(란프틀 등 2020)를 형성한다. 구체적으로, 우리의 로컬 퓨저는 냉동 SD 블록에 통합된 국부적 특징의 원천 역할을 한다. 우리는 또한 _깊이 반점 훈련_에 대한 세부 사항과 합성 이미지 삼중선이 물체의 상대적 깊이 배치의 공급원으로 어떻게 레버링되는지 제공한다.\n' +
      '\n' +
      '합성 이미지 트리플렛은 추론 동안 다양한 깊이 스코프가 있는 중복되는 객체를 나타낼 수 있기 때문에 모델은 훈련 중에 오브젝트에 의해 가려진 다른 요소를 인식하도록 학습할 필요가 있다. 3D 세계에서 간단하지만, 2D 영상에서 다른 사람에게 폐색된 객체에 대한 모델을 알려주는 것은 비개인적인 것이지만, 일단 이미지가 캡처되면 그 뒤에 있는 객체에 대한 공간적 정보는 영원히 손실된다는 사실 때문이다. 이러한 한계를 극복하기 위해 먼저 영상 구성에 사용되는 과정(Fang et al., 2019)을 채택하여 종합 이미지 삼중체를 생성하는데, 이는 다음 섹션에서 자세히 설명하는 깊이 무능 훈련(DDT)을 위한 훈련 샘플 역할을 한다. 전경 이미지 \\(I_{f}\\in\\mathbb{R}\\in\\mathb{R} W\\times W\\tcer 3}\\)는 단일 소스 이미지 \\(I_{b}\\in\\mathb{HCI} W\\tome 3}\\), 배경 이미지 \\(I_{b}\\I_{f} W\\i} W\\i,I_{H\\in\\mathb{H} W\\i}\\in\\in\\in\\mathb} W\\i} W\\i} W\\i} W\\i} W\\I_{H\\I_{H\\I_{H\\i} W\\I_{H\\i} W\\I_{H\\i} W\\I_{H\\i} W\\I_{H\\i} W\\I_{H\\i} W\\i} W\\I_{H\\i} W\\i} W\\i} W\\i} W\\I_{H\\i} W\\i} W\\i} W\\i} W\\i} W\\ 전경 이미지 \\(I_{f}\\)는 \\(I_{f}=I_{s}\\)의 하다마드 제품을 사용하여 도출되며, 이는 \\(I_{s}\\)의 두드러진 객체만을 남겼다. i\\(I_{b}\\)를 생성하기 위해, 우리는 Stable Diffusion의 인포팅 모듈(Rombach et al, 2022)을 사용한다. 이는 \\(I_{s}\\otep(1-\\tilde{M})\\의 결과를 치환함으로써 달성되며, 여기서 \\(\\tilde{M}\\)는 이진 확장형 \\(M\\)이다. 개념적으로, 이 과정은 염기성 객체가 없는 \\(I_{s}\\)의 묘사와 마찬가지로 생각할 수 있으며, 이는 우리의 모델 _see_를 효과적으로 뒤처진다. 이 선택에 대한 세부 사항은 부록 A.2를 참조하십시오.\n' +
      '\n' +
      'Depth Disentanglement Training.Once the synthetic image triplets \\(\\{I_{f},I_{b},M\\}\\) are prepared, we proceed to extract the depth maps of \\(I_{f}\\) and \\(I_{b}\\) to train our local fuser, which we refer to as depth disentanglement training (DDT). Our local fuser incorporates these two depth maps, which passes through its own individual stream consisting of ResNet blocks, and are concatenated along its channel dimension. Apart from previous works that directly fuse different local conditions before entering a network, DDT first process each depth map of \\(I_{f}\\) and \\(I_{b}\\) in their own independent layers. Reminiscent of early and late fusion methodologies of salient object detection (Zhou et al., 2021), we consider DDT a variant of late fusion, where the network first distinguishes each representation in a disentangled manner. Once concatenated, features containing spatial information about objects\n' +
      '\n' +
      'Figure 2: **Model Architecture**. Our model consists of a local fuser, a global fuser, and the cloned encoder/center block \\(\\{E^{\\prime},C^{\\prime}\\}\\). The input depth maps are fed into the local fuser, producing four latent representations of different spatial resolutions, which are incorporated into \\(E^{\\prime}\\). The CLIP image embeddings are fed into the global fuser, producing 2 extra tokens to be concatenated with the text token embeddings. Masks \\(M\\) are flattened and repeated to produce \\(M^{\\prime}=\\operatorname{concat}(J,\\varphi(M),1-\\varphi(M))\\), which serves as a source of soft guidance of the cross-attention layers.\n' +
      '\n' +
      '추출층에 의해 서로 다른 해상도를 따라 다양한 깊이가 추출된다. 그런 다음 이러한 특징을 복제된 SD 블록과 냉동 SD 블록에 통합하여 부록 A.2에 추가 세부 정보를 제공하고 추론 동안 다양한 깊이로 중첩된 물체를 나타내기 위해 모델이 불량한 오브젝트에 의해 가려진 요소를 인식해야 하기 때문에 모델을 훈련시키기 위해 DDT를 공식화한다. 우리의 모델을 두드러진 객체, 즉 합성-배열 합성-그 뒤에 있는 것에 대한 명시적인 깊이 표현을 제공함으로써 우리의 모델은 여러 물체의 상대적 깊이를 효과적으로 구별할 수 있다. 그림 3은 이전 작업에서 수행한 바와 같이 \\(I_{s}\\) 깊이 지도에 대한 모델을 훈련하는 것과 비교하여 DDT로 우리의 모델을 훈련하는 효과를 보여준다. 우리 지역 퓨서가 전경 염도 객체와 배경 깊이 맵 사이의 상대적 깊이 연관만을 전달하는 깊이 맵에 대해 훈련되었음에도 불구하고, 우리의 모델이 서로 다른 염도 객체를 국소화하는 데 확장된다는 것을 발견했다.\n' +
      '\n' +
      '### Global Fuser\n' +
      '\n' +
      '우리의 로컬 퓨저는 상대 객체 장소의 공급원으로 깊이 맵을 통합하지만, 우리의 글로벌 퓨저는 세계 의미학을 특정 지역에 지역화하기 위해 _소프트 안내_를 침출한다. CLIP 이미지 인코더(라드포드 등, 2021)에서 도출된 이미지 임베딩을 글로벌 시맨틱 조건으로 사용한다. 이 선택은 대조적으로 훈련된 상대방인 CLIP 텍스트 인코더로부터 텍스트 임베딩을 통합하도록 설계된 SD의 훈련 방법론에 의해 안내된다. 텍스트 임베딩(\\mathbf{y}_{\\text{text}}\\)은 교차 의도 메커니즘을 통해 중간 SD 블록에 통합된다. 이 설정에서 텍스트 임베딩은 키와 값에 대한 컨텍스트 역할을 하는 반면 중간 비지도 래치들은 쿼리로서 작용한다. 이전 작품(Nichol et al, 2021; Ramesh et al., 2022; Huang et al., 2023)은 DM의 교차 의도 계층 내에서 텍스트 임베딩과 CLIP 이미지 임베딩을 병합했지만, 이 접근법은 공간 접지 정보의 제공이 부족하다. 결과적으로, 글로벌 의미학은 정확한 국소화 능력이 결여되어 생성된 전체 이미지 상에서 조건화된다. 이러한 한계를 극복하기 위해 우리의 방법은 합성 이미지 삼중수에서 \\(I_{f}\\)를 추출하는 데 사용된 이진 전경 마스크 \\(M\\)를 인용한다.\n' +
      '\n' +
      '구체적으로, 우리는 먼저 적층된 피드포워드 층으로 구성된 글로벌 퓨저를 사용하여 \\(I_{s}\\) 및 \\(I_{b}\\)의 이미지 임베딩을 투사한다. 글로벌 퓨저는 별도의 전경/배경 하천으로 구성되며, 각각의 이미지 임베딩이 각각 \\(N\\) 글로벌 토큰에 투사되고 재결합되어 \\(\\mathbf{y}_{\\text{fg}}\\) 및 \\(\\mathbf{y}_{\\text{bg}}\\)를 생성한다. 우리 글로벌 퓨저는 로컬 퓨즈 모듈과 달리 피드포워드 레이어 내에서 각 스트림을 융합하지 않습니다. 확장된 컨텍스트(\\mathbf{y}},\\math{f{illcat},\\math{f}}}, \\mathda_{\\text{fg}}}},\\math{fg}}}{\\f}}})는 서로 연결된 \\(\\math{fg}:\\math{fg}},\\math{fg}},\\math{f}_{\\f{fg}},\\math{f}}}{\\bf{\\d{f}}}}{\\bf}{\\f}}{\\-{\\f}}{\\f}}}{\\bg}}}{\\bg}}{\\f}}{\\f}}{\\f}}{\\f}}{\\f}}{\\f}}}{\\f}}}{\\f}}{\\f}}}{\\f}}}{\\f}}}{\\f}}}}{\\f}}}{\\f}}}}{\\f} (\\lambda_{\\text{fg}}\\) 및 \\(\\lambda_{\\text{bg}}}\\)는 훈련 중 \\(1\\)로 설정된 각 토큰의 무게를 제어하는 스칼라 하이퍼파라미터를 나타낸다. 유사성 기질(Q=W_{Q}\\cdot\\mathbf{z}_{t}\\) 및 \\(K=W_{K}\\cdot\\mathbf{t}}}}\\)에 의해 제공되며(K=W_{K}\\cdot\\mathbf{t}}}}} <\\) 유사 기질은 \\(K=W_{K}\\cdot\\mathbf{dot\\crtbf}{t}{t}.{K}\\(K ={K}\\cdot\\cdot\\crtbf}{t}{dot\\dot\\crtbf}{dot\\dot\\crtbf}{dot\\dot\\crtbf}{t}}{dot\\dot\\dot\\crtbf}{t}{t}{dot\\dot\\bf}{t}}{dot\\dot\\sf}{t}}}{t}}}{t}}}}}}\n' +
      '\n' +
      'Once \\(S\\) is calculated, we apply soft guidance by first creating a Boolean matrix \\(M^{\\prime}\\), which has the same dimensionality of \\(S\\). Given that \\(S\\in\\mathbb{R}^{i\\times j}\\), \\(M^{\\prime}\\) is defined as \\(M^{\\prime}=\\operatorname{concat}(J,\\varphi(M),1-\\varphi(M))\\), where \\(J\\in\\mathbf{1}^{i\\times j-2N}\\) denotes an all ones matrix, \\(\\varphi(M)\\in\\mathbb{B}^{i\\times N}\\) denotes the reshaped, flattened and repeated boolean mask \\(M\\), and \\(1-\\varphi(M)\\) denotes the complement of \\(\\varphi(M)\\) of the same shape. By overriding \\(S\\) with the Hadamard product \\(S^{\\prime}=S\\otimes M^{\\prime}\\), the attention operation \\(\\operatorname{softmax}(S^{\\prime})\\cdot V\\) is completed. Intuitively, soft guidance can be thought of as masking out parts of\n' +
      '\n' +
      'Figure 3: **Depth disentanglement training. Our model trained on DDT (Left) successfully recognizes that objects portrayed by the foreground depth map (Top left) should be placed closer that the background depth map (Bottom left), and fully occludes objects that are larger. On the other hand, when trained on just the depth maps of \\(I_{s}\\) (Right), our model struggles to disentangle the depth maps, resulting in either objects becoming fused (samples (a), (c)) or completely ignoring the foreground object (sample (b)).**\n' +
      '\n' +
      '\\(\\mathbf{y}_{t}}\\)의 교차 선택 계산은 \\(\\mathbf{y}_{\\{fg}}\\)에 참석하지 않아야 하며, \\(\\mathbf{fg}}}\\) 및 \\(\\mathbf{y}_{\\}_{t}}}\\)의 교차 의도 계산은 \\(\\mathbf{y}_{\\)의 해당 평탄한 값에서만 수행되어야 한다. 우리는 \\(\\mathbf{y}_{\\text{텍스트}}\\)의 토큰을 전체 잠재된 토큰에 참석시키고 추가 토큰을 제한함으로써 생성된 샘플이 \\(M\\)의 공간 정보가 몰라도 조건화된 글로벌 의미학을 지역화된 영역에 반영하면서 텍스트 조건에 유지될 수 있음을 발견했다.\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      'LDM은 호 등은 먼저 제안한 \\(q(\\mathbf{x}_{0})\\에 결합된 재가중 가변 하부 변이체를 사용하여 노이즈 예측 방식으로 최적화되어 있다. 우리는 이 제형을 확장하고 모델을 Eq로 최적화한다. 1은 \\(p(\\mathbf{z}|y)\\의 조건부 분포를 학습하는데, 여기서 \\(y\\)는 CLIP 텍스트 임베딩과 함께 볼펜mm 및 이미지 임베딩 조건의 세트를 나타낸다. 위에서 언급한 바와 같이 복제된 인코더 \\(E^{\\prime}\\), 중심 블록 \\(C^{\\prime}\\) 및 지역/글로벌 퓨저 모듈의 가중치를 공동 최적화하는 동안 초기 SD 모델을 동결한다(\\theta^{\\prime}\\).\n' +
      '\n' +
      '}\\math{{}\\math{{}\\math{{.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '우리의 합성 이미지 트리플츠(I_{f},I_{b}, M\\)는 COCO-Stuff(Caesar et al., 2018), Pick-a-Pic(Kirstain et al., 2023)의 두 가지 별개의 데이터세트로부터 생성된다. 우리는 독자를 이 선택 뒤에 있는 우리의 추론에 대해 4.3절에게 참조한다. 164K 이미지가 적용된 COCO-Stuff 데이터셋은 픽셀별 주석을 가지고 있는 객체들을 "다운"(웰 정의 형상)과 "스택(배경 영역)"으로 분류한다. 우리는 사물의 범주에 있는 사물을 소수 대상으로 볼 수 있다는 사실을 레버리지하고, 80개의 사물의 집합 중 어느 하나에 속하면 색인 마스크의 각 화소를 1로 설정하여 \\(M\\)를 만들고, 그렇지 않으면 0을 생성한다. 텍스트 프롬프트는 각 이미지에 대해 사용할 수 있는 5개의 이미지로부터 무작위로 선택된다. Pick-a-Pic 데이터 세트는 SD 및 그 변이체에 의해 생성된 584K 합성 이미지 텍스트 쌍을 포함하고 선호도 점수 모델을 훈련시키기 위한 노력으로 수집되었다. 각각의 텍스트 프롬프트는 2개의 생성된 이미지와 페어링되고, 주어진 프롬프트에 대한 충실도 및 의미 정렬 측면에서 선호 이미지를 나타내는 라벨을 보유한다. 선호 이미지를 유지하고 부적절한 콘텐츠를 필터링하기만 하면 138K 이미지-텍스트 쌍으로 마무리됩니다. Pick-a-Pic은 COCO-Stuff과 달리 염기성 객체에 대한 그라운드 진리 라벨을 보유하지 않기 때문에, 우리는 \\(M\\)를 생성하기 위해 두드러진 객체 검출 모듈(Qin et al, 2020)을 사용한다. 이 두 데이터 세트를 결합하여 섹션 3.2에 자세히 설명된 과정을 통해 302K 합성 이미지 트리플츠(\\{I_{f}, I_{b}, M\\}\\)를 생성한다.\n' +
      '\n' +
      '깊이 맵 표현을 위해 단일 깊이 추정 네트워크(Ranftl et al., 2020), 그리고 글로벌 의미 조건에 CLIP 이미지 인코더(Radford et al., 2021)를 사용한다. 단일 모델로 공식화되었지만, 우리는 지역 및 글로벌 퓨서를 독립적으로 훈련하고 결합된 가중치를 가속화하는 것이 더 빠른 수렴으로 이어진다는 것을 경험적으로 발견했다. 훈련하는 동안 이미지는 재구성되고 중앙은 \\의 해상도로 크롭된다(512\\tenn 512\\). 우리는 복제된 \\(E^{\\prime}\\) 및 \\(C^{\\prime}\\)를 사용하여 로컬 퓨저를 훈련시키고, 28epochs에 대한 글로벌 퓨저, 24epochs에 대한 글로벌 퓨저, 8 NVIDIA V100s에 걸쳐 32의 배치 크기로 9epoch에 대한 전체 모델을 구했다. 훈련 동안 우리는 모델이 다양한 조합을 일반화하기 위해 학습하도록 각 조건에 대한 독립적인 중도 탈락 확률을 설정했다. 우리의 평가를 위해 50단계로 DDIM(송 et al., 2020), CFG(호 및 살미만, 2021) 척도를 사용하여 \\(768\\·768\\)의 이미지를 생성한다.\n' +
      '\n' +
      '### Evaluation\n' +
      '\n' +
      '질적 평가를 통해 깊이 맵을 국소 조건으로 포함하는 다른 기준선 모델, 전 지구적 조건으로 CLIP 이미지 임베딩 또는 둘 다에 비해 이 방법의 결과를 보여준다. 기준선 모델은 표 1. GLIGEN(Li et al., 2023), 제어넷(장 및 아크로알라라, 2023)에 나열되어 있으며 단일 깊이 맵을 국소 조건으로 수용하도록 훈련된다. 유니 컨트롤넷(Zhao et al., 2023), T2I-Ad캡터(Mou et al., 2023)는 CLIP 이미지 임베딩의 소스로서 깊이 맵과 예시 이미지를 수용하도록 훈련된다. 그림 4에서 깊이 맵과 예시 이미지를 모두 글로벌 의미론의 공급원으로 수용하는 모델과 비교하여 모델의 질적 결과를 보여준다. 우리의 모델은 두 개의 깊이 맵과 두 개의 예시 이미지를 수용하기 때문에 그림 4의 각 모달리티에 대해 동일한 이미지를 조건한다는 점에 주목하며, 다른 모델들은 각 조건의 아이디어를 파악하는데, 예를 들어 이미지와 텍스트 프롬프트가 제공하는 깊이 맵과 의미 정보 사이의 균형을 찾는 데 우리의 모델이 초과함을 알 수 있다. 첫 번째 열을 예로 들자면 유니-제어넷은 종종 글로벌 의미론을 통합하지 못하는 반면, T2I-캡터는 종종 "가짜" 또는 "보츠"와 같은 텍스트 신호를 무시하고 예시 영상에서 의미론을 과도하게 조절한다. 우리의 접근법은 글로벌 의미론을 정확하게 반영하는 동시에 깊이 맵에 의해 제공되는 텍스트 기반 세부 정보와 구조적 정보를 강조하는 이러한 측면을 능가한다. 추가적인 질적 결과를 위해 독자를 그림 5에 참조한다.\n' +
      '\n' +
      '정량적 평가는 기준 메트릭으로 생성된 이미지의 품질을 평가하기 위해 FID(Heusel et al, 2017) 및 인셉션 스코어(살리만 et al, 2016)와 생성된 이미지의 의미 정렬을 텍스트 프롬프트에 평가하기 위해 CLIPS코어(Hessel et al, 2021)를 사용한다. 표 1은 COCO-Stuff 검증 세트의 5K 이미지에 대해 평가한 결과를 보고한다. 추론(CnC, 양조에 대해) 동안 독립적으로 훈련되고 결합된 지역 및 글로벌 퓨저를 사용한 모델은 적절한 성능을 보여주지만 깊이 전용 실험의 FID 및 IS를 제외하고 대부분의 메트릭에서 핀셋된 모델이 탁월함을 알 수 있다. 이는 기준 모델이 검증 이미지로부터 추출된 단일 깊이 맵에서만 취하지만, 우리의 모델은 데이터 세트의 사전 분포를 반영하지 못할 수 있는 침입된 영역을 갖는 추가 깊이 맵에서 취하기 때문일 수 있다. 우리의 모델이 상충되는 현지화 정보의 깊이 지도를 처리해야 한다는 사실은 또 다른 가능한 설명이다. 모든 모델은 SD 이전에 공유 생성 모델을 고려할 때 유사한 CLIPS코어를 보고하지만 부드러운 지도의 통합 덕분에 예시 이미지에서만 생성할 때 핀셋 모델이 탁월하다. 우리는 우리의 안정 모델이 실질적인 성능을 달성한다고 결론지었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c|c c c} Model/Condition & \\multicolumn{3}{c}{Depth} & \\multicolumn{3}{c|}{Semantics} & \\multicolumn{3}{c}{Depth+Semantics} \\\\  & FID (\\(\\downarrow\\)) & IS (\\(\\uparrow\\)) & CLIP Score (\\(\\uparrow\\)) & FID (\\(\\downarrow\\)) & IS (\\(\\uparrow\\)) & CLIP Score (\\(\\uparrow\\)) & FID (\\(\\downarrow\\)) & IS(\\(\\uparrow\\)) & CLIP Score (\\(\\uparrow\\)) \\\\ \\hline GLIGEN & 18.887 & 29.602 & 25.815 & - & - & - & - & - & - \\\\ ControlNet & **17.303** & **31.652** & 25.741 & - & - & - & - & - & - \\\\ Uni-ControlNet & 19.277 & 31.287 & 25.620 & 23.632 & 28.364 & 24.096 & 18.945 & 28.218 & 24.839 \\\\ T2I-Adapter & 20.949 & 31.485 & 26.736 & 35.812 & 23.254 & 23.666 & 30.611 & 23.938 & 24.579 \\\\ \\hline\n' +
      '24.659&25.421 & 24.659\\\\ & 25.305 & 24.659 & 25.211 & 21.555 & 27.555 & 25.9932 & 21.9932 & 21.9932 & 21.9932 & 19.804 & 21.9932 & 21.9932 & 21.9932 & 21.9932 & 21.985 & 21.9932 & 21.9932 & 21.38 및 21.9932 & 21.9932 & 21.38 및 21.38 및 21.38 및 21.9932 & 21.985 & 21.985 & 21.38 및 21.9932 & 21.985 & 21.985 & 21.38 및 21.9932 & 21.985 & 21.9932 & 21.38 및 21.38 및 21.9932 & 21.09.930 및 21.985 & 21.985 & 21.38 및 21.9932 & 21.9932 & 21.38 및 21.9932 및 21.09.59 및 21.9932 & 21.38 및 21.9932 & 21.38 및 21.\n' +
      '**CnC Finetuned** & 22.257 & 27.981 & **26.870** & **17.254** & **32.131** & **25.940** & **18.191** & **29.304** & **25.880** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: COCO-Stuff 밸브 세트에 대한 평가 메트릭. 우리는 이러한 조건을 지원하지 않는 모델로 인해 GLIGEN과 ControlNet에서 의미론적 및 깊이+제학 결과를 생략한다. 가장 큰 결과는 **bold***입니다.\n' +
      '\n' +
      '그림 4: ** 샘플은 다른 기준선 모델과 비교하여 a***에 비해 CnC는 주어진 깊이 맵, 예시 이미지 및 텍스트 프롬프트 사이의 균형을 맞췄다.\n' +
      '\n' +
      '추가적인 핀셋링 단계로 인해 푸더가 서로의 컨디셔닝 프로세스를 더 잘 적응하고 이해할 수 있습니다. Pick-a-Pic 검증 세트에 대한 결과에 대한 A.3절을 참조한다.\n' +
      '\n' +
      '또한 "표 2"에 나열된 COCO-Stuff 및 Pick-a-Pic 검증 세트에 대해 평가된 정량적 재구성 메트릭을 보고하며, 우리의 모델은 지상 진리 검증 이미지의 이미지 삼중선에서 추출된 깊이 맵 및 CLIP 이미지 임베딩을 사용하고 기준 모델은 모달리티당 하나 이상의 조건을 지원하지 않기 때문에 지상 진리 이미지로부터 추출된 깊이 맵 및 CLIP 이미지 임베딩을 사용한다. 우리는 LPIPS(Zhang et al., 2018)를 지각적 유사성의 메트릭으로, SSIM(왕 등은 2004)을 구조적 유사성의 메트릭으로 채택한다. 우리는 또한 z축으로 확장된 구조적 유사성의 추가 척도로서 생성된 상대방으로부터 추출한 지상 진리 깊이 맵과 깊이 맵의 MAE를 보고한다. COCO-Stuff의 SSIM 값과 별도로 우리는 모델이 다른 모델을 큰 폭으로 능가한다는 것을 발견했다. 그림 6에서 볼 수 있듯이, 우리는 우리의 모델이 현장 깊이를 보존하면서 다양한 지역 내 객체들을 충실히 재현할 수 있다는 것을 발견했다. 다른 기준선 모델은 객체 지역화에 성공하지만 깊이 관점을 합성하는 데 어려움을 겪으며 상대적으로 평평하게 보이는 이미지를 낳는다.\n' +
      '\n' +
      '개념 출혈(Podell et al., 2023)으로 알려져 있는 현상은 서로 다른 의미론으로 이어져 의도하지 않은 결과를 초래한다. 소프트 지도가 가능합니다\n' +
      '\n' +
      '그림 5: ** 정성적 결과** 포그라운드/배경 조건은 각 샘플의 왼쪽에 있다.\n' +
      '\n' +
      '그림 6: ** 정성적 재구성 비교** 삼그룹은 COCO-Stuff(Left) 및 Pick-a-Pic(그렇죠)의 검증 샘플에서 추출한 조건을 사용하여 생성된다.\n' +
      '\n' +
      '글로벌 의미학은 이러한 바람직하지 않은 효과를 방지하면서 국부적 영역에 조절된다. 그림 7은 두 가지 상반된 의미론이 국한되어 있는 이 능력을 보여준다. Hi(\\lambda_{\\text{fg}}\\)를 1로 고정하고 \\(\\lambda_{\\text{bg}}\\)를 꾸준히 증가시킴으로써 배경 의미론의 효과가 증폭된다. 그러나 부드러운 지도로 인해 배경 의미론이 심해지면서 전경 물체의 모순된 의미성은 그대로 유지된다. 부드러운 안내 동안 \\(M\\)의 공간 정보가 손실되지만 출혈로 인한 모든 의미학에 대한 장벽을 충실히 생성한다는 것을 발견했다. 추가 회수를 위해 A.4절을 볼 수 있습니다.\n' +
      '\n' +
      '### Discussions\n' +
      '\n' +
      '4.1절에서 언급한 바와 같이, 우리가 모델을 훈련시키기 위해 사용하는 두 데이터 세트는 다른 데이터 세트와 매우 다르다. 즉, COCO-Stuff의 이미지는 일상적인 장면을 포함하는 반면 Pick-a-Pic의 이미지는 근본적으로 합성되어 실제 생활 시나리오에 대한 모든 설명을 초월하는 프롬프트로부터 SD의 변이체에 의해 생성된다. 이 설계 선택은 의도적인 것이지만, 우리는 대부분의 기준 모델이 MS-COCO(Lin et al., 2014)의 변이체에 대해 훈련된다는 사실을 먼저 지적한다. 이러한 모델은 새로운 조건을 도입하는 방법으로서 실제 이미지에서만 훈련하는 것이 적절하다는 것을 보여주지만, Kirstain et al.(2023) 및 Podell et al.(2023)는 COCO 제로 샷 FID가 생성된 이미지의 인간 선호도와 시각적 미학과 _음성 상관 관계가 있다고 보고한다. COCO와 그 변이체의 이미지는 새로운 조건을 도입하는 데 목적을 제공하지만 전처리된 DM의 이전에 학습된 것과 일치하는 또 다른 데이터 세트를 활용하여 사전 표류로부터 안전망을 제공한다고 주장한다. COCO-Stuff의 상세한 근거 진실 픽셀별 주석들을 활용하고 우리의 모델을 Pick-a-Pic에 의해 제공되기 전에 원래 표상으로부터 추가적인 표현을 배우게 함으로써, 우리는 두 세계 중 최고를 활용하며 DM 이전에 사용자에게 선호되는 동안 조건의 강력한 후방을 제공한다.\n' +
      '\n' +
      '5 콘퓨전.\n' +
      '\n' +
      '우리는 현장에서 두 가지 주요 과제를 다루는 새로운 텍스트-조건 확산 모델인 목적 및 정복(CnC)을 제시했는데, 이는 여러 물체의 3차원 배치와 여러 출처에서 글로벌 의미학의 지역별 국소화이다. CnC는 지역 및 글로벌 퓨저의 두 가지 주요 구성 요소를 사용하는데, 이는 각각 새로운 제6차 장애훈련(DDT)과 소프트 안내 기법을 사용한다. DDT가 물체의 절대 깊이 배치를 추론하고 부드러운 지도가 국소 지역에 의미론을 통합할 수 있음을 보여준다. COCO-stuff 및 Pick-a-Pic 데이터 세트에 대한 평가는 광범위한 실험 결과를 통해 입증된 바와 같이 이러한 문제를 해결하는 CnC의 능력을 보여준다. 현재 프레임워크는 이용 가능한 조건의 수와 비참한 공간 근거를 전경 및 배경으로 제한하기 때문에 이미지를 더 깊이 분해하여 원시물과 중간 지면을 묘사하여 향후 작업을 위해 레버리지한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c} Model/Condition & \\multicolumn{3}{c|}{COCO-Stuff} & \\multicolumn{3}{c}{Pick-a-Pic} \\\\  & SSIM(\\(\\uparrow\\)) & LPIPS(\\(\\downarrow\\)) & MAE(\\(\\downarrow\\)) & SSIM(\\(\\uparrow\\)) & LPIPS(\\(\\downarrow\\)) & MAE(\\(\\downarrow\\)) \\\\ \\hline Uni-ControlNet & **0.2362** & 0.6539 & 0.1061 & 0.2506 & 0.6504 & 0.1111 \\\\ T2I-Adapter & 0.1907 & 0.6806 & 0.1201 & 0.2238 & 0.6724 & 0.1270 \\\\ \\hline\n' +
      '0.6636 & 0.6431 & 0.1080 \\\\ 0.6421 & 0.1061 & 0.1080 \\\\ < 0.2345 및 0.2345 및 0.2336 & 0.1080 \\\\****CnC** & 0.2345 및 0.\n' +
      '**CnC Finetuned** & 0.2248 & **0.6509** & **0.0990** & **0.2690** & **0.6216** & **0.1027** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: COCO-Stuff 및 Pick-a-Pic 밸브-sets에서 평가된 정량적 재구성 메트릭이다.\n' +
      '\n' +
      '그림 7: ** 상충되는 의미와의 소프트 가이드의 효과** 로컬 퓨저 내의 스트림별 동일한 깊이 맵을 조건하고, 프롬프트 "An Igloo"로 각 샘플을 생성한다. r\\(\\lambda_{\\text{fg}}\\)를 고정하고 \\(\\lambda_{\\text{bg}}\\)를 증가시켜 배경 글로벌 정자의 효과가 실질적으로 증가한다. 부드러운 지침은 두 가지 글로벌 정자가 서로 출혈되는 것을 방지하고 _e.g._개념 출혈을 방지하여 igloo의 의미성을 효과적으로 유지한다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '사려 깊은 조언과 토론에 NAVER 클라우드의 이미지비전 팀에 감사드립니다. 네이버 스마트 기계학습(NSML) 플랫폼(김 et al., 2018)에서 훈련 및 실험을 수행하였다. 이 연구는 BK21 FOUR에 의해 뒷받침되었다.\n' +
      '\n' +
      '## Ethics statement\n' +
      '\n' +
      '확산 모델은 생성 모델의 한 종류로서 유익하고 잠재적으로 유해한 방법 모두에서 사용될 수 있는 합성 함량을 생성할 가능성이 있다. 우리의 작업은 이러한 모델의 이해와 역량을 발전시키는 것을 목표로 하지만 책임 있는 사용의 중요성을 인정한다. 우리는 실무자들이 그러한 모델을 배치할 때 보다 광범위한 사회적 의미를 고려하고 악성 애플리케이션에 대한 안전장치를 구현하도록 권장한다. 구체적으로, 우리 작업에서 이전의 공급원으로 사용하는 확산 모델은 웹스트랩 컬렉션인 LAION(Schuhmann et al., 2021) 데이터셋에서 훈련된다. 데이터셋의 크리에이터가 부적절한 데이터를 걸러내는 최선의 의도에도 불구하고 LAION에는 인종 고정관념, 폭력, 음란물 등 모델들이 내면화하는 데 부적절할 수 있는 콘텐츠가 포함되어 있다. 이러한 문제를 인식하여 유해한 편향과 잘못된 정보의 영구화를 방지하기 위해 그러한 모델을 사용하는 데 엄격한 조사의 필요성을 강조한다.\n' +
      '\n' +
      '## Reproducibility statement\n' +
      '\n' +
      '소스 코드 및 전처리 모델은 [https://github.com/tomtom1103/목적 및 수정](https://github.com/tomtom1103/목적 및 목적 정복)에서 찾을 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Caesar et al. (2018) Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 1209-1218, 2018.\n' +
      '* Dhariwal and Nichol (2021) Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.\n' +
      '* Fang et al. (2019) Hao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao Gou, Yong-Lu Li, and Cewu Lu. Instaboost: Boosting instance segmentation via probability map guided copy-pasting. In _Proceedings of the IEEE/CVF international conference on computer vision_, pp. 682-691, 2019.\n' +
      '* 고엘 등은 (2023) 비디트 고엘, 엘리아 페루조, 예판 장, 데지아 주, 니쿠 세베, 트레보 다렐, 장양 왕, 후미프리 시이를 밝혔다. 페어 확산: 구조 및 응용 쌍 확산 모델이 있는 대상 수준 이미지 편집: __ 구조 및 활용 쌍 확산 모델이 있다. arXiv 프리프린트 arXiv:2303.17546_, 2023.\n' +
      '* Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. _Advances in neural information processing systems_, 27, 2014.\n' +
      '* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 770-778, 2016.\n' +
      '* Hessel et al. (2021) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. _arXiv preprint arXiv:2104.08718_, 2021.\n' +
      '* Heusel et al.(2017) 마르틴 Heusel, Hubert Ramsauer, 토머스 유니터타이너, 베르나르 네슬러, 세파 호흐레이터. 2개의 시간 규모의 업데이트 규칙에 의해 훈련된 팬은 로컬 nash 평형으로 수렴한다. _ 시간 규모의 업데이트 규칙에 의해 수렴된다. 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '*호앤살리만스(2021) 조나단 호와 팀 살리만스. 분류자가 없는 확산.In_NeurI PS 2021 워크샵. 딥러닝 네오 래티 ve모델산 dDowns tre amApplicat 이온_,2021에 있다.\n' +
      '* Hessel et al. (2018)* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '*호 등은 (2022) 조나단호, 치토완 사하라리아, 윌리엄 찬, 데이비드 J 플렛, 모하마드 노루지, 팀 살미만 등이 있다. 고충도 이미지 생성을 위한 __고충도 이미지 생성을 위한 카스캅드 확산 모델. __고충도 이미지 생성을 위한 카스캔 확산 모델. 기계 학습 연구 저널 23(1):2249-2281, 2022.\n' +
      '* Huang et al. (2023) Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable image synthesis with composable conditions. _arXiv preprint arXiv:2302.09778_, 2023.\n' +
      '* Kim et al. (2018) Hanjoo Kim, Minkyu Kim, Dongjoo Seo, Jinwoong Kim, Heungseok Park, Soeun Park, Hyunwoo Jo, KyungHyun Kim, Youngil Yang, Youngkwan Kim, et al. Nsml: Meet the mlaas platform with a real-world case study. _arXiv preprint arXiv:1810.09957_, 2018.\n' +
      '*케터인 등은 유발 케스테인, 아담 폴리크, 우리엘 싱어, 샤불랜드 마티아나, 조 펜나, 오머 레비 등이 있다. 픽-아-픽: 텍스트-이미지 생성을 위한 사용자 선호도의 공개 데이터셋. __ 텍스트-이미지 생성을 위한 사용자 선호도의 오픈 데이터셋. arXiv 프리프린트 arXiv:2305.01569_, 2023.\n' +
      '* Li et al. (2023) Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 22511-22521, 2023.\n' +
      '* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pp. 740-755. Springer, 2014.\n' +
      '* 마우 등은 (2023) 총무, 신타오 왕, 리앙빈 시, 지안 장, 중앙기, 예잉 샹, 샤오후 키이다. T2i 어댑터: 텍스트 대 이미지 확산 모델에 대한 더 통제 가능한 능력을 파내기 위한 학습 어댑터. __이미지 확산 모델을 위한 학습 어댑터. arXiv 프리프린트 arXiv:2302.08453_, 2023.\n' +
      '* Nichol et al. (2021) Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.\n' +
      '* Niu et al. (2021) Li Niu, Wenyan Cong, Liu Liu, Yan Hong, Bo Zhang, Jing Liang, and Liqing Zhang. Making images real again: A comprehensive survey on deep image composition. _arXiv preprint arXiv:2106.14490_, 2021.\n' +
      '* Park et al. (2019) Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 2337-2346, 2019.\n' +
      '* 포델 등은 (2023) 더스틴 포델, 지온 영어, 카일 레이시, 안드레아스 블라트만, 팀 도코렌, 조 뮬러, 조펜나, 로빈 람바흐 등을 들 수 있다. 고해상도 이미지 합성을 위한 잠재 확산 모델 개선: __고해상도 이미지 합성을 위한 잠재 확산 모델 개선. arXiv 프리프린트 arXiv:2307.01952_, 2023.\n' +
      '* Qin et al.(2020) Xue빈 Qin, Zichen Zhang, Chenyang Huang, 마사드 드하건, 오스마르 R Zaiane, 마르틴 자이르랜드. U2-net: 유리체 객체 검출을 위해 중첩된 u-구조로 더 깊이 계발한다. 패턴 인식_, 106:107404, 2020.\n' +
      '* Radford et al.(2021) 알레크 라드포드, 종욱 김, 크리스 홀리스, 아디아 레즈, 가브리엘 고, 샌히니 아가왈, 기리시 사스트리, 아미다 아셀, 파멜라 미슈킨, 잭 클라크 등 자연 언어 감독으로부터 시각적 모델을 전수할 수 있다. 머신러닝_, pp. 8748-8763에 관한 _국제회의에서 2021년 PMLR.\n' +
      '* 라즈(2022) 아디야 레즈, 프라풀라 다라리왈, 알렉스 니콜, 케이시 추, 마크 첸 등이 있다. 클립 래치들을 가진 __ 계층적 텍스트-조건 이미지 생성  _ 클립 래치들을 갖는 계층적 텍스트-조건 이미지 생성이다. arXiv 프리프린트 arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '* Ranftl et al. (2020) Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. _IEEE transactions on pattern analysis and machine intelligence_, 44(3):1623-1637, 2020.\n' +
      '* Ramesh et al. (2020)* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10684-10695, 2022.\n' +
      '* Ronneberger et al. (2015) Olaf Ronneberger, 필리필 피셔 및 토머스 Brox. U-net: 생물의학적 이미지 분할을 위한 콘볼루션 네트워크. E_ 의료 이미지 컴퓨팅 및 컴퓨터 보조 개입-MICCAI 2015: 제18차 국제 회의, 뮌헨, 독일, 2015년 10월 5-9일, 합의, 부분 III 18_ pp. 234-241. 스프링거.\n' +
      '* Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.\n' +
      '* 살리만스 등 (2016) 팀 살리만스, 이안 굿펠로우, 우제키 자레마바, 비키 체웅, 알레크 라드포드, 시 첸 등이 있다. 간을 훈련시키기 위한 개선된 기술 __ ＆개선된 기술. 신경 정보 처리 시스템_, 29, 2016의 발전이다.\n' +
      '(2021) 크리스토프 슈하만, 리처드, 리처드, 로마이 nBeaum 온, 리처드, 로바이 nBeaum 존재, 로베르 tKaczm 아크지크, 엠블리 s, 아라루스 hKatta, aoC oomb es, Jenia Jitse v, 그리고 K omat suzaki -400m:O지출은 에토프콜리 p-텍스트 페어를 필터링한 400mi lli -텍스트 페어를 통해 0.00M. arXi vprepr intarXiv : 2111.02114_,2021.\n' +
      '* Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International conference on machine learning_, pp. 2256-2265. PMLR, 2015.\n' +
      '* 송 등은 송(2020)의 지밍 송, 첸린 멍, 스테파노 에르몬 등이 있다. 덴노징 확산 암묵적인 모델입니다. 2020학년도 _국제학습설명회 회의.\n' +
      '* Song et al. (2022) Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and Daniel Aliaga. Objectstitch: Generative object compositing. _arXiv preprint arXiv:2212.00932_, 2022.\n' +
      '* Suvorov et al. (2022) Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. In _Proceedings of the IEEE/CVF winter conference on applications of computer vision_, pp. 2149-2159, 2022.\n' +
      '* Szegedy et al. (2016) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 2818-2826, 2016.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* Wang et al. (2004) Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE transactions on image processing_, 13(4):600-612, 2004.\n' +
      '* Yang et al. (2023) Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 18381-18391, 2023.\n' +
      '*Zeng et al.(2023) 유쩡, 저장린, 지안밍 장, 청 류, 존 콜로모세, 제이슨 쿠엔, 비슈알 마텔. Scenecomposer: Any 수준의 의미 이미지 합성입니다. 컴퓨터 비전 및 패턴 인식_, pp 22468-22478, 2023에 대한 IEEE/CVF 회의의 _검토에서.\n' +
      '* Zhang & Agrawala (2023) Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. _arXiv preprint arXiv:2302.05543_, 2023.\n' +
      '\n' +
      '* Zhang et al. (2018) Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 586-595, 2018.\n' +
      '* Zhao et al. (2023) Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. _Advances in Neural Information Processing Systems_, 2023.\n' +
      'Shoz 헴바오, Shoz 헴하오, LuY uan,a ndKwanYee KWon g.Uni-con trolne t: 올인-in-onec 존재비 로트 o텍스트 -i mngif 융합 모델._n Bao. 인스핀 니펠트 식별 시스템_\n' +
      '\n' +
      'Appendix\n' +
      '\n' +
      '일단은.\n' +
      '\n' +
      '이미지 구성(Niu et al, 2021)은 단일화된 합성 이미지를 생성하기 위해 주어진 전경을 배경으로 혼합하는 작업을 포함한다. 전통적인 방법은 전형적으로 객체 배치, 이미지 블렌딩/하모니화 및 그림자 생성으로 구성된 순차적 파이프라인을 따른다. 이러한 단계는 두 이미지 컴포넌트 간의 시각적 불일치를 최소화하는 것을 목표로 한다. 최근 생성 모델, 특히 GAN 굿펠로우 등(2014년), DMs 호 등(2020년)의 발전으로 이미지 구성 도전은 생성 과제로 반박되었다. GAN 기반 모델은 연구 기여 수, 확산 기반 모델 측면에서 주도해 온 반면, 오바마스트티 송(2022)과 피인트 바예 양(2023) 같은 작품에서 예시한 바와 같이 이미지 구성을 위한 원샷 솔루션으로서 DM의 가능성을 보여줌으로써 다단계 전통적인 방법으로부터 이탈을 제공한다. 그러나 우리의 접근법은 전형적인 이미지 구성에서 분기된다는 점에 유의해야 한다. 우리의 모델은 전경 및 배경의 뚜렷한 정체성을 보존하기 위한 것이 아니라, 이를 메우기 위한 텍스트 및 글로벌 의미학에 대한 국부적 표현으로 활용한다. 우리의 작업은 본질적으로 다른 과제를 해결하는 것을 목표로 하지만, 우리는 합성 이미지 트리플트를 레버리지하고 생성하고자 하는 타겟 이미지를 처리하는 방식으로 이미지 구성과 평행선을 도출한다.\n' +
      '\n' +
      'CnC에는 결정 사항이 있습니다.\n' +
      '\n' +
      '지방 Fuser.Depth 이젠탕스 트레이닝(DDT)에 대한 자세한 내용은 로컬 퓨저를 훈련시키기 위해 합성 이미지 트리플츠(\\{I_{f},I_{b},M\\}\\)를 침출한다. DDT는 먼저 그림 2와 같이 자체 전경/배경 하천에 \\(I_{f}\\) 및 \\(I_{b}\\)의 깊이 맵을 통합하며 스트림의 특징은 채널 차원을 따라 연결되며 4개의 공간 해상도로 \\(I_{f}\\) 및 \\(I_{b}\\)에 대한 공간적 특징을 모두 통합하는 특징이 추출된다. 추출된 각각의 특징은 이후에 0 컨볼루션 레이어를 통과하며, Zhao et al(2023)에서 수행된 바와 같이 특징 변성 레이어(박 등 2019)를 통해 최종적으로 \\(E^{\\prime}\\)에 통합된다. 냉동 SD는 잔류 스킵 연결에 의해 통합된 디코더 \\(D\\)에서 \\(E^{\\prime}\\) 및 \\(C^{\\prime}\\)로부터 국부적인 신호를 수신한다. \\(E\\),\\(E\\),\\bf}}\\(D\\) 및\\(C^{\\f}\\)는 각각\\(\\mathbf}}}\\), \\(\\mathbf{d}\\), \\(\\mathbf{d}\\) 및 \\(\\mathbf{f}\\)의 해당 출력으로 캡처된다.\n' +
      '\n' +
      '종종{d}\\mathrm{d}_{i-1}\\mathbf{d}}\\mathbf{e}}\\mathbf{e}}\\mathbf{e}}{j}}\\mathbf{e}}<\\math{e}}{j}}{math{c}}{math{e}}{math{e}}{mathbf{e}}{math{e}}}{mathbf{e}}{math{e}}{mathbf{e}}{d{e}}{d{e}}{d{e}}{d{e}}{d{e}}{d{e}}{math{d{e}}}{d{e}}}{d{e}}}}{d{d{e}}}{d{d{e}}}}}{d{d{e}}}}{d{d{e}}}}}{d{d{e}}}}}}\n' +
      '\n' +
      '글로벌 퓨저에 대한 자세한 내용은 아래의 글로벌 퓨저를 훈련시키기 위해 교차 의도 계층에 소프트 가이드를 통합하는 과정을 요약하는 알고리즘을 제공한다.\n' +
      '\n' +
      '```\n' +
      '0:\\(I_{s}\\), \\(I_{b}\\), \\(M\\), \\(\\mathbf{y}_{\\text{text}}\\), \\(\\lambda_{\\text{fg}}=1\\), \\(\\lambda_{\\text{bg}}=1\\), \\(\\mathbf{z}_{t}\\), CLIP image encoder, Global Fuser, \\((E,C,D)\\)(Frozen layers of SD)\n' +
      '0:\\(I_{s}\\),\\(I_{\\),\\(I_{\\),\\(M\\),\\(M\\),\\(\\),\\(\\mathbf{y}_ {\\tex t{text}_ {\\ed t{text}_ {\\)\n' +
      '2:\\((\\mathbf{y}_{\\text{fg}},\\mathbf{y}_{\\text{bg}})\\leftarrow\\textsc{Global Fuser}(E_{s},E_{b})\\)\n' +
      '}(\\mathbf{y}_{\\text{illcat})\\(\\lambda_{\\text{fg}\\mathbf{fg}_{\\text{fg}}\\lambda_{\\text{fg}}\\lambda_{\\text{bg}}\\mathbff{g}_{\\text{g}_{\\text{bg}_{\\text{g}_{\\text{fg}):\\mathbath{ff}_{\\text{g}.\n' +
      '4: \\(E,C,D\\)도(E,C,D\\)도(E,C,D\\)도 \\(E,C,D\\)도의 모든 교차 선택 계층에 대한 4: \\(E,C,D\\)도에서 모든 교차 선택 계층에 대한 것이다.\n' +
      '5:\\((Q,K,V)\\leftarrow(W_{Q}\\cdot\\mathbf{z}_{t},W_{K}\\cdot\\mathbf{y}_{\\text{full}},W _{V}\\cdot\\mathbf{y}_{\\text{full}})\\)\n' +
      '6:\\(S\\leftarrow(QK^{T}/\\sqrt{d})\\)\\(\\triangleright\\)\\(S\\in\\mathbb{R}^{i\\times j}\\)\n' +
      '7:\\(J\\leftarrow\\mathbf{1}^{i\\times(j-2N)}\\)\\(\\triangleright\\) Initialize \\(J\\) as an all ones matrix\n' +
      '8:\\(\\varphi(M)\\leftarrow\\mathrm{Reshape}\\), \\(\\mathrm{Flatten}\\), \\(\\mathrm{Repeat}(M)\\)\\(\\triangleright\\)\\(\\varphi(M)\\in\\mathbb{B}^{i\\times N}\\)\n' +
      '9:\\(M^{\\prime}\\leftarrow\\mathrm{concat}(J,\\varphi(M),1-\\varphi(M))\\)\n' +
      '10:\\(S^{\\prime} 경쟁자 M^{\\prime}\\)\n' +
      '11:\\(\\mathbf{z}_{t}\\leftarrow\\mathrm{softmax}(S^{\\prime})\\cdot V\\)\n' +
      '12:endfor\n' +
      '```\n' +
      '\n' +
      'I_{s}\\(I_{s}\\)의 단일 훈련 시간표(I_{b}\\)를 생성하기 위해 \\(I_{b},I_{f},I_{b}, M\\})에 \\(I_{b}\\)을 사용하여 표적 프롬시를 위해 특별히 훈련된 SD의 변이체를 사용하여 "빈경치, 고도로 상세하지 않은 사람"으로 설정한다. 또한 널리 채택된 인포팅 모델인 라마(수보로프 등 2022년)를 테스트하고 \\(I_{b}\\) 깊이 맵의 품질을 중심으로 인포팅 모듈로서의 적합성을 측정합니다. 그림 9에서 라마에서 생성된 \\(I_{b}\\)가 파이프라인의 요구 사항과 잘 정렬되지 않을 수 있는 특정 유물을 나타낸다는 것을 관찰했다. 라마의 주목할 만한 특징은 \\(I_{b}\\)의 깊이 맵이 종종 염기성 물체의 모양을 유지하여 로컬 퓨저로 중계된 정보에 영향을 미칠 수 있다는 것이다. 한편, SD 인포팅 모듈은 \\(I_{b}\\)의 생성에 대한 접착을 증명한다. 그림 9의 1열 \\(I_{b}\\)를 중심으로 \\(I_{s}\\)에 한 번 존재하지 않는 특정 객체가 생성되었음을 알 수 있다. SD의 인포팅 모듈의 이러한 속성은 깊이 있는 무력화 훈련으로 레버리지에 매력적이라고 생각하는데, 두드러진 물체의 상대적 배치에 대한 정보를 증류하기 위해서는 우리의 모델이 훈련 중에 두드러진 객체에 의해 폐색된 _see_ 오브젝트가 효과적으로 유지되도록 하는 것이 중요하다. 합성 이미지 트리플트 생성 파이프라인의 시각화를 위해 그림 8을 확인할 수 있다.\n' +
      '\n' +
      '### Additional Results\n' +
      '\n' +
      '보다 정량적 결과는 표 3의 Pick-a-Pic 검증 세트에 대한 추가 정량적 결과를 제공하고 있으며, COCO-Stuff 검증 세트에 대한 경향에 따라 깊이 전용 실험의 FID 및 IS 값을 제외하고 모든 메트릭에서 다른 모델에 비해 적절한 모델을 제공한다. 흥미로운 사실은 표 1에 나타난 COCO-Stuff 검증 결과를 비교할 때 각 모델의 성능 순위가 크게 일치함을 관찰했다. 그러나 FID 및 IS 메트릭에 대한 특정 값은 크게 악화되는 반면 CLIP 스코어는 주목할만한 개선을 보여준다. 이러한 경향의 한 가지 잠재적인 이유는 이러한 메트릭에 사용된 인셉션-V3(Szegedy et al, 2016)과 같은 미리 학습된 모델의 기본 특성과 관련이 있다. 이 실험에서 비교되는 두 이미지 세트는 합성이지만 이러한 모델은 실제 이미지, 본질적으로 실제 이미지 특징 및 패턴을 캡처하는 실제 이미지 상에서 훈련된다. Pick-a-Pic 이미지의 합성 특성은 이러한 실제 기대에서 상당히 분기될 수 있으며, 따라서 인플루언서가 나타날 수 있다.\n' +
      '\n' +
      'Figure 8: The process of generating our synthetic image triplets.\n' +
      '\n' +
      'Figure 9: Comparison of LaMa and SD for inpainting, and its corresponding depth maps. Although images inpainted by LaMa seem to have their salient objects removed, their corresponding depth maps contain artifacts of the salient object.\n' +
      '\n' +
      'FID 점수를 매길 수 있습니다. 더욱이 비교 중인 두 데이터 세트가 합성이라고 하더라도 합성 Pick-a-Pic 데이터세트에서의 특징의 분산과 분포는 일반적인 실제 데이터 세트와 충분히 구별되어 FID 및 IS 점수의 관찰된 차이를 초래할 수 있다. 이는 합성 데이터 세트 대 실제 데이터 세트에 대한 모델을 평가하는 것과 관련된 뉘앙스를 강조하며 그러한 평가에서 결론을 도출할 때 신중한 고려가 필요함을 강조한다.\n' +
      '\n' +
      'Ordering of localized objects.In Figure 10, we compare our model\'s ability to place objects in front of another through the use of local conditions against Uni-ControlNet. Uni-ControlNet is able to take in 7 local conditions, and report that the local conditions pair listed in Figure 10 are the most robust in handling conflicting conditions, _i.e._ two overlapping objects. Although some samples do show the foreground local condition being placed in front of its counterpart, Uni-ControlNet often fails in conveying a sense of depth in its samples, resulting in the two objects to be generated on the same z-axis. On the other hand, even if the two depth maps conditioned have relatively same depth values, our model is able to consistently occlude overlapping parts of the background depth map.\n' +
      '\n' +
      'Additional details on Reconstruction.In Figure 11, we provide additional samples of the depth maps extracted from the reconstruction experiment detailed in Section 4.2. Although depth maps produced by MiDaS does not reflect the true metric depth of an object, comparing the depth maps of the ground truth images and the reconstructed images shed light onto how well models reflects the images and depth maps exposed to them during training. While the reconstructed depth maps of baseline models hold the overall shape of an object, it can be seen that our model succeeds in capturing the relative depth of regions relative to the salient object. Additionally, we elucidate our choice in selecting MAE as a metric for gauging the quality of reconstruction. Although depth maps produced by MiDaS do not predict metric depth as mentioned above, our model and baseline models were trained to generate images based on image-depth map pairs. Comparing depth maps\n' +
      '\n' +
      '그림 10: Uni-대조군Net에 대한 국부적 조건의 주문 능력을 비교한다. 유니 컨트롤넷은 열거된 조합이 7개의 지역 조건의 다른 조합에 대해 가장 효과적이라고 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c|c c c} Model/Condition & \\multicolumn{3}{c|}{Depth} & \\multicolumn{3}{c|}{Semantics} & \\multicolumn{3}{c}{Depth+Semantics} \\\\  & FID (\\(\\downarrow\\)) & IS (\\(\\uparrow\\)) & \\multicolumn{2}{c|}{CLIP} & \\multicolumn{2}{c|}{FID (\\(\\downarrow\\))} & IS (\\(\\uparrow\\)) & \\multicolumn{2}{c|}{CLIP} & \\multicolumn{2}{c}{FID (\\(\\downarrow\\))} & IS(\\(\\uparrow\\)) & \\multicolumn{2}{c}{CLIP} \\\\  & FID (\\(\\downarrow\\)) & IS (\\(\\uparrow\\)) & \\multicolumn{2}{c|}{Score (\\(\\uparrow\\))} & \\multicolumn{2}{c|}{FID (\\(\\downarrow\\))} & \\multicolumn{2}{c|}{Score (\\(\\uparrow\\))} & \\multicolumn{2}{c}{FID (\\(\\downarrow\\))} & \\multicolumn{2}{c}{IS(\\(\\uparrow\\))} & \\multicolumn{2}{c}{Score (\\(\\uparrow\\))} \\\\ \\hline GLIGEN & 22.540 & 12.733 & 28.227 & - & - & - & - & - & - \\\\ ControlNet & **21.183** & **13.685** & 28.112 & - & - & - & - & - & - \\\\ Uni-ControlNet & 24.561 & 13.260 & 28.053 & 28.964 & 12.809 & 25.245 & 22.808 & 12.006 & 26.722 \\\\ T2I-Adapter & 26.262 & 13.309 & 28.017 & 47.996 & 11.408 & 25.033 & 34.698 & 10.745 & 26.583 \\\\ \\hline Cnc & 28.192 & 11.460 & 27.347 & 36.272 & 10.353 & 24.301 & 25.524 & 11.131 & 27.109 \\\\ CnC Finetuned & 32.155 & 11.512 & **28.274** & **26.042** & **12.838** & **27.681** & **22.484** & **12.602** & **28.094** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: Pick-a-Pic 밸브 세트에 대한 평가 메트릭. 우리는 이러한 조건을 지원하지 않는 모델로 인해 GLIGEN과 ControlNet에서 의미론적 및 깊이+제학 결과를 생략한다. 가장 큰 결과는 **bold***입니다.\n' +
      '\n' +
      '지상 진리 깊이 맵이 주어진 모델 "예측자" 깊이 지도가 얼마나 잘 되는지 생각해 볼 수 있으며, 이는 다시 모델이 이미지와 깊이 맵 간의 관계를 얼마나 잘 학습했는지 나타낸다. 그림 12에서 우리는 MAE 측면에서 그라운드 진리 깊이 맵이 재구성된 깊이 맵과 얼마나 유사한지 간의 관계를 시각화한다. 각 세트는 가장 낮은/가장 낮은 MAE 값을 갖는 상위 50쌍에서 무작위로 선택되었다. 가장 낮은 깊이 맵(MAE) 점수를 가진 쌍은 복원된 이미지가 그라운드 진리 이미지에 존재하는 상대 깊이를 충실히 묘사하는 등 재구성 품질을 직접적으로 초래한다는 것을 알 수 있다. 반면, MAE 점수가 가장 높은 쌍은 하위 비교 재구성된 이미지를 초래한다. 그림 12(b)의 두 번째 행을 예로 들면, 복원된 이미지는 지상 진리 이미지에 존재하는 나무와 사람의 상대적 깊이를 포착하는 데 실패함을 알 수 있다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '다양한 공간 조건에서 물체를 공간 영역으로 효과적으로 주문하는 우리의 모델의 능력은 공간적 정보의 공간적 정보가 증류된 깊이 무력화 훈련에서 비롯되며, 배후에 있는 것은 로컬 퓨저의 각각의 하천으로 증류된다. 이를 위해 해당 조건이 이미지의 공간 정보를 보유하고 있다는 점을 감안할 때, 우리의 모델은 다양한 유형의 지역 조건에 대해 학습될 수 있음을 알 수 있다. 우리는 로컬 퓨저의 능력을 탐색하고 그림 13의 깊이 맵과 비교하여 캐니 모서리에 대한 훈련의 영향을 보여주는데, 이 가장자리의 값이 바이너리인 방식으로 깊이 맵과 다른 특성과 편향을 보유하며 더 미세한 등급이 매겨진 디테일로 깊이를 나타내는 능력을 차단한다. 이러한 특성 때문에 DDT는 캐니 모서리에도 물체의 상대적 배치를 배우는 반면, 캐니 모서리를 사용하는 것은 고유한 장단점을 가지고 있음을 알 수 있다. 그림 13(a)와 (c)는 깊이 지도를 사용하는 것이 선호되는 경우를 보고하고, (b)와 (d)는 반대 의견을 보고한다. 우리는 사과가 비교적 평평해 보이는 경우(c)와 같이 깊이의 감각을 생성하는 데 종종 실패한다는 것을 알게 된다. 그러나, 이 속성은 평평하게 시작하는 베이스 이미지를 레버링할 때 선호될 수 있다. 그림 13(b)와 (d)는 이러한 사례를 보여주는데, 여기서 평탄한 베이스 이미지들의 깊이 맵들( 포스터 및 벡터 그래픽들 등)은 공간 정보를 캡처하지 못하여 하위-파라 이미지를 생성한다. DDT가 쇠약함 또는 깊이 지도가든 주어진 표현의 유도적 편향을 효과적으로 레버리지할 수 있고 특별한 경우 DDT가 주어진 표현의 변이체를 효과적으로 레버리지할 수 있다는 것을 발견했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c}  & \\multicolumn{3}{c|}{COCO-Stuff} & \\multicolumn{3}{c}{Pick-a-Pic} \\\\ Model/Condition & FID (\\(\\downarrow\\)) & IS (\\(\\uparrow\\)) & CLIPScore (\\(\\uparrow\\)) & FID (\\(\\downarrow\\)) & IS (\\(\\uparrow\\)) & CLIPScore (\\(\\uparrow\\)) \\\\ \\hline Uni-ControlNet (Canny Edge) & **17.119** & **30.440** & 25.726 & 21.955 & **12.469** & 28.517 \\\\ T2I-Adapter (Canny Edge) & 20.051 & 28.449 & 25.850 & 30.547 & 12.230 & 28.412 \\\\ \\hline\n' +
      '**CnC (Canny Edge, Ours)** & 17.745 & 29.809 & **26.283** & **20.501** & 12.215 & **28.786** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Canny edge evaluation metrics on the COCO-Stuff and Pick-a-Pic val-set. Best results are in **bold**.\n' +
      '\n' +
      '그림 11: 재구성된 이미지에서 추출한 깊이 맵의 정성적 비교는 그림이다.\n' +
      '\n' +
      '그림 12: 깊이 맵 쌍의 정성적 비교는 각 데이터 세트에서 가장 낮은/가장 높은 MAE 값을 나타낸다. 각 세트는 각각 COCO-Stuff 및 Pick-a-Pic의 가장 낮은/가장 높은 MAE 값의 상위 50쌍에서 무작위로 선택되었다.\n' +
      '\n' +
      '그림 13: 로컬 퓨저의 표현으로서 캐니 엣지에 대한 구조 연구는 다음과 같다. (a)와 (c)는 깊이 맵이 더 나은 이미지를 생성하는 경우를 보고하고 (b)와 (d)는 캐니 모서리가 선호될 수 있는 사례를 보고한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:20]\n' +
      '\n' +
      '그림 15: 갈등 의미론. 전경 글로벌 시맨틱 이미지는 용암이 흐르는 이미지이고, 배경 글로벌 시맨틱 이미지는 눈밭의 이미지이다. 우리는 텍스트 프롬프트를 "화산"으로 수정하고 하이퍼파라미터 \\(\\lambda_{\\text{fg}}\\)와 \\(\\lambda_{\\text{bg}}\\)의 효과를 보여준다.\n' +
      '\n' +
      '그림 16: 상충되는 의미와의 소프트 지도의 효과와 각각의 평균 코사인 유사성 점수에 대한 추가 결과는 그림 16이다. 우리는 로컬 퓨저 내의 각 스트림에 대해 동일한 깊이 이미지를 조건화하고 신속한 컨디셔닝 없이 샘플을 생성한다.\n' +
      '\n' +
      '그림 17: 제11 맵 표현. 우리는 우리의 모델이 다양한 버전의 깊이 맵과 잘 일반화된다는 것을 발견한다. Version 1은 \\(I_{s}\\)에서 추출한 깊이 맵을 의미한다. Version 2는 \\(I_{f}\\)에서 추출한 깊이 맵을 의미한다. Version 3은 \\(M\\otep\\mathrm{ 심층맵}(I_{f})\\에서 추출한 깊이 지도를 의미한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:23]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>