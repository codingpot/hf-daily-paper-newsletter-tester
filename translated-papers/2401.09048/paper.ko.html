<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '확산 기반 3D 주문형 이미지 합성\n' +
      '\n' +
      '종현 이\\({}^{1,2,}\\), 한삼초\\({}^{1,2}\\), 영준유\\({}^{2}\\), 서영범김\\({}^{1}\\), 용현정\\({}^{2}\\)\n' +
      '\n' +
      '한국대학교는\\({}^{1}\\,{}^{2}\\)\n' +
      '\n' +
      '{tomtomll103, chosam95, shkim1}@korea.ac.kr\n' +
      '\n' +
      '{youngjoon.yoo,yonghyun.jeong}@navercorp.com\n' +
      '\n' +
      '첫 번째 아드니쿠는NAVER630 82 15238 087639388 대응 담당자에게 인 터닝스 ip을 링한다.\n' +
      '\n' +
      '카.krtomtomll103, 보삼95, shkim1}@korea.ac.krtomtomll103, chosam95, shkim1}@korea}@korea.\n' +
      '\n' +
      '{youngjoon.yoo,yonghyun.jeong}@navercorp.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '텍스트-조건 확산 모델에서 정확한 레이아웃 표현의 원천으로서 텍스트의 한계를 추가하는데, 많은 작품은 생성된 이미지 내에서 특정 속성을 조건하기 위해 추가적인 신호를 통합한다. 성공적이기는 하지만 이전 작품은 3차원 평면으로 확장된 해당 속성의 구체적인 위치를 설명하지 않는다. 이러한 맥락에서 우리는 3차원 객체 배치에 대한 제어를 여러 예시 이미지로부터 글로벌 양식적 의미론의 단절된 표현과 통합하는 조건부 확산 모델을 제시한다. 구체적으로, 우리는 먼저 _깊이 이젠트 학습_을 도입하여 객체들의 상대적 깊이를 추정기로 활용함으로써, 모델이 합성 이미지 트리플트의 사용을 통해 비세그먼트 오브젝트의 절대 위치를 식별할 수 있도록 한다. 또한 추가 국소화 신호를 사용하지 않고 글로벌 의미학을 표적 영역에 부과하는 방법인 _soft 안내_를 소개합니다. 우리의 통합 프레임워크인 목적 및 정복(CnC)은 이러한 기술을 통일하여 여러 조건을 불쾌하게 국소화한다. 우리는 우리의 접근법이 다양한 깊이에서 객체에 대한 인식을 허용하면서 다양한 글로벌 의미론으로 국소화된 객체를 구성하기 위한 다재다능한 프레임워크를 제공한다는 것을 보여준다.\n' +
      '\n' +
      '그림 1: 목적 및 정복은 3D 깊이 인식 방식으로 지역 조건과 글로벌 조건을 모두 현지화할 수 있다. 그림에 대한 자세한 내용은 1절에서 확인할 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Following the recent progress in text-conditional diffusion models (Rombach et al., 2022; Ramesh et al., 2022; Saharia et al., 2022; Nichol et al., 2021), many subsequent studies have emerged to address their inherent limitation in accurately representing the global layout of generated images. These follow-up works enrich the text-based conditioning capabilities of diffusion models by incorporating additional conditions such as segmentation maps (Zeng et al., 2023; Goel et al., 2023), depth maps (Zhang and Agrawala, 2023; Mou et al., 2023), bounding boxes (Li et al., 2023), and inpainting masks (Yang et al., 2023). These modifications effectively retain the extensive knowledge encapsulated in the pretrained priors.\n' +
      '\n' +
      'Despite these advancements, two primary challenges persist in the current literature. Firstly, while existing models are efficient in generating an object under locally constrained conditions like depth maps and bounding boxes, which inherently capture structural attributes, they confine the generative space to a two-dimensional plane. This limitation makes them less adept at handling object placement within a three-dimensional (3D) or z-axis (depth) perspective, and hence vulnerable to generating images without properly reflecting the depth-aware placement of multiple objects. Secondly, the issue of applying global conditions, such as style and semantics, from multiple image sources to specific regions of the target image in a controlled manner has yet to be resolved.\n' +
      '\n' +
      'To address the existing limitations on local and global conditions and also enhance the capabilities of image generation models, we introduce Compose and Conquer (CnC). Our proposed CnC consists of two building blocks: a local fuser and global fuser designed to tackle each problem. First, the local fuser operates with a new training paradigm called _depth disentanglement training_ (DDT) to let our model understand how multiple objects should be placed in relation to each other in a 3D space. DDT distills information about the relative placement of salient objects to the local fuser by extracting depth maps from synthetic image triplets, originally introduced in the field of image composition. Second, the global fuser employs a method termed _soft guidance_, which aids our model in localizing global conditions without any explicit structural signals. Soft guidance selectively masks out regions of the similarity matrix of cross-attention layers that attend to the specific regions of each salient object.\n' +
      '\n' +
      'Figure 1 demonstrates the main capabilities of our model trained on DDT and soft guidance. In Figure 1(a), DDT lets our model infer relative depth associations of multiple objects within one image, and generates objects that are placed in different depths of the z-axis with foreground objects effectively occluding other objects. In Figure 1(b), we show that by applying soft guidance, our model can localize global semantics in a disentangled manner. By utilizing the local and global fuser simultaneously as demonstrated in Figure 1(c), our model gives users the ability to compose multiple localized objects with different global semantics injected into each localized area, providing a vast degree of creative freedom.\n' +
      '\n' +
      'We quantitatively evaluate our model against other baseline models and gauge the fidelity of samples and robustness to multiple input conditions, and demonstrate that our model substantially outperforms other models on various metrics. We also evaluate our model in terms of reconstruction ability, and the ability to _order_ objects into different relative depths. We shed light onto the use of DDT, where we demonstrate that DDT dissipates the need to provide additional viewpoints of a scene to infer the relative depth placement of objects. Furthermore, we show that soft guidance not only enables our model to inject global semantics onto localized areas, but also prevents different semantics from bleeding into other regions.\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약된다.\n' +
      '\n' +
      '* We propose _depth disentanglement training_ (DDT), a new training paradigm that facilitates a model\'s understanding of the 3D relative positioning of multiple objects.\n' +
      '* We introduce _soft guidance_, a technique that allows for the localization of global conditions without requiring explicit structural cues, thereby providing a unique mechanism for imposing global semantics onto specific image regions.\n' +
      '* By combining these two propositions, we present Compose and Conquer (CnC), a framework that augments text-conditional diffusion models with enhanced control over three-dimensional object placement and injection of global semantics onto localized regions.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      'Conditional Diffusion Models.Diffusion models (DMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020) are generative latent variable models that are trained to reverse a forward process that gradually transforms a target data distribution into a known prior. Proving highly effective in its ability to generate samples in an unconditional manner, many following works (Dhariwal and Nichol, 2021; Ho et al., 2022; Nichol et al., 2021; Rombach et al., 2022; Ramesh et al., 2022) formulate the diffusion process to take in a specific condition to generate corresponding images. Out of said models, Rombach et al. (2022) proposes LDM, a latent text-conditional DM that utilizes an autoencoder, effectively reducing the computational complexity of generation while achieving high-fidelity results. LDMs, more commonly known as Stable Diffusion, is one of the most potent diffusion models open to the research community. LDMs utilize a twofold approach, where an encoder maps \\(\\mathbf{x}\\) to its latent representation \\(\\mathbf{z}\\), and proceeds denoising \\(\\mathbf{z}\\) in a much lower, memory-friendly dimension. Once fully denoised, a decoder maps \\(\\mathbf{z}\\) to the original image dimension, effectively generating a sample.\n' +
      '\n' +
      'Beyond Text Conditions.While text-conditional DMs enable creatives to use free-form prompts, text as the sole condition has limitations. Namely, text-conditional DMs struggles with localizing objects and certain semantic concepts with text alone, because text prompts of large web-scale datasets used to train said models (Schuhmann et al., 2021) do not provide explicit localized descriptions and/or semantic information. Addressing this limitation, many works have introduced methods to incorporate additional conditional signals to the models while preserving its powerful prior, _e.g._ freezing the model while training an additional module. Among these models, ControlNet (Zhang and Agrawala, 2023) and T2I-Adapter (Mou et al., 2023) train additional modules that incorporate modalities such as depth maps and canny edge images to aid generation of localized objects. However, these models only support a single condition, lacking the ability to condition multiple signals or objects. Taking inspiration from ControlNet, Uni-ControlNet (Zhao et al., 2023) extends its framework to accept multiple local conditions and a single global condition at once. Whereas the works detailed above all leverage Stable Diffusion as a source of their priors, Composer (Huang et al., 2023) operates in the pixel-space. Although being able to process multiple conditions at once, both Composer and Uni-ControlNet struggle in processing incompatible conditions, or conditions that overlap with each other. They also do not provide methods to localize global semantics onto a localized region. In contrast, our approach directly addresses these challenges by proposing two novel methods, depth disentanglement training and soft guidance, which enables the composition of multiple local/global conditions onto localized regions.\n' +
      '\n' +
      '3가지 방법론.\n' +
      '\n' +
      '그림 2에 예시된 아키텍처는 제안된 방법의 전반적인 틀을 보여준다. CnC는 전처리된 텍스트-조건 DM의 로컬 퓨저, 글로벌 퓨저 및 구성 요소로 구성된다. 당사의 로컬 퓨저는 깊이 맵을 통해 이미지의 상대적 z축 위치를 캡처하고, 우리의 글로벌 퓨저는 지정된 영역에 CLIP 이미지 임베딩(라드포드 등 2021)으로부터 글로벌 의미학을 부과한다. 지역 및 글로벌 퓨저의 설명은 아래에 자세히 설명되어 있습니다.\n' +
      '\n' +
      '정확하게.\n' +
      '\n' +
      '추가 조건 신호(Li et al, 2023; Mou et al., 2023; Zhang 및 Agrawala, 2023; Zhao et al., 2023; Zhao et al., 2023)를 포함하는 이전 연구에 따르면 우리는 이전 출처로 Stable Diffusion(SD)로 알려져 있는 LDM 변이체를 사용한다. 구체적으로 SD는 시끄러운 잠재 기능이 12개의 공간 다운샘플링 블록, 1개의 센터 블록(C\\), 12개의 공간 업샘플링 블록을 연속적으로 통과하는 구조와 같은 UNet(론네버거 등 2015)을 사용한다. 각 블록은 ResNet(He et al, 2016) 블록 또는 트랜스포머(Vaswani et al, 2017) 블록으로 구성되며, 양조성은 각각 12개 블록의 각 그룹을 인코더 \\(E\\) 및 디코더 \\(D\\)로 지칭한다. 제어넷 장과 아크로알라라(2023) 및 유니 컨트롤넷 자오 등(2023)에 의해 영감을 받은 Stable Diffusion 아키텍처는 먼저 전체 모델을 동결하고 인코더 및 센터 블록의 훈련 가능한 사본을 복제하는 모델에서 2배 활용되며, 이는 \\(E^{\\prime}\\) 및 \\(C^{\\prime}\\)로 표시된다. 우리는 SD에서 전체 모델의 가중치와 Uni-대조군Net에서 \\(E^{\\prime}\\) 및 \\(C^{\\prime}\\)의 가중치를 초기화한다. 복제된 인코더 \\(E^{\\prime}\\) 및 센터 블록 \\(C^{\\prime}\\)은 우리 모델의 시작점으로 작용하는 로컬 퓨저로부터 국부적인 신호를 수신한다. 아래 섹션에서 모델 아키텍처, 두 건물 블록의 방법론 및 대응하는 훈련 패러다임을 자세히 설명합니다.\n' +
      '\n' +
      '### Local Fuser\n' +
      '\n' +
      '먼저 추출된 깊이 맵을 포함하는 로컬 퓨저에 대한 세부 정보를 제공하며, 이는 지역 조건으로 전처리된 단안 깊이 추정 네트워크(란프틀 등 2020)를 형성한다. 구체적으로, 우리의 로컬 퓨저는 냉동 SD 블록에 통합된 국부적 특징의 원천 역할을 한다. 우리는 또한 _깊이 반점 훈련_에 대한 세부 사항과 합성 이미지 삼중선이 물체의 상대적 깊이 배치의 공급원으로 어떻게 레버링되는지 제공한다.\n' +
      '\n' +
      '합성 이미지 트리플렛은 추론 동안 다양한 깊이 스코프가 있는 중복되는 객체를 나타낼 수 있기 때문에 모델은 훈련 중에 오브젝트에 의해 가려진 다른 요소를 인식하도록 학습할 필요가 있다. 3D 세계에서 간단하지만, 2D 영상에서 다른 사람에게 폐색된 객체에 대한 모델을 알려주는 것은 비개인적인 것이지만, 일단 이미지가 캡처되면 그 뒤에 있는 객체에 대한 공간적 정보는 영원히 손실된다는 사실 때문이다. 이러한 한계를 극복하기 위해 먼저 영상 구성에 사용되는 과정(Fang et al., 2019)을 채택하여 종합 이미지 삼중체를 생성하는데, 이는 다음 섹션에서 자세히 설명하는 깊이 무능 훈련(DDT)을 위한 훈련 샘플 역할을 한다. 전경 이미지 \\(I_{f}\\in\\mathbb{R}\\in\\mathb{R} W\\times W\\tcer 3}\\)는 단일 소스 이미지 \\(I_{b}\\in\\mathb{HCI} W\\tome 3}\\), 배경 이미지 \\(I_{b}\\I_{f} W\\i} W\\i,I_{H\\in\\mathb{H} W\\i}\\in\\in\\in\\mathb} W\\i} W\\i} W\\i} W\\i} W\\I_{H\\I_{H\\I_{H\\i} W\\I_{H\\i} W\\I_{H\\i} W\\I_{H\\i} W\\I_{H\\i} W\\I_{H\\i} W\\i} W\\I_{H\\i} W\\i} W\\i} W\\i} W\\I_{H\\i} W\\i} W\\i} W\\i} W\\i} W\\ 전경 이미지 \\(I_{f}\\)는 \\(I_{f}=I_{s}\\)의 하다마드 제품을 사용하여 도출되며, 이는 \\(I_{s}\\)의 두드러진 객체만을 남겼다. i\\(I_{b}\\)를 생성하기 위해, 우리는 Stable Diffusion의 인포팅 모듈(Rombach et al, 2022)을 사용한다. 이는 \\(I_{s}\\otep(1-\\tilde{M})\\의 결과를 치환함으로써 달성되며, 여기서 \\(\\tilde{M}\\)는 이진 확장형 \\(M\\)이다. 개념적으로, 이 과정은 염기성 객체가 없는 \\(I_{s}\\)의 묘사와 마찬가지로 생각할 수 있으며, 이는 우리의 모델 _see_를 효과적으로 뒤처진다. 이 선택에 대한 세부 사항은 부록 A.2를 참조하십시오.\n' +
      '\n' +
      'Depth Disentanglement Training.Once the synthetic image triplets \\(\\{I_{f},I_{b},M\\}\\) are prepared, we proceed to extract the depth maps of \\(I_{f}\\) and \\(I_{b}\\) to train our local fuser, which we refer to as depth disentanglement training (DDT). Our local fuser incorporates these two depth maps, which passes through its own individual stream consisting of ResNet blocks, and are concatenated along its channel dimension. Apart from previous works that directly fuse different local conditions before entering a network, DDT first process each depth map of \\(I_{f}\\) and \\(I_{b}\\) in their own independent layers. Reminiscent of early and late fusion methodologies of salient object detection (Zhou et al., 2021), we consider DDT a variant of late fusion, where the network first distinguishes each representation in a disentangled manner. Once concatenated, features containing spatial information about objects\n' +
      '\n' +
      '그림 2: ** 모델 건축***입니다. 우리의 모델은 로컬 퓨저, 글로벌 퓨저 및 복제된 인코더/중앙 블록 \\(\\{E^{\\prime},C^{\\prime}\\}\\)로 구성된다. 입력 깊이 맵들은 로컬 퓨저에 공급되어 \\(E^{\\prime}\\)에 통합된 다양한 공간 해상도의 4개의 잠재 표현을 생성한다. CLIP 이미지 임베딩은 글로벌 퓨저에 공급되어 텍스트 토큰 임베딩과 연결되도록 2개의 추가 토큰을 생성한다. 과제 \\(M\\)는 평평하고 반복되어 \\(M^{\\prime}=\\operatoral{concat}(J,\\varphi(M), 1-\\varphi(M))를 생성하며, 이는 교차 의도 계층의 부드러운 지침의 원천 역할을 한다.\n' +
      '\n' +
      'in varying depths are extracted along different resolutions by an extraction layer. These features are then incorporated into the cloned and frozen SD blocks, which we provide additional details in our appendix A.2. We formulate DDT to train our model because to represent overlapping objects with varied depths during inference, the model needs to recognize the elements obscured by the salient object. By providing our model with an explicit depth representation of what lies behind the salient object--albeit synthetic--, our model is able to effectively distinguish the relative depths of multiple objects. Figure 3 demonstrates the effect of training our model with DDT compared to training our model on depth maps of \\(I_{s}\\), as done in previous works. Even though that our local fuser was trained on depth maps that convey only the relative depth associations between the foreground salient object and background depth maps, we find that our model extends to localizing different salient objects.\n' +
      '\n' +
      '### Global Fuser\n' +
      '\n' +
      'While our local fuser incorporates depth maps as a source of relative object placements, our global fuser leverages _soft guidance_ to localize global semantics onto specific regions. We use image embeddings derived from the CLIP image encoder (Radford et al., 2021) as our global semantic condition. This choice is informed by the training methodology of SD, which is designed to incorporate text embeddings from its contrastively trained counterpart, the CLIP text encoder. The text embeddings \\(\\mathbf{y}_{\\text{text}}\\) are integrated into the intermediate SD blocks through a cross-attention mechanism. In this setup, the text embeddings serve as the context for keys and values, while the intermediate noised latents act as the queries. Although prior works (Nichol et al., 2021; Ramesh et al., 2022; Huang et al., 2023) have merged CLIP image embeddings with text embeddings within the cross-attention layers of the DM, this approach lacks the provision of spatial grounding information. As a result, the global semantics are conditioned on the entire generated image, lacking precise localization capabilities. To overcome this limitation, our method leverages the binary foreground mask \\(M\\) used to extract \\(I_{f}\\) in our synthetic image triplet.\n' +
      '\n' +
      'Soft Guidance.In detail, we first project the image embeddings of \\(I_{s}\\) and \\(I_{b}\\) using our global fuser, which consists of stacked feedforward layers. The global fuser consists of separate foreground/background streams, where each image embedding is projected and reshaped to \\(N\\) global tokens each, resulting in \\(\\mathbf{y}_{\\text{fg}}\\) and \\(\\mathbf{y}_{\\text{bg}}\\). Unlike our local fuser module, our global fuser doesn\'t fuse each stream within the feedforward layers. We instead choose to concatenate \\(\\mathbf{y}_{\\text{fg}}\\) and \\(\\mathbf{y}_{\\text{bg}}\\) directly to \\(\\mathbf{y}_{\\text{text}}\\), where the extended context \\(\\mathbf{y}_{\\text{full}}=\\operatorname{concat}(\\mathbf{y}_{\\text{text}}, \\lambda_{\\text{fg}}\\mathbf{y}_{\\text{fg}},\\lambda_{\\text{bg}}\\mathbf{y}_{\\text {bg}})\\) is utilized in the cross-attention layers of our cloned and frozen modules. \\(\\lambda_{\\text{fg}}\\) and \\(\\lambda_{\\text{bg}}\\) denotes scalar hyperparameters that control the weight of each token, which are set to \\(1\\) during training. In the cross-attention layers, the similarity matrix \\(S\\) is given by \\(S=(QK^{T}/\\sqrt{d})\\) with \\(Q=W_{Q}\\cdot\\mathbf{z}_{t}\\) and \\(K=W_{K}\\cdot\\mathbf{y}_{\\text{full}}\\), where \\(\\mathbf{z}_{t}\\) is a noised variant of \\(\\mathbf{z}\\) with the diffusion forward process applied \\(t\\) steps.\n' +
      '\n' +
      '일단 \\(S\\)가 계산되면, 우리는 먼저 \\(S\\)의 동일한 차원을 갖는 Boolean 매트릭스 \\(M^{\\prime}\\)를 생성하여 부드러운 지침을 적용한다. \\(J\\in\\math j-2N}\\)는 모든 매트릭스(J,\\vath{i)를 나타내는 모든 매트릭스(J,\\vati{i)\\(M,\\vath{b(M)\\in\\mathi{B}\\i)를 의미하고, 여기서\\(M,\\vath{B(M)\\in\\math{b\\i첨부(M)\\i{b\\i{i\\i{b\\i{b\\i{i\\i{b\\i{b\\i{i\\i{b\\i{B(M)\\vati{b\\i{b\\i{b\\i{b\\i{b\\i\\i{b\\i\\i{b\\i{b\\i\\i{b\\i{b\\i\\i{b\\i\\i{b\\i{b\\i\\i\\i{b\\i\\i<\\i <\\i{B})는\\(M)\\i{B(M)\\i{B(M)\\i{B(M)\\i:\\i\\i Hadamard 제품 \\(S^{\\prime}=S\\otome M^{\\prime}\\)으로 \\(S\\)를 과잉 공급함으로써, 주의 운영 \\(\\operatorRep{softmax}(S^{\\prime})\\cdot V\\)을 완성한다. 직관적으로 부드러운 지도는 마스킹하는 것으로 생각할 수 있다.\n' +
      '\n' +
      'Figure 3: **Depth disentanglement training. Our model trained on DDT (Left) successfully recognizes that objects portrayed by the foreground depth map (Top left) should be placed closer that the background depth map (Bottom left), and fully occludes objects that are larger. On the other hand, when trained on just the depth maps of \\(I_{s}\\) (Right), our model struggles to disentangle the depth maps, resulting in either objects becoming fused (samples (a), (c)) or completely ignoring the foreground object (sample (b)).**\n' +
      '\n' +
      '\\(S\\) where \\(\\mathbf{z}_{t}\\) should not be attending to, _e.g._ forcing the cross-attention computations of \\(\\mathbf{y}_{\\text{fg}}\\) and \\(\\mathbf{y}_{\\text{bg}}\\) to be performed only on its corresponding flattened values of \\(\\mathbf{z}_{t}\\). We find that by letting the tokens of \\(\\mathbf{y}_{\\text{text}}\\) attend to the whole latent and restricting the extra tokens, generated samples are able to stay true to their text conditions while also reflecting the conditioned global semantics in their localized areas, even though that spatial information of \\(M\\) is forfeited.\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      'LDMs are optimized in a noise-prediction manner, utilizing a variation of the reweighted variational lower bound on \\(q(\\mathbf{x}_{0})\\), first proposed by Ho et al. (2020). We extend this formulation and optimize our model with Eq. 1 to learn the conditional distribution of \\(p(\\mathbf{z}|y)\\), where \\(y\\) denotes the set of our conditions of dethpmms and image embeddings accompanied with CLIP text embeddings. As mentioned above, we freeze the initial SD model while jointly optimizing the weights of the cloned encoder \\(E^{\\prime}\\), center block \\(C^{\\prime}\\), and the local/global fuser modules, denoted as \\(\\theta^{\\prime}\\).\n' +
      '\n' +
      '}\\math{{}\\math{{}\\math{{\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '우리의 합성 이미지 트리플츠(I_{f},I_{b}, M\\)는 COCO-Stuff(Caesar et al., 2018), Pick-a-Pic(Kirstain et al., 2023)의 두 가지 별개의 데이터세트로부터 생성된다. 우리는 독자를 이 선택 뒤에 있는 우리의 추론에 대해 4.3절에게 참조한다. 164K 이미지가 적용된 COCO-Stuff 데이터셋은 픽셀별 주석을 가지고 있는 객체들을 "다운"(웰 정의 형상)과 "스택(배경 영역)"으로 분류한다. 우리는 사물의 범주에 있는 사물을 소수 대상으로 볼 수 있다는 사실을 레버리지하고, 80개의 사물의 집합 중 어느 하나에 속하면 색인 마스크의 각 화소를 1로 설정하여 \\(M\\)를 만들고, 그렇지 않으면 0을 생성한다. 텍스트 프롬프트는 각 이미지에 대해 사용할 수 있는 5개의 이미지로부터 무작위로 선택된다. Pick-a-Pic 데이터 세트는 SD 및 그 변이체에 의해 생성된 584K 합성 이미지 텍스트 쌍을 포함하고 선호도 점수 모델을 훈련시키기 위한 노력으로 수집되었다. 각각의 텍스트 프롬프트는 2개의 생성된 이미지와 페어링되고, 주어진 프롬프트에 대한 충실도 및 의미 정렬 측면에서 선호 이미지를 나타내는 라벨을 보유한다. 선호 이미지를 유지하고 부적절한 콘텐츠를 필터링하기만 하면 138K 이미지-텍스트 쌍으로 마무리됩니다. Pick-a-Pic은 COCO-Stuff과 달리 염기성 객체에 대한 그라운드 진리 라벨을 보유하지 않기 때문에, 우리는 \\(M\\)를 생성하기 위해 두드러진 객체 검출 모듈(Qin et al, 2020)을 사용한다. 이 두 데이터 세트를 결합하여 섹션 3.2에 자세히 설명된 과정을 통해 302K 합성 이미지 트리플츠(\\{I_{f}, I_{b}, M\\}\\)를 생성한다.\n' +
      '\n' +
      'Implementation Details.For depth map representations, we utilize a monocular depth estimation network (Ranftl et al., 2020), and the CLIP image encoder (Radford et al., 2021) for global semantic conditions. Although formulated as a single model, we empirically find that training the local and global fuser independently, and finetuning the combined weights lead to faster convergence. During training, images are resized and center cropped to a resolution of \\(512\\times 512\\). We train our local fuser with the cloned \\(E^{\\prime}\\) and \\(C^{\\prime}\\) for 28 epochs, our global fuser for 24 epochs, and finetune the full model for 9 epochs, all with a batch size of 32 across 8 NVIDIA V100s. During training, we set an independent dropout probability for each condition to ensure that our model learns to generalize various combinations. For our evaluation, we employ DDIM (Song et al., 2020) sampling with 50 steps, and a CFG (Ho and Salimans, 2021) scale of 7 to generate images of \\(768\\times 768\\).\n' +
      '\n' +
      '### Evaluation\n' +
      '\n' +
      'Qualitative Evaluation.We demonstrate the results of our method compared to other baseline models that incorporate either depth maps as a local condition, CLIP image embeddings as a global condition, or both. The baseline models are listed in Table 1. GLIGEN (Li et al., 2023) and ControlNet (Zhang and Agrawala, 2023) are trained to accept a single depth map as a local condition. Uni-ControlNet (Zhao et al., 2023) and T2I-Adapter (Mou et al., 2023) are trained to accept a depth map and an exemplar image as a source of CLIP image embeddings. In Figure 4, we show qualitative results of our model compared to models that accept both depth maps and exemplar images as a source of global semantics. Since our model accepts two depth maps and two exemplar images, note that we condition the same images for each modality in Figure 4. While other modelsdo grasp the ideas of each condition, it can be seen that our model exceeds in finding a balance between the structural information provided by the depth maps and semantic information provided by exemplar images and text prompts. Taking the first column as an example, Uni-ControlNet often fails to incorporate the global semantics, while T2I-Adapter often overconditions the semantics from the exemplar image, ignoring textual cues such as "lake" or "boats". Our approach adeply interweaves these aspects, accurately reflecting global semantics while also emphasizing text-driven details and structural information provided by depth maps. For additional qualitative results, we refer the readers to Figure 5.\n' +
      '\n' +
      '정량적 평가는 기준 메트릭으로 생성된 이미지의 품질을 평가하기 위해 FID(Heusel et al, 2017) 및 인셉션 스코어(살리만 et al, 2016)와 생성된 이미지의 의미 정렬을 텍스트 프롬프트에 평가하기 위해 CLIPS코어(Hessel et al, 2021)를 사용한다. 표 1은 COCO-Stuff 검증 세트의 5K 이미지에 대해 평가한 결과를 보고한다. 추론(CnC, 양조에 대해) 동안 독립적으로 훈련되고 결합된 지역 및 글로벌 퓨저를 사용한 모델은 적절한 성능을 보여주지만 깊이 전용 실험의 FID 및 IS를 제외하고 대부분의 메트릭에서 핀셋된 모델이 탁월함을 알 수 있다. 이는 기준 모델이 검증 이미지로부터 추출된 단일 깊이 맵에서만 취하지만, 우리의 모델은 데이터 세트의 사전 분포를 반영하지 못할 수 있는 침입된 영역을 갖는 추가 깊이 맵에서 취하기 때문일 수 있다. 우리의 모델이 상충되는 현지화 정보의 깊이 지도를 처리해야 한다는 사실은 또 다른 가능한 설명이다. 모든 모델은 SD 이전에 공유 생성 모델을 고려할 때 유사한 CLIPS코어를 보고하지만 부드러운 지도의 통합 덕분에 예시 이미지에서만 생성할 때 핀셋 모델이 탁월하다. 우리는 우리의 안정 모델이 실질적인 성능을 달성한다고 결론지었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c|c c c} Model/Condition & \\multicolumn{3}{c}{Depth} & \\multicolumn{3}{c|}{Semantics} & \\multicolumn{3}{c}{Depth+Semantics} \\\\  & FID (\\(\\downarrow\\)) & IS (\\(\\uparrow\\)) & CLIP Score (\\(\\uparrow\\)) & FID (\\(\\downarrow\\)) & IS (\\(\\uparrow\\)) & CLIP Score (\\(\\uparrow\\)) & FID (\\(\\downarrow\\)) & IS(\\(\\uparrow\\)) & CLIP Score (\\(\\uparrow\\)) \\\\ \\hline GLIGEN & 18.887 & 29.602 & 25.815 & - & - & - & - & - & - \\\\ ControlNet & **17.303** & **31.652** & 25.741 & - & - & - & - & - & - \\\\ Uni-ControlNet & 19.277 & 31.287 & 25.620 & 23.632 & 28.364 & 24.096 & 18.945 & 28.218 & 24.839 \\\\ T2I-Adapter & 20.949 & 31.485 & 26.736 & 35.812 & 23.254 & 23.666 & 30.611 & 23.938 & 24.579 \\\\ \\hline\n' +
      '24.659&25.421 & 24.659\\\\ & 25.305 & 24.659 & 25.211 & 21.555 & 27.555 & 25.9932 & 21.9932 & 21.9932 & 21.9932 & 19.804 & 21.9932 & 21.9932 & 21.9932 & 21.9932 & 21.985 & 21.9932 & 21.9932 & 21.38 및 21.9932 & 21.9932 & 21.38 및 21.38 및 21.38 및 21.9932 & 21.985 & 21.985 & 21.38 및 21.9932 & 21.985 & 21.985 & 21.38 및 21.9932 & 21.985 & 21.9932 & 21.38 및 21.38 및 21.9932 & 21.09.930 및 21.985 & 21.985 & 21.38 및 21.9932 & 21.9932 & 21.38 및 21.9932 및 21.09.59 및 21.9932 & 21.38 및 21.9932 & 21.38 및 21.\n' +
      '**CnC Finetuned** & 22.257 & 27.981 & **26.870** & **17.254** & **32.131** & **25.940** & **18.191** & **29.304** & **25.880** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: COCO-Stuff 밸브 세트에 대한 평가 메트릭. 우리는 이러한 조건을 지원하지 않는 모델로 인해 GLIGEN과 ControlNet에서 의미론적 및 깊이+제학 결과를 생략한다. 가장 큰 결과는 **bold***입니다.\n' +
      '\n' +
      '그림 4: ** 샘플은 다른 기준선 모델과 비교하여 a***에 비해 CnC는 주어진 깊이 맵, 예시 이미지 및 텍스트 프롬프트 사이의 균형을 맞췄다.\n' +
      '\n' +
      '추가적인 핀셋링 단계로 인해 푸더가 서로의 컨디셔닝 프로세스를 더 잘 적응하고 이해할 수 있습니다. Pick-a-Pic 검증 세트에 대한 결과에 대한 A.3절을 참조한다.\n' +
      '\n' +
      '또한 "표 2"에 나열된 COCO-Stuff 및 Pick-a-Pic 검증 세트에 대해 평가된 정량적 재구성 메트릭을 보고하며, 우리의 모델은 지상 진리 검증 이미지의 이미지 삼중선에서 추출된 깊이 맵 및 CLIP 이미지 임베딩을 사용하고 기준 모델은 모달리티당 하나 이상의 조건을 지원하지 않기 때문에 지상 진리 이미지로부터 추출된 깊이 맵 및 CLIP 이미지 임베딩을 사용한다. 우리는 LPIPS(Zhang et al., 2018)를 지각적 유사성의 메트릭으로, SSIM(왕 등은 2004)을 구조적 유사성의 메트릭으로 채택한다. 우리는 또한 z축으로 확장된 구조적 유사성의 추가 척도로서 생성된 상대방으로부터 추출한 지상 진리 깊이 맵과 깊이 맵의 MAE를 보고한다. COCO-Stuff의 SSIM 값과 별도로 우리는 모델이 다른 모델을 큰 폭으로 능가한다는 것을 발견했다. 그림 6에서 볼 수 있듯이, 우리는 우리의 모델이 현장 깊이를 보존하면서 다양한 지역 내 객체들을 충실히 재현할 수 있다는 것을 발견했다. 다른 기준선 모델은 객체 지역화에 성공하지만 깊이 관점을 합성하는 데 어려움을 겪으며 상대적으로 평평하게 보이는 이미지를 낳는다.\n' +
      '\n' +
      '개념 출혈(Podell et al., 2023)으로 알려져 있는 현상은 서로 다른 의미론으로 이어져 의도하지 않은 결과를 초래한다. 소프트 지도가 가능합니다\n' +
      '\n' +
      '그림 5: ** 정성적 결과** 포그라운드/배경 조건은 각 샘플의 왼쪽에 있다.\n' +
      '\n' +
      '그림 6: ** 정성적 재구성 비교** 삼그룹은 COCO-Stuff(Left) 및 Pick-a-Pic(그렇죠)의 검증 샘플에서 추출한 조건을 사용하여 생성된다.\n' +
      '\n' +
      '글로벌 의미학은 이러한 바람직하지 않은 효과를 방지하면서 국부적 영역에 조절된다. 그림 7은 두 가지 상반된 의미론이 국한되어 있는 이 능력을 보여준다. Hi(\\lambda_{\\text{fg}}\\)를 1로 고정하고 \\(\\lambda_{\\text{bg}}\\)를 꾸준히 증가시킴으로써 배경 의미론의 효과가 증폭된다. 그러나 부드러운 지도로 인해 배경 의미론이 심해지면서 전경 물체의 모순된 의미성은 그대로 유지된다. 부드러운 안내 동안 \\(M\\)의 공간 정보가 손실되지만 출혈로 인한 모든 의미학에 대한 장벽을 충실히 생성한다는 것을 발견했다. 추가 회수를 위해 A.4절을 볼 수 있습니다.\n' +
      '\n' +
      '### Discussions\n' +
      '\n' +
      '또. Pick-a-P icrerea 혼합물은 Parere nata altimim의 다이너리, Pareimima Deyying S.nodimi의 디브레이즈, Parere natc 새로운 콘덴티티에서 디비브 항소프 SD를 생성하는데, 이 반모델은 Parere nyyyyyyyyyyyyyyyyyyyyy L. (202 3) 및 포델 에탈. 수은은 다른 데이터인 로크 트라시(20) 로프 디프토프 스레프 리네트 드라프레시(Mwati)를 추가하며, 이튿날의 다른 데이터 세프 레디프(CDP-Fn)는 더 많은 것을 피팅한다.\n' +
      '\n' +
      '5 콘퓨전.\n' +
      '\n' +
      'We presented Compose and Conquer (CnC), a novel text-conditional diffusion model addressing two main challenges in the field: three-dimensional placement of multiple objects and region-specific localization of global semantics from multiple sources. CnC employs two main components: the local and global fuser, which respectively leverages the new Depth Disentanglement Training (DDT) and soft guidance techniques. We show that DDT infers the absolute depth placement of objects, and soft guidance is able to incorporate semantics on to localized regions. Evaluations on the COCO-stuff and Pick-a-Pic datasets illustrates CnC\'s proficiency in addressing these challenges, as demonstrated through extensive experimental results. Since the current framework limits the number of available conditions and the disentangled spatial grounds to the foreground and background, we leave the further decomposition of images into depth portraying primitives and the middle ground to leverage for future work.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c} Model/Condition & \\multicolumn{3}{c|}{COCO-Stuff} & \\multicolumn{3}{c}{Pick-a-Pic} \\\\  & SSIM(\\(\\uparrow\\)) & LPIPS(\\(\\downarrow\\)) & MAE(\\(\\downarrow\\)) & SSIM(\\(\\uparrow\\)) & LPIPS(\\(\\downarrow\\)) & MAE(\\(\\downarrow\\)) \\\\ \\hline Uni-ControlNet & **0.2362** & 0.6539 & 0.1061 & 0.2506 & 0.6504 & 0.1111 \\\\ T2I-Adapter & 0.1907 & 0.6806 & 0.1201 & 0.2238 & 0.6724 & 0.1270 \\\\ \\hline\n' +
      '0.6636 & 0.6431 & 0.1080 \\\\ 0.6421 & 0.1061 & 0.1080 \\\\ < 0.2345 및 0.2345 및 0.2336 & 0.1080 \\\\****CnC** & 0.2345 및 0.\n' +
      '**CnC Finetuned** & 0.2248 & **0.6509** & **0.0990** & **0.2690** & **0.6216** & **0.1027** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: COCO-Stuff 및 Pick-a-Pic 밸브-sets에서 평가된 정량적 재구성 메트릭이다.\n' +
      '\n' +
      '그림 7: ** 상충되는 의미와의 소프트 가이드의 효과** 로컬 퓨저 내의 스트림별 동일한 깊이 맵을 조건하고, 프롬프트 "An Igloo"로 각 샘플을 생성한다. r\\(\\lambda_{\\text{fg}}\\)를 고정하고 \\(\\lambda_{\\text{bg}}\\)를 증가시켜 배경 글로벌 정자의 효과가 실질적으로 증가한다. 부드러운 지침은 두 가지 글로벌 정자가 서로 출혈되는 것을 방지하고 _e.g._개념 출혈을 방지하여 igloo의 의미성을 효과적으로 유지한다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '사려 깊은 조언과 토론에 NAVER 클라우드의 이미지비전 팀에 감사드립니다. 네이버 스마트 기계학습(NSML) 플랫폼(김 et al., 2018)에서 훈련 및 실험을 수행하였다. 이 연구는 BK21 FOUR에 의해 뒷받침되었다.\n' +
      '\n' +
      '## Ethics statement\n' +
      '\n' +
      '확산 모델은 생성 모델의 한 종류로서 유익하고 잠재적으로 유해한 방법 모두에서 사용될 수 있는 합성 함량을 생성할 가능성이 있다. 우리의 작업은 이러한 모델의 이해와 역량을 발전시키는 것을 목표로 하지만 책임 있는 사용의 중요성을 인정한다. 우리는 실무자들이 그러한 모델을 배치할 때 보다 광범위한 사회적 의미를 고려하고 악성 애플리케이션에 대한 안전장치를 구현하도록 권장한다. 구체적으로, 우리 작업에서 이전의 공급원으로 사용하는 확산 모델은 웹스트랩 컬렉션인 LAION(Schuhmann et al., 2021) 데이터셋에서 훈련된다. 데이터셋의 크리에이터가 부적절한 데이터를 걸러내는 최선의 의도에도 불구하고 LAION에는 인종 고정관념, 폭력, 음란물 등 모델들이 내면화하는 데 부적절할 수 있는 콘텐츠가 포함되어 있다. 이러한 문제를 인식하여 유해한 편향과 잘못된 정보의 영구화를 방지하기 위해 그러한 모델을 사용하는 데 엄격한 조사의 필요성을 강조한다.\n' +
      '\n' +
      '## Reproducibility statement\n' +
      '\n' +
      '소스 코드 및 전처리 모델은 [https://github.com/tomtom1103/목적 및 수정](https://github.com/tomtom1103/목적 및 목적 정복)에서 찾을 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Caesar et al. (2018) Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 1209-1218, 2018.\n' +
      '* Dhariwal and Nichol (2021) Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.\n' +
      '* Fang et al. (2019) Hao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao Gou, Yong-Lu Li, and Cewu Lu. Instaboost: Boosting instance segmentation via probability map guided copy-pasting. In _Proceedings of the IEEE/CVF international conference on computer vision_, pp. 682-691, 2019.\n' +
      '* 고엘 등은 (2023) 비디트 고엘, 엘리아 페루조, 예판 장, 데지아 주, 니쿠 세베, 트레보 다렐, 장양 왕, 후미프리 시이를 밝혔다. 페어 확산: 구조 및 응용 쌍 확산 모델이 있는 대상 수준 이미지 편집: __ 구조 및 활용 쌍 확산 모델이 있다. arXiv 프리프린트 arXiv:2303.17546_, 2023.\n' +
      '* Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. _Advances in neural information processing systems_, 27, 2014.\n' +
      '* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 770-778, 2016.\n' +
      '* Hessel et al. (2021) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. _arXiv preprint arXiv:2104.08718_, 2021.\n' +
      '* Heusel et al. (2017) Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* Ho & Salimans (2021) Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In _NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications_, 2021.\n' +
      '* Hessel et al. (2018)* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* Ho et al. (2022) Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. _The Journal of Machine Learning Research_, 23(1):2249-2281, 2022.\n' +
      '*황 등은 (2023) 리앙화황, 디첸, 유류, 유준 선, 델리 자오, 진렌 주 등이 있다. 복합 조건_ arXivp reprinta rXiv:2302.09778_,2 023을 사용한 창의적이고 제어 가능한 이미지 합성이다.\n' +
      '* Kim et al. (2018) Hanjoo Kim, Minkyu Kim, Dongjoo Seo, Jinwoong Kim, Heungseok Park, Soeun Park, Hyunwoo Jo, KyungHyun Kim, Youngil Yang, Youngkwan Kim, et al. Nsml: Meet the mlaas platform with a real-world case study. _arXiv preprint arXiv:1810.09957_, 2018.\n' +
      '* Kirstain 등은 알(2023) 유발 케스터인, 아담 폴리크, 우리엘 싱어, 샤불랜드 마티아나J oeP enna,a ndO merL 에티피:P ick-a-픽: 아토토 퍼지 세로 참고문 또는t 외 이미지 반복을 하지 않는다. rXivpr 이프린타 Xivar Xiv:2305.01569_,20 23.\n' +
      '* Li et al. (2023) Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 22511-22521, 2023.\n' +
      '* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pp. 740-755. Springer, 2014.\n' +
      '* 마우 등은 (2023) 총무, 신타오 왕, 리앙빈 시, 지안 장, 중앙기, 예잉 샹, 샤오후 키이다. T2i-캡터: 학습 스티디구우 tm광석 콘 트롤은 문자에 사용할 수 있는 능력 - 문자에 대한 시간-이미지 온코딩에 사용할 수 있다. arXivp reprin tarXiv:2 302.08453_,2023.\n' +
      '* Nichol et al. (2021) Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.\n' +
      '* Niu et al. (2021) Li Niu, Wenyan Cong, Liu Liu, Yan Hong, Bo Zhang, Jing Liang, and Liqing Zhang. Making images real again: A comprehensive survey on deep image composition. _arXiv preprint arXiv:2106.14490_, 2021.\n' +
      '* Park et al. (2019) Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 2337-2346, 2019.\n' +
      '* Podell et al. (2023) Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* Qin et al. (2020) Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar R Zaiane, and Martin Jagersand. U2-net: Going deeper with nested u-structure for salient object detection. _Pattern recognition_, 106:107404, 2020.\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.\n' +
      '* Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '* Ranftl et al. (2020) Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. _IEEE transactions on pattern analysis and machine intelligence_, 44(3):1623-1637, 2020.\n' +
      '* Ramesh et al. (2020)* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10684-10695, 2022.\n' +
      '* Ronneberger et al. (2015) Olaf Ronneberger, 필리필 피셔 및 토머스 Brox. U-net: 생물의학적 이미지 분할을 위한 콘볼루션 네트워크. E_ 의료 이미지 컴퓨팅 및 컴퓨터 보조 개입-MICCAI 2015: 제18차 국제 회의, 뮌헨, 독일, 2015년 10월 5-9일, 합의, 부분 III 18_ pp. 234-241. 스프링거.\n' +
      '* Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.\n' +
      '* Salimans et al. (2016) Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. _Advances in neural information processing systems_, 29, 2016.\n' +
      '* Schuhmann et al. (2021) Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International conference on machine learning_, pp. 2256-2265. PMLR, 2015.\n' +
      '* Song et al. (2020) Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _International Conference on Learning Representations_, 2020.\n' +
      '* Song et al. (2022) Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and Daniel Aliaga. Objectstitch: Generative object compositing. _arXiv preprint arXiv:2212.00932_, 2022.\n' +
      '* 수보로프 등은 (2022) 로마 수보로프, 엘리자베타 로바바, 안톤 마시크신, 아나스타시아 레미자바, 아르세니 아슈카, 알레세 실레트로프, 내진 콩, 하셜 곡가, 기웅 공원, 빅토르 레미츠키 등을 들 수 있다. 진화-로버는 대형 마스크를 푸리에 컨볼로 담백합니다. 컴퓨터 비전_, pp 2149-2159의 적용에 대한 IEEE/CVF 겨울 회의의 _발표에서 2022년.\n' +
      '* Szegedy et al. (2016) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 2818-2826, 2016.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* 양 등은 알(2023) 비니시 n 양, 샤이 천구, BoZha n g, TingZ 한지 g,Xu e jinChen,Xia oanS 언, 동 첸, 앤판 게르-바 세상타이드 틴프레이션 모 델즈이다. 프로그래핀 gsoftheIEEE/CV F.\n' +
      '* Yang et al. (2023) Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 18381-18391, 2023.\n' +
      '* Zeng et al. (2023) Yu Zeng, Zhe Lin, Jianming Zhang, Qing Liu, John Collomosse, Jason Kuen, and Vishal M Patel. Scenecomposer: Any-level semantic image synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 22468-22478, 2023.\n' +
      '* Zhang & Agrawala (2023) Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. _arXiv preprint arXiv:2302.05543_, 2023.\n' +
      '\n' +
      '* Zhang et al. (2018) Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 586-595, 2018.\n' +
      '* Zhao et al. (2023) Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. _Advances in Neural Information Processing Systems_, 2023.\n' +
      '* Zhou et al. (2021) Tao Zhou, Deng-Ping Fan, Ming-Ming Cheng, Jianbing Shen, and Ling Shao. Rgb-d salient object detection: A survey. _Computational Visual Media_, 7:37-69, 2021.\n' +
      '\n' +
      'Appendix\n' +
      '\n' +
      '일단은.\n' +
      '\n' +
      'Image Composition.Image composition (Niu et al., 2021) involves the task of blending a given foreground with a background to produce a unified composite image. Traditional methods typically follow a sequential pipeline comprising of object placement, image blending/harmonization, and shadow generation. These steps aim to minimize the visual discrepancies between the two image components. With the recent advances in generative models, notably GANs Goodfellow et al. (2014) and DMs Ho et al. (2020), the image composition challenge has been reframed as a generative task. While GAN-based models have led in terms of the number of research contributions, diffusion-based models, as exemplified by works such as ObjectSticht Song et al. (2022) and Paint By Example Yang et al. (2023), showcase the potential of DMs as a one-shot solution for image composition, offering a departure from the multi-step traditional methods. However, it is essential to note that our approach diverges from typical image composition. Rather than aiming to preserve the distinct identity of the foreground and background, our model utilizes them as localized representations for text and global semantics to fill. Although our work aims to solve an inherently different task, we draw parallels to image compositioning in the way we leverage synthetic image triplets and handle the target image to be generated.\n' +
      '\n' +
      'CnC에는 결정 사항이 있습니다.\n' +
      '\n' +
      'Details on the Local Fuser.Depth disentanglement training (DDT) leverages synthetic image triplets \\(\\{I_{f},I_{b},M\\}\\) in order to train the local fuser. DDT first incorporates the depth maps of \\(I_{f}\\) and \\(I_{b}\\) in their own foreground/background streams, as shown in Figure 2. The features from the streams are concatenated along their channel dimension, and features that incorporate both spatial features about \\(I_{f}\\) and \\(I_{b}\\) are extracted in four spatial resolutions. Each extracted feature subsequently passes through a zero convolution layer, and are finally incorporated into \\(E^{\\prime}\\) through feature denormalization layers (Park et al., 2019), as done in Zhao et al. (2023). The frozen SD receives the localized signals from \\(E^{\\prime}\\) and \\(C^{\\prime}\\) at its decoder \\(D\\), integrated by residual skip connections. Denoting outputs of the \\(i\\)-th blocks of \\(E\\), \\(D\\), and \\(C\\) as \\(\\mathbf{e}_{i}\\), \\(\\mathbf{d}_{i}\\), and \\(\\mathbf{c}\\) respectively, and the corresponding outputs of \\(E^{\\prime}\\) and \\(C^{\\prime}\\) as \\(\\mathbf{e}^{\\prime}{}_{i}\\), and \\(\\mathbf{c}^{\\prime}\\), the integration is captured as:\n' +
      '\n' +
      '종종{d}\\mathrm{d}_{i-1}\\mathbf{d}}\\mathbf{e}}\\mathbf{e}}\\mathbf{e}}{j}}\\mathbf{e}}<\\math{e}}{j}}{math{c}}{math{e}}{math{e}}{mathbf{e}}{math{e}}}{mathbf{e}}{math{e}}{mathbf{e}}{d{e}}{d{e}}{d{e}}{d{e}}{d{e}}{d{e}}{math{d{e}}}{d{e}}}{d{e}}}}{d{d{e}}}{d{d{e}}}}}{d{d{e}}}}{d{d{e}}}}}{d{d{e}}}}}}\n' +
      '\n' +
      '글로벌 퓨저에 대한 자세한 내용은 아래의 글로벌 퓨저를 훈련시키기 위해 교차 의도 계층에 소프트 가이드를 통합하는 과정을 요약하는 알고리즘을 제공한다.\n' +
      '\n' +
      '```\n' +
      '\\(I_{s}}}\\),\\(I_{s}\\), \\(I_{s}}=1\\), \\(I_{f{g}), \\(I_{{t}=1\\), \\(\\mathda_{text{g}), \\(\\mathda_{text{g}) \\(I_{ff} <\\), \\(I_{t}=1\\), \\) \\(I_{t}=1\\), \\(I_{t}) \\(I_\\) \\) \\(I_\\) \\(I) \\(I) \\(I) \\(I) \\(I) \\(I) \\(I) \\) \\(I) \\(I) \\(I) \\(I) \\(I) \\(I) \\(I) \\(I) \\(I) \\(I) \\(I) \\(I) \\(I) \\(\n' +
      '1:\\(E_{s},E_{b}) 사다리꼴 상징체{CLIP 이미지 인코더}(I_{s},I_{b})\n' +
      '2:\\((\\mathbf{y}_{\\text{fg}},\\mathbf{y}_{\\text{bg}})\\leftarrow\\textsc{Global Fuser}(E_{s},E_{b})\\)\n' +
      '}(\\mathbf{y}_{\\text{illcat})\\(\\lambda_{\\text{fg}\\mathbf{fg}_{\\text{fg}}\\lambda_{\\text{fg}}\\lambda_{\\text{bg}}\\mathbff{g}_{\\text{g}_{\\text{bg}_{\\text{g}_{\\text{fg}):\\mathbath{ff}_{\\text{g}.\n' +
      '4: \\(E,C,D\\)도(E,C,D\\)도(E,C,D\\)도 \\(E,C,D\\)도의 모든 교차 선택 계층에 대한 4: \\(E,C,D\\)도에서 모든 교차 선택 계층에 대한 것이다.\n' +
      '5:\\((Q,K,V)\\leftarrow(W_{Q}\\cdot\\mathbf{z}_{t},W_{K}\\cdot\\mathbf{y}_{\\text{full}},W _{V}\\cdot\\mathbf{y}_{\\text{full}})\\)\n' +
      '6:\\(S\\leftarrow(QK^{T}/\\sqrt{d})\\)\\(\\triangleright\\)\\(S\\in\\mathbb{R}^{i\\times j}\\)\n' +
      '7:\\(J\\leftarrow\\mathbf{1}^{i\\times(j-2N)}\\)\\(\\triangleright\\) Initialize \\(J\\) as an all ones matrix\n' +
      '8:\\(M) 자작나무 갈매기{Reshape}\\),\\(\\mathrm{Flatten}),\\(M)\\(\\matholdbb{B},^{{i\\i\\)\n' +
      '9:\\(M^{\\prime}\\leftarrow\\mathrm{concat}(J,\\varphi(M),1-\\varphi(M))\\)\n' +
      '10:\\(S^{\\prime} 경쟁자 M^{\\prime}\\)\n' +
      '11:\\(\\mathbf{z}_{t}\\leftarrow\\mathrm{softmax}(S^{\\prime})\\cdot V\\)\n' +
      '12:endfor\n' +
      '```\n' +
      '\n' +
      'I_{s}\\(I_{s}\\)의 단일 훈련 시간표(I_{b}\\)를 생성하기 위해 \\(I_{b},I_{f},I_{b}, M\\})에 \\(I_{b}\\)을 사용하여 표적 프롬시를 위해 특별히 훈련된 SD의 변이체를 사용하여 "빈경치, 고도로 상세하지 않은 사람"으로 설정한다. 또한 널리 채택된 인포팅 모델인 라마(수보로프 등 2022년)를 테스트하고 \\(I_{b}\\) 깊이 맵의 품질을 중심으로 인포팅 모듈로서의 적합성을 측정합니다. 그림 9에서 라마에서 생성된 \\(I_{b}\\)가 파이프라인의 요구 사항과 잘 정렬되지 않을 수 있는 특정 유물을 나타낸다는 것을 관찰했다. 라마의 주목할 만한 특징은 \\(I_{b}\\)의 깊이 맵이 종종 염기성 물체의 모양을 유지하여 로컬 퓨저로 중계된 정보에 영향을 미칠 수 있다는 것이다. 한편, SD 인포팅 모듈은 \\(I_{b}\\)의 생성에 대한 접착을 증명한다. 그림 9의 1열 \\(I_{b}\\)를 중심으로 \\(I_{s}\\)에 한 번 존재하지 않는 특정 객체가 생성되었음을 알 수 있다. SD의 인포팅 모듈의 이러한 속성은 깊이 있는 무력화 훈련으로 레버리지에 매력적이라고 생각하는데, 두드러진 물체의 상대적 배치에 대한 정보를 증류하기 위해서는 우리의 모델이 훈련 중에 두드러진 객체에 의해 폐색된 _see_ 오브젝트가 효과적으로 유지되도록 하는 것이 중요하다. 합성 이미지 트리플트 생성 파이프라인의 시각화를 위해 그림 8을 확인할 수 있다.\n' +
      '\n' +
      '### Additional Results\n' +
      '\n' +
      '보다 정량적 결과는 표 3의 Pick-a-Pic 검증 세트에 대한 추가 정량적 결과를 제공하고 있으며, COCO-Stuff 검증 세트에 대한 경향에 따라 깊이 전용 실험의 FID 및 IS 값을 제외하고 모든 메트릭에서 다른 모델에 비해 적절한 모델을 제공한다. 흥미로운 사실은 표 1에 나타난 COCO-Stuff 검증 결과를 비교할 때 각 모델의 성능 순위가 크게 일치함을 관찰했다. 그러나 FID 및 IS 메트릭에 대한 특정 값은 크게 악화되는 반면 CLIP 스코어는 주목할만한 개선을 보여준다. 이러한 경향의 한 가지 잠재적인 이유는 이러한 메트릭에 사용된 인셉션-V3(Szegedy et al, 2016)과 같은 미리 학습된 모델의 기본 특성과 관련이 있다. 이 실험에서 비교되는 두 이미지 세트는 합성이지만 이러한 모델은 실제 이미지, 본질적으로 실제 이미지 특징 및 패턴을 캡처하는 실제 이미지 상에서 훈련된다. Pick-a-Pic 이미지의 합성 특성은 이러한 실제 기대에서 상당히 분기될 수 있으며, 따라서 인플루언서가 나타날 수 있다.\n' +
      '\n' +
      '그림 8: 합성 이미지 트리플트를 생성하는 프로세스입니다.\n' +
      '\n' +
      '그림 9: 인포팅에 대한 라마 및 SD의 비교 및 해당 깊이 맵이다. 라마에 의해 침투된 이미지는 두드러진 물체를 제거하는 것처럼 보이지만, 그들의 대응하는 깊이 맵에는 두드러진 물체의 유물이 포함되어 있다.\n' +
      '\n' +
      'ing the FID scores. Moreover, even if both datasets under comparison are synthetic, the variance and distribution of features in the synthetic Pick-a-Pic dataset could be distinct enough from typical real-world datasets to lead to the observed differences in FID and IS scores. This highlights the nuances associated with evaluating models on synthetic versus real datasets and emphasizes the need for careful consideration when drawing conclusions from such evaluations.\n' +
      '\n' +
      'Ordering of localized objects.In Figure 10, we compare our model\'s ability to place objects in front of another through the use of local conditions against Uni-ControlNet. Uni-ControlNet is able to take in 7 local conditions, and report that the local conditions pair listed in Figure 10 are the most robust in handling conflicting conditions, _i.e._ two overlapping objects. Although some samples do show the foreground local condition being placed in front of its counterpart, Uni-ControlNet often fails in conveying a sense of depth in its samples, resulting in the two objects to be generated on the same z-axis. On the other hand, even if the two depth maps conditioned have relatively same depth values, our model is able to consistently occlude overlapping parts of the background depth map.\n' +
      '\n' +
      'Additional details on Reconstruction.In Figure 11, we provide additional samples of the depth maps extracted from the reconstruction experiment detailed in Section 4.2. Although depth maps produced by MiDaS does not reflect the true metric depth of an object, comparing the depth maps of the ground truth images and the reconstructed images shed light onto how well models reflects the images and depth maps exposed to them during training. While the reconstructed depth maps of baseline models hold the overall shape of an object, it can be seen that our model succeeds in capturing the relative depth of regions relative to the salient object. Additionally, we elucidate our choice in selecting MAE as a metric for gauging the quality of reconstruction. Although depth maps produced by MiDaS do not predict metric depth as mentioned above, our model and baseline models were trained to generate images based on image-depth map pairs. Comparing depth maps\n' +
      '\n' +
      '그림 10: Uni-대조군Net에 대한 국부적 조건의 주문 능력을 비교한다. 유니 컨트롤넷은 열거된 조합이 7개의 지역 조건의 다른 조합에 대해 가장 효과적이라고 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c|c c c} Model/Condition & \\multicolumn{3}{c|}{Depth} & \\multicolumn{3}{c|}{Semantics} & \\multicolumn{3}{c}{Depth+Semantics} \\\\  & FID (\\(\\downarrow\\)) & IS (\\(\\uparrow\\)) & \\multicolumn{2}{c|}{CLIP} & \\multicolumn{2}{c|}{FID (\\(\\downarrow\\))} & IS (\\(\\uparrow\\)) & \\multicolumn{2}{c|}{CLIP} & \\multicolumn{2}{c}{FID (\\(\\downarrow\\))} & IS(\\(\\uparrow\\)) & \\multicolumn{2}{c}{CLIP} \\\\  & FID (\\(\\downarrow\\)) & IS (\\(\\uparrow\\)) & \\multicolumn{2}{c|}{Score (\\(\\uparrow\\))} & \\multicolumn{2}{c|}{FID (\\(\\downarrow\\))} & \\multicolumn{2}{c|}{Score (\\(\\uparrow\\))} & \\multicolumn{2}{c}{FID (\\(\\downarrow\\))} & \\multicolumn{2}{c}{IS(\\(\\uparrow\\))} & \\multicolumn{2}{c}{Score (\\(\\uparrow\\))} \\\\ \\hline GLIGEN & 22.540 & 12.733 & 28.227 & - & - & - & - & - & - \\\\ ControlNet & **21.183** & **13.685** & 28.112 & - & - & - & - & - & - \\\\ Uni-ControlNet & 24.561 & 13.260 & 28.053 & 28.964 & 12.809 & 25.245 & 22.808 & 12.006 & 26.722 \\\\ T2I-Adapter & 26.262 & 13.309 & 28.017 & 47.996 & 11.408 & 25.033 & 34.698 & 10.745 & 26.583 \\\\ \\hline Cnc & 28.192 & 11.460 & 27.347 & 36.272 & 10.353 & 24.301 & 25.524 & 11.131 & 27.109 \\\\ CnC Finetuned & 32.155 & 11.512 & **28.274** & **26.042** & **12.838** & **27.681** & **22.484** & **12.602** & **28.094** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: Pick-a-Pic 밸브 세트에 대한 평가 메트릭. 우리는 이러한 조건을 지원하지 않는 모델로 인해 GLIGEN과 ControlNet에서 의미론적 및 깊이+제학 결과를 생략한다. 가장 큰 결과는 **bold***입니다.\n' +
      '\n' +
      '지상 진리 깊이 맵이 주어진 모델 "예측자" 깊이 지도가 얼마나 잘 되는지 생각해 볼 수 있으며, 이는 다시 모델이 이미지와 깊이 맵 간의 관계를 얼마나 잘 학습했는지 나타낸다. 그림 12에서 우리는 MAE 측면에서 그라운드 진리 깊이 맵이 재구성된 깊이 맵과 얼마나 유사한지 간의 관계를 시각화한다. 각 세트는 가장 낮은/가장 낮은 MAE 값을 갖는 상위 50쌍에서 무작위로 선택되었다. 가장 낮은 깊이 맵(MAE) 점수를 가진 쌍은 복원된 이미지가 그라운드 진리 이미지에 존재하는 상대 깊이를 충실히 묘사하는 등 재구성 품질을 직접적으로 초래한다는 것을 알 수 있다. 반면, MAE 점수가 가장 높은 쌍은 하위 비교 재구성된 이미지를 초래한다. 그림 12(b)의 두 번째 행을 예로 들면, 복원된 이미지는 지상 진리 이미지에 존재하는 나무와 사람의 상대적 깊이를 포착하는 데 실패함을 알 수 있다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '다양한 공간 조건에서 물체를 공간 영역으로 효과적으로 주문하는 우리의 모델의 능력은 공간적 정보의 공간적 정보가 증류된 깊이 무력화 훈련에서 비롯되며, 배후에 있는 것은 로컬 퓨저의 각각의 하천으로 증류된다. 이를 위해 해당 조건이 이미지의 공간 정보를 보유하고 있다는 점을 감안할 때, 우리의 모델은 다양한 유형의 지역 조건에 대해 학습될 수 있음을 알 수 있다. 우리는 로컬 퓨저의 능력을 탐색하고 그림 13의 깊이 맵과 비교하여 캐니 모서리에 대한 훈련의 영향을 보여주는데, 이 가장자리의 값이 바이너리인 방식으로 깊이 맵과 다른 특성과 편향을 보유하며 더 미세한 등급이 매겨진 디테일로 깊이를 나타내는 능력을 차단한다. 이러한 특성 때문에 DDT는 캐니 모서리에도 물체의 상대적 배치를 배우는 반면, 캐니 모서리를 사용하는 것은 고유한 장단점을 가지고 있음을 알 수 있다. 그림 13(a)와 (c)는 깊이 지도를 사용하는 것이 선호되는 경우를 보고하고, (b)와 (d)는 반대 의견을 보고한다. 우리는 사과가 비교적 평평해 보이는 경우(c)와 같이 깊이의 감각을 생성하는 데 종종 실패한다는 것을 알게 된다. 그러나, 이 속성은 평평하게 시작하는 베이스 이미지를 레버링할 때 선호될 수 있다. 그림 13(b)와 (d)는 이러한 사례를 보여주는데, 여기서 평탄한 베이스 이미지들의 깊이 맵들( 포스터 및 벡터 그래픽들 등)은 공간 정보를 캡처하지 못하여 하위-파라 이미지를 생성한다. DDT가 쇠약함 또는 깊이 지도가든 주어진 표현의 유도적 편향을 효과적으로 레버리지할 수 있고 특별한 경우 DDT가 주어진 표현의 변이체를 효과적으로 레버리지할 수 있다는 것을 발견했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c}  & \\multicolumn{3}{c|}{COCO-Stuff} & \\multicolumn{3}{c}{Pick-a-Pic} \\\\ Model/Condition & FID (\\(\\downarrow\\)) & IS (\\(\\uparrow\\)) & CLIPScore (\\(\\uparrow\\)) & FID (\\(\\downarrow\\)) & IS (\\(\\uparrow\\)) & CLIPScore (\\(\\uparrow\\)) \\\\ \\hline Uni-ControlNet (Canny Edge) & **17.119** & **30.440** & 25.726 & 21.955 & **12.469** & 28.517 \\\\ T2I-Adapter (Canny Edge) & 20.051 & 28.449 & 25.850 & 30.547 & 12.230 & 28.412 \\\\ \\hline\n' +
      '**CnC (Canny Edge, Ours)** & 17.745 & 29.809 & **26.283** & **20.501** & 12.215 & **28.786** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: COCO-Stuff 및 Pick-a-Pic 밸브 세트에 대한 캐니 에지 평가 메트릭. 가장 큰 결과는 **bold***입니다.\n' +
      '\n' +
      '그림 11: 재구성된 이미지에서 추출한 깊이 맵의 정성적 비교는 그림이다.\n' +
      '\n' +
      '그림 12: 깊이 맵 쌍의 정성적 비교는 각 데이터 세트에서 가장 낮은/가장 높은 MAE 값을 나타낸다. 각 세트는 각각 COCO-Stuff 및 Pick-a-Pic의 가장 낮은/가장 높은 MAE 값의 상위 50쌍에서 무작위로 선택되었다.\n' +
      '\n' +
      '그림 13: 로컬 퓨저의 표현으로서 캐니 엣지에 대한 구조 연구는 다음과 같다. (a)와 (c)는 깊이 맵이 더 나은 이미지를 생성하는 경우를 보고하고 (b)와 (d)는 캐니 모서리가 선호될 수 있는 사례를 보고한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:20]\n' +
      '\n' +
      '그림 15: 갈등 의미론. 전경 글로벌 시맨틱 이미지는 용암이 흐르는 이미지이고, 배경 글로벌 시맨틱 이미지는 눈밭의 이미지이다. 우리는 텍스트 프롬프트를 "화산"으로 수정하고 하이퍼파라미터 \\(\\lambda_{\\text{fg}}\\)와 \\(\\lambda_{\\text{bg}}\\)의 효과를 보여준다.\n' +
      '\n' +
      '그림 16: 테그 후미 nt 헤페노는 에멀젼스, 니디 tsr 에스시클레아 이매틱스 코어, 에코 페이즈는 트레미 nt 헬 오송 사용자, 리빙아 펠릿을 낭비한다.\n' +
      '\n' +
      'Figure 17: Depth map Ablations. We find that our model generalizes well to different versions of depth maps. Version 1 refers to depth maps extracted from \\(I_{s}\\). Version 2 refers to depth maps extracted from \\(I_{f}\\). Version 3 refers to depth maps extracted from \\(M\\otimes\\mathrm{depthmap}(I_{f})\\).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:23]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>