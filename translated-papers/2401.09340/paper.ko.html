<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# SceneVerse:\n' +
      '\n' +
      '계획된 모델을 위한 3D 비전-언어 학습.\n' +
      '\n' +
      ' 유야야 자송 니수오네그 자니아\\({}^{**}\\)\n' +
      '\n' +
      '리위, 황우.\n' +
      '\n' +
      '베이징종합인공지능연구소(BIGAI).\n' +
      '\n' +
      '[https://scene-verse.github.io](https://scene-verse.github.io)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '3D 물리적 환경과 언어를 정렬하는 데 중점을 둔_3D 비전 라운딩은 체화된 에이전트의 개발에 초석으로 서 있다. 2D 영역의 최근 진보와 비교하여 3D 장면의 접지 언어는 다양한 객체 구성, 풍부한 속성 및 복잡한 관계로 인한 3D 장면의 고유한 복잡성, (i) 쌍을 이루는 3D 비전-언어 데이터의 부족, (ii) 접지된 3D 데이터로부터 지식을 증류시키기 위한 통일된 학습 프레임워크의 부재 등 몇 가지 중요한 도전에 직면해 있다. 이 연구에서 우리는 실내 환경에서 3D 비전-언어 학습을 체계적으로 업스케일링할 수 있는 가능성을 조사하여 3D 비전-언어에서 이 세 가지 주요 과제를 해결하는 것을 목표로 한다. 우리는 약 \\(68\\)K 3D 실내 장면을 포함하고 인간 주석 및 확장 가능한 장면-사진 기반 생성 접근법에서 파생된 \\(2.5\\)M 비전-언어 쌍을 포함하는 첫 **100만 규모의** 3D 비전-언어 데이터세트 SceneVerse를 소개한다. 우리는 이 스케일링이 3D 비전 언어 학습을 위해 통합 사전 훈련 프레임워크인 스카이크(GPS)를 준비한다는 것을 보여준다. 광범위한 실험을 통해 기존의 모든 3D 시각적 접지 벤치마크에 대한 최첨단 성능을 달성하여 GPS의 효과를 보여주고 있다. 스카네버스와 GPS의 방대한 잠재력은 도전적인 3D 비전-언어 작업에서 제로샷 전달 실험을 통해 공개된다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'The foundation of human cognitive development lies in the grounding of language within the physical world [46, 73, 97]. Recent progress in Large Language Models (LLMs) [11, 75], often referred to as "foundation models" [10], has markedly promoted the alignment between vision and language [3, 51, 66] through utilizing billion-scale vision-language datasets [71, 96]. Nonetheless, with these advancements predominantly focusing on the 2D domain, the grounded understanding of 3D physical environments remains in an incipient stage [1, 5, 16]. Recognizing the pivotal role of grounded 3D experiences in shaping human cognition [7, 8] and the delayed research development in this area, there is a compelling need to intensify the exploration into the vision-language learning challenge, specifically in the context of 3D scenes.\n' +
      '\n' +
      '2D 비전-언어(2D-VL) 성과로부터 통찰력을 보고, 성공의 주요 요인은 쌍을 이루는 시력-언어 데이터[15, 45, 71]의 주목할 만한 스케일 업이었다. 그러나 이러한 원칙을 2D에서 3D로 직접 적용하는 것은 도전으로 인해 취약하다. 주로 3D 데이터 수집은 스캐닝 장치에 크게 의존하여 2D 이미지를 수집하는 것보다 본질적으로 훨씬 복잡하고 비용이 많이 든다. 3D 장면 데이터[9, 23, 58, 87]의 부피를 증가시키기 위한 꾸준한 노력에도 불구하고 대부분의 데이터 세트는 수천 장면으로 제한되었으며, 이는 실질적으로 기존 2D 데이터 세트의 규모보다 뒤쳐진다. 이 격차는 다양한 속성, 다양한 배열 및 복잡한 객체 간 관계를 갖는 다양한 객체 인스턴스를 특징으로 하는 3D 장면들의 고유한 복잡성에 의해 더욱 확대되었다. 3D 장면의 이러한 독특한 측면은 물체에 대한 정확한 설명과 이들의 관계를 더 어렵게 만들 뿐만 아니라 철저한 장면 묘사에 필요한 언어 설명의 수를 상당히 증가시킨다. 결과적으로, 이는 지상 장면 이해에 중요한 고품질 쌍을 이루는 장면-언어 데이터의 충분한 공급을 얻는 데 중요한 도전을 제시한다.\n' +
      '\n' +
      'To confront these challenges, we propose consolidating current efforts to build up SceneVerse, the first **million-scale** dataset aimed at advancing 3D vision-language (3D-VL) learning for grounded scene understanding. At the scene level, we unify 3D scene data from existing datasets [9, 23, 40, 67, 78] and supplement the collection with synthetic scenes [27, 95]. This compilation represents the most extensive 3D scene data gathered to date, amounting to \\(68,406\\) scenes for grounding. Additionally, we propose an automated generation pipeline utilizing 3D scene graphs [4, 79] and LLMs to create comprehensive, high-quality scene-language pairs. This refined collection, including \\(190,836\\) human annotated pairs and totaling \\(2.5\\)M scene-language pairs, provides detailed and comprehensive portrayals of both object-level and scene-level descriptions within the 3D scene.\n' +
      '\n' +
      '우리는 대규모 사전 학습을 통해 스켄버스에서 데이터 스케일 업이 제공하는 잠재력을 철저히 조사한다. 구체적으로 장면 수준 및 객체 수준 정렬 목표로 설계되고 보조 손실 및 설계가 없는 스켄(GPS)을 위한 새롭고 통일된 사전 훈련 프레임워크를 제시한다. 다단계 대비 정렬을 통해 기존의 모든 3D 시각적 접지 벤치마크에 걸쳐 유의한 성능 개선을 관찰하여 간단하고 효과적인 사전 훈련 과정을 통해 새로운 최첨단 결과를 달성했다. 또한, 우리는 스켄버스와 GPS가 제공하는 방대한 가능성을 제로 샷 전송 설정에서 3D-VL 작업에서 공개하고 있다. 마지막으로, 우리는 미래의 방향을 지적하기 위해 광범위한 절제 실험을 통해 스켄버스에서 데이터-스케일링 효과에 대한 보다 포괄적인 이해도를 제공한다.\n' +
      '\n' +
      '우리의 주요 기여금은 다음과 같이 요약될 수 있다.\n' +
      '\n' +
      '* We introduce SceneVerse, the first million-scale 3D-VL dataset for grounded scene understanding. SceneVerse encompasses \\(68\\)K 3D scenes coupled with \\(2.5\\)M scene-language pairs, sourced through a combination of human annotation and automated generation methods. This represents a significant improvement in terms of data diversity and scale compared to prior datasets.\n' +
      '* We propose GPS, an efficient transformer-based model trained with multi-level scene-text alignment that achieves state-of-the-art results on all existing 3D-VL grounding benchmarks, benefiting from pre-training on multi-level scene-language pairs in SceneVerse.\n' +
      '* 우리는 데이터 스케일업 및 모델 설계와 함께 미리 훈련된 모델이 2D-VL 모델에서 볼 수 있는 성공과 일치하여 그라운드 장면 이해에서 새로운 제로 샷 일반화 능력을 나타낸다는 것을 보여준다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '정렬된 3D 언어 데이터를 달성하기 위한 데이터베이스는 본질적으로 어려운 작업이다. 3D 객체 모델링에서 샤프넷[14]과 같은 선구적 작품은 온라인 리플렉스에서 3D 자산을 조달하여 고품질 3D 객체 데이터셋[22, 60, 81]의 후속 증식으로 이어진다. 특히, 최근 개발에는 3D-언어 정렬에 대한 객체 수준 캡션[83]의 통합과 함께 오바마자버스 [25, 26]을 사용한 인터넷 규모의 데이터 수집이 포함된다. 이러한 데이터 세트에 대해 훈련된 모델은 분류 [52], 생성 [53], 자막 작업[55]에서 분명하고 오브젝트에 대한 향상된 이해도를 보여준다.\n' +
      '\n' +
      'In contrast, developing datasets for grounded 3D scene understanding is even more challenging due to the extensive requirements for scene acquisition and annotation. Existing works curate RGB-D and scanned indoor scene datasets [9, 13, 23, 58, 67, 78] initially used for benchmarking classical grounding tasks like 3D object detection and segmentation [30, 42, 59, 72, 77]. These semantically labeled scenes are subsequently applied in fine-grained scene grounding tasks like object referral [1, 16, 93], captioning [17, 19, 20, 88], vision-language-navigation [38, 56, 63, 80] and reasoning [5, 37, 57]. Recent work exploits the representation of 3D scene graphs (3DSGs) [4, 69, 79], which concisely describes scenes with hierarchical structures. This representation is notably advantageous for planning [2, 68] and captioning [33], owing to its compatibility with LLMs. Nevertheless, as shown in Tab. 1, these datasets are significantly constrained in both scene and language scales, underscoring the need for scaling up fine-grained scene-language-aligned data to enhance grounded scene understanding.\n' +
      '\n' +
      '**Vision-Language Learning** Recent years have witnessed tremendous progress in 2D vision-language learning [3, 24, 49, 51, 66, 70, 76], empowered by transformer-based pre-training models [11, 28, 62] and large-scale image-language datasets [15, 71]. A central theme across language and 2D-VL domains is the effectiveness of data scaling [43], as demonstrated by improved alignment and expanded capabilities in open-vocabulary understanding [32, 44, 47, 50] through a simplified contrastive pre-training pipeline [66].\n' +
      '\n' +
      'However, in grounded scene understanding, the primary challenge for models has been the limited availability of paired 3D scene-language data, which restricts the application of insights gained from 2D-VL. Current models for 3D scene grounding [6, 18, 35, 40, 41, 54, 82, 86, 94] heavily rely on task-specific knowledge in both model and loss designs or advanced optimization strategies [98]. To bridge this gap, there has been a growing emphasis on employing pre-trained 2D-VL models for 3D-VL [34, 36, 64, 74, 83, 91, 92]. Nonetheless, these models predominantly draw on information available from 2D-VL models (_e.g_., object attribute, affordance, _etc_.), falling short on capturing crucial information like object spatial relationships, which are only attainable through 3D data. This urges the need for a multi-level alignment between language and 3D scenes, particularly regarding 3D-specific information. Considering the nascent stage of existing 3D pre-training methods [29, 84, 98], we believe SceneVerse and GPS have the potential to spear-head new avenues in 3D-VL research.\n' +
      '\n' +
      '## 3 SceneVerse\n' +
      '\n' +
      'SceneVerse는 지상 장면 이해를 위해 고안된 최초의 100만 규모의 데이터세트이다. 우리의 3D 장면들은 실제 및 합성 환경의 다양한 기존 데이터 세트들로부터 큐레이션된다. 3D 장면 그래프와 LLM의 힘을 살려, 대상 수준 및 장면 수준 설명 모두에 대해 종합적이고 고품질 언어를 생성하기 위한 자동화 파이프라인을 소개합니다. 우리는 현재까지 가장 광범위한 인간 미공개 객체 참조를 통합하여 이 분야에서 새로운 훈련 소스 및 벤치마크를 제공한다.\n' +
      '\n' +
      '### Scene Curation\n' +
      '\n' +
      'To address the scarcity of available 3D scene data, we construct SceneVerse by unifying 3D scene data from various existing datasets. We use real-world scene datasets, including ScanNet [23], ARKitScenes [9], HM3D [67], 3RScan [78] and MultiScan [58], alongside synthetic environments from Structured3D [95] and ProcTHOR [27]. The inclusion of these synthetic datasets is mainly motivated by their potential as scalable data sources for 3D-VL alignment. To ensure cohesion across various sources, we conduct preprocessing steps such as room segmentation, point subsampling, axis alignment, normalization, and semantic label alignment. Each scan is represented by a point cloud \\(\\mathrm{P}\\in\\mathbb{R}^{N\\times 8}\\), wherein each point is defined by its 3D coordinates, RGB color, instance id and semantic label. In total, we curate \\(68,406\\) 3D scenes in SceneVerse.\n' +
      '\n' +
      '3D 스켄 브러시 건설.\n' +
      '\n' +
      'Our 3D scene graph is defined as a set of tuples \\(\\mathcal{G}=(\\mathcal{V},\\mathcal{E})\\), where the nodes \\(\\mathcal{V}\\) comprises \\(\\mathcal{V}_{1}\\bigcup\\mathcal{V}_{2}\\bigcup\\ldots\\bigcup\\mathcal{V}_{K}\\), with \\(\\mathcal{V}_{k}\\) representing the set of nodes at a particular hierarchical level. Each node \\(v\\) represents one distinct 3D object instance, parameterized by its centroid \\(\\mathbf{p}_{i}\\in\\mathbb{R}^{3}\\) and bounding box size of \\(\\mathbf{b}_{i}=(b_{x},b_{y},b_{z})\\in\\mathbb{R}^{3}\\). The edges \\(\\mathcal{E}\\) represent spatial relationships between nodes.\n' +
      '\n' +
      'To construct the scene graph \\(\\mathcal{G}\\), we first instantiate the nodes with the instance annotation from the point clouds and assign object classes with their corresponding semantic labels. Following prior work[1, 79], we consider the following spatial relations.\n' +
      '\n' +
      '**Vertical proximity** This encompasses both in-contact relationships (_e.g_., support, inside, embed), and non-contact ones (_e.g_., above, below).\n' +
      '\n' +
      '**Horizontal proximity** Horizontal relationships describe the proximity relations like in front of, next to, behind, _etc_. Relationships like left, right are contextually dependent on a reference view, where another\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline \\multirow{2}{*}{Dataset} & \\multicolumn{2}{c}{3D Data} & \\multicolumn{3}{c}{Language} \\\\ \\cline{2-5}  & Scene & Object & Anno. & Syn. & Total \\\\ \\hline ScanRefer[16] & \\multirow{2}{*}{\\(|\\)} & \\multirow{2}{*}{\\(|\\)} & 52K & - & 52K \\\\ ReferIt3D[1] & & & 42K & 200K & 242K \\\\ ScanQA[5] & 1.5K & 33K & 27K & - & 27K \\\\ SQA3D[57] & & & - & 33K & 33K \\\\ Multi3DRefer[93] & \\multirow{2}{*}{\\(|\\)} & \\multirow{2}{*}{\\(|\\)} & 52K & 10K & 62K \\\\ Cap3D[55] & & & 66K & 58K & 666K & 724K \\\\ ScanScribe[98] & 3K & 56K & 94K & 184K & 278K \\\\ \\hline \\hline SceneVerse & 68K & 1.5M & 190K & 2.3M & 2.5M \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: ** SceneVerse와 기존 3DVL Datasets의 비교는 표 1: **이다. SceneVerse는 사전 작업의 데이터 척도를 크기만큼 확장한다. 아니, 인간 주석. 신: 템플릿 또는 LLM 생성 설명**만터 객체는 뷰 방향을 설정하기 위해 사용된다. 두 객체 사이의 거리 또한 객체가 우주에서 멀리 있는지 또는 근처에 있는지 여부를 설명하기 위해 계산된다.\n' +
      '\n' +
      '다객체 관계는 다중 객체, _e.g_의 공간적 배열, 정렬 및 사이를 모델링한다.\n' +
      '\n' +
      '노드 계층화는 Wetrave rseeal ltheobj 엑노데스토칼 큐가 엉덩이의 공간적 상대, 즉 인터세그 오아나우토 무시베리카트 이온pr이 정확한 수정을 위해 수정된 것이다. 포밍된 병인은 세스 n 에그 인스트 감산드르 라 티오 니십이 미니화를 억제하여 무균 리커토Appen dixA.2를 억제한다.\n' +
      '\n' +
      'LLMs.\n' +
      '\n' +
      'The scene-language pairs in SceneVerse aim to capture varying aspects of the 3D scene, which include detailed object attribute descriptions in object captioning, spatial relationships between objects in object referral, and global scene descriptions in scene captioning. Based on the 3D scene graph, we utilize both templates and LLMs to automatically generate descriptions on these three granularities.\n' +
      '\n' +
      'Object CaptioningObject captions aim to provide detailed descriptions of an object\'s visual and physical properties, facilitating object-level grounding with its distinctive features. Given the multi-view images, we utilize the point cloud of the object \\(v\\in\\mathcal{V}\\) to identify its occurrence in the images through rendering. The images are then cropped with the rendered bounding boxes and processed through BLIP2 [48] to generate initial object captions. To refine the captions, we select the top 10 sentences with the highest CLIP [66] similarity score and minimal occlusion. The selected sentences are fed into a LLM to obtain a coherent summary of the object captions. In this process, we explicitly instruct the language model to identify and correct the potential errors. The detailed object captioning pipeline is illustrated in Appendix A.3.\n' +
      '\n' +
      'Object ReferralObject relationship captions refer to objects by articulating their spatial relationships in the scene. Spatial relationship triplets \\((v_{i},v_{j},e_{ij})\\) are first extracted from the constructed 3D scene graph. We design various templates to generate descriptions for each relationship type, assigning the entities in the form of (target-object, spatial-relation, anchor-object(s)). This results in examples like "the chair is next to the armchair", "facing the sofa, there is a suitcase far to the right of the shoes", and "the fridge is between cabinet and sofa". To add complexity to the template-based descriptions, we design "star-reference" templates, where the reference to the target object is generated by describing its relationship to 3 randomly chosen adjacent objects in the scene graph. Our designed templates span passive and active tenses, as well as inversion clauses, contributing to the richness of the generated text. To enhance the naturalness of the descriptions, we employ LLM for sentence rephrasing. Fig. 2 presents statistics for the descriptions before and after rephrasing.\n' +
      '\n' +
      '그림 2: **SceneVerse 수집 및 통계. 3D 장면(a)을 감안할 때, 당사의 자동화 파이프라인(c)은 장면 캡션, 객체 캡션 및 객체 추천을 포함한 세 가지 유형의 설명을 생성한다. (b) 다른 언어원 및 데이터 구성의 비교****(b) 다른 언어원 및 데이터 구성의 비교.\n' +
      '\n' +
      'Skeyo bonininthesc enealwit htheiratleanutesa ndfunc tionality입니다. 유레이는 에크홀데프 스캅스 에코 앱티오 ns를 공급하며, 세이브 아필레 에페아 테라스 ts는 엔소프 라프로 바이로 사이프 ts이며, 로 오트 요페안 도베라 트라베테와 함께 오트 요페타 타르드 이스트린타 데이터셋을 사용한다.\n' +
      '\n' +
      '후만인에 의한 설명.\n' +
      '\n' +
      'In addition to automatically generated scene-text pairs, SceneVerse includes the most comprehensive set of human-annotated, context-rich object referrals to date, serving as a valuable benchmark for assessing grounded scene understanding capabilities. The human annotations contain \\(96,863\\) descriptions in ARKitScenes [9], HM3D [67] and MultiScan [58]. During the annotation process, one human annotator was assigned to write at least 20 words to distinctly refer to a single 3D object within a 3D scene. Each referral text then undergoes independent verification by two additional reviewers, both mandated to accurately locate the referenced object based on the 3D scene and the annotated referral text. Any object referrals that do not pass the verification by either reviewer are flagged for re-annotation.\n' +
      '\n' +
      '소매 및 통계.\n' +
      '\n' +
      '총 SceneVerse는 총 \\(68,406\\) 룸 레벨 3D 스캔으로 구성되며 소스 구성은 그림 1에 나와 있다. 2(b). 데이터세트에는 이전 작업 [1, 79]에 따른 \\(21\\) 유형의 관계를 포함하는 \\(1.5\\)M 객체 인스턴스가 포함되어 있다. 언어 설명을 위해 우리는 라마[75]와 GPT-3.5 [61]에 의해 재연된 LLM에 의해 \\(1\\)M 템플릿 기반 텍스트와 \\(1\\)M 문장을 생성한다. 모든 재피싱 및 요약 프롬프트는 전체 관계 세트와 함께 부록 A.3에 자세히 설명되어 있으며 자동화된 언어 생성 파이프라인의 효능을 확인하기 위해 12K 생성 객체 수준 설명을 인간 검증을 위해 무작위로 선택하여 \\(96.93\\%\\) 합격률을 달성하는 품질 검사(QC)를 수행한다. 이것은 우리의 제안된 장면 사진 기반 생성 접근법이 고품질 언어 설명을 생성하는 능력을 보여주며 미래의 확장성에 강력한 기초를 제공한다.\n' +
      '\n' +
      '4 G가 스톱을 위한 Pre-트레이닝을 조직했다.\n' +
      '\n' +
      'In this section, we introduce GPS, an efficient transformer-based model trained with multi-level contrastive losses for aligning 3D scenes and texts. As shown in Fig. 3, we echo the language descriptions collected at different levels to form scene-language pairs at both object-level, referral-object-level, and scene-level for contrastive objectives in GPS. We describe the design of each level in the following sections.\n' +
      '\n' +
      '### Object-level Grounding\n' +
      '\n' +
      '3D 장면 패턴(\\{b{T}\\:\\dath{S,\\{S}) 오프-시트페나프(\\{f{T}<\\{f}} a\\{math{2}}\\math{dot,\\b{n}}{n}^{{N}\\\\\\)를 통해 트라베스(\\{f}s.\n' +
      '\n' +
      '\\[\\begin{split}\\mathcal{L}_{\\text{obj}}=-\\frac{1}{2}\\sum_{(p,q)} \\left(\\log\\frac{\\exp\\left(D^{\\text{obj}}(p,q)\\right)}{\\sum_{r}\\exp\\left(D^{ \\text{obj}}(p,r)\\right)}+\\right.\\\\ \\left.\\log\\frac{\\exp\\left(D^{\\text{obj}}(p,q)\\right)}{\\sum_{r} \\exp\\left(D^{\\text{obj}}(r,q)\\right)}\\right),\\end{split} \\tag{1}\\]\n' +
      '\n' +
      '(D^{f}_{p}.{p}d\\) C.\n' +
      '\n' +
      '### Scene-level Grounding\n' +
      '\n' +
      'With aligned object features, we encode the scene by incorporating object spatial locations into the extracted object features. Specifically, we use a spatial transformer model to encode extracted object features \\(\\{\\mathbf{f}_{i}^{O}\\}\\) with their spatial location features \\(\\{\\mathbf{l}_{i}\\}\\) following [18, 98]:\n' +
      '\n' +
      '\\[\\mathbf{f}^{S}=\\mathrm{SpatialAttn}(\\{\\mathbf{f}_{i}^{O}\\},\\{\\mathbf{l}_{i}\\})\\]\n' +
      '\n' +
      '(}\\) 아도토파노포비토비르(ncodingwimimestesthefeatureofob) 직종(nco)은 gn.\n' +
      '\n' +
      '제안된 GPS 모델*****의 대조 정렬은 모델 학습을 위한 마스크 언어 모델링 객관적인 \\(\\mathcal{L}_{\\text{L}_{\\text{obj}}\\) 뿐만 아니라 \\(\\mathcal{L}_{\\text{L}_{\\text{MLM}}\\) 세 가지 수준(\\mathcal{L}_{\\text{L}_{\\ill{L}_{\\)에서 3가지 수준, \\(\\mathcal{L}_{\\{L}_{\\)에서 대조 정렬, \\(\\mathcal{L}_{\\ill{L}_{\\)과 \\(\\mathcal{L}_{\\ill{L}_{\\)에서 대조 정렬), \\(\\mathcal{L}_{\\)에서 대조 정렬, \\(\\mathcal{L}_{\\ill{L}_{\\) 및 \\(\\mathcal{L}_{\\ill{I}_{\\:{\\/{\\)에서 대조 정렬, \\\n' +
      '\n' +
      '우리는 이러한 장면 수준 객체 특징(\\{\\\\\\mathbf{f}_{i}^{S}\\}\\)에서 작동하며 장면 캡션 \\(\\mathbf{T}^{\\text{scene}}\\)과 정렬한다. 구체적으로, 우리는 객체 특징을 투영층으로 공급하여 모든 객체 특징들에 대한 맥스풀링을 사용하여 장면 특징 \\(\\mathbf{g}^{S}\\)을 얻는다. 객체 레벨 접지과 유사하게 튜닝 가능한 언어 모델을 통해 장면 자막을 통과시켜 텍스트 특징 \\(\\mathbf{g}^{T}\\)를 획득하고 장면 수준 대비 정렬을 수행한다.\n' +
      '\n' +
      '카페인{scene}}=D^frac{scene}}(p,q))}}\\log{\\sum_{TP}}}}\\log{\\sum_{TP}}}(p,q)}\\log\\frac{{r}} \\log\\frac{{r}}(D^{\\obacterium{r}})}<\\log\\\\fusion{{r}}:\\\\fusion{fart{TP}}} <\\\\fusion{erg{TP}}}}{d\\fusion{TP}}}}}}{\\\\\\\\\\\\\\{s\\\\\\{TP}}{d\\fusion{finc{TP}}}{d\\fusion{d\\fusion{fusion{TP}}}{d\\fusion{d\\ce{TP}}}}}}{d\\fusion{fusion{TP}}}:\\fusion{d\\obacterium{d\\obacterium{TP}}}}:\\fusion{d\\obacterium{v}}})}}(p,q)}}\n' +
      '\n' +
      '장면의 특징(p,q)=(D^bf{g}_{p}^math{g}}^mathbf}}{p}}{q}/\\tau)은 장면의 특징 \\(\\mathbf{g}_{p}_{p}^{S}}/\\tau)과 훈련 배치에서 정렬된 각 한 쌍의 현장-텍스트 쌍에 대한 장면 특징 \\(\\mathbf{g}<\\mathbf{g}/\\mathbf{g}}.{p}.{p}/\\mathbf{g}}/\\mathbf{g}_{p}_{p}_{p}{p}_{p}{p}_{p}{p}_{p}{p}_{p}{p}_{p}{p}_{p}{p}_{p}{p}}{p}{p}}{p}{p}{p}{p}{p}{p}{p}{p}{p}{p}\n' +
      '\n' +
      '### Referral-object-level Grounding\n' +
      '\n' +
      '표정에서 드러난 관계를 모델링하기 위해, 우리는 장면에서 물체 추천에 대한 자기 의도 기반 추론 변압기를 사용한다. 이 변압기는 장면-객체(\\{\\mathbf{f}_{i}^{S}\\}\\)와 객체 추천 \\(\\mathbf{T}^{\\{T}^{\\{ref}}\\)를 특징으로 하며 텍스트 설명과 객체 관계 간의 관계를 배우는 자기 의사를 수행한다. 우리는 대상당 추천 기능을 추출하기 위해 장면 수준 라운딩과 동일한 튜닝 가능한 언어 인코더를 사용한다. 우리는 현장-객체 특징과 함께 이 텍스트 특징을 자기-의도 변압기에 통과시켜 정렬된 객체 특징 \\(\\mathbf{h}_{i}^{S}\\) 및 문장 수준 추천 특징 \\(\\mathbf{h}^{T}\\)를 얻는다. 그런 다음 다음 추천 대상 수준 대비 정렬을 수행합니다.\n' +
      '\n' +
      '}}{{f}^mathbf{h}^{f}^{f}\\tau\\}\n' +
      '\n' +
      'where \\(\\bar{\\mathbf{h}}^{S}\\) denotes the feature of the referred object, \\(p\\) iterates over all objects within the same scene. Notably, in contrast to inter-scene contrast that was done in object- and scene-level alignment, we force the selection of positive pairs to be within the same scene to provide intra-scene contrast for fine-grained object grounding. This mimics the success of intra-image and inter-image contrasts commonly used for region-word alignment in 2D-VL models [90].\n' +
      '\n' +
      '3D 장면과 언어 사이의 다단계 정렬을 배우기 위해, 지느러미 트라소베에 테스쿨을 배웁니다.\n' +
      '\n' +
      '\\[\\mathcal{L}=\\mathcal{L}_{\\text{obj}}+\\mathcal{L}_{\\text{scene}}+\\mathcal{L}_{ \\text{ref}}+\\mathcal{L}_{\\text{MLM}}.\\]\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '본 절에서는 다음과 같은 질문을 다루는 평가 결과를 제시한다.\n' +
      '\n' +
      '세버퍼프 3Dv에서 데이터 스케일링은 ualgro 언딩이 얼마나 효과적인가? 스토브 스크라 레업버스 kfor 유전 메랄레프-트레이닝바스 ed3D-VLmod els?\n' +
      '* How well is the GPS pre-training pipeline? Does it exhibit similar properties of 2D-VL models in 3D-VL tasks?\n' +
      '** 미래의 연구를 위해 스켄버스와 GPS가 제공하는 잠재력은 무엇입니까? 무슨 일이 없어졌나요?\n' +
      '\n' +
      '다음 섹션에서는 이러한 주요 주제에 대한 모델 성능에 대해 자세히 설명한다. 페이지 제한으로 인해, 우리는 구현 세부 사항 및 더 많은 실험 분석을 위해 독자를 부록 B 및 C로 지시한다.\n' +
      '\n' +
      '3D 정품 교환\n' +
      '\n' +
      'SettingsWe evaluate our model on three commonly-used datasets for 3D visual grounding: ScanRefer [16], Nr3D, and Sr3D [1]. For Nr3D and Sr3D, we follow Achioptas _et al_. [1] and report the grounding accuracies of models using ground-truth object masks. For ScanRefer, we follow Zhu _et al_. [98] and use Mask3D [72] to generate object proposals. Results are reported as Acc\\(@0.5\\) to evaluate the correctness of predictions whose object bounding boxes overlap the ground truth with IoU \\(>0.5\\). For comparisons, we compare with existing baselines by providing the results of pre-trained GPS and dataset-specific fine-tuned GPS. Please see more details in the Appendix C.\n' +
      '\n' +
      'Results and AnalysesAs shown in Tab. 2, GPS trained on SceneVerse achieves state-of-the-art results on all existing 3D-VL grounding benchmarks. Initially, when GPS is trained directly on the training sets of benchmark datasets, labeled as Ours (_scratch_), it underperforms compared to existing models that employ more complex structures or loss designs. This result underscores the data-intensive nature of the contrastive alignment paradigm. However, when presented with extensive training data in SceneVerse, the results of our model without additional fine-tuning, _i.e_., Ours (_pre-train_), significantly improves and already achieves state-of-the-art results on benchmarks like ScanRefer. Moreover, the dataset-specific fine-tuned model, _i.e_., Ours (_fine-tuned_), _consistently outperforms existing baselines with only a simple projection MLP_ added on top of the pre-trained model, jointly optimized during fine-tuning without any other auxiliary architecture or loss objective. These results underscore the strong potential of both the SceneVerse and GPS for 3D-VL tasks.\n' +
      '\n' +
      '### Zero-Shot Transfer\n' +
      '\n' +
      '스카네베르테 데이터와 GPS 모델의 효과를 더 잘 평가하기 위해 4개의 벤치마크, 스카이머, Sr3D, Nr3D 및 SceneVerse-val에서 모델의 능력을 테스트하기 위해 제로 샷 전달 실험을 추가로 수행한다. 멀티스캔에서 \\(271\\) 장면의 주석이 달린 객체 참조기(8.5K\\)를 사용하여 SceneVerse-val를 생성하고, 홀드 아웃 테스트 세트를 생성하기 위한 4:1 열차/테스트 분할 후 장면을 무작위로 분할한다. 우리는 주로 (i) _제로 샷_: 표적 데이터세트로부터 모든 장면을 제거하고, 운반되지 않은 장면에서 테스트하여 훈련된 모델, (ii) _제로 샷 텍스트_: 표적 데이터셋의 훈련 세트로부터의 3D 장면을 포함하는 데이터에 대해 훈련되었지만, 비선 장면-텍스트 분포로 독점적으로 테스트하였다. 구체적으로, _제로 샷 텍스트_ 설정에 대해, 우리는 SceneVerse에서 생성된 텍스트를 _제로 샷_ 모델에 대한 미세 조정 소스로 사용한다. 우리는 주로 최근 사전 훈련 기반 모델 3D-VisTA와 모델을 비교한다. 부록 C에서 실험 설정 및 구현에 대한 자세한 내용을 참조한다.\n' +
      '\n' +
      '** 결과 및 분석** 우리는 Tab에서 0샷 전달 실험의 결과를 제시한다. 3개, 타브. 4는 다음과 같은 주요 관측치를 가지고 있다.\n' +
      '\n' +
      '* Our GPS model demonstrates superior generalization to unseen scenes compared to the 3D-VisTA model. In zero-shot transfer scenarios, our model consistently outperforms 3D-VisTA across established benchmarks and SceneVerse-val. This indicates the effectiveness of contrastive alignment over traditional classification objectives, aligning with the advancements seen in 2D-VL models for open-vocabulary grounding and transfer capabilities\n' +
      '* SceneVerse dataset substantially enhances 3D-VL grounding capabilities through zero-shot transfer, especially when provided with relatively limited training data, _i.e._, SceneVerse-val. As demonstrated in Tab. 4, there is a significantly improved performance when comparing models trained on SceneVerse in a zero-shot manner to those trained from scratch. This indicates that SceneVerse can effectively capture knowledge for general 3D scene grounding. Consequently, _this underscores its potential as a go-to pre-training dataset for 3D-VL tasks_.\n' +
      '* The impact of our extensive collection and scalable generation of scene-text pairs is further evidenced by the results in the _zero-shot text_ setting. Notably, as shown in Tab. 3, these automatically generated scene-text pairs supply ample knowledge for comprehending the scene distribution. This contributes significantly to the substantial improvement over the _zero-shot_ performance.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Method & Overall & Easy & Hard & V-Dep. & V-Indep. \\\\ \\hline\n' +
      '3D-VisTA(_scratch__) & 40.7&37.3 & 37.3&37.3 \\\\-VisTA(_scratch_) & 40.7 & 53.3 \\\\-mm-VisTA(_scratch_) & 40.7 & 21.3 \\\\.3 & 44.3 \\\\-VisTA(_scratch_) & 40.7 및 53.3 \\\\-VisTA(_scratch_4.6 & 44.3 \\\\.3－S.3 \\\\-VisTA.1 & 95.3 ip.3 \\\\-VisTA.7 & 53.3 \\\\-VisTA.7 & 53.3 \\\\-m.3 ＋.3 －4.7 & 53.7 & 53.3 \\\\-VisTA(_4.7 & 53.3 ip.3 ＆.3        \n' +
      '앤5.7 & 52\\\\ 2.7 & 35.4 & 53.7 & 52\\\\ 2.3D-VisTA(_제로샷_) & 52.9 & 52.9 & 59.6 & 52.9 & 52.9 & 52.9 & 52.9 & 52.9 & 52.9 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.9 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.3D-VisTA.3D-VisTA.3D-VisTA.3D-VisTA.3D-VisTA.3D-VisTA.3D-VisTA\n' +
      '3D-VisTA (_zero-shot text_) & 58.1 & 70.0 & 39.6 & 52.5 & 64.1 \\\\ \\hline Ours (_scratch_) & 38.5 & 50.2 & 20.8 & 33.7 & 43.9 \\\\ Ours (_zero-shot_) & 59.2 & 69.4 & 44.0 & 53.1 & 66.3 \\\\ Ours (_zero-shot text_) & 60.6 & 70.9 & 45.1 & 54.8 & 67.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: **Zero-shot transfer on SceneVerse-val. We evaluate models following settings in Nr3D/Sr3D using GT object proposals.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{4}{c}{Nr3D} & \\multicolumn{4}{c}{Sr3D} & \\multicolumn{4}{c}{ScanRefer Acc@0.5} \\\\ \\cline{2-13}  & Overall & Easy & Hard & V-Dep. & V-Indep. & Overall & Easy & Hard & V-Dep. & V-Indep. & Overall & Unique & Multiple \\\\ \\hline\n' +
      '3DVG-Trans [94] & 40.8 & 48.5 & 34.8 & 43.7 & 51.4 & 54.2 & 44.9 & 44.6 & 51.7 & 34.7 & 60.6 & 28.4 \\\\ TGNN [39] & 37.3 & 44.2 & 30.6 & 35.8 & 38.0 & 45.0 & 48.5 & 36.9 & 45.8 & 45.0 & 29.7 & 56.8 & 23.2 \\\\ TransRefer3D [35] & 48.0 & 56.7 & 39.6 & 42.5 & 50.7 & 57.4 & 60.5 & 50.2 & 49.9 & 57.7 & - & - & - \\\\ InstanceRefer [89] & 38.8 & 46.0 & 31.8 & 34.5 & 41.9 & 48.0 & 51.1 & 40.5 & 45.8 & 48.1 & 32.9 & 66.8 & 24.7 \\\\ FFL-3DOG [31] & 41.7 & 48.2 & 35.0 & 37.1 & 44.7 & - & - & - & - & - & 34.0 & 67.9 & 25.7 \\\\ LAR [6] & 48.9 & 58.4 & 42.3 & 47.4 & 52.1 & 59.4 & 63.0 & 51.2 & 50.0 & 59.1 & - & - & - \\\\ SAT [86] & 56.5 & 64.9 & 48.4 & 54.4 & 57.6 & 57.9 & 61.2 & 50.0 & 49.2 & 58.3 & 30.1 & 50.8 & 25.2 \\\\\n' +
      '53.7&29.8 \\\\-SPS[54] & 51.5 & 58.5 & 51.5 & 51.5 & 99.5 & 63.2 & 61.2 & 63.4 & 29.8 \\\\-SPS[54] & 51.5 & 58.5 & 5.5 & 63.2 & 63.8.2 & 63.8.2 & 63.5 & 58.5 & 51.5 & 65.8.2 & 68.2 & 63.8.2 & 63.8.2 & 63.8.2 & 63.2 & 5.5 & 5.5 & 5.5 및 5.8.2 & 63.8.4 & 63.8.4 & 5.5 & 5.5 & 5.5 & 5.4 & 5.5 & 5.5 & 5.8.5 & 5.5 & 5.4 & 5.4 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.\n' +
      '3DJCG [12] & - & - & - & - & - & - & - & - & - & 37.3 & 64.3 & 30.8 \\\\ BUTD-DETR [41] & 54.6 & 60.7 & 48.4 & 46.0 & 58.0 & 67.0 & 68.6 & 63.2 & 53.0 & 67.6 & 39.8 & 66.3 & 35.1 \\\\ MVT [40] & 59.5 & 67.4 & 52.7 & 59.1 & 60.3 & 64.5 & 66.9 & 58.8 & 58.4 & 64.7 & 33.3 & 66.5 & 25.3 \\\\ ViL3DRel [18] & 64.4 & 70.2 & 57.4 & **62.0** & 64.5 & 72.8 & 74.9 & 67.9 & 63.8 & 73.2 & 37.7 & 68.6 & 30.7 \\\\ EDA [82] & 52.1 & 58.2 & 46.1 & 50.2 & 53.1 & 68.1 & 70.3 & 62.9 & 54.1 & 68.7 & 42.3 & 68.6 & 37.6 \\\\\n' +
      '표 2:* *DVisualGro undi n gres u ltso n Nr3D, Sr3D 및 S c는 aRe f 소거.\n' +
      '3D-VisTA [98] & 64.2 & 72.1 & 56.7 & 61.5 & 65.1 & 76.4 & 78.8 & 71.3 & 58.9 & 77.3 & 45.8 & 75.1 & 39.1 \\\\ \\hline Ours (_scratch_) & 58.7 & 67.0 & 50.9 & 55.8 & 59.8 & 68.4 & 70.5 & 63.4 & 53.1 & 69.0 & 40.4 & 71.3 & 34.7 \\\\ Ours (_pre-train_) & 55.2 & 62.8 & 48.0 & 45.5 & 58.8 & 74.1 & 76.4 & 68.5 & 54.1 & 75.0 & 47.1 & 77.4 & 41.6 \\\\ Ours (_fine-tuned_) & **64.9** & **72.5** & **57.8** & 56.9 & **67.9** & **77.5** & **80.1** & **71.6** & **62.8** & **78.2** & **48.1** & **77.9** & **42.7** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **3D Visual Grounding results on Nr3D, Sr3D, and ScanRefer. We use “_direct_” for our model trained on SceneVerse with no additional fine-tune head, and “_fine-tune_” for the data-specific fine-tuned version of our model. We highlight the best results in bold.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Method & Nr3D & Sq3D & ScanRefer@0.25 & ScanRefer@0.5 \\\\ \\hline\n' +
      '3D-VisTA (_scratch_) & 57.5 & 69.6 & 45.9 & 41.5 \\\\\n' +
      '5&69. &45&9 41.5\\ \\ 9.3D-VisTA(_scratch_)&57 5&69. & 45&9 41.5 \\＆3D-VisTA(_scratch_)&57.\n' +
      '3D-VisTA (_crrow-shot_) & 43.1 & 36.1 & 41.1 & 36.4 \\\\ \\hline Ours (_scratch_) & 58.7 & 68.4 & 44.5 & 40.4 \\\\ Ours (_crrow-shot_) & 32.4 & 33.3 & 35.2 & 31.1 \\\\ Ours (_crrow-shot text_) & 41.9 & 38.1 & 40.7 & 35.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: **Zero-shot 전달 결과는 확립된 벤치마크에 대한 표 3: **Zero-shot 전달 결과****.\n' +
      '\n' +
      '선택학 및 논의.\n' +
      '\n' +
      '이 섹션에서는 주로 스켄버스에서 수집된 데이터에 초점을 맞춘 절제 연구를 제시한다. 우리의 목표는 데이터 스케일링의 효과를 보다 명확하게 설명하는 것이다. 모델 아키텍처에 대한 절제 연구에 관한 논의를 위해 독자를 부록 D라고 하며, 이 섹션에서는 다음과 같은 점을 구체적으로 논의한다.\n' +
      '\n' +
      '**는 데이터-스케일링이 얼마나 중요한가?** 우리는 GPS를 사전 훈련하는 동안 사용된 데이터의 양에 대해 절제 연구를 수행한다. 우리는 ScanRefer 및 SceneVerse-val에서 모델 성능에 대한 데이터-스케일링의 효과를 보여주기 위해 SceneVerse의\\(\\frac{1}{8}\\), \\(\\frac{1}{4}\\), \\(\\frac{1}{2}\\)로 훈련된 모델을 고려한다. 그림과 같이. 4, 우리는 두 설정 모두에 대한 데이터 스케일 증가에 대한 일관된 성능 향상을 관찰한다. 우리는 이러한 스케일링 효과가 3D-VL 접지뿐만 아니라 의미 세분화 [72, 85]와 같은 다른 3D 과제에 유익하다는 것을 보여주기 위해 부록 D에서 추가 실험을 제공한다.\n' +
      '\n' +
      '**How is the generated data compared with human-annotated data?** We assess the performance of models trained using various scene-text sources, specifically focusing on their performance in the ScanRefer dataset without additional fine-tuning. As shown in Tab. 5, models trained with our template-based generated texts and Large Language Model (LLM)-refined texts show significant improvements over models trained solely on ScanRefer. More importantly, these variants of our model already achieve state-of-the-art results compared with previous baselines. This indicates the effectiveness of our text-generation pipeline. Finally, we observe that adding human-annotated data is still beneficial for model performance. However, the improvement is relatively marginal over models trained on our generated data.\n' +
      '\n' +
      '**What is the role of the synthetic scenes in this scale-up process?** With synthetic data providing large-scale and diverse scene data for 3D-VL tasks, we evaluate the models\' domain transfer (Sim2Real) capability. Specifically, we compare models trained on all real scenes in SceneVerse against models trained exclusively on two synthetic subsets of SceneVerse, _i.e_., Structured3D and ProcTHOR. As shown in Tab. 6, models trained on synthetic subsets demonstrate remarkable performance on their corresponding test sets while suffering when transferred to real or other synthetic scenes. In contrast, the model trained on real scene-text pairs exhibits less severe performance drops when generalizing to synthetic scenes. This result affirms the domain gap between real and synthetic scenes in 3D-VL grounding and shows that a simple scale-up in the amount of scenes is insufficient when the scene naturalness can not be guaranteed. Considering the scalability of our quality-ensured language generation and also the scaling effect shown in our experiments, the rate-determining step for further scaling-up 3D-VL comes to the collection of diverse, high-quality, and realistic scenes that capture natural 3D scene distributions.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '이 작업에서 우리는 지상 장면 이해의 맥락에서 3D-VL을 스케일링한다. 인간 주석 및 제안된 장면-텍스트 생성 접근법에서 제공하는 다양한 장면 및 다중 수준 장면 설명을 포함하는 100만 규모의 3D-VL 데이터세트 SceneVerse를 소개합니다. 스카네베이스를 활용하여 수집된 데이터에 대해 다중 수준의 장면-언어 대조 정렬으로 훈련된 모델인 스켄에 대한 그라운드 프레-트레이닝을 제안한다. 광범위한 실험을 통해 GPS가 기존의 모든 3D-VL 접지 작업에 대한 최첨단 결과를 달성한다는 것을 보여준다. 우리는 이전 기저부에 비해 스켄버스에 대해 훈련된 GPS의 개선된 일반화 성능을 보여주기 위해 제로 샷 전달 실험을 추가로 수행한다. 스카네버스에서 우리의 노력과 성공적인 스케일업 시도가 3D-VL에서 새로운 연구 패러다임의 길을 열어줄 수 있기를 바랍니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Real & Synthetic & SceneVerse-val & S3D & ProcTHOR \\\\ \\hline All & ✗ & 64.8 & 37.1 & 43.4 \\\\ ✗ & S3D & 7.0 & 85.1 & 16.1 \\\\ ✗ & ProcTHOR & 4.2 & 16.3 & 91.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '_datas ca.*M 델스콘 시트는 재트라니 nand제로-s 핫트랜스포터 에틴 gsonSc aR eferandSceneVSc.Figur e4 :**Mode lowo rmance_v를 모두 개선한다.\n' +
      '\n' +
      'Figure 4: **Model performance _v.s._ data scale.** Models consistently improve in both the pre-train and zero-shot transfer settings on ScanRefer and SceneVerse-val with data scaling-up.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Template & LLM & Anno. & Acc@0.25 & Acc@0.5 \\\\ \\hline ✗ & ✗ & ✗ & 43.5 & 38.4 \\\\ ✓ & ✗ & ✗ & 50.9 & 46.1 \\\\ ✓ & ✓ & ✗ & 51.1 & 46.3 \\\\ ✓ & ✓ & ✓ & 52.0 & 47.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '교육***에 사용되는 장면-텍스트 쌍 유형에 대한 표 5: ** 증폭이다. 추가적인 지느러미가 없는 스칸 리커에 대한 모델 결과를 보고한다.\n' +
      '\n' +
      '## 7 Acknowledgement\n' +
      '\n' +
      '저자는 결과 시각화 프레임워크를 설계하는 BIGAI의 야오위 장, 데이터 생성 및 정제에 대한 제안을 위해 BIGAI의 장용황과 시오그쿤 링후, BIGAI의 동료들에게 도움이 되는 토론과 제안에 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In _Proceedings of European Conference on Computer Vision (ECCV)_, 2020.\n' +
      '* [2] Christopher Agia, Krishna Murthy Jatavallabhula, Mohamed Khodeir, Ondrej Miksik, Vibhav Vineet, Mustafa Mukadam, Liam Paull, and Florian Shkurti. Taskography: Evaluating robot task planning over large 3d scene graphs. In _Proceedings of Conference on Robot Learning (CoRL)_, 2022.\n' +
      '* [3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [4] Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir R Zamir, Martin Fischer, Jitendra Malik, and Silvio Savarese. 3d scene graph: A structure for unified semantics, 3d space, and camera. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2019.\n' +
      '* [5] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanga: 3d question answering for spatial scene understanding. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [6] Eslam Bakr, Yasmeen Alsaedy, and Mohamed Elhoseiny. Look around and refer: 2d synthetic semantics knowledge distillation for 3d visual grounding. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [7] Lawrence W Barsalou. Perceptual symbol systems. _Behavioral and brain sciences_, 22(4):577-660, 1999.\n' +
      '* [8] Lawrence W Barsalou. Grounded cognition. _Annu. Rev. Psychol._, 59:617-645, 2008.\n' +
      '* [9] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: A diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. In _Proceedings of Advances in Neural Information Processing Systems Datasets and Benchmarks (NeurIPS Datasets and Benchmarks Track)_, 2021.\n' +
      '* [10] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.\n' +
      '* [11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2020.\n' +
      '* [12] Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong Xu. 3djcg: A unified framework for joint dense captioning and visual grounding on 3d point clouds. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [13] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. _Proceedings of International Conference on 3D Vision (3DV)_, 2017.\n' +
      '* [14] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. _arXiv preprint arXiv:1512.03012_, 2015.\n' +
      '* [15] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2021.\n' +
      '* [16] Dave Zhenyu Chen, Angel X Chang, and Matthias Niessner. Scanrefer: 3d object localization in rgb-d scans using natural language. In _Proceedings of European Conference on Computer Vision (ECCV)_, 2020.\n' +
      '* [17] Dave Zhenyu Chen, Qirui Wu, Matthias Niessner, and Angel X Chang. D3net: a speaker-listener architecture for semi-supervised dense captioning and visual grounding in rgb-d scans. In _Proceedings of European Conference on Computer Vision (ECCV)_, 2022.\n' +
      '* [18] Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Language conditioned spatial relation reasoning for 3d object grounding. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [19] Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang Yu, and Tao Chen. End-to-end 3d dense captioning with vote2capdetr. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [20] Zhenyu Chen, Ali Gholami, Matthias Niessner, and Angel X Chang. Scan2cap: Context-aware dense captioning in rgb-d scans. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2021.\n' +
      '* [21] Zhenyu Chen, Ronghang Hu, Xinlei Chen, Matthias Niessner, and Angel X Chang. Unit3d: A unified transformer for 3d dense captioning and visual grounding. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [22] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas F Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [23] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niessner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In _Proceedingsof Conference on Computer Vision and Pattern Recognition (CVPR)_, 2017.\n' +
      '* [24] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. _arXiv preprint arXiv:2305.06500_, 2023.\n' +
      '* [25] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A universe of 10m+ 3d objects. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2023.\n' +
      '* [26] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [27] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Procthor: Large-scale embodied ai using procedural generation. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [28] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)_, 2018.\n' +
      '* [29] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. Pla: Language-driven open-vocabulary 3d scene understanding. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [30] Zhipeng Ding, Xu Han, and Marc Niethammer. Votenet: A deep learning label fusion method for multi-atlas segmentation. In _Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)_, 2019.\n' +
      '* [31] Mingtao Feng, Zhen Li, Qi Li, Liang Zhang, XiangDong Zhang, Guangming Zhu, Hui Zhang, Yaonan Wang, and Ajmal Mian. Free-form description guided 3d visual graph network for object grounding in point cloud. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [32] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In _Proceedings of European Conference on Computer Vision (ECCV)_, 2022.\n' +
      '* [33] Qiao Gu, Alihusein Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, et al. Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. _arXiv preprint arXiv:2309.16650_, 2023.\n' +
      '* [34] Huy Ha and Shuran Song. Semantic abstraction: Open-world 3d scene understanding from 2d vision-language models. In _Proceedings of Conference on Robot Learning (CoRL)_, 2022.\n' +
      '* [35] Dailan He, Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei Huang, Aixi Zhang, and Si Liu. Transfer3d: Entity-and-relation aware transformer for fine-grained 3d visual grounding. In _Proceedings of ACM International Conference on Multimedia (MM)_, 2021.\n' +
      '* [36] Deepit Hegde, Jaya Maria Jose Valanarasu, and Vishal Patel. Clip goes 3d: Leveraging prompt tuning for language grounded 3d recognition. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [37] Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen, Joshua B Tenenbaum, and Chuang Gan. 3d concept learning and reasoning from multi-view images. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [38] Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, and Stephen Gould. Vln bert: A recurrent vision-and-language bert for navigation. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2021.\n' +
      '* [39] Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, and Tyng-Luh Liu. Text-guided graph neural networks for referring 3d instance segmentation. In _Proceedings of AAAI Conference on Artificial Intelligence (AAAI)_, 2021.\n' +
      '* [40] Shijia Huang, Yilun Chen, Jiaya Jia, and Liwei Wang. Multi-view transformer for 3d visual grounding. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [41] Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, and Katrina Fragkiadaki. Bottom up top down detection transformers for language grounding in images and point clouds. In _Proceedings of European Conference on Computer Vision (ECCV)_, 2022.\n' +
      '* [42] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2020.\n' +
      '* [43] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* [44] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [45] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. In _International Journal of Computer Vision (IJCV)_, 2017.\n' +
      '* [46] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. _Behavioral and brain sciences_, 40:e253, 2017.\n' +
      '* [47] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. In _Proceedings of International Conference on Learning Representations (ICLR)_, 2022.\n' +
      '\n' +
      '* [48] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In _ICML_, 2023.\n' +
      '* [49] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _Proceedings of International Conference on Machine Learning (ICML)_, 2022.\n' +
      '* [50] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [51] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2023.\n' +
      '* [52] Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, and Hao Su. Openshape: Scaling up 3d shape representation towards open-world understanding. _arXiv preprint arXiv:2305.10764_, 2023.\n' +
      '* [53] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [54] Junyu Luo, Jiahui Fu, Xianghao Kong, Chen Gao, Haibing Ren, Hao Shen, Huaxia Xia, and Si Liu. 3d-sps: Single-stage 3d visual grounding via referred point progressive selection. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [55] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2023.\n' +
      '* [56] Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, and Caiming Xiong. Self-monitoring navigation agent via auxiliary progress estimation. In _Proceedings of International Conference on Learning Representations (ICLR)_, 2019.\n' +
      '* [57] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated question answering in 3d scenes. In _Proceedings of International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* [58] Yongsen Mao, Yiming Zhang, Hanxiao Jiang, Angel Chang, and Manolis Savva. Multiscale: Scalable rgbd scanning for 3d environments with articulated objects. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [59] Ishan Misra, Rohit Girdhar, and Armand Joulin. An end-to-end transformer model for 3d object detection. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [60] Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna Tripathi, Leonidas J Guibas, and Hao Su. Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* [61] OpenAI. Introducing chatgpt. [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt), 2022.\n' +
      '* [62] OpenAI. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* [63] Alexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic transformer for vision-and-language navigation. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [64] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliascchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [65] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2017.\n' +
      '* [66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [67] Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, et al. Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. In _Proceedings of Advances in Neural Information Processing Systems Datasets and Benchmarks (NeurIPS Datasets and Benchmarks Track)_, 2021.\n' +
      '* [68] Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf. Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning. In _Proceedings of Conference on Robot Learning (CoRL)_, 2023.\n' +
      '* [69] Antoni Rosinol, Andrew Violette, Marcus Abate, Nathan Hughes, Yun Chang, Jingnan Shi, Arjun Gupta, and Luca Carlone. Kimera: From slam to spatial perception with 3d dynamic scene graphs. _International Journal of Robotics Research (IJRR)_, 2021.\n' +
      '* [70] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photoorealistic text-to-image diffusion models with deep language understanding. _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [71] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [72] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3d: Mask transformer for 3d semantic instance segmentation. In _Proceedings of International Conference on Robotics and Automation (ICRA)_, 2023.\n' +
      '* [73] Linda Smith and Michael Gasser. The development of embodied cognition: Six lessons from babies. _Artificial life_, 11(1-2):13-29, 2005.\n' +
      '* [74] Ayca Takmaz, Elisabetta Fedele, Robert W Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. Open-mask3d: Open-vocabulary 3d instance segmentation. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2023.\n' +
      '* [75] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [76] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2021.\n' +
      '* [77] Thang Vu, Kookhoi Kim, Tung M Luu, Thanh Nguyen, and Chang D Yoo. Softgroup for 3d instance segmentation on point clouds. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [78] Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, and Matthias Niessner. Rio: 3d object instance re-localization in changing indoor environments. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2019.\n' +
      '* [79] Johanna Wald, Helisa Dhamo, Nassir Navab, and Federico Tombari. Learning 3d semantic scene graphs from 3d indoor reconstructions. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2020.\n' +
      '* [80] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* [81] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, et al. Omnibject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [82] Yanmin Wu, Xinhua Cheng, Renrui Zhang, Zesen Cheng, and Jian Zhang. Eda: Explicit text-decoupling and dense alignment for 3d visual grounding. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [83] Le Xue, Mingfei Gao, Chen Xing, Roberto Martin-Martin, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. Ulip: Learning a unified representation of language, images, and point clouds for 3d understanding. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [84] Jihan Yang, Runyu Ding, Zhe Wang, and Xiaojuan Qi. Regionplc: Regional point-language contrastive learning for open-world 3d scene understanding. _arXiv preprint arXiv:2304.00962_, 2023.\n' +
      '* [85] Yu-Qi Yang, Yu-Xiao Guo, Jian-Yu Xiong, Yang Liu, Hao Pan, Peng-Shuai Wang, Xin Tong, and Baining Guo. Swin3d: A pretrained transformer backbone for 3d indoor scene understanding. _arXiv preprint arXiv:2304.06906_, 2023.\n' +
      '* [86] Zhengyuan Yang, Songyang Zhang, Liwei Wang, and Jiebo Luo. Sat: 2d semantics assisted training for 3d visual grounding. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [87] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Niessner, and Angela Dai. Scannet++: A high-fidelity dataset of 3d indoor scenes. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [88] Zhihao Yuan, Xu Yan, Yinghong Liao, Yao Guo, Guanbin Li, Shuguang Cui, and Zhen Li. X-trans2cap: Cross-modal knowledge transfer using transformer for 3d dense captioning. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [89] Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang, Sheng Wang, Zhen Li, and Shuguang Cui. Instancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [90] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Lianian Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [91] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li. Point-clip: Point cloud understanding by clip. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [92] Renrui Zhang, Liuhui Wang, Yu Qiao, Peng Gao, and Hongsheng Li. Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [93] Yiming Zhang, ZeMing Gong, and Angel X Chang. Multi3drefer: Grounding text description to multiple 3d objects. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [94] Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvg-transformer: Relation modeling for visual grounding on point clouds. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [95] Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao, and Zihan Zhou. Structured3d: A large photo-realistic dataset for structured 3d modeling. In _Proceedings of European Conference on Computer Vision (ECCV)_, 2020.\n' +
      '* [96] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus of images interleaved with text. _arXiv preprint arXiv:2304.06939_, 2023.\n' +
      '* [97] Yixin Zhu, Tao Gao, Lifeng Fan, Siyuan Huang, Mark Edmonds, Hangxin Liu, Feng Gao, Chi Zhang, Siyuan Qi,Ying Nian Wu, et al. Dark, beyond deep: A paradigm shift to cognitive ai with humanlike common sense. _Engineering_, 6(3):310-345, 2020.\n' +
      '* Zhu et al. [2023] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 3d-vista: Pre-trained transformer for 3d vision and text alignment. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '\n' +
      '## SceneVerse:\n' +
      '\n' +
      '계획된 모델을 위한 3D 비전-언어 학습.\n' +
      '\n' +
      'Supplementary Material\n' +
      '\n' +
      '부록 A에서는 3D 장면 전처리, 장면 그래프 구축, 자동 언어 생성 등 SceneVerse에 대한 자세한 내용을 소개한다. 애플리케이션스 B는 보다 많은 모델 및 구현 세부 사항을 제시한다. 부록 C는 본 논문의 실험에 대한 설정 및 구현에 대한 보다 심층적인 요약과 SceneVerse의 이점을 입증하기 위한 시맨틱 분할에 대한 남용 연구 및 추가 실험을 포함한다.\n' +
      '\n' +
      '부록 A SceneVerse.\n' +
      '\n' +
      '### 3D Scenes\n' +
      '\n' +
      '사용 가능한 3D 장면 데이터의 부족을 해결하기 위해 기존 다양한 데이터세트로부터 3D 장면 데이터를 일원화하여 SceneVerse를 구성한다. 큐레이션은 스턴넷[23], ARKitScenes[9], HM3D[67], 3RScan[78] 및 멀티Scan[58]과 같은 실제 장면의 데이터셋을 사용하여 구조화된 3D[95] 및 ProcTHOR[27]의 합성 환경과 연동된다. 이러한 합성 데이터 세트의 통합은 주로 3D-VL 정렬을 위한 확장 가능한 데이터 소스로서의 잠재력에 의해 주도된다. 훈련 과정을 용이하게 하기 위해 다음과 같은 전처리 단계를 진행한다.\n' +
      '\n' +
      '**Room Segmentation** The 3D scenes in HM3D and ProcTHOR are released at the building level, encompassing multiple rooms and sometimes spanning over 50 meters. To align with existing benchmarks [1, 16], we leverage the associated metadata to segment the 3D point cloud at the room level, facilitating subsequent operations in scene graph construction and language description generation. Additionally, we implement a filtering process to exclude extremely large rooms and those with fewer than 4 objects in the scene.\n' +
      '\n' +
      '**Point Cloud Normalization** In order to mitigate the data disparities arising from diverse capture devices across various data sources, we subsample each point cloud to a maximum of \\(240,000\\) points. Each point cloud then undergoes a transformation centered on the central point on the floor, followed by rotation to align the room layout with the axis following the approach by Chen _et al_. [18].\n' +
      '\n' +
      '**Semantic Label Alignment** Given the divergence in semantic label sets across different datasets, we undertake a comprehensive effort to map all the object class labels to the \\(607\\) semantic labels in ScanNet [23] to facilitate close-vocabulary object classification [65] in the existing model framework [98]. We construct the mapping in each dataset through LLM and manual verification. Note that the object-level grounding in GPS can directly deal with open-set object labels or captions, similar to CLIP [36].\n' +
      '\n' +
      'After the preprocessing, each scan is represented by a point cloud \\(\\mathrm{P}\\in\\mathbb{R}^{N\\times 8}\\), wherein each point is defined by its 3D coordinates, RGB color, instance id and semantic label. In total, we curate \\(68,406\\) 3D scenes in SceneVerse.\n' +
      '\n' +
      '3D 스켄 브러시 건설.\n' +
      '\n' +
      'Sec. 3.2에서 포인트 클라우드의 3D 장면 그래프를 구성하기 위한 자동화 파이프라인을 소개합니다. 여기서는 보다 많은 구현 세부 사항과 관계 정의를 제공합니다.\n' +
      '\n' +
      '#### a.2.1 Relationships\n' +
      '\n' +
      '우리의 3D 장면 그래프는 Tab에서 볼 수 있듯이 \\(21\\) 유형의 관계를 캡처한다. A.1. 그림 A.1에서 볼 수 있듯이 이러한 관계가 3D 공간에서 어떻게 정의되는지 예시한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c} \\hline \\hline Category & Relation \\\\ \\hline \\(|\\) & supported by \\\\ In-contact vertical & embedded into \\\\  & placed in \\\\ \\(|\\) & inside \\\\ \\hline \\multirow{4}{*}{Non-contact vertical} & hanging on \\\\  & affixed on \\\\  & mounted on \\\\ Non-contact vertical & above \\\\  & higher than \\\\  & below \\\\  & lower than \\\\ \\hline \\multirow{4}{*}{Horizontal} & near(far) to the left of \\\\  & near(far) to the right of \\\\  & is behind \\\\ \\cline{1-1}  & is in front of \\\\ \\cline{1-1}  & close to \\\\ \\cline{1-1}  & adjacent to \\\\ \\cline{1-1}  & besides \\\\ \\cline{1-1}  & next to \\\\ \\hline Multi-object & between \\\\ \\cline{1-1}  & aligned \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: ** SceneVerse*** 3D 장면 그래프는 4가지 범주 범위의 21가지 유형의 관계를 캡처한다.\n' +
      '\n' +
      '2.2.2의 스카엔그래프건설###############.\n' +
      '\n' +
      '점 클라우드 표현에서 내재적 소음과 불완전성으로 인해 포인트 구름에서 자동으로 정확하고 포괄적인 관계를 추출하는 것은 비개인적인 과제이다. 아래에서는 Alg 1에 설명된 대로 3D 장면 그래프 구축 과정을 자세히 설명합니다.\n' +
      '\n' +
      '우리는 먼저 포인트 클라우드(\\mathrm{p_{i}}\\in\\mathbb{R}^{3}\\)의 인스턴스 주석과 각 노드를 객체 중심형 \\(\\mathrm{bb{R}^{3}\\)으로 인스턴스 노드를 인스턴스화하고 축 정렬된 바운딩 박스 \\(\\mathrm{b_{{R}^{3}^{i}}\\)의 크기와 크기(\\mathrm{b_{{R}^{3}^{R}^{3}^{R}^{R}^{3}^{R}^{R}^{R}^{3}^{R}^{R}^{3}^{R}^{i}. 다음으로, 우리는 그들의 공간적 관계를 결정하기 위해 모든 노드를 횡단한다(라인 4-22). 특히, 사물 노드가 장면에서 다른 객체들과의 접촉 내 수직 관계가 부족한 경우, 우리는 _"항가능"_와 같은 객체를 지정하고 그들의 비접촉 수직 관계(라인 9-13)를 계산한다. 이러한 물체의 예로는 그림, 커튼, _etc_가 있다. 마지막으로, 우리는 복수의 객체들(라인 23): i) 사이의 관계를 설정하는데, 목표 객체가 좌우로 표시된 두 개의 에지들과 연결되면, 대상 객체는 이웃하는 두 개의 노드들과 함께 관계 트리플트를 형성한다. 이(ii) X축 또는 Y축 방향의 객체 그룹의 중심점 좌표의 오프셋이 지정된 오프셋 임계값(\\delta\\)보다 작은 경우, 이 객체 그룹은 정렬 관계를 형성한다. 오프셋 임계치(\\delta\\)는 장면의 크기를 기준으로 조정될 것이다. 추가적으로, 우리는 장면 그래프를 검증하기 위해 자동 검증 절차를 사용하여 우리가 구축한 장면 그래프의 품질을 더욱 향상시킨다(라인 24). 검증 동작 중 하나는 상식에 기초하여 사물 간의 매핑과 관계 설명을 수동으로 유지하는 것을 포함한다. 예를 들어, 사람들은 보통 "움직이는 것"이 아니라 TV와 벽의 관계를 설명하기 위해 "마운트 온"을 사용한다. 따라서 (TV, 매달려, 벽)부터 (TV, 장착, 벽)까지 자동 정제해 드립니다.\n' +
      '\n' +
      '구성된 3D 장면 그래프(\\mathcal{V},\\mathcal{V},\\mathcal{V}})\\에서 노드 \\(\\mathcal{V}_{1}\\311cup\\mathcal{V}_{2}\\빅cup\\mathcal{V}_{K}\\)는 특정 계층 수준에서 노드 세트를 나타내는 \\(\\mathcal{V})를 포함한다. 위계는 지지 관계에 의해 결정되며, 예를 들어 바닥이 지원하는 오브젝트는 \\(\\mathcal{V}_{0}\\)를 구성하는 반면, 표가 지원하는 오브젝트는 \\(\\mathcal{V}_{1}\\), _etc_를 형성한다. 한 노드 \\(v\\in\\mathcal{V}_{k}\\)에서 유래한 가장자리는 인근 계층(\\mathcal{V}_{k}\\cup\\mathcal{V}_{k}\\cup\\mathcal{V}_{k+1}\\cup\\mathcal{V}_{k+1}\\)에서만 종료될 수 있다. 즉, 장면 그래프 내의 에지들은 동일한 계층적 레벨 내의 노드들을 독점적으로 연결하거나, 또는 한 레벨 이상의 노드들을 연결한다.\n' +
      '\n' +
      '```\n' +
      '입력 :\\(M\\) 객체 포인트 구름(P_{1}, P_{2},\\ldots, P_{m}\\) 출력 :\\(3\\)D 장면 그래프(\\mathcal{G},\\mathcal{V})\n' +
      '1:1, \\(1\\)에서 \\(i\\)까지의\\(i\\)에서 \\(M\\)도(M\\)까지 1:1:\\(i\\)\n' +
      '2: Create 노드 \\(v_{i}\\mathcal{V}\\) 및 오브젝트 포인트 클라우드 \\(P_{i}\\)의 경계 상자 크기 \\(b_{i}\\)를 사용한 Create 노드 \\(v_{i}\\)\n' +
      '3:endfor\n' +
      '4:\\(i\\f rom\\ (1\\)t o\\ (M\\)do)\n' +
      '5:for\\(j\\) from \\(i+1\\) to \\(M\\)do\n' +
      '6:\\(\\mathrm{RelsType}_{v}\\leftarrow\\mathrm{VerticalInContact}(v_{i},v_{j})\\)\n' +
      '7:\\((v_{i}_{v}})에서\\(\\mathcal{G}},e_{i,j\\)까지의 접촉 내 수직 관계 트리플츠.\n' +
      '8:endfor\n' +
      '9:\\(v_{i}\\)와 수평으로 관련된 노 오브젝트(v_{i}\\)\n' +
      'H\\(1\\)에서 \\(M\\) 및 \\(i\\neq k\\)까지의\\(k\\) 및 \\(i\\(i\\neq k\\)도 10:10:\\(k\\)\n' +
      '11:\\(\\mathrm{RelsType}_{v}\\leftarrow\\mathrm{VerticalNonContact}(v_{i},v_{k})\\)\n' +
      '12: 부비접촉 수직 관계 트리플츠(v_{i},v_{k},e_{v}\\)~\\(\\mathcal{G}\\)\n' +
      '13:endfor\n' +
      '14:endif\n' +
      '15:endfor\n' +
      '16:for\\(v_{i}\\in\\mathcal{V}\\)do\n' +
      '17: let \\(\\{v_{i_{1}},v_{i_{2}},...,v_{i_{N}}\\}\\) be the \\(N\\) different nodes with the same in-contact vertical parent node \\(v_{i}\\)\n' +
      '18:for\\(j\\) from \\(1\\) to \\(N\\)do\n' +
      '19:\\(\\mathrm{RelsType}_{h}\\leftarrow\\mathrm{Horizontal}(v_{i},v_{i_{j}})\\)\n' +
      '20:\\((v_{i},v_{i_{j}\\) 내지\\(\\mathcal{G}},e_{i,j})의 수평 관계 a\\(v_{i})\n' +
      '21:endfor\n' +
      '22:endfor\n' +
      '23: 업데이트 \\(\\mathcal{G}\\lelearrow\\mathrm{멀티코드}(\\mathcal{G})\n' +
      '자동 검증 절차가 있는 자동 확인 절차 24:업데이트 \\(\\mathcal{G}\\)\n' +
      '```\n' +
      '\n' +
      '원번호 1*\n' +
      '\n' +
      '어깨기는 소매.\n' +
      '\n' +
      'Sec. 3.3에서 템플릿과 LLM을 모두 채택하여 SceneVerse에서 장면 언어 쌍을 자동으로 생성한다. 이 섹션에는 더 많은 기술적 세부 사항 및 예가 제공된다.\n' +
      '\n' +
      'Pipelineing Pipelineing Pipeline#########.3.1을 대상으로 선별 선택 피프라인##############.\n' +
      '\n' +
      '객체 캡션은 물체의 시각적 및 물리적 특성에 대한 자세한 설명을 제공하는 것을 목표로 하며, 고유한 특징으로 객체 수준의 접지력을 촉진한다. 세부 객체 캡션 파이프라인은 Alg 2에서 요약되며, 다중 뷰 이미지 \\(\\{I_{1},I_{2},\\ldots,I_{n}\\}\\)를 통해 대상체의 포인트 클라우드(P_{o}\\)를 사용하여 이미지 스루 렌더링에서 가시점 \\(P_{o,v}^{vis}\\)를 얻는다. 폐쇄 점수 \\(s^{occ}_{o,v}\\)는 가시점 수와 객체점 클라우드 사이의 비율로 계산된다. 그런 다음 이미지를 렌더링된 바운딩 박스로 크롭하고 BLIP2 [48]를 통해 처리하여 초기 객체 캡션 \\(C_{o,v}\\)을 생성한다. 각 초기 자막에 대해 \\(s^{clip}_{o,v}\\)로 표시된 텍스트와 크롭된 이미지 사이의 CLIP [66] 유사성 점수를 계산한다. 정제된 객체 캡션을 얻기 위해 CLIP 점수가 가장 높고 폐색이 최소인 상위 \\(10\\) 초기 캡션을 선택한다. 선택된 문장들은 객체 캡션에 대한 일관된 요약들을 얻기 위해 LLM에 공급된다. 이 과정에서 우리는 잠재적 오류를 식별하고 보정하기 위해 언어 모델을 명시적으로 지시한다.\n' +
      '\n' +
      '#### Scene Caption\n' +
      '\n' +
      'In this apartment, there are 5 cabinets, 1 bed, 3 trash cans, 1 microwave, and 1 TV. The cabinets are positioned in front of the trash cans, while the bed is in front of the cabinet. The trash cans are also behind the cabinet and to the left of the bed. The TV is inside one of the cabinets. The bed is positioned behind the cabinet and to the right of the trash cans. This apartment seems to be well-equipped with storage options and has a comfortable sleeping area.\n' +
      '\n' +
      '#### Scene Caption\n' +
      '\n' +
      'In this room, there is an architectural floor and wall. The wall are attached to the floor, creating a room with a big door. There are blind hanging on the wall, close to the window. The room has a wide window, a heater connected to a wall, and a ceiling overhead. The room is furnished with a sofa, a table, and a chair. There are cushion and beanbag on the sofa, and a plant and lamp nearby. The room also has a TV, a whiteboard, and some clutter on the floor. The overall style of the room is comfortable and modern.\n' +
      '\n' +
      '#### Scene Caption\n' +
      '\n' +
      '이 방에는 침대 2개, 창 2개, 램프 3개, 이불 3개, TV 6개, 베개 2개, 커튼 4개, 선반이 있습니다. TV는 선반보다 높은 위치에 있는 반면 소파는 침대 오른쪽에 있습니다. 베개 중 하나는 침대 내부에 있고 침대는 소파 왼쪽에 있습니다. 또한 램프는 전원 배출구보다 높게 위치하여 램프보다 낮다. 방은 여유와 오락을 위해 다양한 물건들로 편안한 생활공간으로 보인다.\n' +
      '\n' +
      'A.3.2.2 자동언어세대####\n' +
      '\n' +
      '템플릿 기반 템플릿을 생성하여 각 유형의 관계에 대한 설명을 생성한다. 우리는 관련된 물체의 수와 공간적 관계를 기반으로 템플릿을 세 가지 유형으로 분류했다.\n' +
      '\n' +
      '**** 쌍별**: 쌍별 템플릿은 장면에서 타겟 객체와 앵커 객체 사이의 위치 관계를 설명하는 데 사용된다. 우리는 강화 기반 설명을 풍부하게 하고 활동적이고 수동적인 긴장, 반전 조항에 걸쳐 다양한 템플릿을 설계한다. 대표적인 예가 아래에 나와 있습니다.\n' +
      '\n' +
      '- The target-object (is) spatial-relation the anchor-object.\n' +
      '\n' +
      '그림 A.2: ** 객체 캡션의 예.*** 코일 로크 에타 레트로브 제넥틴 ** 굵게***입니다.\n' +
      '\n' +
      'Figure A.3: **Examples of scene captioning.**\n' +
      '\n' +
      '* (is) 공간 상관 앵커 객체라는 타겟 객체이다.\n' +
      '* There is a target-object that (is) spatial-relation the anchor-object.\n' +
      '* 공간 상관 앵커 오브젝트는 대상 객체이다.\n' +
      '* Spatial-relation the anchor-object, a target-object is placed.\n' +
      '**** ** 다중 대상**: 아르테토 bjectf 또는msa b etweeno ra 리그노 엘라스토프러 엘라스토시 nt를 주저할 때 사용된다. Tt는 오렐로트 지저분자를 사용하는 경우 *Pair-wise**t를 사용한다.\n' +
      '\n' +
      '그림 A.4: ** 오브젝트의 예.* 노트. 그린 바운딩 박스가 타겟 객체를 나타내고 노란색 바운딩 박스가 앵커 오브젝트(들)를 나타낸다는 것을 나타낸다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:18]\n' +
      '\n' +
      '공간 변압기의 특징을 집계하고 장면 자막의 [CLS] 특징과 정렬하기 위한 층입니다. 추천-객체-레벨 접지들을 위해, 우리는 획득한 객체 특징들뿐만 아니라 추천 언어 특징을 4층 자기-의도 변압기에 더 통과시키고 Sec 4.3에 기술된 접지 목적을 사용하여 참조 표현의 참조 오브젝트의 특징 및 [CLS] 특징에 매칭한다.\n' +
      '\n' +
      '** 훈련*** 객체 수준의 사전 학습을 위해 1500epochs 및 평가 기간이 없는 경우 학습률이 \\(1\\t 10^{-2}\\)인 AdamW 최적기를 사용한다. 훈련 중 512의 배치 크기를 사용하고 최소 학습률 \\(1\\t 10^{-3}\\)로 학습률 스케줄링을 위한 코사인 어닐링 방식을 레버리지한다. 언어 인코더(1\\ 10^{-5}\\)의 학습률, 공간 변압기에 대한 \\(1\\t10^{-4}\\)의 학습률, 자기 선택 변압기에 대한 \\(1\\t10^{-4}\\)의 학습률, 그리고 나머지 모든 학습 가능 파라미터(_e._e._e._e._CS.A.A.A.A.5\\)에 대해 \\(1\\ 10^{-4}\\)에 대해 \\(1\\ 10^{-4)의 학습률, 자기 선택 변압기에 대한 \\(1\\ 10^{-4}\\)에 대해 \\(1\\ 10^{-4}\\)에 대해 \\(1\\ 10^{-4}\\)에 대해 \\(5\\ 10^{-4}\\)에 대해 \\(1\\ 10^{-4}\\)의 학습률, 자기 선택 변압기의 학습률(1\\ 10^{-4}\\)의 학습률(1\\ 10^{-4}\\)의 학습률, 그리고 나머지 모든 학습 가능 매개변수(5 모든 실험에 대해 500개의 평가 기간을 가진 150개의 epoch에 대한 모델과 \\(0.1\\)의 최소 학습률 비율로 학습률을 위한 코사인 어닐링 방식을 훈련시킨다. 모든 사전 훈련 실험은 약 2일 동안 스켄버스에 대한 가장 긴 사전 훈련과 함께 8개의 NVIDIA-A100 GPU에서 실행된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{42.7pt} p{284.5pt}} \\hline \\hline Description type & Prompt \\\\ \\hline Object caption & Summarize caption below. The summary should be a description of the **target-object**. Focus on the **target-object**’s attribute, like color, shape and material, _etc._ Identify and correct the potential errors. \\\\  & caption: _A bed in a hotel room. A white comforter on a bed. A bed with a striped comforter._. \\\\  & **target-object**: _Bed_ \\\\ \\hline Object referral & Rewrite the following caption using one random sentence structure. You should give me only one rewritten sentence without explanation. _The bed is between desk and nightstand._ \\\\ \\cline{2-3}  & Rewrite the following caption. You should give me only one rewritten sentence about **target-object** without explanation. Make sure **target-object** is the subject of the sentence, not anchor-object(s). If the sentence is in full inversion, keep the inversion. \\\\  & caption: _The armchair is next to the sofa._ \\\\  & **target-object**: _Armchair_ \\\\  & anchor-object(s): _Sofa_ \\\\ \\hline \\multicolumn{3}{p{42.7pt}}{Rewrite the following caption using one random sentence structure. You need to focus on the location and relations of the **target-object** that appears in the sentence. If multiple **target-object** appear in the sentence, you need to focus on the first **target-object** that appears. You can also add the **target-object**’s function and comfort level based on the sentence, e.g., how the objects can be used by humans and human activities in the scene. You should give me only one rewritten sentence without explanation. _Far from the bowl and peppershaker, the vase is to the left, it is also on the top of countertop._ \\\\  & **target-object**: _Vase_ \\\\ \\hline Scene captioning & Your task is to provide a summary for a scene from a given scene graph. The scene contains some objects, which compose a scene graph in json format. \\\\  & There are 3 types of descriptions in scene graph: “scene type” denotes the type of the scene. “objects count” then listed the objects in the scene and their quantity, it should be noted that the actual objects in the room may be more than listed. “objects relations” describe the spatial relations with objects. \\\\  & Also describe the scene concerning commonsense, e.g., how the objects can be used by human and human activity in the scene. The description should conform to the given scene graph. The spatial relations between objects can only be inferred from the “objects relations” in scene graph. Don’t describe each object in the scene, pick some objects of the scene for summary. Don’t describe each relations in the scene, pick some relations of the scene for summary. You can also summarize the room’s function, style, and comfort level based on the arrangement and count of objects within the room. The summary should be about the object types, object attributes, relative positions between objects. Your summary must not exceed 80 words. You must write using one random sentence structure. \\\\  & scene graph: { ’scene_type’: ’Bedroom’, ’object_count’: {’nightstand’:2,...}, ’relation’: {’nightstand’, ’on’, ’floor’}, {’backback’, ’in} \\\\  & front of’, bed\\(\\}\\),...) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **l 세부 세부표 s s <표 2: **l 세부표 2: <표 2: **l 자세한 세부표 s s <표 2>: <표 2: **l 자세한 세부표 2: <표 2>\n' +
      '\n' +
      'In this section, we provide details on experimental settings, model implementation, and additional results.\n' +
      '\n' +
      '3D 정품 교환\n' +
      '\n' +
      '모든 데이터 세트를 설정하면 제공되는 훈련 세트만 있는 모든 모델을 평가합니다. 이전 작품 [98]에 이어 Tab 2에서 모든 데이터 세트의 검증 세트에 대한 모델 성능을 보고하는데, 최적화가 없는 객체 제안을 생성하기 위해 오프 시트 마스크3D 분할 모델을 사용했다.\n' +
      '\n' +
      'Sec. 5.1에서 간략하게 언급한 바와 같이, 우리는 주로 3D 시각적 접지 실험에서 3개의 모델 설정, 즉 _scratch_, _pre-train_ 및 _fine-tuned_를 고려했다. i_pre-train_ 설정의 경우 부록 B.2에서 언급된 동일한 설정을 따르며, _scratch_ 및 _fine-tuned_ 설정에서 다른 데이터세트 특이적 미세 조정 모델과 상당히 비교하기 위해 추천 접지 자기 의도 변환기의 객체 특징에 대해 추가 2층 MLP를 추가한다. 훈련 중 추가된 투영층에 대해 \\(1\\t 10^{-4}\\)의 학습률로 100epoch에 대한 모든 모델 가중치와 함께 이 접지 헤드를 미세 조정하고 부록 B.2에 설명된 구현과 동일한 다른 모든 설정을 설정했다.\n' +
      '\n' +
      '### Zero-shot Transfer\n' +
      '\n' +
      'SettingIn the zero-shot experiments, we first construct the held-out test set by aggregating scene-text pairs in SceneVerse from scenes in ScanNet and MultiScan. Specifically, we use the validation set of ScanRefer, Nr3D, and Sr3D. For scene-text pairs in the SceneVerse-val, we construct the test set by randomly sampling \\(\\frac{1}{5}\\) of human-annotated object referrals in the MultiScan dataset. This results in a test set with around 1.7K object referrals randomly drawn from 8.5k human-annotated object referrals in the MultiScan dataset. In the _zero-shot_ settings, we use all scene-text pairs from datasets in SceneVerse except for ScanNet and MultiScan. This includes both human-annotated and generated texts in ARKitScenes, 3RScan, and HM3D. This setting serves to test models\' generalization capability in grounding objects with both unseen scenes and unseen texts. In the _zero-shot text_ setting, we add generated scene-text pairs in ScanNet and MultiScan into the data used in the _zero-shot_ setting, thereby making the held-out test containing mainly unseen object referrals.\n' +
      '\n' +
      'ImplementationIn the zero-shot experiments, we mainly considered three model settings _scratch_, _zero-shot_, and _zero-shot text_. For the _zero-shot_ setting, we pre-train the model following Appendix B.2 without additional grounding heads considering there is no additional training data available in the zero-shot transfer setting. In the _scratch_ and _zero-shot text_ setting, we follow the model implementation described in Appendix C.1 and add an additional 2-layer MLP over the object features from the self-attention transformer. We follow the same fine-tuning setting described in Appendix C.1.\n' +
      '\n' +
      '다.\n' +
      '\n' +
      '이 섹션에서는 추가 실험 결과를 제공합니다. 구체적으로, 우리는 수집된 SceneVerse를 전통적인 3D 의미 세분화 작업에 대한 사전 훈련 데이터 소스로서 레버리지한다. 다음으로 모델 설계에 대한 남용 분석을 제공합니다.\n' +
      '\n' +
      '### Semantic Segmentation\n' +
      '\n' +
      'SettingTo test if the scaling effect of SceneVerse is universally beneficial for 3D understanding tasks, we use 3D semantic segmentation as a signature task to illustrate the effectiveness of SceneVerse. Notably, a recent work that introduced the Swin3D model [85] has identified the importance of pre-training for 3D semantic segmentation [85]. Following the same setting, we test if the proposed Swin3D model could be further improved by substituting the pre-training data to SceneVerse. Specifically, we test models\' performance on the ScanNet semantic segmentation task with 20 semantic categories and report the mean IoU and mean Acc on the validation set of ScanNet. As the original implementation of Swin3D pre-training requires surface normals as additional inputs, we reimplement the model and pre-train all models with only point coordinates and colors.\n' +
      '\n' +
      'ComparisonAs shown in Tab. A.3, we observe a significant model performance improvement (\\(\\sim\\)6%) by training Swin3D-S model on our SceneVerse dataset. Comparing our pre-training set to Structured 3D, we also observe consistent model performance improvement, showcasing the benefit of scaling-effect in SceneVerse. Moreover, we fine-tune the model on ScanNet after pre-training on SceneVerse. This process further brings improvement in model performance on semantic segmentation. We believe these results serve as strong pieces of evidence validating the effectiveness of data scaling in SceneVerse and also\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Methods & Init. & SceneVerse Pre. & mIoU & mAcc \\\\ \\hline Swin3D\\({}_{n}\\)-S\\(\\dagger\\) & ✗ & ✗ & 75.2 & - \\\\ Swin3D\\({}_{n}\\)-S\\(\\dagger\\) & S3D & ✗ & 75.6 & - \\\\ \\hline Swin3D-S & ✗ & ✗ & 63.2 & 72.8 \\\\ Swin3D-S & S3D & ✗ & 64.1 & 75.1 \\\\ Swin3D-S (_pre-train_) & ✗ & ✓ & 67.7 & 78.0 \\\\ Swin3D-S (_pre-train_) & S3D & ✓ & 69.5 & 80.1 \\\\ Swin3D-S (_fine-tuned_) & S3D & ✓ & **70.6** & **80.2** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 A.3: **Santic 세분화 결과는 <스카넷 검증 집합>과 같다. (\\dagger\\)**는 표면 정규화를 추가 입력으로 하는 모델을 나타낸다. S3D는 양 _et al_[85]가 제공하는 구조화된 3D에서 미리 훈련된 원래 Swin3D 모델 가중치로 초기화된 모델을 나타낸다.\n' +
      '\n' +
      '3D 시각적 접지 외에 모든 3D 과제에 대한 잠재적인 이점을 제공합니다.\n' +
      '\n' +
      '1.1 모델 어플레이션 촉매######### 2.1.1 모델 압플레이션 촉매 2.1.1 모델 2##########\n' +
      '\n' +
      '이 절에서는 다단계 대비 정렬 설계에 대한 남용 분석을 제공한다. 우리는 주로 모델의 목표 제거를 학대라고 생각한다. 우리는 추천-객체-레벨 정렬 목적을 기본 설정으로 선택하고 (i) 객체-레벨 정렬 목적, (ii) 마스킹된 언어 모델링 목적, (iii) 장면-레벨 정렬 목적 등 제거를 고려한다. 객체-레벨 정렬 목적을 제거하기 위해, 우리는 객체 포인트 클라우드 인코더의 1단계 사전 학습을 제거하고 추천-객체-레벨 정렬 내에서 이 모듈을 공동으로 학습한다. Tab A.4에서 볼 수 있듯이, 우리는 추가 미세 조정 없이 SceneVerse-val에 대한 다양한 모델 설정을 테스트한다. 먼저, 우리는 장면의 정렬 목표가 \\(\\ Res\\)5% 성능 강하와 함께 SceneVerse-val의 추천 객체 접지에도 중요하다는 것을 보여준다. 객체 수준 정렬(\\(\\ason\\) 2% 드롭) 및 마스킹된 언어 모델링 목표(\\(\\ Res\\) 1.5% 드롭) 없이 학습된 모델에 대해서도 유사한 관찰이 이루어질 수 있다. 이러한 결과는 전체 모델 디자인의 효과를 긍정한다고 생각합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline Obj-lvl & MLM & Scene-lvl & Overall & Easy & Hard \\\\ \\hline ✗ & ✗ & ✗ & 64.8 & 75.4 & 48.7 \\\\ ✓ & ✗ & ✗ & 65.2 & 77.1 & 47.4 \\\\ ✓ & ✓ & ✗ & 62.4 & 73.4 & 45.8 \\\\ ✓ & ✓ & ✓ & **66.9** & **77.8** & **50.3** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 A.4: ** 모델 절제가 SceneVerse-val에 대한 모델 절제****모델 절제에 대해 표.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>