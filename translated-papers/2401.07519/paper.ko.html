<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '제2의 제로샷 아이덴티티 홍보세대: 제2의 프로젝트 샷 아이덴티티 기술 회사입니다.\n' +
      '\n' +
      'Qixun Wang\n' +
      '\n' +
      'InstantX Team\n' +
      '\n' +
      '{haofanwang.ai@gmail.com}\n' +
      '\n' +
      '[https://instantid.github.io](https://instantid.github.io)\n' +
      '\n' +
      'Xu Bai\n' +
      '\n' +
      'InstantX Team\n' +
      '\n' +
      '{haofanwang.ai@gmail.com}\n' +
      '\n' +
      '[https://instantid.github.io](https://instantid.github.io)\n' +
      '\n' +
      'Haofan Wang\n' +
      '\n' +
      'Corresponding author\n' +
      '\n' +
      'InstantX Team\n' +
      '\n' +
      '{haofanwang.ai@gmail.com}\n' +
      '\n' +
      '[https://instantid.github.io](https://instantid.github.io)\n' +
      '\n' +
      'Zekui Qin\n' +
      '\n' +
      'InstantX Team\n' +
      '\n' +
      '{haofanwang.ai@gmail.com}\n' +
      '\n' +
      '[https://instantid.github.io](https://instantid.github.io)\n' +
      '\n' +
      'Anthony Chen\n' +
      '\n' +
      'InstantX Team\n' +
      '\n' +
      '{haofanwang.ai@gmail.com}\n' +
      '\n' +
      '[https://instantid.github.io](https://instantid.github.io)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '교과전환, 드림보스, LoRA 등의 방법으로 개인화된 이미지 합성에 상당한 진전이 있었다. 그러나 그들의 실제 적용 가능성은 높은 저장 요구, 긴 미세 조정 프로세스 및 다중 참조 이미지의 필요에 의해 방해된다. 반대로 기존의 ID 임베딩 기반 방법은 단일 순방향 추론만 요구하는 반면, 수많은 모델 파라미터에 걸쳐 광범위한 미세 조정, 지역 사회 사전 훈련 모델과의 호환성이 부족하거나 높은 안면 충실도를 유지하지 못하는 문제에 직면한다. 이러한 한계를 추가하여 강력한 확산 모델 기반 솔루션인 **EEantID**를 소개합니다. 당사의 플러그 앤 플레이 모듈은 단일 얼굴 이미지만을 사용하여 다양한 스타일로 이미지 개인화를 강력하게 처리하면서 높은 충실도를 보장합니다. 이를 달성하기 위해 강한 의미론적, 약한 공간 조건을 부과하여 얼굴과 랜드마크 이미지를 텍스트 프롬프트와 통합함으로써 새로운 아이덴티티넷을 설계한다. 인스턴트ID는 탁월한 성능과 효율성을 보여 신원 보존이 가장 중요한 실제 응용 분야에서 매우 유익하다는 것을 입증한다. 또한, 우리의 작업은 SD1.5 및 SDXL과 같은 인기 있는 미리 학습된 텍스트 대 이미지 확산 모델과 원활하게 통합되어 적응 가능한 플러그인 역할을 한다. 당사의 코드 및 사전 훈련된 체크포인트는 [https://github.com/preantID/preantID] (https://github.com/preantID/preantID)에서 사용할 수 있습니다.\n' +
      '\n' +
      '이미지 합성, 이미지 맞춤형, ID 보존 보존\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '이미지 생성 기술은 GLIDE[12], DALL-E 2[15], Imagen[19], Stable Diffusion (SD)[16]과 같은 대형 텍스트 대 이미지 확산 모델이 등장하면서 괄목할 만한 발전을 보였다. 이러한 발전의 초점은 개인화 및 맞춤형 생성이며, 이는 하나 이상의 참조 이미지를 기반으로 스타일, 주제 또는 캐릭터 ID에서 일관된 이미지를 생성하려는 것이다. 이 접근법은 전자 상거래 광고, AI 초상화, 이미지 애니메이션 및 가상 트립온과 같은 응용 분야에서 광범위한 잠재력에 상당한 관심을 기울였다. 그러나 주목할 만한 과제는 인간 피험자의 복잡한 정체성 세부 사항을 정확하게 보존하는 맞춤형 이미지를 생성하는 것이다. 이 작업은 특히 인간의 얼굴 정체성(ID)이 더 많은 양의 의미학을 포함하고 일반 스타일이나 물체에 비해 더 높은 세부 및 충실도를 필요로 하기 때문에 요구되며, 이는 주로 거친 양식 질감과 색상에 초점을 맞추고 있다. 세부 텍스트 설명에 의존하는 기존 바닐라 텍스트 대 이미지 모델은 맞춤형 세대의 강력한 의미론적 관련성을 달성하는 데 짧다. 제어 가능성을 높이기 위해 제어넷[25] 및 T2I-캡터[11] 등의 최근 작품이 소개되었다. 이러한 발전은 대규모 사전 훈련된 텍스트 대 이미지 모델에 공간 컨디셔닝 제어를 추가하여 사용자 인출 스케치, 깊이 맵, 신체 포즈 및 의미 세분화 맵과 같은 요소를 통해 미세 개질된 구조적 제어를 용이하게 한다. 또한 T2I-캡터 스타일 어댑터 및 유니-컨트롤넷[26] 글로벌 컨트롤러와 같은 참조되는 스타일 또는 콘텐츠에 대한 적응이 이 영역에 무리가 있었다. 이러한 발전에도 불구하고, 생성된 이미지들의 참조에 대한 충실도는 부분적으로만 남아 있다.\n' +
      '\n' +
      '기준 이미지로 이미지 생성을 향상시키기 위해 현재 개인화된 생성 방법은 테스트 중 미세 조정의 필요성을 기반으로 두 가지 유형으로 분류할 수 있다. 첫 번째 유형은 드림보트[18], 텍스트 전환[4], 로우-랭크 적응[6](LoRA)와 같은 선도적인 예시와 함께 미세 조정이 필요한 방법을 포함한다. 이러한 방법은 참조 이미지에 새로운 측면을 더 잘 반영하기 위해 미리 학습된 텍스트 대 이미지 모델을 미세 조정하려는 것을 목표로 한다. 높은 정확도를 달성하는 데 효과적이지만 미세 조정 과정은 자원 집약적이고 시간이 많이 소요되어 실용성을 제한한다. 또한, 이들은 종종 동일한 문자 ID와 같은 제한된 데이터 시나리오에서 다양한 설정 및 투쟁에 여러 참조가 필요하다.\n' +
      '\n' +
      '두 번째 유형은 추론 중에 미세 조정 작업을 우회합니다. 그것은 많은 양의 도메인 특이적 데이터를 구축하고 경량 어댑터를 구축하는 것을 포함한다.\n' +
      '\n' +
      '그림 1: 저작자 순서대로 인판트ID의 멋진 팀이 인판트ID로 생성되었다.\n' +
      '\n' +
      '기준 이미지로부터 특징 추출을 위해. 그런 다음 이 어댑터는 교차 의도를 사용하여 확산 생성 공정에 이러한 특징을 통합한다. IP-Ad캡터[24]는 텍스트와 이미지 특징을 분리하기 위해 고유한 교차 의도 메커니즘을 사용하여 시각적 프롬프트로 참조 이미지를 주입할 수 있는 대표적인 작품 중 하나이다. 그러나 주로 CLIP[14]의 이미지 인코더에 의존하는 이 접근법은 약하게 정렬된 신호만을 생성하는 경향이 있어 고 충실도 맞춤형 이미지를 생성하는 데 짧다.\n' +
      '\n' +
      '이러한 제약에 비추어 즉각적인 동일성 보존 이미지 합성에 초점을 맞춘 새로운 접근(**EEantID**)을 소개한다. 본 연구에서는 단순 플러그 앤 플레이 모듈을 도입하여 고 충실도와 효율성의 격차를 해소하여, 높은 충실도를 유지하면서 하나의 얼굴 이미지만을 사용하여 어떤 스타일에서도 이미지 개인화를 잘 처리할 수 있도록 한다. 기준 이미지로부터 얼굴 동일성을 보존하기 위해, 우리는 안면 이미지, 랜드마크 이미지 및 텍스트 프롬프트가 이미지 생성 프로세스를 안내하는 강력한 의미론적 및 약한 공간 조건을 추가하여 복잡한 세부 사항을 유지하기 위해 새로운 얼굴 인코더를 설계한다. 우리는 (1) 안정성과 호환성: UNet[17]의 전체 매개변수 대신 경량 어댑터를 훈련하는 데 중점을 두고 모듈 플레이블을 만들고 커뮤니티에서 사전 훈련된 모델과 호환될 수 있습니다; (2) 튀는 방법은 추론을 위한 단일 정방향 전파만 요구하여 미세 조정의 필요성을 제거해야 한다. 이 기능은 인판트ID가 실제 응용 분야에 매우 경제적이고 실용적이며, (3) 수퍼리어 성능: 하나의 참조 이미지만으로 인판트ID가 최첨단 결과를 달성하여 높은 충실도와 유연성을 보여준다. 놀랍게도, 여러 참조 이미지에 의존하는 LoRAs와 같은 훈련 기반 방법의 성능을 일치시키거나 능가할 수 있다.\n' +
      '\n' +
      '요약하면, 우리의 기여는 3배입니다.\n' +
      '\n' +
      '\\(\\불릿\\) 우리는 미리 훈련된 텍스트 대 이미지 확산 모델을 위한 혁신적인 ID 보존 적응 방법인 인판트ID를 제시하여 충실도와 효율성의 격차를 해독한다. 실험 결과는 이 영역에서 다른 최첨단 방법과 비교하여 제안된 방법의 우수한 성능을 보여준다.\n' +
      '\n' +
      '\\(\\불렛\\) 인판트ID는 동일한 염기 확산 모델에서 미세 조정되는 다른 맞춤형 모델과 플러싱 가능하고 호환되어 추가 비용 없이 미리 학습된 모델의 ID 보존이 가능하다. 또한, 인판트ID는 원본 스테이블 디퓨전 모델에서 관찰된 바와 같이 텍스트 편집에 대한 상당한 제어를 유지하여 다양한 스타일로 ID의 원활한 통합을 가능하게 한다.\n' +
      '\n' +
      '\\(팔레트\\) 인판타니의 우수한 성능과 효율은 새로운 뷰 합성, ID 보간, 멀티ID 및 멀티 스타일 합성과 같은 다양한 실제 응용 분야에 대한 엄청난 잠재력을 무시한다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '영상 내확산 모델들.\n' +
      '\n' +
      '텍스트 대 이미지 확산 모델[15, 19, 16, 12, 1, 7]은 최첨단 이미지 생성 결과를 달성하며 최근 몇 년 동안 커뮤니티로부터 전례 없는 관심을 받았다. 공통적인 관행은 CLIP[14]와 같은 사전 훈련된 언어 인코더를 통해 텍스트 프롬프트가 잠재된 언어로 인코딩되고 잠재력을 사용하여 확산 과정을 안내하는 것이다. 예를 들어, GLIDE[12]는 이미지 생성과 편집을 모두 지원하기 위해 캐스케이드 텍스트 유도 확산 구조를 채택한다. 디스코[21]는 CLIP[14]의 텍스트 인코더를 사용하여 텍스트 입력을 처리하고 확산 과정에서 CLIP 지침을 소개한다. 안정적 확산 [16]은 원래의 픽셀 공간 대신 잠재 이미지 공간에서 확산 과정이 수행되는 잠재 확산 모델(LDM)의 강조된 작품 중 하나로 계산 비용을 크게 감소시킨다. 후속 조치로서 Stable Diffusion XL[13] (SDXL)는 더 큰 UNet과 제2 텍스트 인코더를 도입하여 생성된 이미지에 대한 더 강력한 텍스트 제어를 얻는 강력한 텍스트 대 이미지 생성 모델이다.\n' +
      '\n' +
      '기업 중심 이미지 세대입니다.\n' +
      '\n' +
      '특정 피사체의 제한된 이미지 세트를 사용하여 텍스트 설명을 기반으로 맞춤형 이미지를 생성하는 객체 구동 텍스트-이미지 생성이 주목할 만한 발전을 보였다. 꿈보드와 같은 이전 주제 중심 접근 방식[18], 텍스트 전환 [4], ELITE[22], E4T[5], ProFusion[28]은 미세 조정 과정에서 목표 개념을 설명하기 위해 특별한 프롬프트 토큰(S^{*}\\)을 미세 조정했다. 대조적으로, 다른 최근의 방법은 추가 미세 조정 없이 피사체 구동 텍스트 대 이미지 생성을 가능하게 하기 위해 노력한다. 이러한 방법들은 일반적으로 코어 미리 학습된 텍스트 대 이미지 모델을 동결된 상태로 유지하면서 추가적인 모듈들을 트레이닝하는 것을 포함한다. 이 접근법의 주요 예는 텍스트 특징 및 이미지 특징에 대한 교차 의도 계층을 분리하여 교차 의도 메커니즘을 해독하는 것을 목표로 하는 IP-아드레터[24]이다. 동시 작업인 Anydoor[2]는 다양한 지역 변이를 허용하면서 질감 세부 사항을 유지하도록 설계된 세부 기능으로 일반적으로 사용되는 동일성 특징을 보완한다.\n' +
      '\n' +
      '임산대를 보존합니다.\n' +
      '\n' +
      'ID를 보존하는 이미지 생성은 교과 중심 세대의 특수한 경우이지만, 강한 의미론을 가진 얼굴 속성에 초점을 맞추고 실제 시나리오에서 광범위한 적용을 찾는다. 기존의 작품들은 주로 시험 시간 미세 조정 의존도에 따라 두 가지로 구분할 수 있다. 저랭크 적응[6] (LoRA)은 맞춤형 데이터셋에 대해 학습하기 전에 최소 수의 새로운 가중치를 모델에 삽입하는 인기 있는 경량 트레이닝 기법이다. 그러나 LoRA는 새로운 캐릭터별로 개별 훈련이 필요하여 유연성을 제한합니다. 대조적으로, 최근의 발전은 최적화가 없는 방법을 도입하여 추가 미세 조정 또는 반전 과정을 우회한다. Face0[20]은 CLIP 공간에서 투사된 얼굴 임베딩과 함께 마지막 3개의 텍스트 토큰을 오버랩하고 관절 임베딩을 조건으로 사용하여 확산 과정을 안내한다. 포토메이커[10]는 유사한 접근법을 채택하지만 이미지 인코더에서 트랜스포머[3] 층의 미세 조정 부분에 의해 ID 중심 임베딩을 추출하고 클래스 및 이미지 임베딩을 병합하는 능력을 향상시킨다. 페이스스튜디오[23]는 하이브리드-지도 동일성-보존 이미지 합성 프레임워크를 제시하며, 얼굴 임베딩이 선형 사영을 통해 CLIP 비전 임베딩과 CLIP 텍스트 임베딩 모두에 통합되는 경우 병합된 안내 임베딩이 교차 언급으로 UNet에 융합된다. IP-Ad캡터-FaceID[24]는 CLIP 이미지 임베딩 대신 얼굴 인식 모델로부터의 얼굴 ID 임베딩을 이용하여 ID 일관성을 유지한다. 그럼에도 불구하고, 이러한 방법은 UNet의 전체 매개변수를 훈련하거나 기존의 전처리된 커뮤니티 모델과 호환성을 희생하거나 높은 얼굴 충실도를 보장하는 데 짧아야 한다. 이러한 한계를 해결하기 위해 훈련 없는 방법과 훈련 집약적 방법 사이의 나눗셈을 정교하게 교잡하는 플레이블 모듈을 소개한다. 추론 중에 미세 조정이 필요하지 않은 우리의 방법은 SD1.5 및 SDXL과 같은 쉽게 이용 가능한 사전 훈련된 확산 모델과 원활하게 정렬하여 얼굴 보존에 탁월한 충실도를 달성했다.\n' +
      '\n' +
      '## 3 Methods\n' +
      '\n' +
      '인스턴트ID는 효율적인 경량 어댑터로, 아이디 보존 능력을 갖춘 사전 훈련된 텍스트 대 이미지 확산 모델을 쉽게 종료하고, \'자유 점심\'과 유사하다. 이 부분에서는 먼저 3.1절에서 텍스트 대 이미지 확산 모델과 맞춤형 이미지 생성에 대한 예선을 논의한 다음 3.2절에서 당사의 인판트ID의 세부 설계를 소개하고, 따라서 3.3절에서 우리의 훈련 및 추론 과정의 구체적인 내용을 상세히 설명한다.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '** 스테이블 디확산****입니다. 우리의 방법은 오토 인코더[8]가 있는 픽셀 공간 대신 저차원 잠재 공간에서 계산적으로 효율적으로 확산 과정을 수행하는 스테이블 디확산[16] 위에 구축된다. 구체적으로, 입력 이미지 \\(x_{i}\\in R^{H\\cer 3}\\)를 감안할 때 인코더는 먼저 잠재 표현으로 매핑한다: \\(z_{0}=\\xi(x_{i})\\, 여기서 \\(z_{0}\\xi(x_{i}),\\(z_{0}\\in R^{h\\in R^{h\\in R^{h\\)은 \\(z_{0}(x_{i(x_{i(x_{i}:\\xi(x_{i(x_{i}:\\xi(x_{i}:\\xi(x_{i}:\\xi(x_{i}:\\xi(x_{i}:\\xi(x_{i})\\in R^{h\\in R^{h\\in R^{h\\in R^{h\\in R^{h\\in R^{h\\in R^{h\\in R^{i}-{i})\\in R^{h\\in R^{h\\in R^ 확산 과정은 미리 훈련된 CLIP[14] 텍스트 인코더를 통해 생성된 텍스트 프롬프트의 임베딩을 나타내는 시끄러운 잠복형 \\(z_{t}\\), 현재 타임스톤(t\\) 및 조건 \\(C\\)을 나타내는 데노징 UNet[17]\\(\\)을 채택한다. 전체적인 훈련 목표는 정의된 것으로 정의된다.\n' +
      '\n' +
      '\\[\\mathcal{L}=\\mathbb{E}_{z_{t}_{z_{t},t,C,\\epsilon\\\\mathcal{N}[0,1]}[||\\\\epsilon- \\epsilon_{\\theta}(z_{t},t,C)||_{{2}].\n' +
      '\n' +
      '** 제어 가능한 디확산 모델***입니다. 우리는 사전 훈련된 확산 모델에 공간 제어를 조건으로 추가할 수 있는 제어넷[25]을 예시로 사용하여 기본 텍스트 프롬프트의 능력을 넘어 확장된다. 제어넷은 Stable Diffusion의 UNet 아키텍처를 UNet의 훈련 가능한 복제물과 통합한다. 이 복제물은 인코더 블록들 및 중간 블록 내의 제로 컨볼루션 레이어들을 특징으로 한다. 제어Net의 전체 과정은 다음과 같이 실행되며,\\[y_{c}=\\mathcal{F}(x,\\theta)+\\mathcal{Z}(x+\\mathcal{F}(x+\\mathcal{Z}(c,\\thecal_{z1}),\\theta_{z2}),\\theta_{z2}.\n' +
      '\n' +
      '\\(\\mathcal{F}\\)은 UNet 아키텍처,\\(x\\)는 잠복형, \\(\\theta{Z}\\)은 미리 훈련된 모델의 동결된 무게, \\(\\mathcal{Z}\\)는 중량 \\(\\theta_{z1}\\), \\(\\theta_{z2}\\), \\(\\ta_{c}\\)는 제어Net의 훈련 가능한 중량(\\_{z2}\\)를 갖는 0 컨벌루션(\\_{z2}\\)를 갖는 0 컨벌루션(\\_{z2}\\)는 제어넷(\\_{z2}\\)를 갖는 0 컨벌루션(\\_{z2}\\), \\(\\_{z2}\\), \\(\\_{z2}\\)를 갖는 0 컨볼루셔널(\\_{z2}\\)에 해당하며,\\(\\_{z2}\\), \\(\\_{z2}\\)는 제어Net(\\_{z2}\\)는 제어넷(\\_{z2}\\ 간단히 말해서, 제어넷은 UNet Block에 잔차를 추가하여 공간 조건 정보(스케치, 포즈, 깊이 맵 등)를 인코딩한 후 원래의 네트워크에 내장한다.\n' +
      '\n' +
      '**이미지 프로빗캡터*** IP-아드레터[24]는 원본 텍스트 대 이미지 모델을 수정하지 않고 텍스트 프롬프트와 병행하여 이미지 프롬프트 능력을 달성하기 위한 새로운 접근법을 소개한다. 독특한 디커플링된 교차 의도 전략을 사용하며, 다른 파라미터들을 변하지 않고 남기는 동안 추가적인 교차 의도 계층들을 통해 이미지 특징을 임베딩한다. 상기 디커플링된 교차의도는 상기 디커플링된 교차의도와 같이 예시될 수 있다.\n' +
      '\n' +
      '\\[Z_{ 새로운}(Q,K^{i}, V^{i}, V^{i}), 즉\\tag{3}\\\\}.\n' +
      '\n' +
      'I\\(Q\\), \\(K^{t}\\), \\(V^{t}\\)는 텍스트 교차의도, \\(K^{i}\\) 및 \\(V^{i}\\)에 대한 주의 동작의 질의, 키 및 값 행렬이 이미지 교차 의사를 위한 것이다. 질의 특징 \\(Z\\)과 이미지 특징 \\(c_{i}\\), \\(Q=ZW_{q}\\)\\(K^{i}=c_{i}=W^{i}_{k}\\), \\(V^{i}=c_{i}=c_{i}_{v}_{v}_{v}\\), \\(V^{i}_{i}_{i}_{i}_{i}_{i}_{i}_{i}_{i}_{i}_{i}_{i}_{i}. \\(W^{i}_{k}\\) 및 \\(W^{i}_{k}\\)만이 훈련 가능한 가중치이다.\n' +
      '\n' +
      '### Methodology\n' +
      '\n' +
      '**오버뷰***입니다. 하나의 참조 ID 이미지만을 감안할 때, 인판트ID는 높은 충실도를 보장하면서 단일 참조 ID 이미지에서 다양한 포즈 또는 스타일로 맞춤형 이미지를 생성하는 것을 목표로 한다. 그림 2는 우리의 방법에 대한 개요를 제공한다. 강력한 의미론적 얼굴 정보를 캡처하는 (1) An ID 임베딩, (2) 경량화된 교차 의도를 갖는 가벼운 적응 모듈, 시각적 프롬프트로서 이미지의 사용을 용이하게 하고, (3) 참조 얼굴 이미지로부터의 세부 특징을 추가 공간 제어로 인코딩하는 안아이덴티티넷의 세 가지 중요한 구성 요소를 통합한다.\n' +
      '\n' +
      '**ID 구현*****입니다. 시각적 신속한 추출을 위해 미리 훈련된 CLIP 이미지 인코더에 의존하는 IP-Ad캡터[24], 페이스스튜디오[23], 포토메이커[10]과 같은 이전 접근 방식과 달리, 우리의 작업은 더 강력한 의미론적 세부사항의 필요성과 ID 보존 작업의 충실도를 향상시킨다. CLIP의 고유한 한계는 약하게 정렬된 데이터에 대한 학습에 있으며, 이는 그것의 인코딩된 특징이 구성, 스타일 및 색상과 같은 광범위하고 모호한 의미 정보를 주로 캡처한다는 것을 의미한다. 이러한 특징은 텍스트 임베딩의 일반적인 보충제로 작용할 수 있지만, 더 강한 의미론과 더 높은 충실도가 가장 중요한 정확한 ID 보존이 필요한 작업에 짧다. 최근 몇 년 동안 얼굴 표현, 특히 얼굴 인식 및 ReID에서 광범위한 연구는 얼굴 인식 및 재구성과 같은 복잡한 작업에서 얼굴 재현의 효능을 입증했다. 그러나 확산 모델이 있는 이미지 생성에서 직접적인 적용은 제거되지 않았다. 우리의 연구에서 우리는 사전 훈련된 얼굴 모델을 레버리지하여 기준 얼굴 이미지에서 얼굴 ID 임베딩을 검출하고 추출하여 이미지 생성 과정을 안내하기 위해 강력한 정체성 특징을 제공한다. 이것은 우리가 중요한 질문으로 이어집니다: **어떻게 우리는 확산 모델에 정체성 특징을 효과적으로 주입합니까?**의 후속 섹션에서 이 질문을 탐색하고 답변한다.\n' +
      '\n' +
      '** 이미지 어댑터****입니다. 사전 훈련된 텍스트 대 이미지 확산 모델에서 이미지를 촉발하는 능력은 특히 텍스트로 설명하기 어려운 콘텐츠에 대한 텍스트 프롬프트를 크게 향상시킨다. 우리의 접근법에서 우리는 그림 2와 같이 이미지 프롬프트에 대한 IP-클라우드와 유사한 전략을 채택한다.\n' +
      '\n' +
      '그림 2: **에 제안된 InstantID의 전체 파이프라인이 있다. 우리의 모델은 높은 안면 충실도를 유지하기 위해 세 부분으로 구성되어 있습니다. 먼저, 우리는 CLIP 대신 얼굴 인코더를 채택하여 의미론적 얼굴 특징을 추출하고 훈련 가능한 프로젝션 레이어를 사용하여 텍스트 특징의 공간에 투사한다. 우리는 투사된 기능을 얼굴 임베딩으로 가져갑니다. 그런 다음, 빠르게 이미지를 지원하기 위해 디커플링된 교차 권한을 갖는 경량 적응 모듈을 도입한다. 마지막으로, 우리는 추가 약한 공간 제어와 함께 참조 안면 영상에서 복잡한 특징을 인코딩하기 위해 아이덴티티넷을 제안한다. 이덴티티넷에서는 텍스트 정보 없이 얼굴 임베딩에 의해 생성 과정이 완전히 안내된다. 새로 추가된 모듈만 업데이트되는 반면, 미리 학습된 텍스트 대 이미지 모델은 유연성을 보장하기 위해 냉동 상태로 남아 있다. 훈련 후, 사용자는 무료로 높은 충실도로 모든 스타일에 대한 아이디 보존 이미지를 생성할 수 있습니다.**\n' +
      '\n' +
      '편향점, 디커플링된 크로스 포지셔닝이 있는 경량 적응 모듈을 도입하여 이미지를 조속한 대로 지원한다. 그러나 거친 정렬된 CLIP 임베딩과 달리 이미지 프롬프트로 ID 임베딩을 사용하여 분기한다. 이 선택은 보다 정교하고 의미 있게 풍부한 신속한 통합을 달성하는 것을 목표로 한다.\n' +
      '\n' +
      '**식별넷***입니다. 이미지 프롬프트와 텍스트 프롬프트(예: IP-아터터)를 통합하는 방법의 가용성에도 불구하고, 우리는 이러한 기술이 거친 변경 개선만을 제공하고 이러한 수준의 통합이 ID 보존 이미지 생성에 충분하지 않다고 주장한다. 우리는 이러한 한계를 사전 훈련된 확산 모델의 고유한 훈련 메커니즘과 특성에 속한다. 예를 들어, 이미지 및 텍스트 프롬프트가 주의 레이어 이전에 연결되면, 모델은 확장된 토큰 시퀀스에 대한 미세-곡물 제어로 투쟁한다. 그러나 교차 의도에 텍스트와 이미지 토큰을 직접 추가하는 것은 텍스트 토큰에 의해 발휘되는 제어를 약화시키는 경향이 있다. 더욱이, 개선된 충실도를 위한 이미지 토큰의 강도를 높이기 위한 시도는 텍스트 토큰의 편집 능력을 무심코 손상시킬 수 있다. 이것은 대안적인 특징 임베딩 방법인 컨트롤넷을 채택하여 우리가 다루는 작업에서 중요한 도전을 제시한다. 이 방법은 전형적으로 공간 정보를 제어 가능한 모듈에 대한 입력으로 활용하고 확산 모델에서 UNet 설정과의 일관성을 유지하고 교차 의도 계층에서 조건부 요소로 텍스트를 포함한다.\n' +
      '\n' +
      '통제Net의 적응에는 주로 1) 미세화 오픈포즈 안면 키포인트의 설치, 조건부 입력을 위해 5개의 얼굴 키포인트(눈 2개, 코 1개, 입 2개)만을 사용한다. 우리는 텍스트 프롬프트를 제거하고 아이디 임베딩을 제어넷의 교차 의도 레이어의 조건으로 사용한다. 아래 동기에 대해 논의합니다.\n' +
      '\n' +
      '먼저 제어망에서 공간 제어가 필수적이지만 특히 얼굴 영역에 대해 민감하게 적용되어야 함을 인식합니다. 우리는 얼굴 랜드마크를 공간 제어 신호로 선택하지만 세부 키 포인트보다 일반화된 제약을 위해 5개의 키 포인트(눈 2개, 코 1개, 입 2개)로 제한했다. 이 접근법은 두 가지 목적을 제공한다. 인스턴트ID는 수동으로 크롭된 얼굴 이미지가 아닌 실제 인간 영상에서 훈련되며, 얼굴은 종종 현장의 작은 부분이기 때문에 정확한 키 포인트 검출이 어렵다. b) 편집성을 유지하기 위해 공간 제약이 미치는 영향을 줄이고 얼굴 모양이나 입 폐쇄와 같은 중복 안면 정보에 대한 과도한 강조를 방지하는 것을 목표로 한다. 그럼에도 불구하고, 우리는 공간적 제약 없이 인간의 얼굴의 자유도가 과도하게 커 만족할 만한 결과의 발생을 복잡하게 만들 수 있다는 것을 알게 된다. 따라서 거친 키 포인트의 맵은 균형 잡힌 솔루션을 제공합니다.\n' +
      '\n' +
      '둘째, 텍스트 프롬프트를 제거하고 ID 임베딩을 대신 교차의도 레이어의 조건으로 사용하는 이유는 이 접근법으로 네트워크가 ID 관련 표현에만 집중할 수 있게 하여 얼굴과 배경에 대한 일반화된 설명에 영향을 받지 않기 때문이다.\n' +
      '\n' +
      '수능과 위험 전략.\n' +
      '\n' +
      '훈련 동안, 우리는 미리 훈련된 확산 모델의 매개변수를 동결시키는 동안 이미지 아캡터 및 아이덴티티넷의 매개변수만 최적화한다. 우리는 인간의 주제를 특징으로 하는 이미지 텍스트 쌍에 전체 인판트ID 파이프라인을 훈련하여 원래 안정적인 확산 작업에 사용된 것과 유사한 훈련 목적을 사용한다.\n' +
      '\n' +
      '}_{z_{t}, C,C_{i} (0,1)\\epsilon\\mathcal{N} (z_{t},t,C,{i})\n' +
      '\n' +
      '여기서 \\(C_{i}\\)는 아이덴티티넷에 대한 과제별 이미지 조건이다. 훈련 과정에서 이덴티티넷의 텍스트 프롬프트 조건을 제거했기 때문에 랜덤하게 텍스트나 이미지 조건을 떨어뜨리지 않습니다.\n' +
      '\n' +
      '우리의 작업의 주요 특징은 이미지 프롬프트 어댑터 내의 텍스트 교차의도 및 이미지 교차의도 간의 분리와 이미지 프롬프트 어댑터와 아이덴티티넷의 디커플링이다. 이러한 설계 선택은 이러한 이미지 조건의 가중치를 독립적으로 유연하게 조정할 수 있도록 하여 보다 통제되고 표적화된 훈련 및 추론 과정을 보장한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '블 디퓨전으로 구현하여 다양성을 보장하기 위해 5천만 개의 이미지 텍스트 쌍으로 구성된 대규모 오픈 소스 데이터세트 LIAION-Face[27]에서 훈련합니다. 또한, 우리는 생성 품질을 더욱 향상시키기 위해 BLIP2[9]에서 자동으로 생성된 주석들로 인터넷으로부터 1000만 개의 고품질 인간 이미지를 수집합니다. 우리는 1인 이미지에 초점을 맞추고 인간 이미지에서 얼굴 ID 임베딩을 검출하고 추출하기 위해 미리 훈련된 얼굴 모델인 항엘리프프21을 사용한다. 우리는 크로싱된 얼굴 데이터 세트 대신 원래 인간 이미지를 훈련합니다. 훈련 중에 이미지캡터 및 아이덴티티넷의 파라미터만 업데이트되는 반면, 미리 학습된 텍스트 대 이미지 모델은 동결된 상태로 남아 있다. 우리의 실험은 SDXL-1.0 모델을 기반으로 하며 GPU당 배치 크기가 2인 48개의 NVIDIA H800 GPU(80GB)를 사용하여 수행된다.\n' +
      '\n' +
      '폐경 1: [국경://github.com/디페신라이트/비표면] (https://github.com/디페핀사이트/비단면)\n' +
      '\n' +
      '### Qualitative Results\n' +
      '\n' +
      '우리는 방법의 견고성, 신속한 편집성 및 호환성을 입증하기 위해 다양한 환경에서 질적 결과를 제시한다.\n' +
      '\n' +
      '**이미지는 온리***입니다. 우리는 완전히 기준 이미지에 의해 생성 과정을 안내하는 빈 프롬프트를 사용한다. 그림 3 \'빈 프롬프트\'에서 볼 수 있듯이 신속한 결과가 없는 우리의 결과는 표현, 나이, 정체성과 같은 풍부한 의미론적 얼굴 콘텐츠를 강력하게 유지하는 인판트ID의 능력을 보여준다. 그러나 성별과 같은 특정 의미론은 완벽하게 정확하지 않을 수 있다(제2열 참조).\n' +
      '\n' +
      '**이미지 + 프롭****입니다. 여러 신속한 설정(그림 3, 열 2-4)에서 우리는 정체성 일관성을 보장하고, 효과적으로 성별, 옷, 모발 색상을 변화시키면서 텍스트 제어 능력의 저하를 관찰하지 않는다.\n' +
      '\n' +
      '**이미지 + 프롭 + 공간 제어***입니다. 우리의 방법은 제어넷과 같은 사전 훈련된 공간 제어 모델과 호환됩니다. 그림 3, 열 5-9는 이러한 호환성을 보여, 사전 훈련된 제어넷(카니, 깊이) 모델을 사용하여 유연한 공간 제어를 도입할 수 있음을 보여준다.\n' +
      '\n' +
      '그림 4: ** 참조 이미지 수의 효과*** 다중 참조 이미지에 대해 이미지 프롬프트로 ID 임베딩의 평균 평균을 취한다. 인스턴트ID는 하나의 단일 기준 이미지만 있으면 좋은 결과를 얻을 수 있다.\n' +
      '\n' +
      '그림 3: ** 인판트ID의 견고성, 편집성 및 호환성의 입증*** 컬럼 1은 추론 중에 프롬프트가 비어 있는 이미지 결과를 보여준다. 2-4열은 텍스트 프롬프트를 통해 편집성을 보여준다. 5-9열은 기존 제어네츠와의 호환성(카니&깊이)을 보여준다.\n' +
      '\n' +
      '### Ablative Study\n' +
      '\n' +
      '추론 중 각 내부 모듈의 효과와 생성된 결과에 미치는 영향을 평가한다. 그림 9는 부록에서 이해넷만으로는 좋은 ID 보유를 달성하고 이미지캡터 추가는 얼굴 디테일 복구를 더욱 향상시킨다는 것을 보여준다.\n' +
      '\n' +
      '우리는 또한 그림 4의 생성 품질에 대한 참조 이미지의 수의 영향을 조사하는데, 참조 이미지는 일반적으로 생성 품질을 향상시키지만 단일 이미지에서도 우리의 방법은 놀라운 충실도를 달성한다. 훈련 기반 방법에서 동일인의 이미지 수는 일반적으로 세대 품질에 직접적인 영향을 미친다는 점에 유의한다. 예를 들어, LoRAs 훈련은 일반적으로 수십 개의 이미지를 필요로 하며, 이미지 유사도가 너무 높으면 과적합으로 인해 문제가 될 것이다. 그러나 우리의 경우 모든 참조 이미지의 평균 임베딩을 사용하고, 이는 더 나아가 생성 품질을 향상시키는 데 도움이 된다.\n' +
      '\n' +
      '접수.\n' +
      '\n' +
      'IP 어댑터의****. 방법의 효과를 입증하기 위해 개인 맞춤형 생성에 대한 기존 최첨단 방법과 하나의 단일 참조 이미지를 비교한다. 구체적으로, 미리 학습된 IP-Ad캡터, IP-Ad캡터-FaceID 및 IP-Ad캡터-FaceID-Plus의 결과를 InstantID와 비교한다. 그림 5로부터 두 가지 실질적인 관찰이 있으며, 1) ID 임베딩은 얼굴 보존 과제에 기본이다. 이미지들의 거친 표현만을 제공하는 IPA에 사용되는 CLIP 임베딩과 달리 ID 임베딩은 정체성, 연령, 성별과 같은 풍부한 의미 정보를 본질적으로 전달한다. 이 고수준의 의미 콘텐츠는 정확하고 상세한 얼굴 보존에 중요합니다. 2) IPA-FaceID 및 IPA-FaceID-Plus에서 수행된 바와 같이 교차 의도 수준에서만 ID 임베딩을 도입하면 확실히 얼굴 충실도가 향상된다. 그러나 이 접근법은 텍스트 제어에 무심코 영향을 미칠 수 있다. 주목할 만한 예는 얼굴 영역을 배경 스타일로 원활하게 통합할 수 없다는 것이다. 이러한 한계는 얼굴 충실도와 텍스트 제어 사이의 상충 관계를 강조한다. 대조적으로, 우리의 인판트ID 방법은 다른 참조 이미지의 강력한 처리를 보여주며 결과에서의 더 높은 충실도를 달성했다. 더 중요한 것은 ID의 무결성을 효과적으로 유지하여 다양한 스타일로 원활하게 블렌딩하는 것이다. 이 비교는 양식적 유연성과 제어를 유지하면서 정체성 보존에 있어 인판테니의 우월성을 강조한다.\n' +
      '\n' +
      'LoRAs****의 경우 InstantID의 우수성을 추가로 입증하기 위해 다중 참조 이미지를 사용하여 훈련된 LoRA 모델과 비교 분석을 수행했다. 이를 위해 시비타이로부터 재키찬2 및 에마 왓슨3과 유사한 것과 같은 미리 훈련된 여러 캐릭터 LoRA 모델을 선택했다. 비교 결과 유쾌한 놀라운 결과를 얻었으며, 인판트ID는 테스트 시간 미세 조정의 필요 없이 단일 이미지만을 사용하여 경쟁 결과를 달성할 수 있다. 이 발견은 그림 5: **와 다른 문자와 스타일로 조건화된 다른 방법을 비교할 때 중요하다.** 왼쪽에서 왼쪽에서 IP-Ad캡터-SDXL, IP-Ad캡터-SDXL-FaceID(*)는 실험 버전, IP-Ad캡터-SD1.5-FaceID, IP-Ad캡터-SD1.5-FaceID-플러스이다. 그림에서 보는 바와 같이 CLIP 임베딩에 의존하는 IP-Ad캡터가 안면 충실도를 달성할 수 없고, 또한 스타일을 생성하기 위해 신속한 제어의 저하로 이어진다는 것을 발견했다. IP-Ad캡터-FaceID는 얼굴 임베딩을 도입하여 얼굴 충실도를 향상시키지만 여전히 높은 충실도를 달성할 수 없다. IP-Ad캡터-FaceID-Plus는 얼굴의 임베딩과 좋은 얼굴 충실도를 달성할 수 있는 CLIP를 결합하지만 스타일 분해 문제가 있어 얼굴이 배경 스타일로 혼용되지 못하고 있다. 대조적으로, 제안된 인판트ID는 높은 충실도를 유지하면서 다양한 스타일과 호환됩니다.\n' +
      '\n' +
      '그림 6: ** 인판트ID와 사전 훈련된 캐릭터 LoRAs.**의 비교는 아무런 훈련 없이 LoRAs로서 경쟁적인 결과를 얻을 수 있다.\n' +
      '\n' +
      'LoRA 모델과 함께. LoRA 모델은 한편으로는 데이터 품질과 수량에 대한 구체적인 요구 사항을 가지고 있어 실제 시나리오에서 적용 가능성을 제한할 수 있다. 반면, 맞춤형 모델의 추가 훈련이 필요합니다. 대조적으로, 인판트ID는 단일 이미지로 효율적인 단일 단계 추론을 가능하게 하여 다양한 다운스트림 응용 분야에 큰 도움이 될 수 있는 놀라운 효율성을 보여준다. 이러한 효율성은 광범위한 훈련 또는 다중 참조 이미지의 필요 부족과 함께 이미지 생성 영역에서 매우 실용적이고 다재다능한 도구로서의 위치 인판트ID를 포함한다.\n' +
      '\n' +
      '또한, 우리는 ROOP5 및 Re 배우6과 같은 커뮤니티 프로젝트에 통합되는 것으로 알려져 있는 비확산 모델 구조를 가진 널리 사용되는 블랙박스 페이스 캡핑 모델인 인사이트스페이스 스위퍼4와 비교했는데, 이는 그림 7에 묘사된 바와 같이 인사이트스페이스 스위퍼가 대부분의 표준 시나리오에서 효과적이지만 인사이트페이스 스위퍼가 배경, 특히 비현실적인 스타일과 같은 블렌딩 얼굴에서 더 큰 유연성을 나타냄을 보여준다. 이러한 적응성은 다양한 다양한 예술적 표현을 처리하는 데 있어 인판트ID의 진보된 능력을 보여준다.\n' +
      '\n' +
      '폐경 4: [https://github.com/haofanwang/inwapper] (https://github.com/haofanwang/inswapper)\n' +
      '\n' +
      '폐경 5: [https://github.com/s0md3v/roop] (https://github.com/s0md3v/roop)\n' +
      '\n' +
      '부츠 6: [https://github.com/Gouff/sourieff/Gourieff/sd-webui-reactor] (https://github.com/Gourieff/sd-webui-reactor)\n' +
      '\n' +
      '### Applications\n' +
      '\n' +
      '또한, 인판트ID의 저가 플러그 앤 플레이 캐릭터 ID 보존은 여러 개의 다운스트림 애플리케이션으로 문을 연다. 아이디어를 촉발하고 잠재력을 보여주기 위해 몇 가지 흥미로운 사용 사례를 탐구한다. 여기에는 새로운 뷰 합성이 포함되며, 여기서 인판트ID는 문자 일관성을 유지하면서 새로운 관점과 각도의 생성을 가능하게 한다. 우리는 또한 다양한 캐릭터들 간의 정체성 보간으로 묘사되어 여러 정체성으로부터 특징을 원활하게 혼합할 수 있는 인판트ID의 능력을 보여준다. 또한, 우리는 인판트ID가 여러 문자와 관련된 복잡한 시나리오를 관리할 수 있는 방법을 보여주는 다중 동일성 합성을 강조한다. 이 애플리케이션들 각각은 이 애플리케이션들 각각 에스토스토어 In.\n' +
      '\n' +
      '그림 7: ** 인판트ID와 인사이트스페이스 스위퍼**를 비교하면 비현실적인 스타일에서는 얼굴과 배경의 통합에 더 유연하다.\n' +
      '\n' +
      '다양한 창의·실천적 맥락에서 경쟁력 있는ID의 다재다능성과 효과성이 있다. 결과는 부록(그림 10, 그림 11 및 그림 12 참조)에 각각 있다.\n' +
      '\n' +
      '5축제와 미래일.\n' +
      '\n' +
      '본 논문에서는 간단한 플러그 앤 플레이 모듈로 제로샷 아이덴티티 보존 세대를 위한 해결책으로 인판트ID를 제안하여, 높은 충실도를 유지하면서 하나의 얼굴 이미지만을 사용하여 임의의 스타일로 이미지 개인화를 부가적으로 처리할 수 있도록 한다. 저희 인판트ID에는 두 가지 핵심 디자인이 있습니다. 얼굴 디테일 충실도를 높이는 이미지 어댑터와 복잡한 얼굴 기능을 보존하기 위해 강력한 ID 제어를 보장하는 아이덴티티넷입니다. 우리의 실험 결과는 원본 모델의 텍스트 편집 능력을 보존하는 능력과 함께 인판트ID의 견고성과 호환성을 긍정한다. 이 제품의 플레이블 특성은 LoRAs 및 제어넷과 같은 다른 모델과의 원활한 통합을 더 가능하게 하여 추가 비용을 일으키지 않고 적용 가능성을 넓힌다. 인스탄트ID의 탁월한 성능과 효율성은 정체성 보존이 가장 중요한 다양한 현실 세계 시나리오에서 응용의 길을 열어준다. 그러나, 인판트ID의 효과에도 불구하고, 특정 도전은 여전히 해결되어야 한다. 우리의 모델에 ID 임베딩은 성별, 연령과 같은 의미 정보가 풍부한 반면 얼굴 속성은 고도로 결합되어 얼굴 편집에 도전한다. 향후 발전은 유연성을 향상시키기 위해 이러한 얼굴 속성 특징을 해독하는 것을 포함할 수 있다. 더욱이, 우리는 우리가 사용한 얼굴 모델에 내재된 편향과 관련된 인판트ID의 몇 가지 한계를 관찰할 수 있다. 나아가 우리 모형에서 인간 얼굴의 유지, 특히 공세나 문화적 부적절한 상상력의 잠재적 생성을 둘러싼 윤리적 고려 사항은 향후 작업에서 신중한 고려를 보증하는 중요한 관심사이다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro, B., et al.: ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324 (2022)\n' +
      '* [2] Chen, X., Huang, L., Liu, Y., Shen, Y., Zhao, D., Zhao, H.: Anydoor: Zero-shot object-level image customization. arXiv preprint arXiv:2307.09481 (2023)\n' +
      '* [3] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net (2021), [https://openreview.net/forum?id=YicbFdNTTy](https://openreview.net/forum?id=YicbFdNTTy)\n' +
      '* [4] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G., Cohen-Or, D.: An image is worth one word: Personalizing text-to-image generation using textual inversion (2022). [https://doi.org/10.48550/ARXIV.2208.01618](https://doi.org/10.48550/ARXIV.2208.01618), [https://arxiv.org/abs/2208.01618](https://arxiv.org/abs/2208.01618)\n' +
      '* [5] Gal, R., Arar, M., Atzmon, Y., Bermano, A.H., Chechik, G., Cohen-Or, D.: Designing an encoder for fast personalization of text-to-image models. arXiv preprint arXiv:2302.12228 (2023)\n' +
      '* [6] Hu, E.J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al.: Lora: Low-rank adaptation of large language models. In: ICLR (2021)\n' +
      '* [7] Huang, L., Chen, D., Liu, Y., Shen, Y., Zhao, D., Zhou, J.: Composer: Creative and controllable image synthesis with composable conditions. In: International Conference on Machine Learning (2023), [https://api.semanticscholar.org/CorpusID:257038979](https://api.semanticscholar.org/CorpusID:257038979)\n' +
      '* [8] Kingma, D.P., Welling, M.: Auto-encoding variational bayes. In: Bengio, Y., LeCun, Y. (eds.) 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings (2014), [http://arxiv.org/abs/1312.6114](http://arxiv.org/abs/1312.6114)\n' +
      '* [9] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023)\n' +
      '* [10] Li, Z., Cao, M., Wang, X., Qi, Z., Cheng, M.M., Shan, Y.: Photomaker: Customizing realistic human photos via stacked id embedding (2023)\n' +
      '* [11] Mou, C., Wang, X., Xie, L., Zhang, J., Qi, Z., Shan, Y., Qie, X.: T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453 (2023)\n' +
      '* [12] Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., Chen, M.: Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In: International Conference on Machine Learning (2021), [https://api.semanticscholar.org/CorpusID:245335086](https://api.semanticscholar.org/CorpusID:245335086)\n' +
      '* [13] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image synthesis (2023)\n' +
      '* [14] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)* [15] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 **1**(2), 3 (2022)\n' +
      '* [16] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [17] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. pp. 234-241. Springer (2015)\n' +
      '* [18] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2023)\n' +
      '* [19] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems **35**, 36479-36494 (2022)\n' +
      '* [20] Valevski, D., Wasserman, D., Matias, Y., Leviathan, Y.: Face0: Instantaneously conditioning a text-to-image model on a face (2023)\n' +
      '* [21] Wang, T., Li, L., Lin, K., Lin, C.C., Yang, Z., Zhang, H., Liu, Z., Wang, L.: Disco: Disentangled control for referring human dance generation in real world. arXiv preprint arXiv:2307.00040 (2023)\n' +
      '* [22] Wei, Y., Zhang, Y., Ji, Z., Bai, J., Zhang, L., Zuo, W.: Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848 (2023)\n' +
      '* [23] Yan, Y., Zhang, C., Wang, R., Zhou, Y., Zhang, G., Cheng, P., Yu, G., Fu, B.: Facestudio: Put your face everywhere in seconds (2023)\n' +
      '* [24] Ye, H., Zhang, J., Liu, S., Han, X., Yang, W.: Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721 (2023)\n' +
      '* [25] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3836-3847 (2023)\n' +
      '* [26] Zhao, S., Chen, D., Chen, Y.C., Bao, J., Hao, S., Yuan, L., Wong, K.Y.K.: Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems (2023)\n' +
      '* [27] Zheng, Y., Yang, H., Zhang, T., Bao, J., Chen, D., Huang, Y., Yuan, L., Chen, D., Zeng, M., Wen, F.: General facial representation learning in a visual-linguistic manner. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18697-18709 (2022)\n' +
      '* [28] Zhou, Y., Zhang, R., Sun, T., Xu, J.: Enhancing detail preservation for customized text-to-image generation: A regularization-free approach. arXiv preprint arXiv:2305.13579 (2023)\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n' +
      '그림 11: 서로 다른 인물들 간의 식별 보간이다.\n' +
      '\n' +
      '그림 12: 지역 통제와 함께 다중 동일성 합성을 보여준다.\n' +
      '\n' +
      '그림 10: 주어진 포즈 아래 노벨뷰 합성입니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>