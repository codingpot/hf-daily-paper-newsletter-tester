<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '언어 모델링을 위한 다차원적인 로컬-SGD 훈련.\n' +
      '\n' +
      'Bo Liu\\({}^{*}\\)\n' +
      '\n' +
      '1\n' +
      '\n' +
      ' 라키타 헌카리아\\({}^{2}\\)\n' +
      '\n' +
      ' Arthur Douillard\\({}^{2}\\)\n' +
      '\n' +
      ' 사티엔 케일\\({}^{3}\\)\n' +
      '\n' +
      ' 안드로이 A. 러슈\\({}^{2}\\)\n' +
      '\n' +
      ' 자준선성({}^{2}\\)\n' +
      '\n' +
      ' 아더 스즈마티({}^{2}\\)\n' +
      '\n' +
      'and Marc\'Aurelio Ranzato\\({}^{2}\\)\n' +
      '\n' +
      '1Work done as an intern at Google DeepMind, 1The University of Texas at Austin, 2Google DeepMind, 3Google Research\n' +
      '\n' +
      '부츠 1: 때때로 페데레이티드 평균(FedAvg)이라고도 하는 로컬-SGD 용어는 여기에서 사용자가 다른 작업자에게 데이터 할당에 대한 통제를 갖는 분산 최적화에서 뿌리를 강조하는 데 사용된다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '각 디바이스가 통신당 하나 이상의 SGD 업데이트를 수행하는 분산 최적화에 대한 접근 방식인 피지 평균화라고도 하는 국소 확률적 구배 하강(지방-SGD)도 있다. 이 작업은 언어 모델을 훈련하기 위한 _asynchronous_ 로컬-SGD에 대한 실증 연구를 제시하고, 즉 각 작업자는 SGD 단계를 마치자마자 글로벌 파라미터를 업데이트한다. 작업자 하드웨어 이질성, 모델 크기, 근로자 수, 최적화자가 학습 성과에 어떤 영향을 미칠 수 있는지 조사하여 포괄적인 조사를 실시한다. 우리는 순진한 구현과 함께 비동기적 로컬-SGD가 (글로벌) 모델 파라미터를 더 자주 업데이트했음에도 불구하고 동기적 대응물보다 수렴하기 위해 더 많은 반복을 취한다는 것을 발견했다. 작업자 구배가 핵심 도전으로 엉망일 때 전 세계 매개변수에 대한 운동량 가속도를 식별한다. 지연된 네스테로프 운동량 업데이트를 활용하여 작업자의 지역 훈련 단계를 연산 속도에 따라 조정하는 새로운 방법을 제안한다. C4 데이터세트 상의 최대 150M 파라미터로 평가된 이 접근법은 업데이트 단계당 엄격성 측면에서 동기 로컬-SGD의 성능과 일치하며 벽 시계 시간 측면에서 크게 능가한다.\n' +
      '\n' +
      '비동기식 훈련, 언어 모델링, 대규모 분산 학습 2024\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 언어 모델(LLM)은 많은 응용 분야에 혁명을 일으켜 기계가 인간 언어와 상호 작용하는 방식을 변화시켰다. 이 혁명의 초석은 이러한 모델을 대규모 규모로 훈련하고 있다. 이러한 대규모 교육을 합리적인 시간으로 관리하기 위해서는 여러 장치에 걸쳐 계산을 배포할 필요가 있었다. 그러나 이 분산 학습에 대한 표준 접근법은 빠른 상호 연결과 함께 공동 위치된 장치를 사용한다.\n' +
      '\n' +
      '보다 강력한 대형 모델을 구축하기 위해 지리적으로 서로 멀리 떨어진 광범위한 범위의 계산 자원을 효과적으로 활용할 수 있기를 희망할 수 있다. 그러나 수많은 먼 기기를 사용하는 것은 통신 지연의 중요한 장애물에 직면한다. 디바이스들이 중앙 서버로 되돌리기 전에 컴퓨팅 구배에만 초점을 맞출 때, 통신 시간은 계산 시간을 과대평가하여 효율성의 병목 현상을 생성할 수 있다.\n' +
      '\n' +
      'Local Stochastic Gradient Descent (Local-SGD) is a collection of optimization methods that can reduce communication bottlenecks.1 These methods involve each device performing multiple local gradient steps before syncing their parameter updates with a parameter server. While Local-SGD enhances training efficiency by reducing communication frequency, it can suffer from the _straggler_\n' +
      '\n' +
      'Figure 1: Illustration of async. v.s. sync. training with 2 workers (in blue and red). Sync. training suffers from the straggler effect, while async. training reduces the idling time of the fast worker.\n' +
      '\n' +
      '이종 기기로 인한 효과_ 예를 들어, 더 빠른 장치는 더 느린 장치를 따라잡기 위해 유휴하게 기다리고 있어 시스템의 전반적인 효율성을 약화시킨다. 더욱이, 모든 디바이스들은 파라미터 서버와의 높은 대역폭 연결을 필요로 하는 동시에 통신하도록 강제된다. 동기형 로컬-SGD는 작업자의 업데이트가 가능한 즉시 서버에서 모델을 업데이트할 수 있게 하여 계산 활용도를 높이고 통신 대역폭 요구사항을 최소화할 수 있어 보다 실행 가능한 솔루션(그림 1에서 하향 조정)을 제시한다.\n' +
      '\n' +
      'In this study, we explore the viability of asynchronously training LLMs using Local-SGD. We expand upon previous works that have attempted to alternate steps on subsets of workers or randomly drop certain subset of workers during synchronous Local-SGD (Douillard et al., 2023; Ryabinin et al., 2021). The main content is structured into three parts:\n' +
      '\n' +
      '우리는 이 연구에서 템프 테드토카 l-SGD(D ouilla rdetal,2023; Ryexplo 리디동기 LLMg LLMimim LLSD)에 대한 동기 추리 동안 테드토나 라터스 아 및 oml 수퍼 파이어세터를 먹는 ks를 재조명한다.\n' +
      '\n' +
      '2. 최적화 챌린지(섹션 4)에서 비동기적 지방-SGD에 적합한 다양한 기존 최적화 전략에 대한 실증 연구를 수행한다. 여기에는 작업자 측 최적화(제너 최적화)와 서버 측 최적화(제터 최적화)가 모두 포함된다. 운동량을 효과적으로 활용하는 데 있어 핵심 과제를 발견합니다. 특히 적응 운동량 방법은 일반적으로 내부 및 외부 최적화의 융합을 가속화하는 반면, 비동기 로컬-SGD의 효능은 특히 동기 구현과 비교할 때 운동량 기술을 모두 사용할 때 상대적으로 감소한다.\n' +
      '\n' +
      '3개의 제안 솔루션(섹션 5)은 딜레이 네스테로프 모멘텀 업데이트(DN)와 다이나믹 로컬 업디테스(DyLU)의 단순하고 효과적인 두 가지 기술을 소개한다. 이러한 기술은 조합되고 훈련하는 란을 조합하고 평가할 때 이러한 기술을 사용한다.\n' +
      '\n' +
      '그림 2: 동기화를 이용한 언어모델 비교평가. 비누와 비네크. 20M 매개변수 모델에서 4명의 이질적인 작업자를 가진 지역-SGD 방법. 최첨단 동기입니다. 지역-SGD 방법인 디LoCo(Douillard et al, 2023)는 각각 아담W와 네스테로프 모멘텀을 근로자측 및 서버측 최적화기로 사용한다. 이 최적화기 조합은 아스네크의 가장 강력한 것으로 남아 있다. 지역-SGD 훈련(그림 5 참조)은 그러나 저성능 디LoCo는 상당히 낮다. 외부 최적화를 위한 딜레이 네스테로프(DN)(알고리즘 3)와 동적 로컬 업도트(DyLU)(섹션 5)를 통합함으로써, 우리는 동기화 간의 업데이트에 대한 엄격성 측면에서 성능 격차를 크게 가교한다. 비누와 비네크. 언어 모델링에 대한 훈련. 더욱이, 제안된 방법은 벽 시계 시간 대 비장 측면에서 디LoCo를 크게 능가한다.\n' +
      '\n' +
      ' 기저 모델은 비동기적인 로컬-SGD가 지역 업데이트의 총 수에 대한 경막성 측면에서 동기적 로컬-SGD에 접근하고 비동기적 로컬-SGD 대 비동기적 로컬-SGD를 더욱 향상시킬 수 있도록 한다. 그림 2에 자세히 설명된 바와 같이 경막 대 벽시계 측면에서 동기적 지방-SGD가 있다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '본 연구에서는 \\(k\\) 데이터 음영 전반에 걸쳐 공유 모델 매개변수 \\(\\theta\\)의 분산 최적화에 중점을 두고 \\(\\mathcal{D}=\\{\\mathcal{D}_{1},\\ldots,\\mathcal{D}_{k}\\) 작업자를 사용하여\\(k\\) 작업자를 사용하여 1차 목표를 설명한다.\n' +
      '\n' +
      '부츠 2: 근로자 수(\\(k\\))는 데이터 샤드 수와 동일하지만, 우리의 방법은 데이터 샤드보다 작업자가 적을 때도 적용 가능하다.\n' +
      '\n' +
      '}.{j}{mathcal{D}}{mathb{D}}\\{{{}\\]\n' +
      '\n' +
      'Hi(\\ell(\\cdot;\\theta)\\가 손실 함수(예를 들어 언어 모델링에서 다음 토큰 예측에 대한 교차 엔트로피 손실)를 나타내고, \\(|\\cdot|\\)는 설정된 크기를 나타낸다.\n' +
      '\n' +
      '이 작업에서 로컬-SGD의 정의를 확장하여 원래 로컬-SGD 방법뿐만 아니라 고급 최적화 기술을 통합하는 변이체를 포함한다. 특히 언어 모델링에서 동기 로컬-SGD의 기준을 설정하는 디LoCo(Douillard et al., 2023)에 초점을 맞추고 있다. 알고리즘 1에서 Each 근로자 \\(i\\)는 데이터 샤드 \\(\\mathcal{D}_{i}_{i}\\)에 대한 _inner 최적화기_를 사용하여 로컬 업데이트를 수행하여 파라미터 변경(슈도- 업그레이팅) \\(슈도델타_{i}^{(t)}=\\theta_{i}^{(t)}^{(t)를 서버에 다시 전송하기 전에 데이터 샤드 \\(\\mathcal{D}_{i}_\\)에 대한 inner 최적화기_inner 최적기_\\)를 사용하여 로컬 업데이트를 수행한다. 그런 다음 서버는 응집된 외부 구배 \\(\\Delta^{{(t)}=\\frac{1}{k}\\sum_{i =1}^{k}\\delta_{i}^{{(t)}\\\\)를 계산하고 \\(\\Delta^{{(t)}\\\\)를 사용하여 _outer 최적화기__outer 최적화기를 적용하여 \\(\\Delta^{{(t)를 갱신한다. 디LoCo의 주요 통찰력은 각각 최고의 내부 및 외부 최적제로 AdamW 및 Nrierov 모텐톤의 최적 사용이다.\n' +
      '\n' +
      '## 3 Async. Local-SGD Framework\n' +
      '\n' +
      '이 섹션에서는 중심 서버가 모든 작업자를 제어하고 업데이트를 비동기적으로 응집시키는 비동기 로컬-SGD 파이프라인 설계를 설명한다.\n' +
      '\n' +
      '데이터 샤드 샘플링은 각 디바이스가 자신의 데이터에 부착된 피딩된 학습 설정과 달리 분산 최적화에서 사용자는 어떤 데이터가 어떤 작업자가 동적으로 어떤 작업자에게 할당되는지 선택할 수 있는 권리를 가진다. 작업자가 새로운 로컬 최적화 라운드를 시작할 준비가 되었을 때 다른 데이터 샤드(근로자는 이질적)에 대한 학습 진행의 균형을 맞추기 위해 "학습 진행"에 반비례하는 데이터를 샘플링한다. 구체적으로 \\(n_{i}\\)를 \\(\\mathcal{D}_{i}\\)에서 학습된 데이터 포인트의 수로 정의하는 다음, 샤드 \\(i_{\\text{ 샘플링}}\\)를 샘플링한다.\n' +
      '\n' +
      '}\\fegin{splid}}}\\\\ &\\text{split}}}\\\\ &\\text{i}\\\\ &\\text{i}\\frac{|}\\frac{|\\mathcal{D}_{i}}}{frac{mathcal{D}_{j}_{j}},0.\n' +
      '\n' +
      '즉, 우리는 데이터 샤드(즉, \\(\\frac{n_{i}}{\\sum_{j}}{\\sum_{j}}\\leq\\frac{mathcal{D}_{i}}{\\q\\frac{D}_{\\sum_{j}{\\mathcal{D}_{\\mathcal{D}_{\\mathcal{D}_{j}{\\mathcal{D}_{j}{\\mathcal{D}_{j}{\\mathcal{D}{j}{j}{\\mathcal{D}{j}{j}{\\mathcal{D}{j}{j}{j}{\\mathcal{D}_{j}{j}{j}{j}_{j}_{j}|}_{j}|}_{j}|}_{j}|}_{j}|}_{j}|}_{j}|}_{j}|}_{j}|}_{j}| 샤드가 과소 샘플링된 정도는 샘플링 속도를 결정한다. 그렇게 함으로써, 더 느린 경과를 가진 데이터 샤드가 훈련을 위해 샘플링될 가능성이 더 높기 때문에 화소에 걸친 학습 경과를 균형을 맞출 수 있다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      '```\n' +
      '0: 경구 전치 모델 \\(\\ta^{(0)}\\)\n' +
      '0:\\(k\\) workers\n' +
      '0:Grace period \\(\\tau_{\\text{grace}}\\)\n' +
      '0:Total local updates \\(t_{\\text{max}}\\)\n' +
      '1:\\(t_{\\text{local}}=0\\)\n' +
      '2:\\(\\theta\\leftarrow\\theta^{(0)}\\)\n' +
      '3:\\(\\mathcal{W}\\) = [init_worker() for \\(i\\) in [\\(k\\)]\n' +
      '4\\(성심증{}_{\\text{ 보완}}\\)=[[]]\n' +
      '5:\\(\\text{train}(\\mathcal{W},\\ \\theta)\\)\n' +
      '6:\\(\\tau_{\\text{sync}=\\riangleright\\) \\(\\tau_{\\text{sync})의 시작.\n' +
      '7:while\\(t_{\\text{local}}<t_{\\text{max}}\\)do\n' +
      '8:\\(w\\) = get_worker(\\(\\mathcal{W},\\tau_{\\text{grace}},\\tau_{\\text{sync}}\\))\n' +
      '종업원 9:9:\\(첼트리랑플러라이트\\)를 받아 완성근로자를 얻게 된다.\n' +
      '그런 다음 10:if\\(w\\)가 존재합니다.\n' +
      '서버와 업데이트를 동기화한다.\n' +
      '12:\\(\\tau_{\\text{sync}}=\\min(\\tau_{\\text{sync}},w.\\text{completed\\_time})\\)\n' +
      '13:\\(\\theta\\leftarrow\\text{sync}(\\theta,\\ w.\\text{update})\\)\n' +
      '14:\\(\\mathcal{W}_{\\text{completed}}.\\text{add}(w)\\)\n' +
      '15:\\(t_{\\text{ 로컬}}\\) += \\(w.\\text{ 로컬\\_updates}\\)).\n' +
      '16:else\n' +
      '완성된 근로자를 위한 일자리를 보유하고 있다.\n' +
      '18:\\(\\tau_{\\text{sync}}=\\infty\\)\n' +
      '19:\\(\\text{train}(\\mathcal{W}_{\\text{completed}},\\ \\theta)\\)\n' +
      '20:\\(성실{W}_{\\text{{}}\\) = ["\n' +
      '21:endif\n' +
      '22:endwhile\n' +
      '```\n' +
      '\n' +
      '**알고리즘 2** Async. 지역-SGD 태스크 세글링.\n' +
      '\n' +
      '4 Optimization 챌린지 4 Optimization 챌린지.\n' +
      '\n' +
      'Effect of InnerOpt + OuterOptTo study how optimization affects the language modeling performance in asynchronous Local-SGD, we first experiment with different combinations of the inner and outer optimizers (we use A+B to denote A and B as the inner and outer optimizer, respectively): SGD+Nesterov, SGD+Adam, AdamW+SGD, AdamW+SGD Momentum, AdamW+Adam, AdamW+Nesterov. The hyperparameters for each combination are tuned separately, for AdamW as InnerOpt we kept the default values. We assume there are \\(k=4\\) workers, whose device speed is shown in Figure 4. Then we apply asynchronous Local-SGD finetuning on a 20M-parameter language model for 64,000 steps per worker (256,000 local steps in total), where the initial model checkpoint was pretrained for 24,000 steps with Adam without distributed training. We choose finetuning with Local-SGD as it has been observed that Local-SGD style methods work well in finetuning but is less efficient from scratch (Lin et al., 2018), though others have also observed that Local-SGD works well even for training from scratch (Douillard et al., 2023). The learning rate scheduling and task scheduling follow the procedures described in Section 3. We use inner steps \\(=50\\) across all workers in all experiments by default. The result is shown in Figure 5.\n' +
      '\n' +
      'ObservationThe analysis reveals that combining AdamW as the inner optimizer with Nesterov momentum as the outer optimizer yields the best results, aligning with findings from synchronous training, like the DiLoCo method. Notably, using AdamW as the outer optimizer is less effective. This may be because AdamW, derived from Adam, introduces a normalization effect, which can be counterproduc\n' +
      '\n' +
      '그림 4: 각 장치에 대해 초당 스톤입니다.\n' +
      '\n' +
      '그림 5: 4명의 작업자와 함께 20M 언어 모델에서 비동기 로컬-SGD 교육을 위한 내부 및 외부 최적화기의 다양한 조합을 사용하는 성능이다.\n' +
      '\n' +
      '의사 등급은 실제 구배보다 더 크며 잠재적으로 수렴이 둔화되는 경향이 있다. 내부 최적화에 AdamW를 사용할 경우 SGD, SGD 모텐텀 및 네스테로프가 비슷한 성능을 보인다. 그러나 네스테로프는 학습 곡선을 안정화시킬 뿐만 아니라 최종 성능도 약간 향상시킵니다. 이는 업데이트 메커니즘(표기를 악용하고 \\(t\\)이 \\(t_{\\text{server}}\\)임을 나타내는 경우)에 기인할 수 있다.\n' +
      '\n' +
      '<타_{t <\\\\>} (}\\beta^{2} {t})\\{t} (1+\\beta)\\.\n' +
      '\n' +
      '아이티(\\epsilon\\)가 학습률인 경우 \\(m_{t}\\)는 운동량, \\(g_{t}\\)는 시간 \\(t\\), \\(\\beta\\in(0,1)의 구배이다. 네스테로프와 SGD 모멘텀의 주요 차이점은 네스테로프가 가중치를 조정하여 \\(\\beta\\)) 대신 운동량 성분(\\(\\(\\beta^{2}\\)을 감소시키고 \\(1\\) 대신 구배 성분(\\(1+\\beta\\)을 증가시키는 방법에 있다. 이것은 운동량이 지방-SGD에서 중요하면서도 복잡한 역할을 한다는 것을 시사한다.\n' +
      '\n' +
      '외측 최적기에 대한 운동량항의 모멘텀은 외부 최적기에 대한 모멘텀항의 영향에 더 깊이 정착하기 위해 동기 및 비동기 훈련 환경 모두에서 AdamW+SGD와 AdamW+Nrierov 사이의 비교 분석을 수행했다. 이러한 실험은 앞서 설명한 것과 동일한 조건에서 수행되었다. 결과는 그림 6에 보고되어 있다.\n' +
      '\n' +
      '그림은 비동기적인 로컬-SGD에서 운동량 용어가 없는 AdamW+SGD가 동기적 대응물보다 최종 엄격성과 학습 효율을 향상시킨다는 것을 분명히 보여준다. 그러나, 네스테로프 운동량을 아웃레이트에 통합하면 동기 로컬-SGD의 성능이 크게 향상되어 비동기 버전을 능가한다. 비동기 아담W+네스테로프는 테스트된 모든 내부 및 외부 최적화기 조합(그림 5에서 볼 수 있는 것)에 걸쳐 최고의 연주자로 남아 있다는 점은 주목할 만하다. 이 관찰은 운동량이 언어 모델링을 위한 비동기 로컬-SGD에 유익하지만 동기 설정에서 그 효과가 더 뚜렷하다는 것을 나타낸다.\n' +
      '\n' +
      'Is Staleness the Cause?We further apply the asynchronous DiLoCo algorithm with homogeneous devices. By doing so, we maximally diminish the staled gradient problem in Local-SGD, which refers to the fact that we are using an outdated outer gradient to update the server model. In particular, this means if we have \\(k\\) workers, all of them will return the computed outer gradient back to the server at the same time. Therefore, the only staleness comes from the fact that we are sequentially applying the individual updates instead of aggregating them together and apply it once. Results are summarized in Figure 7.\n' +
      '\n' +
      '이 수그 리슈날렌 ss_Ein ntstalene de raimi 네온스탈레이즈 리드스틸리셉트, le63477 75267563742kknatar, ak=4\\) 작업자 프로비디프 아디엔 ts(덴토테 다스칼라)를 발견한다. 시알아피실라 트리오포프테리아 의사 o-구배:스탄 다르 디네스테로 vmomentu mupdatei 시그날리 네오 디식스 식종 n(4)을 격리한다.\n' +
      '\n' +
      'r\\_{t+}(}4+\\beta^{2} <\\beta^{2} <\\beta^{2} <\\beta^{2} <\\beta^{2}> <\\beta^{2} <\\beta{2}> <\\beta{2} <\\beta{2>} <\\beta^{2>} <\\beta^{2>} <\\beta{2>} <\\beta^{2 <\\beta{2>} <\\beta^{2>} <\\beta^{2>} <\\beta{2>} <\\beta^{2 <\\beta{2>} <\\beta^{2>} <\\beta{2>} <\\beta{2 <\\beta{2>} <\\beta{2 <\\beta{2>} <\\beta{2 <\\beta{2>} <\\beta{2>} <\\beta{2>} <\\beta{2>} <\\beta{2>}<\n' +
      '\n' +
      'From this, we observe that sequential application results in a more rapidly decaying momentum term but amplifies the actual change in parameter \\(\\theta\\). Consequently, a higher \\(\\beta\\) maintains more recent momentum but may lead to greater changes in parameters, and vice versa. Importantly, this imbalance cannot be simply rectified by reducing the learning rate.\n' +
      '\n' +
      'BaselinesWe consider several synchronous baselines from the literature, and their naive application to an asynchronous setting: **1)** Finetune 1 worker (4xbatch): This involves finetuning a single worker with a larger batch size, equating to synchronous SGD. **2)** DiLoCo (Douillard et al., 2023): This synchronous Local-SGD method combines AdamW with Nesterov. **3)** Async. DiLoCo: The asynchronous version of DiLoCo.\n' +
      '\n' +
      '기존의 Fixes는 관찰된 문제를 해결하기 위해 비동기 로컬-SGD 문헌에서 잠재적인 고정을 조사했다. 다음 방법은 **1)** Async로 간주되었다. DiLoCo + 폴리(Xie et al., 2019): Async를 계산합니다. 디LoCo는 \\(g\\lelearrow(1+\\text{staleness})^{-0.5}g\\)를 사용하여 유사 업그레이드를 하향 가중하여 **2)**Async. DiLoCo + 폴리-Thres: 10. **3)*^{-0.5}g\\ 이상으로 스테이플리티로 업데이트를 폐기하는 임계값을 제공한다. * Async. 디LoCo + 델레이 화합물(정 등 2017): 생산 지연 보상(지연 화합물)은 진정한 의사 등급에 근사한다. 1차 테일러 근사치(g(\\theta_{t}})\\ a\\(\\theta_{t})\\ 기울기 g(\\theta_{t}})를 사용하여\\(\\theta_{t})\\ 기울기 g(\\theta_{t})\\ a\\<\\<\\)\\ a\\(\\theta_{t})\\<\\<\\<\\)\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff (\\nabla_{\\mathcal{B}},\\nabla_{\\mathcal{B}})\\o타_{\\prime}}(\\theta_{\\prime}})\\ododot g(\\theta_{\\prime})\\\\odot g(\\nabla_{\\prime})\\odot g(\\theta_{\\prime})\\odot g(\\theta_{\\prime})\\odot g(\\nathime})\\ododot g(\\ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff * Async. 버퍼: 네스테로프 업데이트를 적용하기 전에 첫 번째 외부 패션에서 모든 구배를 측정하고 평균하며, 애덤W+네스테로프를 사용하여 원래 페데버프 알고리즘(Nguyen et al, 2022)의 변화이다. 결과는 그림 8에 나와 있다.\n' +
      '\n' +
      '의사 등급에 대한 관찰 다항식 할인은 한계 이점을 보여준다. 보유 및 지연 보상 기술은 많은 개선을 제공하지 않습니다. 다시 말하지만, 지연 보상이 잘 작동되지 않는다는 사실은 비동기 SGD와 비동기 로컬-SGD의 차이를 지적한다. Async. 버퍼 방법은 융합에 탁월하지만 훈련 초기에 불안정성을 나타낸다. 즉, 방법의 _none_은 동기 디LoCo 기준선들의 성능과 일치한다.\n' +
      '\n' +
      '그림 8: 상이한 비동기 로컬-SGD 접근법의 비교이다. 폴리, 폴리 티즈, 델레이 컴파운드는 비인크를 거의 향상시키지 않습니다. 지역-SGD 공연. 아닌크. 버퍼는 동기 간의 간격을 상당히 닫습니다. 비누와 비네크. 훈련, 훈련 초기 단계에서 불안정성을 도입하면서 훈련한다.\n' +
      '\n' +
      '5개는 솔루션입니다.\n' +
      '\n' +
      '4절에서 요약된 최적화 과제를 해결하기 위해 우리는 두 가지 전략을 개발했다.\n' +
      '\n' +
      '특히, 누에스테로프 업데이트를 지연시킨다. 버퍼 방법은 유망한 성능(그림 8과 같이)을 보여주었다. 또한, 우리의 분석에서는 AdamW+SGD, sans 외부 운동량, 동기적 방법(그림 5)과의 비동기적 훈련이 동기적 방법을 능가한다는 것을 보여주었다. 이러한 통찰력을 기반으로 알고리즘 2에서 동기() 기능을 나타내는 _지연된 네스테로프_(DN) 전략을 제안하며, 이 접근법은 간헐적으로-매우 \\(N\\) 서버 업데이트를 사용하는 것을 포함한다. 네스테로프 업데이트 사이에서 우리는 완충액 \\(\\Delta\\)에서 유사 업그레이트를 집계하고 기울기 하강(또는 구배 하강 및 오래된 운동량의 작은 부분)을 사용하여 모델 파라미터를 업데이트한다. 구배와 운동량 기반 하강성을 균형을 맞추기 위해 매개변수 \\(c\\in[0,1/N]\\)를 소개한다. Hf(0\\)의 \\(c\\) 값은 네스테로프 업데이트 간에 순수한 구배 하강도를 나타내는 반면, \\(1\\)와 동일한 \\(c\\)는 \\(N\\) 업데이트보다 운동량항을 고르게 분배한다. 이 알고리즘의 구체적인 내용은 알고리즘 3에 자세히 설명되어 있으며, Async와 같다. N\\(N\\) 기간에 한 번만 모델 파라미터를 업데이트하는 버퍼(Nguyen et al, 2022)는 구배를 사용하여 계속 업데이트하고, 오래된 운동량의 일부를 통합하고, 모든 \\(N\\) 서버 업데이트에서 한 번 운동량을 업데이트한다.\n' +
      '\n' +
      '```\n' +
      '0: 인티티 모델 매개변수 \\(\\ta_{0}\\)\n' +
      '0: 엄마 부패 \\ (0,1)\n' +
      '0: 엄마 활성화 \\ (c\\in[0,1/N]\\) \\ (c=0\\)에 기본이다.\n' +
      '0: Buffer 크기 \\(m_{0}=0\\) \\(m_{0\\) \\(\\triangleright\\) 운동량 \\(\\Delta=0\\) \\(\\triangleright\\) 응집 구배를 완료하지 않고 유사 등급 \\(g_{t}\\) \\(\\triangleright\\) \\(\\ a\\) \\(m_{0\\) \\(\\,\\) \\(\\,\\) \\(\\,\\) \\(\\,\\) \\(\\-Ex(\\ -0\\) \\(\\,\\) \\(g_{t}\\) \\(\\) \\(g_{t}\\) \\(\\) \\(\\) \\(g_{tincangleright\\) \\(\\) \\(\\) \\(\\,\\ \\(g_{tobacteriumleright\\) \\(\\,\\) \\(\\,\\,\\,\\) \\(g_{triang 다른 \\(m_{t+/\\)\\(m_{t+}{t/\\)\\(mm_{t+}<\\-d\\) <\\<<\\<<\\<<\\<<\\<<<<<<<<<<<<<<<<<<<<<<<>>>> < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <>>>>>> < <\\>>> < <\\>>> < <\\>> < <\\>>> < <\\>> < <\\>> < <\\>> < <\\>>> < <\\>> < <\\>>> < <\\>> < <\\>> < <\\>> < <\\>>> < <\\>> < <\\>>> < < <\\>>>\n' +
      '```\n' +
      '\n' +
      '**알고리즘 3** 딜레이드 네스테로프 업데이트입니다.\n' +
      '\n' +
      '다이나믹 로컬 업데이트 델레이 네스테로프 접근법은 의사 업그레이트를 완충하고 전략적으로 모멘텀 업데이트를 지연시켜 아웃에오폼의 모멘텀 챌린지를 다룬다. 대안적 관점은 동기적 훈련을 모든 근로자의 의사 등급들이 동기화되는 해결책으로 간주한다. 그러나 장치 역량의 다양성은 종종 각 작업자가 동일한 수의 지역 훈련 단계를 실행하면 동시 의사 업그레이드를 방해한다. 생존 가능한 작업은 각 장치의 처리 속도를 기반으로 로컬 트레이닝 단계(예를 들어, \\(w\\)를 맞춤화하는 것을 포함한다. 특히 근로자 \\(초당 단계)의 훈련 속도(초당 단계)로서 \\(v(w)\\를 의미하는데, 근로자의 원하는 훈련 단계를 그대로 계산할 수 있다.\n' +
      '\n' +
      '텍스트{ 단계}(w)}{\\frac{v(w)}{\\frac{v({\\frac{v:{w^{{\\prime}}, \\tag{6}\\]\\]\\[w.\n' +
      '\n' +
      'H\\(H\\)는 가장 빠른 작업자 실행의 수를 나타내고 \\(차층 x\\rfloor\\)는 \\(x\\)보다 크지 않은 가장 큰 정수(DyLU)를 나타낸다. 이러한 조정은 더 느린 작업자가 더 적은 단계를 실행하여 다른 작업자에 걸쳐 완료 시간을 정렬할 수 있게 한다. 이 설정에서 모델 동기화를 위한 유예 기간을 통합하는 것은 스테이플 구배의 영향을 더욱 감소시켜 전반적인 성능을 향상시킵니다.\n' +
      '\n' +
      '3: 여기, 우리는 장치 속도가 선험적으로 알려져 있다고 암묵적으로 가정한다. 만약 이것이 그렇지 않다면 경험적 관측에 기초하여 장치 속도를 추정하는 것은 간단하다.\n' +
      '\n' +
      '6개의 최소 도예.\n' +
      '\n' +
      '향후 연구의 편의성과 새로운 아이디어의 신속한 프로토타이핑의 편의를 위해 관찰된 최적화 도전을 비동기적인 로컬-SGD(그림 9.4)에서 복제하는 최소한의 장난감 예를 제시하며 가우시안 데이터의 혼합물의 혼합물에 대한 분류를 수행하는 것이 과제이다.\n' +
      '\n' +
      '부츠 4: [https://github.com/google-deepmind/asyncdiloco] (https://github.com/google-deepmind/asyncdiloco)에서 콜랩을 확인해 주세요.\n' +
      '\n' +
      '그림 9와 그림 6을 비교한 관찰 결과는 장난감 예시가 동일한 최적화 도전을 나타낸다는 것을 관찰하였다.\n' +
      '\n' +
      '## 7 Experiments\n' +
      '\n' +
      '이 섹션 세부 실험은 두 가지 제안된 방법인 델레이 네스테로프(DN) 및 다이나믹 로컬 업도트(DyLU)의 효능을 평가하기 위해 수행되었다. 또한, 절제 연구는 근로자 수와 모델 크기가 다양하기 때문에 이러한 방법의 효과를 탐구한다.\n' +
      '\n' +
      'Evaluating Delayed Nesterov (DN) and Dynamic Local Updates (DyLU)Figure 2 compares asynchronous Local-SGD with DN and DyLU against baselines such as single worker finetuning and DiLoCo, using the same setup as in Figure 8.\n' +
      '\n' +
      '결과는 DyLU와 결합된 DN이 투과성을 크게 감소시키며, 소변 통과가 yn동기D iLoCo의 활성도 veru pdatesA를 주저한다는 것을 보여준다.\n' +
      '\n' +
      '작업자 Heterogeneity의 차별화 수준을 평가하면 그림 10(각도 곡선) 및 표 1(최종 엄격도)과 같이 다양한 정도의 작업자 장치 이질성에서 제안된 DN+DyLU 방법과 바닐라 비동기 디LoCo 요금이 어떻게 되는지 살펴본다.\n' +
      '\n' +
      '흥미로운 사실은 호모그 제독 장치 s, 즉 호모그 노린르와 같은 정맥, 우드무디엔 ts.\n' +
      '\n' +
      'Footnote 5: We notice that Async. DN+DyLU performs slightly better than DiLoCo when there is no heterogeneity, this is due to the numerical error, as the two methods reduce to the same and the training curves match almost perfectly.\n' +
      '\n' +
      '근로자의 차별적인 무기력으로 재구성하면 20M 모형을 이용하여 근로자 수(4, 8, 16)를 변화시키면서 DN+DyLU를 적용하는데, 그 결과는 그림 11(각도 곡선)과 표 2(최종 각성)에 요약되어 있다.\n' +
      '\n' +
      '근로자가 증가함에 따라 지역-SGD 훈련의 편익은 감소한다. 특히 근로자 16명(16x 배치 크기)과 함께 1인 근로자 고용(16x 배치 크기)이 가장 우수한 실적을 보이고 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Level of heterogeneity & no & slight & moderate & very \\\\ \\hline Pretrained (24K) & 61.64 & 61.64 & 61.64 & 61.64 \\\\ Finetune (4\\% batch size) & 42.47 & 42.47 & 42.47 & 42.47 \\\\ DiLoCo (Doillard et al., 2023) & 41.35 & 41.35 & 41.35 & 41.35 \\\\ Async. DiLoCo & 44.27 & 44.38 & 44.29 & 44.27 \\\\ Async. DN + DyLU (ours) & **41.27** & **41.09** & **41.13** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 1>은 근로자 이질성 수준 (**top-left***, ***top-right****, **bottom-left******, ***slight**********, ***ion********************와 같다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Number of workers \\(k\\) & 4 & 8 & 16 \\\\ \\hline Pretrained (24K) & 61.64 & 61.64 & 61.64 \\\\ Finetune (\\(k\\times\\) batch size) & 42.47 & 41.28 & **40.60** \\\\ DiLoCo (Douillard et al., 2023) & 41.35 & 41.23 & 41.25 \\\\ \\hline Async. DiLoCo & 44.27 & 44.23 & 44.23 \\\\ Async. DN + DyLU (ours) & **41.13** & **41.02** & 40.98 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 2>는 근로자 수를 가리는 것이다.\n' +
      '\n' +
      '그림 9: 장난감 예에 대한 최적화 챌린지를 복제하세요. **Left**: 데이터 세트는 가우스의 혼합물의 혼합물로 구성된다. ** 우측**: 아sync. 지방-SGD는 동기보다 더 나쁨/베터를 수행한다. AdamW+Nrierov/AdamW+SGD를 사용할 때 국소-SGD를 사용한다.\n' +
      '\n' +
      '피규어. 그러나 DN+DyLU는 성능에서 동기 디LoCo와 밀접하게 일치하여 이질적인 환경에서 디LoCo 대안으로서의 잠재력을 보여준다.\n' +
      '\n' +
      '마지막으로 차별화 모델을 사용한 추출은 DN+DyLU를 다양한 크기의 모델(20M, 60M, 150M)에 적용하며, 결과는 그림 12(각도 곡선) 및 표 3(최종 퍼플로우)에 요약되어 있다.\n' +
      '\n' +
      'ObservationBoth synchronous and asynchronous Local-SGD methods outperform the approach of finetuning a single worker with an increased batch size. Notably, this advantage becomes more pronounced during the later stages of convergence, aligning with findings from previous research that highlight Local-SGD\'s superior generalization capabilities (Gu et al., 2023). Additionally, our proposed DN+DyLU method demonstrates consistent efficacy across various model sizes. It\'s important to note that the performance disparity between synchronous and asynchronous DiLoCo does not diminish even as the model size increases.\n' +
      '\n' +
      'Ablation with Different \\(c\\)We apply \\(c\\in\\{0,0.1\\}\\) in Async. DN+DyLU with varying \\(k\\) (4, 8, 16) and model sizes (20M, 60M, 150M), with the 4 "very" heterogeneous workers. This is because when the level of heterogeneity is small, using different \\(c\\) will have smaller difference (e.g., when there is no heterogeneity, any \\(c\\) results in the same algorithm). Results are summarized in Table 4.\n' +
      '\n' +
      '관찰학적으로 \\(c=0\\)와 \\(c=0.1\\) 사이에 유의미한 차이가 없음을 관찰하여 중간 단계에서 약간의 운동량을 추가하는 것이 너무 큰 도움이 되지 않음을 나타낸다. 그 결과, 우리는 2개의 연속 네스테로프 업데이트 간의 SGD 업데이트를 수행하는 것에 해당하는 알고리즘 3의 디폴트 값으로 \\(c=0\\)를 설정하였다. \\(c\\)의 값을 설정하는 것은 전체 알고리즘에 어떤 오버헤드를 도입하지 않는다는 점에 유의한다.\n' +
      '\n' +
      '8은 특정 일꾼입니다.\n' +
      '\n' +
      '이 섹션은 특히 비동기 환경에서 응용 프로그램에 초점을 맞춘 피빙된 학습과 지역-SGD 스타일의 분산 최적화에 대한 문헌의 간결한 개요를 제공한다.\n' +
      '\n' +
      '지역-SGD 및 분산 최적화 로컬-SGD는 통신 빈도를 줄이기 위해 설계된 특정 분산 최적화 기법(Bijral et al., 2016; Coppola, 2015; McDonald et al., 2010; Stich, 2018; Zhang et al., 2016; Zinkevich et al., 2010)이다. 지방-SGD의 핵심 원리는 각 작업자가 글로벌 동기화에 참여하기 전에 여러 지역 훈련 반복을 실행할 수 있도록 하는 것이다. 이 기술은 이후 연방 학습 설정에 적용되어 통신 오버헤드를 줄이는 것을 목표로 하는 FedAvg 방법(McMahan et al, 2017)의 개발로 이어졌다. 지역-SGD와 달리 피드백된 학습은 또한 사용자 프라이버시 문제를 해결하고 일반적으로 이질적인 장치를 포함한다. 통신 오버헤드를 더욱 최소화하기 위해 FedOpt는 SGD 운동량과 Adam(Reddi et al., 2020)과 같은 적응 최적화 방법을 통합한다. 그러나 고객/근로자의 이질성이 증가할수록 융합률이 저하되는 경우가 많다. SCAFFOLD(카리메디 et al., 2020) 및 MIME(카리메디 et al., 2021)와 같은 방법이 이질적인 환경에 대한 이러한 최적화 방법을 적응시키기 위해 도입되었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Number of workers \\(k\\) & 4 & 8 & 16 \\\\ \\hline Async. DN + DyLU (\\(c=0\\)) & **41.13** & 41.02 & **40.98** \\\\ Async. DN + DyLU (\\(c=0.1\\)) & 41.16 & **40.93** & 41.04 \\\\ \\hline Model size & 20M & 60M & 150M \\\\ \\hline Async. DN + DyLU (\\(c=0\\)) & **41.13** & **24.53** & **17.26** \\\\ Async. DN + DyLU (\\(c=0.1\\)) & 41.16 & 24.69 & 17.27 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 알고리즘 3에서 \\(c\\in\\{0,0.1\\}\\)를 변경하는 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Model size & 20M & 60M & 150M \\\\ \\hline Pretrained (24K) & 61.64 & 30.19 & 22.80 \\\\ Finetune (4x batch size) & 42.47 & 24.80 & 17.47 \\\\ DiLoCo (Douillard et al., 2023) & 41.35 & 24.55 & **17.23** \\\\ \\hline Async. DiLoCo & 44.27 & 25.64 & 18.08 \\\\ Async. DN + DyLU (ours) & **41.13** & **24.53** & 17.26 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: | Varying 모형 크기를 나타낸다.\n' +
      '\n' +
      '학습 효율이 가장 느린 작업자에 의해 병목화되는 동기 분산 최적화(Dean et al, 2012, Diskin et al, 2021, Koh et al, 2006, Lian et al, 2015, 2018, Recht et al, 2011)에서 관찰된 "스트레글러 효과"를 완화하기 위해 동기 훈련 비동기 교육이 개발되었다. 비동기 최적화에서 중요한 과제는 단단한 구배 문제이며, 이는 최근에 업데이트된 모델에 오래된 기울기가 적용될 때 발생한다. 지연 보상(정 등 2017)이 있는 근사 동기 SGD로 이 문제를 해결한다.\n' +
      '\n' +
      'Figure 11: Varying the number of workers.\n' +
      '\n' +
      'Figure 10: Varying the heterogeneity in devices.\n' +
      '\n' +
      '오래된 구배를 사용하여 진정한 구배를 만들어 보세요. 비동기식 방법은 또한 연방 학습 맥락(Xie et al., 2019)에서 탐구되었다. 도전에도 불구하고 비동기 훈련은 전 세계적으로 이질적인 장치를 사용하여 언어 모델링(디스킨 등, 2021)에 대한 성공을 입증했다.\n' +
      '\n' +
      '언어 모델링을 위한 로컬-SGD(또는 FedAvg)의 개념은 언어 모델링 영역에서 이전에 적용되었다. 예를 들어, 교차 장치 공급 학습은 사전 및 미세 조정 언어 모델(Borzunov et al., 2022; Diskin et al., 2021; Hilmkl et al., 2021; 프레스러, 2020; Ro et al, 2022; Ryabinin et al., 2021)에 활용되었다. 보다 최근에, 디LoCo는 더 큰 언어 모델을 포함하도록 로컬-SGD 방법론을 확장했으며, 특히 인너캣 + 아웃커롭 페어링으로 아담W + 네스테로프 모멘텀의 사용을 제안한다. 비동기 환경에서 페드버프(Nguyen et al, 2022) 알고리즘은 고객의 의사 조사자를 버퍼링하여 충분한 수의 유사 조사자를 축적한 후에야 서버 모델을 업데이트한다. 타임리FL(장 et al., 2023)은 느린 디바이스가 모델의 일부만 훈련하도록 함으로써 비동기화를 줄이는 것을 목표로 한다.\n' +
      '\n' +
      '## 9 Limitations\n' +
      '\n' +
      '이 연구는 포괄적인 반면 몇 가지 한계를 가지고 있다. 먼저, OuterOpt에서 운동량 업데이트와 관련된 중요한 최적화 문제를 식별하지만 이 문제의 정확한 원인은 여전히 불분명하다. 강력한 이론적 백싱으로 이 문제를 이해하는 것은 향후 연구를 위한 흥미로운 방법을 제시한다. 둘째, 우리의 경험적 관찰은 지역-SGD 방법의 이점이 작업자가 증가함에 따라 감소한다는 것을 시사하며, 이는 근본적인 이유가 아직 이해되지 않은 현상이다. 이 문제는 현재 비동기 로컬-SGD의 확장성을 방해한다. 마지막으로 제안된 방법 DN+DyLU는 향상된 경험적 성능을 보여주지만 추가 조사가 가능한 측면인 형식적 이론적 융합 보증이 부족하다.\n' +
      '\n' +
      '## 10 Conclusion\n' +
      '\n' +
      '이 연구는 언어 모델링에서 비동기적 지방-SGD에 대한 철저한 검사를 제시한다.\n' +
      '\n' +
      '그림 12: 모델 크기를 변경합니다.\n' +
      '\n' +
      'Our central finding is that while momentum in the outer optimization loop is crucial, it may be less effective in asynchronous scenarios compared to synchronous ones when implemented naively. To bridge this gap, we introduce a novel approach, focusing on sporadic momentum updates using buffered pseudogradients, combined with continuous stochastic pseudogradient updates. Furthermore, our research reveals that tailoring local training steps to each worker\'s computational speed is not only a straightforward but also an efficient strategy to enhance performance.\n' +
      '\n' +
      'However, there is much work to be done. In the standard (as opposed to the "local") gradient descent setting, the optimal batch size in terms of decreasing loss as quickly as possible in terms of number of weight updates is not usually "as large as possible". In our view, similarly, there is hope for asynchronous Local-SGD methods that give better results per local update than synchronous Local-SGD.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '아담 피쉬의 소중한 피드백에 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* S Bijral et al. (2016) 아비엔 S Bijral, Anand D Sarwate 및 Nathan Srebro. 분산 확률 최적화의 데이터 의존성에 대한 __ 분산 확률 최적화의 데이터 의존성에 대한 것이다. arXiv 프리프린트 arXiv:1603.04379_ 2016.\n' +
      '* 보즈노프(2022) 알렉산더 보즈노프, 다미트리 바반코프, 팀 디트머스, 맥스 리야바닌, 예네스 벨카다, 아르테미 차마첸코, 파벨 삼긴, 콜린 라펠 등이 있다. Petals: 거대 모델의 협업 추론 및 미세 조정: 큰 모델의 협력 추론 및 미세 조정. arXiv 프리프린트 라이브러리_, 2022.\n' +
      '* 코폴라(2015) 그레고리 프란치스코 쿠폴라. 자연어 처리를 위한 구조화된 예측 변수의 분산 대마진 교육을 위한 보정 파라미터 혼합이다. 2015년.\n' +
      '* 디안 등은 (2012) 제프리 데안, 그레그 코라도, 라자트 몽라, 카이 크헨, 마티누 데빈, 마크 마오, 마크리우리오 라나토, 앤드루 시니어, 폴 터커, 케양 등의 대규모 척도를 심층 네트워크를 배포했다. 신경 정보 처리 시스템_, 2012년 25.\n' +
      '마이클 디킨, 알렉시 북하바로프, 맥스 리아비닌, 루실 사야비에, 퀘스틴 로케스트, 퀀틴 시미틴, 드미트리 포코프, 드미트리 피르킨, 맥심 카슈노프, 알렉산더 보즈노프, 알베르트 빌라노바 델 모랄, 데니스 모비스, 일리아 고베레브, 야키 제나이트, 토마스 볼프, 제니 페크히메코, 데르노바 델 모랄레스, 데르바노바 델 모랄레스, 야키 페크헤르, 다트르노바 델 모랄레스, 데르나노바 델 모랄레스, 데나노바 델 모랄레스, 이바노바 델 모랄레스, 일리아 고베르, 야키 고베르, 야키 고베르, 야키 고베르, 야신 페르나이트, 토마스 볼프, 야키 페크헤르, 토마스 볼프, 게나디, 토마스 볼프, 게나디 펠크헤르, 게나디 페크헤르, 게나디 페크헤르, 게나디 개방형 협업에서 딥러닝을 분산시켰는데 __ 개방 협업에서 딥러닝을 분배하였다. 2021년 신경 정보 처리 시스템(NeurIPS)_, 2021a에서 발전한다.\n' +
      '* Diskin et al. (2021b) Michael Diskin, Alexey Bukhtiyarov, Max Ryabinin, Lucile Saulnier, Anton Sinitsin, Dmitry Popov, Dmitry V Pyrkin, Maxim Kashirin, Alexander Borzunov, Albert Villanova del Moral, et al. Distributed deep learning in open collaborations. _Advances in Neural Information Processing Systems_, 34:7879-7897, 2021b.\n' +
      '* Douillard et al. (2023) Arthur Douillard, Qixuan Feng, Andrei A Rusu, Rachita Chhaparia, Yani Donchev, Adhiguna Kuncoro, Marc\'Aurelio Ranzato, Arthur Szlam, and Jiajun Shen. Diloco: Distributed low-communication training of language models. _arXiv preprint arXiv:2311.08105_, 2023.\n' +
      '* Gu et al. (2023) Xinran Gu, Kaifeng Lyu, Longbo Huang, and Sanjeev Arora. Why (and when) does local sgd generalize better than sgd? _arXiv preprint arXiv:2303.01215_, 2023.\n' +
      '* 힐킬로 등(2021) 아그린 힐 킬로, 세바스티안 칼, 마테오 바비에리, 레온 리네 수펠트, 에드빈 리스트 제크, 올로프 모그렌 등이 있다. 대형 언어 모델의 미세 조정에 대한 피드백 피드백 학습을 제공합니다. 정보화시스템_페이지에 대한 자연어의 적용에 관한 _국제회의에서 2021년 15-23쪽 스프링거.\n' +
      '(2021) 요르단 호프만, 엘비안 보르츠카야, 아르비제나 루터카우, 엘비즈 데 루터카스, 엘비즈 데 라스 카사, 리사 안네 헨드크, 요하네스 웰블, 원조 클라크, 톰 헤니간, 에릭 니란디, 카티 밀리칸, 조지 밴 데 데 데리체, 보단 다노, 아우렐리아 다노, 에렌 시노니, 에리히 엘센, 잭 Woff만, 엘비안 헨더만, 쿠르드 밀란, 코르드 밀란, 코르드 밀란, 코르드 밀란, 코르네 밀란, 코르네 밀란, 코르네 밀란, 코르네 밀란, 코르네 밀란, 코르네 밀란, 시런 다노, 시린지, 시노의 다노, 에렌 시노, 에렌 시노, 에레엘란, 에렌 시노만, 잭 Woff만, 잭 Woff만, 잭 Woff만, 잭 Woff만, 알(2021). 래, 오리올 비넬, 로랑 시프르.\n' +
      '\n' +
      '훈련은 계산하는 최적의 대형 언어 모델 __ 최적이 아닌 대형 언어 모델. 신경정보처리시스템(NeurIPS)_ 2022년 개발.\n' +
      '* Karimireddy et al. [2020] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In _International conference on machine learning_, pages 5132-5143. PMLR, 2020.\n' +
      '* Karimireddy et al. [2021] Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian U Stich, and Ananda Theertha Suresh. Breaking the centralized barrier for cross-device federated learning. _Advances in Neural Information Processing Systems_, 34:28663-28676, 2021.\n' +
      '* Koh et al. [2006] Byung-Il Koh, Alan D George, Raphael T Haftka, and Benjamin J Fregly. Parallel asynchronous particle swarm optimization. _International journal for numerical methods in engineering_, 67(4):578-595, 2006.\n' +
      '* Lian et al. [2015] Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic gradient for nonconvex optimization. _Advances in neural information processing systems_, 28, 2015.\n' +
      '* Lian et al. [2018] Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic gradient descent. In _International Conference on Machine Learning_, pages 3043-3052. PMLR, 2018.\n' +
      '* Lin et al. [2018] Tao Lin, Sebastian U Stich, Kumar Khitij Patel, and Martin Jaggi. Don\'t use large mini-batches, use local sgd. _arXiv preprint arXiv:1808.07217_, 2018.\n' +
      '* Lin et al. [2020] Tao Lin, Sebastian U. Stich, Kumar Khitij Patel, and Martin Jaggi. Don\'t use large mini-batches, use local sgd. _Proceedings of the International Conference on Learning Representations (ICLR)_, 2020.\n' +
      '* McDonald et al. [2010] Ryan McDonald, Keith Hall, and Gideon Mann. Distributed training strategies for the structured perceptron. In _Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics_, pages 456-464, 2010.\n' +
      '* McMahan et al. [2017] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR, 2017.\n' +
      '* Nguyen et al. [2022] John Nguyen, Khitiz Malik, Hongyuan Zhan, Ashkan Yousefpour, Mike Rabbat, Mani Malek, and Dzmitry Huba. Federated learning with buffered asynchronous aggregation. In _International Conference on Artificial Intelligence and Statistics_, pages 3581-3607. PMLR, 2022.\n' +
      '* 프레스러[2020] 전당포 프레스러. 2020년도 URL[https://barm.com/swarm-트레이닝-v01a.pdf](https://battlen.com/swarm-트레이닝-v01a.pdf) 훈련.\n' +
      '* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 2020.\n' +
      '* Recht et al. [2011] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. _Advances in neural information processing systems_, 24, 2011.\n' +
      '* Reddi et al. [2020] Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. _arXiv preprint arXiv:2003.00295_, 2020.\n' +
      '* Ro et al. [2022] Jae Hun Ro, Theresa Breiner, Lara McConnaughey, Mingqing Chen, Ananda Theertha Suresh, Shankar Kumar, and Rajiv Mathews. Scaling language model size in cross-device federated learning. _arXiv preprint arXiv:2204.09715_, 2022.\n' +
      '* Ryabinin et al. [2021] Max Ryabinin, Eduard Gorbunov, Vsevolod Plokhotnyuk, and Gennady Pekhimenko. Moshpit sgd: Communication-efficient decentralized training on heterogeneous unreliable devices. _Advances in Neural Information Processing Systems_, 34:18195-18211, 2021.\n' +
      '\n' +
      '* 스타치(2018) 세바스티안 우 스티치. 로컬 sgd는 빠르게 수렴하고 거의 통신을 하지 않는다. __ 국소 sgd가 빠르게 수렴하여 거의 통신을 수행하지 않는다. arXiv 프리프린트 arXiv:1805.09767_ 2018.\n' +
      '* Xie et al. (2019) Cong Xie, 산미 기예조, Indranil Gupta. 비동기식 피회 최적화 __비동기식 최적화. arXiv 프리프린트 arXiv:1903.03934_ 2019.\n' +
      '* 장 등은 (2016) 지안 장, 크리스토퍼 데 사, 이오만니스 미틀리아가스, 크리스토퍼 레 등이 있다. 평행 시그드: 평균은 언제 도움이 됩니까? arXiv 프리프린트 arXiv:1606.07365_ 2016.\n' +
      '* Zhang et al. (2023) Tuo Zhang, Lei Gao, Sunwoo Lee, Mi Zhang, and Salman Avestimehr. Timelyfl: Heterogeneity-aware asynchronous federated learning with adaptive partial training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5063-5072, 2023.\n' +
      '* Zheng et al. (2017) Shuxin Zheng, Qi Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhi-Ming Ma, and Tie-Yan Liu. Asynchronous stochastic gradient descent with delay compensation. In _International Conference on Machine Learning_, pages 4120-4129. PMLR, 2017.\n' +
      '*Zinkevich et al. (2010) 마르틴 Zinkevich, 마르커스 웨머, 리홍 리, 알렉스 스놀라. 평행화된 확률적 구배 하강 __평행화된 확률적 구배 하강. __평행화된 확률적 기울기 하강. 신경 정보 처리 시스템_, 23, 2010의 발전이다.\n' +
      '\n' +
      '## Supplementary Materials\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '네트워크 아키텍처는 20M, 60M, 150M 모델의 건축적 차이를 표 6에 나타냈다. 모두 차치닐라 계열(Hoffmann et al, 2022)을 기반으로 한 변압기 디코더 전용입니다.\n' +
      '\n' +
      '우리는 공통 크로슬(Raffel et al., 2020)에서 파생된 데이터세트인 C4 데이터셋에 대한 언어 모델링 작업을 고려한다. 총 단계 수는 모든 모델에 대해 88,000단계로 설정되며, 사전 훈련은 어떤 피드백된 학습 방법 없이 24,000단계, 즉 _포스트 로컬-SGD_(Lin et al., 2020)와 유사하다.\n' +
      '\n' +
      '표 5에서 우리는 이 연구를 위해 고려된 최적화 하이퍼모수들을 윤곽으로 설명한다.\n' +
      '\n' +
      'Douillard et al.(2023)에 이어 인너 옵티미이저 국가는 모든 실험에서 근로자 B가 데이터 샤드 작업자 A를 훈련 종료할 때 AdamW의 최적 상태를 재설정했다. 즉, 각 지역 근로자 측 훈련은 새로운 최적화를 가진 독립적인 훈련 과정이며, 3절에서 설명한 대로 학습률만을 조정한다.\n' +
      '\n' +
      '### Aync. Aocode\n' +
      '\n' +
      '이 절에서는 알고리즘 2에서 기차(O)와 get_worker(O) 기능에 대한 가블록을 제공한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline Hyperparameter & 20M & 60M & 150M \\\\ \\hline Number of layers & 6 & 3 & 12 \\\\ Hidden dim & 256 & 896 & 896 \\\\ Number of heads & 4 & 16 & 16 \\\\ K/V size & 64 & 64 & 64 \\\\ Vocab size & & 32,000 & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '세 가지 평가된 크기에 대한 표 6: ** 모델 구성***이다. 모두 변압기 아키텍처인 키칠라 스타일(Hoffmann et al, 2022)을 기반으로 합니다.\n' +
      '\n' +
      '```\n' +
      '0: 이용 가능한 노동자 \\(\\mathcal{W}\\)\n' +
      '0: 전류 서버 모델 \\(\\ta\\)\n' +
      '1:for\\(w\\in\\mathcal{W}\\)do\n' +
      '2: 삼플 샤드 \\(\\mathcal{D}^{\\prime}\\) \\(Eq. 2)에 대해.\n' +
      '3:\\(w\\).local_updates = DyLU(\\(\\mathcal{D}^{\\prime}\\)) (Eq. 6).\n' +
      '4: 데미드 lr 일정(Eq. 3)\n' +
      '5:\\ (w\\) a\\ = 트레이스_워크러 (\\ (w\\), \\ (\\mathcal{D}^{\\prime}\\), \\ (\\theta\\)\n' +
      '6:endfor\n' +
      '```\n' +
      '\n' +
      '알고리즘 2에서**알고리즘 4** 열차 ()\n' +
      '\n' +
      '```\n' +
      '0: Workers \\(\\mathcal{W}\\)\n' +
      '0: Grace period \\(\\tau_{\\text{grace}}\\)\n' +
      '0: 유예 기간 시작(\\tau_{{sync}\\)\n' +
      '그런 다음 모든 일꾼이 하지 않을 때(성심증{W}\\)\n' +
      '2:return null\n' +
      '3:else\n' +
      '4:\\(w\\) = earliest completed worker in \\(\\mathcal{W}\\).\n' +
      '4:\\(w\\)= 철저할 수 없는 d 근로자\\(\\mathcalW}\\)\n' +
      '6:return\\(w\\)\n' +
      '7:else\n' +
      '8:return null\n' +
      '9:endif\n' +
      '10:endif\n' +
      '```\n' +
      '\n' +
      '알고리즘 2에서**알고리즘 5** get_worker()가 된다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>