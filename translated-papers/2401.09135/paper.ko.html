<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '언어 모델링을 위한 다차원적인 로컬-SGD 훈련.\n' +
      '\n' +
      'Bo Liu\\({}^{*}\\)\n' +
      '\n' +
      '1\n' +
      '\n' +
      ' 라키타 헌카리아\\({}^{2}\\)\n' +
      '\n' +
      ' 아더 더빌라드\\({}^{2}\\)\n' +
      '\n' +
      ' 사티엔 케일\\({}^{3}\\)\n' +
      '\n' +
      ' Andrei A. Rusu\\({}^{2}\\)\n' +
      '\n' +
      ' Jiajun Shen\\({}^{2}\\)\n' +
      '\n' +
      ' Arthur Szlam\\({}^{2}\\)\n' +
      '\n' +
      'Marc\'Aurelio Ranzato\\({}^{2}\\)\n' +
      '\n' +
      '오스틴 텍사스대학 1명, 구글 딥민드 2명, 구글 딥민드 3명, 구글 연구소 3명이 구글 딥민드의 인턴으로 활동했다.\n' +
      '\n' +
      '부츠 1: 때때로 페데레이티드 평균(FedAvg)이라고도 하는 로컬-SGD 용어는 여기에서 사용자가 다른 작업자에게 데이터 할당에 대한 통제를 갖는 분산 최적화에서 뿌리를 강조하는 데 사용된다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '각 디바이스가 통신당 하나 이상의 SGD 업데이트를 수행하는 분산 최적화에 대한 접근 방식인 피지 평균화라고도 하는 국소 확률적 구배 하강(지방-SGD)도 있다. 이 작업은 언어 모델을 훈련하기 위한 _asynchronous_ 로컬-SGD에 대한 실증 연구를 제시하고, 즉 각 작업자는 SGD 단계를 마치자마자 글로벌 파라미터를 업데이트한다. 작업자 하드웨어 이질성, 모델 크기, 근로자 수, 최적화자가 학습 성과에 어떤 영향을 미칠 수 있는지 조사하여 포괄적인 조사를 실시한다. 우리는 순진한 구현과 함께 비동기적 로컬-SGD가 (글로벌) 모델 파라미터를 더 자주 업데이트했음에도 불구하고 동기적 대응물보다 수렴하기 위해 더 많은 반복을 취한다는 것을 발견했다. 작업자 구배가 핵심 도전으로 엉망일 때 전 세계 매개변수에 대한 운동량 가속도를 식별한다. 지연된 네스테로프 운동량 업데이트를 활용하여 작업자의 지역 훈련 단계를 연산 속도에 따라 조정하는 새로운 방법을 제안한다. C4 데이터세트 상의 최대 150M 파라미터로 평가된 이 접근법은 업데이트 단계당 엄격성 측면에서 동기 로컬-SGD의 성능과 일치하며 벽 시계 시간 측면에서 크게 능가한다.\n' +
      '\n' +
      '비동기식 훈련, 언어 모델링, 대규모 분산 학습 2024\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Large language models (LLMs) have revolutionized many applications, transforming the way machines interact with human language. The cornerstone of this revolution is training these models at massive scale. To manage such large-scale training in reasonable amounts of time, it has been necessary to distribute computations across multiple devices. However, the standard approaches to this distributed training uses co-located devices with fast interconnects.\n' +
      '\n' +
      '대략적인 빗자루(L. L. 오스메)의 비말레소네르 리네네프 테메임(L. L.에베노세 에스티아 프레디즈 파시, 이 스웨디브에 있는 올드캅스 아스트루트, 이 스패스트레이즈-로즈 트라쉬는 마들레와 함께 상호작용한다.\n' +
      '\n' +
      'Local Stochastic Gradient Descent (Local-SGD) is a collection of optimization methods that can reduce communication bottlenecks.1 These methods involve each device performing multiple local gradient steps before syncing their parameter updates with a parameter server. While Local-SGD enhances training efficiency by reducing communication frequency, it can suffer from the _straggler_\n' +
      '\n' +
      '그림 1: 아스네크의 일러스트레이션. v.s. 동기. 2명의 근로자(파란색과 빨간색으로)와 교육합니다. 싱크. 훈련은 가려움증 효과와 비인크에 시달립니다. 훈련은 빠른 작업자의 공회전 시간을 줄입니다.\n' +
      '\n' +
      'effect_ caused by heterogeneous devices. For instance, faster devices are idle waiting for slower ones to catch up, undermining the overall efficiency of the system. Moreover, all devices are forced to communicate at the same time requiring high bandwidth connection with the parameter server. Asynchronous Local-SGD presents a more viable solution (illustrated in Figure 1), as it allows the server to update the model as soon as the updates of a worker are available, thereby enhancing computational utilization and minimizing communication bandwidth requirements.\n' +
      '\n' +
      'In this study, we explore the viability of asynchronously training LLMs using Local-SGD. We expand upon previous works that have attempted to alternate steps on subsets of workers or randomly drop certain subset of workers during synchronous Local-SGD (Douillard et al., 2023; Ryabinin et al., 2021). The main content is structured into three parts:\n' +
      '\n' +
      '1. Framework (Section 3).The first part introduces our high-level design for the asynchronous training framework. We discuss how each worker determines which data shard to train on, for how many steps, with what learning rates, and how the server updates models asynchronously.\n' +
      '\n' +
      '2. 최적화 챌린지(섹션 4)에서 비동기적 지방-SGD에 적합한 다양한 기존 최적화 전략에 대한 실증 연구를 수행한다. 여기에는 작업자 측 최적화(제너 최적화)와 서버 측 최적화(제터 최적화)가 모두 포함된다. 운동량을 효과적으로 활용하는 데 있어 핵심 과제를 발견합니다. 특히 적응 운동량 방법은 일반적으로 내부 및 외부 최적화의 융합을 가속화하는 반면, 비동기 로컬-SGD의 효능은 특히 동기 구현과 비교할 때 운동량 기술을 모두 사용할 때 상대적으로 감소한다.\n' +
      '\n' +
      '3개의 제안 솔루션(섹션 5)은 딜레이 네스테로프 모멘텀 업데이트(DN)와 다이나믹 로컬 업디테스(DyLU)의 단순하고 효과적인 두 가지 기술을 소개한다. 이러한 기술은 조합되고 훈련하는 란을 조합하고 평가할 때 이러한 기술을 사용한다.\n' +
      '\n' +
      '그림 2: 동기화를 이용한 언어모델 비교평가. 비누와 비네크. 20M 매개변수 모델에서 4명의 이질적인 작업자를 가진 지역-SGD 방법. 최첨단 동기입니다. 지역-SGD 방법인 디LoCo(Douillard et al, 2023)는 각각 아담W와 네스테로프 모멘텀을 근로자측 및 서버측 최적화기로 사용한다. 이 최적화기 조합은 아스네크의 가장 강력한 것으로 남아 있다. 지역-SGD 훈련(그림 5 참조)은 그러나 저성능 디LoCo는 상당히 낮다. 외부 최적화를 위한 딜레이 네스테로프(DN)(알고리즘 3)와 동적 로컬 업도트(DyLU)(섹션 5)를 통합함으로써, 우리는 동기화 간의 업데이트에 대한 엄격성 측면에서 성능 격차를 크게 가교한다. 비누와 비네크. 언어 모델링에 대한 훈련. 더욱이, 제안된 방법은 벽 시계 시간 대 비장 측면에서 디LoCo를 크게 능가한다.\n' +
      '\n' +
      ' 기저 모델은 비동기적인 로컬-SGD가 지역 업데이트의 총 수에 대한 경막성 측면에서 동기적 로컬-SGD에 접근하고 비동기적 로컬-SGD 대 비동기적 로컬-SGD를 더욱 향상시킬 수 있도록 한다. 그림 2에 자세히 설명된 바와 같이 경막 대 벽시계 측면에서 동기적 지방-SGD가 있다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '본 연구에서는 \\(k\\) 데이터 음영 전반에 걸쳐 공유 모델 매개변수 \\(\\theta\\)의 분산 최적화에 중점을 두고 \\(\\mathcal{D}=\\{\\mathcal{D}_{1},\\ldots,\\mathcal{D}_{k}\\) 작업자를 사용하여\\(k\\) 작업자를 사용하여 1차 목표를 설명한다.\n' +
      '\n' +
      '부츠 2: 근로자 수(\\(k\\))는 데이터 샤드 수와 동일하지만, 우리의 방법은 데이터 샤드보다 작업자가 적을 때도 적용 가능하다.\n' +
      '\n' +
      '}.{j}{mathcal{D}}{mathb{D}}\\{{{}\\]\n' +
      '\n' +
      'Hi(\\ell(\\cdot;\\theta)\\가 손실 함수(예를 들어 언어 모델링에서 다음 토큰 예측에 대한 교차 엔트로피 손실)를 나타내고, \\(|\\cdot|\\)는 설정된 크기를 나타낸다.\n' +
      '\n' +
      '이 작업에서 로컬-SGD의 정의를 확장하여 원래 로컬-SGD 방법뿐만 아니라 고급 최적화 기술을 통합하는 변이체를 포함한다. 특히 언어 모델링에서 동기 로컬-SGD의 기준을 설정하는 디LoCo(Douillard et al., 2023)에 초점을 맞추고 있다. 알고리즘 1에서 Each 근로자 \\(i\\)는 데이터 샤드 \\(\\mathcal{D}_{i}_{i}\\)에 대한 _inner 최적화기_를 사용하여 로컬 업데이트를 수행하여 파라미터 변경(슈도- 업그레이팅) \\(슈도델타_{i}^{(t)}=\\theta_{i}^{(t)}^{(t)를 서버에 다시 전송하기 전에 데이터 샤드 \\(\\mathcal{D}_{i}_\\)에 대한 inner 최적화기_inner 최적기_\\)를 사용하여 로컬 업데이트를 수행한다. 그런 다음 서버는 응집된 외부 구배 \\(\\Delta^{{(t)}=\\frac{1}{k}\\sum_{i =1}^{k}\\delta_{i}^{{(t)}\\\\)를 계산하고 \\(\\Delta^{{(t)}\\\\)를 사용하여 _outer 최적화기__outer 최적화기를 적용하여 \\(\\Delta^{{(t)를 갱신한다. 디LoCo의 주요 통찰력은 각각 최고의 내부 및 외부 최적제로 AdamW 및 Nrierov 모텐톤의 최적 사용이다.\n' +
      '\n' +
      '## 3 Async. 현지-SGD 프레임워크\n' +
      '\n' +
      '이 섹션에서는 중심 서버가 모든 작업자를 제어하고 업데이트를 비동기적으로 응집시키는 비동기 로컬-SGD 파이프라인 설계를 설명한다.\n' +
      '\n' +
      '데이터 샤드 샘플링은 각 디바이스가 자신의 데이터에 부착된 피딩된 학습 설정과 달리 분산 최적화에서 사용자는 어떤 데이터가 어떤 작업자가 동적으로 어떤 작업자에게 할당되는지 선택할 수 있는 권리를 가진다. 작업자가 새로운 로컬 최적화 라운드를 시작할 준비가 되었을 때 다른 데이터 샤드(근로자는 이질적)에 대한 학습 진행의 균형을 맞추기 위해 "학습 진행"에 반비례하는 데이터를 샘플링한다. 구체적으로 \\(n_{i}\\)를 \\(\\mathcal{D}_{i}\\)에서 학습된 데이터 포인트의 수로 정의하는 다음, 샤드 \\(i_{\\text{ 샘플링}}\\)를 샘플링한다.\n' +
      '\n' +
      '}\\fegin{splid}}}\\\\ &\\text{split}}}\\\\ &\\text{i}\\\\ &\\text{i}\\frac{|}\\frac{|\\mathcal{D}_{i}}}{frac{mathcal{D}_{j}_{j}},0.\n' +
      '\n' +
      '즉, 우리는 데이터 샤드(즉, \\(\\frac{n_{i}}{\\sum_{j}}{\\sum_{j}}\\leq\\frac{mathcal{D}_{i}}{\\q\\frac{D}_{\\sum_{j}{\\mathcal{D}_{\\mathcal{D}_{\\mathcal{D}_{j}{\\mathcal{D}_{j}{\\mathcal{D}_{j}{\\mathcal{D}{j}{j}{\\mathcal{D}{j}{j}{\\mathcal{D}{j}{j}{j}{\\mathcal{D}_{j}{j}{j}{j}_{j}_{j}|}_{j}|}_{j}|}_{j}|}_{j}|}_{j}|}_{j}|}_{j}|}_{j}|}_{j}| 샤드가 과소 샘플링된 정도는 샘플링 속도를 결정한다. 그렇게 함으로써, 더 느린 경과를 가진 데이터 샤드가 훈련을 위해 샘플링될 가능성이 더 높기 때문에 화소에 걸친 학습 경과를 균형을 맞출 수 있다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      '```\n' +
      '0: 경구 전치 모델 \\(\\ta^{(0)}\\)\n' +
      '0:\\(k\\) workers\n' +
      '0:Grace period \\(\\tau_{\\text{grace}}\\)\n' +
      '0:Total local updates \\(t_{\\text{max}}\\)\n' +
      '1:\\(t_{\\text{local}}=0\\)\n' +
      '2:\\(\\theta\\leftarrow\\theta^{(0)}\\)\n' +
      '3:\\(\\mathcal{W}\\) = [\\(k\\)]에서 \\(i\\)에 대한 [init_worker(k\\)][init_worker]]\n' +
      '4:\\(성실{W}_{\\text{{}}\\)\n' +
      '5:\\(\\text{train}(\\mathcal{W},\\ \\theta)\\)\n' +
      '6:\\(\\tau_{\\text{sync}=\\riangleright\\) \\(\\tau_{\\text{sync})의 시작.\n' +
      '7:while\\(t_{\\text{local}}<t_{\\text{max}}\\)do\n' +
      '8:\\(w\\) = get_worker(\\(\\mathcal{W},\\tau_{\\text{grace}},\\tau_{\\text{sync}}\\))\n' +
      '9:\\(\\triangleright\\) get a completed worker\n' +
      '그런 다음 10:if\\(w\\)가 존재합니다.\n' +
      '서버와 업데이트를 동기화한다.\n' +
      '12:\\(\\tau_{\\text{sync}}=\\min(\\tau_{\\text{sync}},w.\\text{completed\\_time})\\)\n' +
      '13:\\(\\theta\\leftarrow\\text{sync}(\\theta,\\ w.\\text{update})\\)\n' +
      '14:\\(\\mathcal{W}_{\\text{completed}}.\\text{add}(w)\\)\n' +
      '15:\\(t_{\\text{ 로컬}}\\) += \\(w.\\text{ 로컬\\_updates}\\)).\n' +
      '16:else\n' +
      '완성된 근로자를 위한 일자리를 보유하고 있다.\n' +
      '18:\\(\\tau_{\\text{sync}}=\\infty\\)\n' +
      '19:\\(\\text{train}(\\mathcal{W}_{\\text{completed}},\\ \\theta)\\)\n' +
      '20:\\(성실{W}_{\\text{{}}\\) = ["\n' +
      '21:endif\n' +
      '22:endwhile\n' +
      '```\n' +
      '\n' +
      '**알고리즘 2** Async. 지역-SGD 태스크 세글링.\n' +
      '\n' +
      '4 Optimization 챌린지 4 Optimization 챌린지.\n' +
      '\n' +
      '최적화가 비동기적인 로컬-SGD의 언어 모델링 성능에 어떻게 영향을 미치는지 연구하기 위해 먼저 SGD+Nrierov, SGD+Adam, AdamW+SGD, AdamW+SGD 모멘텀, AdamW+Adam, AdamW+Nicidesov의 내부 및 외부 최적화기(A+B를 사용하여 내부 및 외부 최적기로 A 및 B를 각각 나타내는지)의 서로 다른 조합을 실험한다. 각각의 조합에 대한 하이퍼파라미터는 AdamW에 대해 InnerOpt로서 디폴트 값을 유지하기 위해 별도로 조정된다. 우리는 장치 속도가 그림 4에 표시된 \\(k=4\\) 작업자가 있다고 가정하며, 초기 모델 체크포인트가 분산 훈련 없이 아담으로 24,000단계를 전처리한 근로자 1인당 64,000단계(총 14만6,000단계)에 대해 20M 매개변수 언어 모델에 비동기적인 로컬-SGD 핀셋링을 적용한다. 지방-SGD 스타일 방법이 핀셋링에서 잘 작동하지만 스크래치(Lin et al, 2018)에서 덜 효율적이라는 것이 관찰되었지만 다른 사람들은 로컬-SGD가 스크래치(Douillard et al, 2023)에서 훈련에도 잘 작용한다는 것을 관찰했기 때문에 로컬-SGD로 핀셋링을 선택한다. 학습률 스케줄링 및 작업 스케줄링은 제3절에서 설명한 절차를 따른다. 우리는 기본으로 모든 실험에서 모든 작업자에 걸쳐 내부 단계(\\(=50\\)를 사용한다. 결과는 그림 5에 나와 있다.\n' +
      '\n' +
      '분석 결과, AdamW를 내부 최적자로 하여 노스테로프 운동량을 외부 최적기로 결합하면 디LoCo 방법과 같이 동기 훈련의 결과와 일치하는 최상의 결과가 도출된다는 것을 알 수 있다. 특히, AdamW를 외부 최적화기로 사용하는 것은 덜 효과적이다. 아담에서 유래한 애덤W가 정규화 효과를 도입하여 상쇄할 수 있기 때문일 것이다.\n' +
      '\n' +
      '그림 4: 각 장치에 대해 초당 스톤입니다.\n' +
      '\n' +
      'Figure 5: Performance of using different combinations of inner and outer optimizers for asynchronous Local-SGD training on a 20M language model with 4 workers.\n' +
      '\n' +
      '의사 등급은 실제 구배보다 더 크며 잠재적으로 수렴이 둔화되는 경향이 있다. 아담W가 내부 최적화(t_{\\{server}\\)에 사용될 때, 아담W가 내부 최적화, SGDS GDM 오멘텀, ndN 에스테프가 어떻게 잡힐 수 있는지 알 수 있다.\n' +
      '\n' +
      '<타_{t <\\\\>} (}\\beta^{2} {t})\\{t} (1+\\beta)\\.\n' +
      '\n' +
      '아이티(\\epsilon\\)가 학습률인 경우 \\(m_{t}\\)는 운동량, \\(g_{t}\\)는 시간 \\(t\\), \\(\\beta\\in(0,1)의 구배이다. 네스테로프와 SGD 모멘텀의 주요 차이점은 네스테로프가 가중치를 조정하여 \\(\\beta\\)) 대신 운동량 성분(\\(\\(\\beta^{2}\\)을 감소시키고 \\(1\\) 대신 구배 성분(\\(1+\\beta\\)을 증가시키는 방법에 있다. 이것은 운동량이 지방-SGD에서 중요하면서도 복잡한 역할을 한다는 것을 시사한다.\n' +
      '\n' +
      '외측 최적기에 대한 운동량항의 모멘텀은 외부 최적기에 대한 모멘텀항의 영향에 더 깊이 정착하기 위해 동기 및 비동기 훈련 환경 모두에서 AdamW+SGD와 AdamW+Nrierov 사이의 비교 분석을 수행했다. 이러한 실험은 앞서 설명한 것과 동일한 조건에서 수행되었다. 결과는 그림 6에 보고되어 있다.\n' +
      '\n' +
      'ObservationThe figure clearly shows that in asynchronous Local-SGD, AdamW+SGD, which lacks a momentum term, leads to better final perplexity and learning efficiency than its synchronous counterpart. However, incorporating Nesterov momentum into the OuterOpt significantly boosts the performance of synchronous Local-SGD, outperforming the asynchronous version. It\'s noteworthy that asynchronous AdamW+Nesterov remains the best performer across all tested combinations of inner and outer optimizers (as seen in Figure 5). This observation indicates that while momentum is beneficial in asynchronous Local-SGD for language modeling, its effect is more pronounced in synchronous settings.\n' +
      '\n' +
      '2.6260,81502.13449753의 아스피린크 헤르페스 오세미, 3.38n개의 리포마틴 다발, 1/3의 비육종, 1/3의 기암종, 1/3의 무화과, 1/3의 성조란, 1/3의 무제비, 1-1의 자양반, 1. 이것은 나 노메니스를 추리하는 동안 rforlan 구.\n' +
      '\n' +
      'ObservationFigure 7 reveals a notable finding: even with homogeneity among workers, asynchronous DiLoCo significantly lags behind its synchronous counterpart. This suggests that the _inherent staleness_ from sequentially applying simultaneous updates leads to considerable performance drops. To elucidate this effect, let\'s consider a scenario with \\(k=4\\) workers providing identical outer gradients (denoted as \\(g\\)). The standard Nesterov momentum update is outlined in Equation (4). In a sequential application of pseudo-gradients:\n' +
      '\n' +
      '\\[\\begin{split} m_{t+1}&=\\beta^{4}m_{t}+(1+\\beta+ \\beta^{2}+\\beta^{3})g\\\\ \\theta_{t+1}&=\\theta_{t}-\\epsilon\\big{(}(4+4\\beta +3\\beta^{2}+2\\beta^{3}+\\beta^{4})g\\\\ &\\quad+\\beta^{2}(1+\\beta+\\beta^{2}+\\beta^{3})m_{t}\\big{)}.\\end{split} \\tag{5}\\]\n' +
      '\n' +
      'From this, we observe that sequential application results in a more rapidly decaying momentum term but amplifies the actual change in parameter \\(\\theta\\). Consequently, a higher \\(\\beta\\) maintains more recent momentum but may lead to greater changes in parameters, and vice versa. Importantly, this imbalance cannot be simply rectified by reducing the learning rate.\n' +
      '\n' +
      '우리는 문헌의 여러 동기적 기저부와 비동기적 설정에 대한 순진한 응용: **1)*** Finetune 1 근로자(4xbatch): 이것은 동기 SGD와 동일하고 더 큰 배치 크기를 가진 단일 작업자를 고용하는 것을 포함한다. 이 동기적 지방-SGD 방법은 AdamW와 네스테로프의 ***3)**D 방법을 결합한 것이다. * Async. 디LoCo: 디LoCo의 비동기 버전입니다.\n' +
      '\n' +
      '기존의 Fixes는 관찰된 문제를 해결하기 위해 비동기 로컬-SGD 문헌에서 잠재적인 고정을 조사했다. 다음 방법은 **1)** Async로 간주되었다. DiLoCo + 폴리(Xie et al., 2019): Async를 계산합니다. 디LoCo는 \\(g\\lelearrow(1+\\text{staleness})^{-0.5}g\\)를 사용하여 유사 업그레이드를 하향 가중하여 **2)**Async. DiLoCo + 폴리-Thres: 10. **3)*^{-0.5}g\\ 이상으로 스테이플리티로 업데이트를 폐기하는 임계값을 제공한다. * Async. 디LoCo + 델레이 화합물(정 등 2017): 생산 지연 보상(지연 화합물)은 진정한 의사 등급에 근사한다. 1차 테일러 근사치(g(\\theta_{t}})\\ a\\(\\theta_{t})\\ 기울기 g(\\theta_{t}})를 사용하여\\(\\theta_{t})\\ 기울기 g(\\theta_{t})\\ a\\<\\<\\)\\ a\\(\\theta_{t})\\<\\<\\<\\)\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff (\\nabla_{\\mathcal{B}},\\nabla_{\\mathcal{B}})\\o타_{\\prime}}(\\theta_{\\prime}})\\ododot g(\\theta_{\\prime})\\\\odot g(\\nabla_{\\prime})\\odot g(\\theta_{\\prime})\\odot g(\\theta_{\\prime})\\odot g(\\nathime})\\ododot g(\\ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff * Async. 버퍼: 네스테로프 업데이트를 적용하기 전에 첫 번째 외부 패션에서 모든 구배를 측정하고 평균하며, 애덤W+네스테로프를 사용하여 원래 페데버프 알고리즘(Nguyen et al, 2022)의 변화이다. 결과는 그림 8에 나와 있다.\n' +
      '\n' +
      'ObservationPolynomial discounting of the pseudo-gradient shows marginal benefits. Thresholding and delay compensation techniques don\'t offer much improvements. Again, the fact that delay compensation is not working well points out the difference between asynchronous SGD and asynchronous Local-SGD. The Async. Buffer method excels at convergence but exhibits instability early in training. Crucially, _none_ of the methods match the performance of the synchronous DiLoCo baseline.\n' +
      '\n' +
      'Figure 8: Comparison of different asynchronous Local-SGD approaches. Poly, PolyThres, and Delay Comp. barely improve the async. Local-SGD performance. Async. Buffer significantly closes the gap between sync. and async. training, while introducing instability in early stage of training.\n' +
      '\n' +
      '5개는 솔루션입니다.\n' +
      '\n' +
      '4절에서 요약된 최적화 과제를 해결하기 위해 우리는 두 가지 전략을 개발했다.\n' +
      '\n' +
      '특히, 누에스테로프 업데이트를 지연시킨다. 버퍼 방법은 유망한 성능(그림 8과 같이)을 보여주었다. 또한, 우리의 분석에서는 AdamW+SGD, sans 외부 운동량, 동기적 방법(그림 5)과의 비동기적 훈련이 동기적 방법을 능가한다는 것을 보여주었다. 이러한 통찰력을 기반으로 알고리즘 2에서 동기() 기능을 나타내는 _지연된 네스테로프_(DN) 전략을 제안하며, 이 접근법은 간헐적으로-매우 \\(N\\) 서버 업데이트를 사용하는 것을 포함한다. 네스테로프 업데이트 사이에서 우리는 완충액 \\(\\Delta\\)에서 유사 업그레이트를 집계하고 기울기 하강(또는 구배 하강 및 오래된 운동량의 작은 부분)을 사용하여 모델 파라미터를 업데이트한다. 구배와 운동량 기반 하강성을 균형을 맞추기 위해 매개변수 \\(c\\in[0,1/N]\\)를 소개한다. Hf(0\\)의 \\(c\\) 값은 네스테로프 업데이트 간에 순수한 구배 하강도를 나타내는 반면, \\(1\\)와 동일한 \\(c\\)는 \\(N\\) 업데이트보다 운동량항을 고르게 분배한다. 이 알고리즘의 구체적인 내용은 알고리즘 3에 자세히 설명되어 있으며, Async와 같다. N\\(N\\) 기간에 한 번만 모델 파라미터를 업데이트하는 버퍼(Nguyen et al, 2022)는 구배를 사용하여 계속 업데이트하고, 오래된 운동량의 일부를 통합하고, 모든 \\(N\\) 서버 업데이트에서 한 번 운동량을 업데이트한다.\n' +
      '\n' +
      '```\n' +
      '0: Initial model parameter \\(\\theta_{0}\\)\n' +
      '0: 엄마 부패 \\ (0,1)\n' +
      '0: Momentum activation \\(c\\in[0,1/N]\\) \\(\\triangleright\\) default to \\(c=0\\)\n' +
      '0: Buffer 크기 \\(m_{0}=0\\) \\(m_{0\\) \\(\\triangleright\\) 운동량 \\(\\Delta=0\\) \\(\\triangleright\\) 응집 구배를 완료하지 않고 유사 등급 \\(g_{t}\\) \\(\\triangleright\\) \\(\\ a\\) \\(m_{0\\) \\(\\,\\) \\(\\,\\) \\(\\,\\) \\(\\,\\) \\(\\-Ex(\\ -0\\) \\(\\,\\) \\(g_{t}\\) \\(\\) \\(g_{t}\\) \\(\\) \\(\\) \\(g_{tincangleright\\) \\(\\) \\(\\) \\(\\,\\ \\(g_{tobacteriumleright\\) \\(\\,\\) \\(\\,\\,\\,\\) \\(g_{triang 다른 \\(m_{t+/\\)\\(m_{t+}{t/\\)\\(mm_{t+}<\\-d\\) <\\<<\\<<\\<<\\<<\\<<<<<<<<<<<<<<<<<<<<<<<>>>> < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <>>>>>> < <\\>>> < <\\>>> < <\\>> < <\\>>> < <\\>> < <\\>> < <\\>> < <\\>>> < <\\>> < <\\>>> < <\\>> < <\\>> < <\\>> < <\\>>> < <\\>> < <\\>>> < < <\\>>>\n' +
      '```\n' +
      '\n' +
      '**알고리즘 3** 딜레이드 네스테로프 업데이트입니다.\n' +
      '\n' +
      '다이나믹 로컬 업데이트 델레이 네스테로프 접근법은 의사 업그레이트를 완충하고 전략적으로 모멘텀 업데이트를 지연시켜 아웃에오폼의 모멘텀 챌린지를 다룬다. 대안적 관점은 동기적 훈련을 모든 근로자의 의사 등급들이 동기화되는 해결책으로 간주한다. 그러나 장치 역량의 다양성은 종종 각 작업자가 동일한 수의 지역 훈련 단계를 실행하면 동시 의사 업그레이드를 방해한다. 생존 가능한 작업은 각 장치의 처리 속도를 기반으로 로컬 트레이닝 단계(예를 들어, \\(w\\)를 맞춤화하는 것을 포함한다. 특히 근로자 \\(초당 단계)의 훈련 속도(초당 단계)로서 \\(v(w)\\를 의미하는데, 근로자의 원하는 훈련 단계를 그대로 계산할 수 있다.\n' +
      '\n' +
      '텍스트{ 단계}(w)}{\\frac{v(w)}{\\frac{v({\\frac{v:{w^{{\\prime}}, \\tag{6}\\]\\]\\[w.\n' +
      '\n' +
      'H\\(H\\)는 가장 빠른 작업자 실행의 수를 나타내고 \\(차층 x\\rfloor\\)는 \\(x\\)보다 크지 않은 가장 큰 정수(DyLU)를 나타낸다. 이러한 조정은 더 느린 작업자가 더 적은 단계를 실행하여 다른 작업자에 걸쳐 완료 시간을 정렬할 수 있게 한다. 이 설정에서 모델 동기화를 위한 유예 기간을 통합하는 것은 스테이플 구배의 영향을 더욱 감소시켜 전반적인 성능을 향상시킵니다.\n' +
      '\n' +
      '3: 여기, 우리는 장치 속도가 선험적으로 알려져 있다고 암묵적으로 가정한다. 만약 이것이 그렇지 않다면 경험적 관측에 기초하여 장치 속도를 추정하는 것은 간단하다.\n' +
      '\n' +
      '6개의 최소 도예.\n' +
      '\n' +
      '향후 연구의 편의성과 새로운 아이디어의 신속한 프로토타이핑의 편의를 위해 관찰된 최적화 도전을 비동기적인 로컬-SGD(그림 9.4)에서 복제하는 최소한의 장난감 예를 제시하며 가우시안 데이터의 혼합물의 혼합물에 대한 분류를 수행하는 것이 과제이다.\n' +
      '\n' +
      '부츠 4: [https://github.com/google-deepmind/asyncdiloco] (https://github.com/google-deepmind/asyncdiloco)에서 콜랩을 확인해 주세요.\n' +
      '\n' +
      '그림 9와 그림 6을 비교한 관찰 결과는 장난감 예시가 동일한 최적화 도전을 나타낸다는 것을 관찰하였다.\n' +
      '\n' +
      '## 7 Experiments\n' +
      '\n' +
      '이 섹션 세부 실험은 두 가지 제안된 방법인 델레이 네스테로프(DN) 및 다이나믹 로컬 업도트(DyLU)의 효능을 평가하기 위해 수행되었다. 또한, 절제 연구는 근로자 수와 모델 크기가 다양하기 때문에 이러한 방법의 효과를 탐구한다.\n' +
      '\n' +
      'Evaluating Delayed Nesterov (DN) and Dynamic Local Updates (DyLU)Figure 2 compares asynchronous Local-SGD with DN and DyLU against baselines such as single worker finetuning and DiLoCo, using the same setup as in Figure 8.\n' +
      '\n' +
      'ObservationThe results demonstrate that DN combined with DyLU significantly reduces perplexity, surpassing the synchronous DiLoCo\'s performance over updates. Additionally, DN+DyLU outperforms in terms of time efficiency, avoiding delays from slower workers.\n' +
      '\n' +
      'Assessing Different Levels of Worker HeterogeneityWe examine how both the proposed DN+DyLU method and vanilla asynchronous DiLoCo fare under varying degrees of worker device heterogeneity, as shown in Figure 10 (perplexity curve) and Table 1 (final perplexity).\n' +
      '\n' +
      '관찰DN+DyLU는 균질한 장치에도 불구하고 모든 이질성 수준에서 일관되게 탁월하며 바닐라 비동기 디LoCo 투쟁은 문제가 부분적으로 유사 조사자의 순차적 적용에 있음을 시사한다. 이는 지연 모멘텀 업데이트의 중요성을 강조한다. 또한, 특정 디바이스 그룹화에서 성능에서의 주기적인 진동이 관찰되어 원래 비동기 알고리즘의 견고성 부족을 더욱 강조한다.\n' +
      '\n' +
      '5: Async가 있음을 알립니다. DN+DyLU는 이질성이 없을 때 디LoCo보다 약간 더 잘 수행하는데, 이는 두 가지 방법이 동일하게 감소하고 훈련 곡선이 거의 완벽하게 일치하기 때문에 수치 오류 때문이다.\n' +
      '\n' +
      '근로자의 차별적인 무기력으로 재구성하면 20M 모형을 이용하여 근로자 수(4, 8, 16)를 변화시키면서 DN+DyLU를 적용하는데, 그 결과는 그림 11(각도 곡선)과 표 2(최종 각성)에 요약되어 있다.\n' +
      '\n' +
      '근로자가 증가함에 따라 지역-SGD 훈련의 편익은 감소한다. 특히 근로자 16명(16x 배치 크기)과 함께 1인 근로자 고용(16x 배치 크기)이 가장 우수한 실적을 보이고 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Level of heterogeneity & no & slight & moderate & very \\\\ \\hline Pretrained (24K) & 61.64 & 61.64 & 61.64 & 61.64 \\\\ Finetune (4\\% batch size) & 42.47 & 42.47 & 42.47 & 42.47 \\\\ DiLoCo (Doillard et al., 2023) & 41.35 & 41.35 & 41.35 & 41.35 \\\\ Async. DiLoCo & 44.27 & 44.38 & 44.29 & 44.27 \\\\ Async. DN + DyLU (ours) & **41.27** & **41.09** & **41.13** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 1>은 근로자 이질성 수준 (**top-left***, ***top-right****, **bottom-left******, ***slight**********, ***ion********************와 같다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Number of workers \\(k\\) & 4 & 8 & 16 \\\\ \\hline Pretrained (24K) & 61.64 & 61.64 & 61.64 \\\\ Finetune (\\(k\\times\\) batch size) & 42.47 & 41.28 & **40.60** \\\\ DiLoCo (Douillard et al., 2023) & 41.35 & 41.23 & 41.25 \\\\ \\hline Async. DiLoCo & 44.27 & 44.23 & 44.23 \\\\ Async. DN + DyLU (ours) & **41.13** & **41.02** & 40.98 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 2>는 근로자 수를 가리는 것이다.\n' +
      '\n' +
      '그림 9: 장난감 예에 대한 최적화 챌린지를 복제하세요. **Left**: 데이터 세트는 가우스의 혼합물의 혼합물로 구성된다. ** 우측**: 아sync. 지방-SGD는 동기보다 더 나쁨/베터를 수행한다. AdamW+Nrierov/AdamW+SGD를 사용할 때 국소-SGD를 사용한다.\n' +
      '\n' +
      'updates. Yet, DN+DyLU closely aligns with synchronous DiLoCo in performance, demonstrating its potential as a DiLoCo alternative in heterogeneous settings.\n' +
      '\n' +
      'Ablation with Different Model SizesLastly, we apply DN+DyLU to models of varying sizes (20M, 60M, 150M), with results summarized in Figure 12 (perplexity curve) and Table 3 (final perplexity).\n' +
      '\n' +
      '동기적 및 비동기적 로컬-SGD 방법을 모두 관찰하는 것은 배치 크기가 증가된 단일 작업자를 고용하는 접근법을 능가한다. 특히, 이러한 이점은 융합의 후기 단계에서 더욱 두드러지게 되며, 이는 지역-SGD의 우수한 일반화 능력(Gu et al., 2023)을 강조하는 이전 연구의 결과와 일치한다. 또한 제안된 DN+DyLU 방법은 다양한 모델 크기에 걸쳐 일관된 효능을 보여준다. 동기식 디LoCo와 비동기식 디LoCo 사이의 성능 차이는 모델 크기가 증가함에 따라 감소하지 않는다는 점에 유의하는 것이 중요하다.\n' +
      '\n' +
      'Ablation with Different \\(c\\)We apply \\(c\\in\\{0,0.1\\}\\) in Async. DN+DyLU with varying \\(k\\) (4, 8, 16) and model sizes (20M, 60M, 150M), with the 4 "very" heterogeneous workers. This is because when the level of heterogeneity is small, using different \\(c\\) will have smaller difference (e.g., when there is no heterogeneity, any \\(c\\) results in the same algorithm). Results are summarized in Table 4.\n' +
      '\n' +
      'S. Latv aluei nA Ld알고리즘3(c=0\\) 또는 이에 대응한 옵소프 S GDu pdimimS GDdvering S GDdatpdat)는 더 많은 것을 의미 없이 관찰한다.\n' +
      '\n' +
      '8은 특정 일꾼입니다.\n' +
      '\n' +
      '이 섹션은 특히 비동기 환경에서 응용 프로그램에 초점을 맞춘 피빙된 학습과 지역-SGD 스타일의 분산 최적화에 대한 문헌의 간결한 개요를 제공한다.\n' +
      '\n' +
      '지역-SGD 및 분산 최적화 로컬-SGD는 통신 빈도를 줄이기 위해 설계된 특정 분산 최적화 기법(Bijral et al., 2016; Coppola, 2015; McDonald et al., 2010; Stich, 2018; Zhang et al., 2016; Zinkevich et al., 2010)이다. 지방-SGD의 핵심 원리는 각 작업자가 글로벌 동기화에 참여하기 전에 여러 지역 훈련 반복을 실행할 수 있도록 하는 것이다. 이 기술은 이후 연방 학습 설정에 적용되어 통신 오버헤드를 줄이는 것을 목표로 하는 FedAvg 방법(McMahan et al, 2017)의 개발로 이어졌다. 지역-SGD와 달리 피드백된 학습은 또한 사용자 프라이버시 문제를 해결하고 일반적으로 이질적인 장치를 포함한다. 통신 오버헤드를 더욱 최소화하기 위해 FedOpt는 SGD 운동량과 Adam(Reddi et al., 2020)과 같은 적응 최적화 방법을 통합한다. 그러나 고객/근로자의 이질성이 증가할수록 융합률이 저하되는 경우가 많다. SCAFFOLD(카리메디 et al., 2020) 및 MIME(카리메디 et al., 2021)와 같은 방법이 이질적인 환경에 대한 이러한 최적화 방법을 적응시키기 위해 도입되었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Number of workers \\(k\\) & 4 & 8 & 16 \\\\ \\hline Async. DN + DyLU (\\(c=0\\)) & **41.13** & 41.02 & **40.98** \\\\ Async. DN + DyLU (\\(c=0.1\\)) & 41.16 & **40.93** & 41.04 \\\\ \\hline Model size & 20M & 60M & 150M \\\\ \\hline Async. DN + DyLU (\\(c=0\\)) & **41.13** & **24.53** & **17.26** \\\\ Async. DN + DyLU (\\(c=0.1\\)) & 41.16 & 24.69 & 17.27 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 알고리즘 3에서 \\(c\\in\\{0,0.1\\}\\)를 변경하는 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Model size & 20M & 60M & 150M \\\\ \\hline Pretrained (24K) & 61.64 & 30.19 & 22.80 \\\\ Finetune (4x batch size) & 42.47 & 24.80 & 17.47 \\\\ DiLoCo (Douillard et al., 2023) & 41.35 & 24.55 & **17.23** \\\\ \\hline Async. DiLoCo & 44.27 & 25.64 & 18.08 \\\\ Async. DN + DyLU (ours) & **41.13** & **24.53** & 17.26 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: | Varying 모형 크기를 나타낸다.\n' +
      '\n' +
      '학습 효율이 가장 느린 작업자에 의해 병목화되는 동기 분산 최적화(Dean et al, 2012, Diskin et al, 2021, Koh et al, 2006, Lian et al, 2015, 2018, Recht et al, 2011)에서 관찰된 "스트레글러 효과"를 완화하기 위해 동기 훈련 비동기 교육이 개발되었다. 비동기 최적화에서 중요한 과제는 단단한 구배 문제이며, 이는 최근에 업데이트된 모델에 오래된 기울기가 적용될 때 발생한다. 지연 보상(정 등 2017)이 있는 근사 동기 SGD로 이 문제를 해결한다.\n' +
      '\n' +
      'Figure 11: Varying the number of workers.\n' +
      '\n' +
      '그림 10: 기기의 이질성을 가리는 것이다.\n' +
      '\n' +
      '오래된 구배를 사용하여 진정한 구배를 만들어 보세요. 비동기식 방법은 또한 연방 학습 맥락(Xie et al., 2019)에서 탐구되었다. 도전에도 불구하고 비동기 훈련은 전 세계적으로 이질적인 장치를 사용하여 언어 모델링(디스킨 등, 2021)에 대한 성공을 입증했다.\n' +
      '\n' +
      'Local-SGD for Language ModelingThe concept of local-SGD (or FedAvg) has previously been applied in the realm of language modeling. Cross-device federated learning, for instance, has been utilized to pretrain and fine-tune language models (Borzunov et al., 2022; Diskin et al., 2021; Hilmkl et al., 2021; Presser, 2020; Ro et al., 2022; Ryabinin et al., 2021). More recently, DiLoCo has extended the local-SGD methodology to encompass larger language models, specifically proposing the use of AdamW + Nesterov momentum as the InnerOpt + OuterOpt pairing. In asynchronous settings, the FedBuff (Nguyen et al., 2022) algorithm buffers pseudogradients from clients, updating the server model only after accumulating a sufficient number of pseudogradients. TimelyFL (Zhang et al., 2023) aims to reduce asynchrony by allowing slower devices to train only parts of the model.\n' +
      '\n' +
      '## 9 Limitations\n' +
      '\n' +
      'This study, while comprehensive, has several limitations. First, we identify a significant optimization challenge linked to momentum updates in the OuterOpt, but the precise cause of this issue remains unclear. Understanding this challenge with robust theoretical backing presents an intriguing avenue for future research. Second, our empirical observations suggest that the advantages of the Local-SGD method diminish with an increasing number of workers, a phenomenon whose underlying reasons are yet to be understood. This issue currently hinders the scalability of asynchronous Local-SGD. Finally, although our proposed method DN+DyLU shows improved empirical performance, it lacks formal theoretical convergence guarantees, an aspect that merits further investigation.\n' +
      '\n' +
      '## 10 Conclusion\n' +
      '\n' +
      '이 연구는 언어 모델링에서 비동기적 지방-SGD에 대한 철저한 검사를 제시한다.\n' +
      '\n' +
      'Figure 12: Varying the model size.\n' +
      '\n' +
      '우리의 중심 발견은 외부 최적화 루프의 운동량이 중요하지만 순진한 구현 시 동기 시나리오에 비해 비동기 시나리오에서 덜 효과적일 수 있다는 것이다. 이러한 격차를 해소하기 위해 지속적인 확률적 유사 방사선 업데이트와 결합된 완충된 유사 조사자를 사용한 산발적인 운동량 업데이트를 중심으로 새로운 접근법을 소개한다. 또한, 우리의 연구는 각 작업자의 계산 속도에 대한 지역 훈련 단계를 테일링하는 것이 간단할 뿐만 아니라 성능을 향상시키는 효율적인 전략임을 보여준다.\n' +
      '\n' +
      '하지만 해야 할 일이 많습니다. 표준("지역" 구배 하강 설정과 반대되는 것)에서 체중 업데이트의 수 측면에서 가능한 한 빨리 손실 감소 측면에서 최적의 배치 크기는 보통 "가능한 한 큰 것"이 아니다. 우리의 견해에서도 마찬가지로 동기적인 로컬-SGD보다 지역 업데이트당 더 나은 결과를 제공하는 비동기적인 로컬-SGD 방법에 대한 희망이 있다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '아담 피쉬의 소중한 피드백에 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* S Bijral et al. (2016) Avleen S Bijral, Anand D Sarwate, and Nathan Srebro. On data dependence in distributed stochastic optimization. _arXiv preprint arXiv:1603.04379_, 2016.\n' +
      '* Borzunov et al. (2022) Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem Chumachenko, Pavel Samygin, and Colin Raffel. Petals: Collaborative inference and fine-tuning of large models. _arXiv preprint library_, 2022.\n' +
      '* Coppola (2015) Gregory Francis Coppola. Iterative parameter mixing for distributed large-margin training of structured predictors for natural language processing. 2015.\n' +
      '* Dean et al. (2012) Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc\'aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. _Advances in neural information processing systems_, 25, 2012.\n' +
      '* Diskin et al. (2021a) Michael Diskin, Alexey Bukhtiyarov, Max Ryabinin, Lucile Saulnier, Quentin Lhoest, Anton Sinitsin, Dmitry Popov, Dmitry Pyrkin, Maxim Kashirin, Alexander Borzunov, Albert Villanova del Moral, Denis Mazur, Ilia Kobelev, Yacine Jernite, Thomas Wolf, and Gennady Pekhimenko. Distributed deep learning in open collaborations. _Advances in Neural Information Processing Systems (NeurIPS)_, 2021a.\n' +
      '* Diskin et al. (2021b) Michael Diskin, Alexey Bukhtiyarov, Max Ryabinin, Lucile Saulnier, Anton Sinitsin, Dmitry Popov, Dmitry V Pyrkin, Maxim Kashirin, Alexander Borzunov, Albert Villanova del Moral, et al. Distributed deep learning in open collaborations. _Advances in Neural Information Processing Systems_, 34:7879-7897, 2021b.\n' +
      '* Douillard et al. (2023) Arthur Douillard, Qixuan Feng, Andrei A Rusu, Rachita Chhaparia, Yani Donchev, Adhiguna Kuncoro, Marc\'Aurelio Ranzato, Arthur Szlam, and Jiajun Shen. Diloco: Distributed low-communication training of language models. _arXiv preprint arXiv:2311.08105_, 2023.\n' +
      '* Gu et al. (2023) Xinran Gu, Kaifeng Lyu, Longbo Huang, and Sanjeev Arora. Why (and when) does local sgd generalize better than sgd? _arXiv preprint arXiv:2303.01215_, 2023.\n' +
      '* Hilmkil et al. (2021) Agrin Hilmkil, Sebastian Callh, Matteo Barbieri, Leon Rene Suffeld, Edvin Listo Zec, and Olof Mogren. Scaling federated learning for fine-tuning of large language models. In _International Conference on Applications of Natural Language to Information Systems_, pages 15-23. Springer, 2021.\n' +
      '크실 t, Leo nReneS uffeld,EdvinLis에서Zec으로, Leo nReneS.EdvinLis, LeofMogre n.Scal은 인공 ningf 또는fine-tunin goflar 겔a nguag emodel.에서_ Inter na talCon fere nceo nApplicati.\n' +
      '\n' +
      '훈련은 계산하는 최적의 대형 언어 모델 __ 최적이 아닌 대형 언어 모델. 신경정보처리시스템(NeurIPS)_ 2022년 개발.\n' +
      '* Karimireddy et al. [2020] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In _International conference on machine learning_, pages 5132-5143. PMLR, 2020.\n' +
      '* Karimireddy et al. [2021] Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian U Stich, and Ananda Theertha Suresh. Breaking the centralized barrier for cross-device federated learning. _Advances in Neural Information Processing Systems_, 34:28663-28676, 2021.\n' +
      '* Koh et al. [2006] Byung-Il Koh, Alan D George, Raphael T Haftka, and Benjamin J Fregly. Parallel asynchronous particle swarm optimization. _International journal for numerical methods in engineering_, 67(4):578-595, 2006.\n' +
      '* Lian et al. [2015] Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic gradient for nonconvex optimization. _Advances in neural information processing systems_, 28, 2015.\n' +
      '* Lian et al. [2018] Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic gradient descent. In _International Conference on Machine Learning_, pages 3043-3052. PMLR, 2018.\n' +
      '* Lin et al. [2018] Tao Lin, Sebastian U Stich, Kumar Khitij Patel, and Martin Jaggi. Don\'t use large mini-batches, use local sgd. _arXiv preprint arXiv:1808.07217_, 2018.\n' +
      '* Lin et al. [2020] Tao Lin, Sebastian U. Stich, Kumar Khitij Patel, and Martin Jaggi. Don\'t use large mini-batches, use local sgd. _Proceedings of the International Conference on Learning Representations (ICLR)_, 2020.\n' +
      '* McDonald et al. [2010] Ryan McDonald, Keith Hall, and Gideon Mann. Distributed training strategies for the structured perceptron. In _Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics_, pages 456-464, 2010.\n' +
      '* McMahan et al. [2017] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR, 2017.\n' +
      '* Nguyen et al. [2022] John Nguyen, Khitiz Malik, Hongyuan Zhan, Ashkan Yousefpour, Mike Rabbat, Mani Malek, and Dzmitry Huba. Federated learning with buffered asynchronous aggregation. In _International Conference on Artificial Intelligence and Statistics_, pages 3581-3607. PMLR, 2022.\n' +
      '* 프레스러[2020] 전당포 프레스러. 2020년도 URL[https://barm.com/swarm-트레이닝-v01a.pdf](https://battlen.com/swarm-트레이닝-v01a.pdf) 훈련.\n' +
      '* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 2020.\n' +
      '* Recht et al. [2011] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. _Advances in neural information processing systems_, 24, 2011.\n' +
      '* Reddi et al. [2020] Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. _arXiv preprint arXiv:2003.00295_, 2020.\n' +
      '* Ro et al. [2022] Jae Hun Ro, Theresa Breiner, Lara McConnaughey, Mingqing Chen, Ananda Theertha Suresh, Shankar Kumar, and Rajiv Mathews. Scaling language model size in cross-device federated learning. _arXiv preprint arXiv:2204.09715_, 2022.\n' +
      '* Ryabinin et al. [2021] Max Ryabinin, Eduard Gorbunov, Vsevolod Plokhotnyuk, and Gennady Pekhimenko. Moshpit sgd: Communication-efficient decentralized training on heterogeneous unreliable devices. _Advances in Neural Information Processing Systems_, 34:18195-18211, 2021.\n' +
      '\n' +
      '* Stich (2018) Sebastian U Stich. Local sgd converges fast and communicates little. _arXiv preprint arXiv:1805.09767_, 2018.\n' +
      '* Xie et al. (2019) Cong Xie, Sanmi Koyejo, and Indranil Gupta. Asynchronous federated optimization. _arXiv preprint arXiv:1903.03934_, 2019.\n' +
      '* Zhang et al. (2016) Jian Zhang, Christopher De Sa, Ioannis Mitliagkas, and Christopher Re. Parallel sgd: When does averaging help? _arXiv preprint arXiv:1606.07365_, 2016.\n' +
      '* Zhang et al. (2023) Tuo Zhang, Lei Gao, Sunwoo Lee, Mi Zhang, and Salman Avestimehr. Timelyfl: Heterogeneity-aware asynchronous federated learning with adaptive partial training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5063-5072, 2023.\n' +
      '* Zheng et al. (2017) Shuxin Zheng, Qi Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhi-Ming Ma, and Tie-Yan Liu. Asynchronous stochastic gradient descent with delay compensation. In _International Conference on Machine Learning_, pages 4120-4129. PMLR, 2017.\n' +
      '* 장테탈. (202 3) 토오Z 매달림, L iGao, 선우 오이, M iZhang 및 살맨 아브 자극 ehr.Ti melyfl: 적응성 에피타티탈트리가 있는 한터 오진-시타신 연대기 침출.\n' +
      '\n' +
      '## Supplementary Materials\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '네트워크 아키텍처는 20M, 60M, 150M 모델의 건축적 차이를 표 6에 나타냈다. 모두 차치닐라 계열(Hoffmann et al, 2022)을 기반으로 한 변압기 디코더 전용입니다.\n' +
      '\n' +
      '우리는 공통 크로슬(Raffel et al., 2020)에서 파생된 데이터세트인 C4 데이터셋에 대한 언어 모델링 작업을 고려한다. 총 단계 수는 모든 모델에 대해 88,000단계로 설정되며, 사전 훈련은 어떤 피드백된 학습 방법 없이 24,000단계, 즉 _포스트 로컬-SGD_(Lin et al., 2020)와 유사하다.\n' +
      '\n' +
      '표 5에서 우리는 이 연구를 위해 고려된 최적화 하이퍼모수들을 윤곽으로 설명한다.\n' +
      '\n' +
      'Douillard et al.(2023)에 이어 인너 옵티미이저 국가는 모든 실험에서 근로자 B가 데이터 샤드 작업자 A를 훈련 종료할 때 AdamW의 최적 상태를 재설정했다. 즉, 각 지역 근로자 측 훈련은 새로운 최적화를 가진 독립적인 훈련 과정이며, 3절에서 설명한 대로 학습률만을 조정한다.\n' +
      '\n' +
      '### Aync. Aocode\n' +
      '\n' +
      '이 절에서는 알고리즘 2에서 기차(O)와 get_worker(O) 기능에 대한 가블록을 제공한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline Hyperparameter & 20M & 60M & 150M \\\\ \\hline Number of layers & 6 & 3 & 12 \\\\ Hidden dim & 256 & 896 & 896 \\\\ Number of heads & 4 & 16 & 16 \\\\ K/V size & 64 & 64 & 64 \\\\ Vocab size & & 32,000 & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '세 가지 평가된 크기에 대한 표 6: ** 모델 구성***이다. 모두 변압기 아키텍처인 키칠라 스타일(Hoffmann et al, 2022)을 기반으로 합니다.\n' +
      '\n' +
      '```\n' +
      '0: 이용 가능한 노동자 \\(\\mathcal{W}\\)\n' +
      '0: 전류 서버 모델 \\(\\ta\\)\n' +
      '1:for\\(w\\in\\mathcal{W}\\)do\n' +
      '2: 삼플 샤드 \\(\\mathcal{D}^{\\prime}\\) \\(Eq. 2)에 대해.\n' +
      '3:\\(w\\) 로컬_updates = DyLU(\\(\\mathcal{D}^{\\prime}\\)) (Eq. 6)이다.\n' +
      '4: 데미드 lr 일정(Eq. 3)\n' +
      '5:\\ (w\\) a\\ = 트레이스_워크러 (\\ (w\\), \\ (\\mathcal{D}^{\\prime}\\), \\ (\\theta\\)\n' +
      '6:endfor\n' +
      '```\n' +
      '\n' +
      '**Algorithm 4** train() in Algorithm 2.\n' +
      '\n' +
      '```\n' +
      '0: Workers \\(\\mathcal{W}\\)\n' +
      '0: Grace period \\(\\tau_{\\text{grace}}\\)\n' +
      '0: 사르토 ft 헵그 레이스프 소거\\(\\tau_{\\text{sync}\\)\n' +
      '1:if all workers in \\(\\mathcal{W}\\) are not done then\n' +
      '2:return null\n' +
      '3:else\n' +
      '4:\\(w\\) = earliest completed worker in \\(\\mathcal{W}\\).\n' +
      '5:if\\(w\\).completed_time - \\(\\tau_{\\text{sync}}\\leq\\tau_{\\text{grace}}\\)then\n' +
      '6:return\\(w\\)\n' +
      '7:else\n' +
      '8:return null\n' +
      '9:endif\n' +
      '10:endif\n' +
      '```\n' +
      '\n' +
      '알고리즘 2에서**알고리즘 5** get_worker()가 된다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>