<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '범용 사용(Krishis et al., 2017), 투명성이 중요한 조작 설정(Krishis et al., 2017), 사회적 규범이 관찰되어야 하는 일상 시나리오(접수원과의 상호작용 등)를 포함하는 목적 및 맥락(Selig et al., 2019). 접근법들은 규칙- 또는 템플릿-기반(Krishis et al., 2017; Goyal et al., 2018; Goyal et al., 2018)일 수 있으며, 이들은 종종 강성 템플릿 또는 규칙들의 세트에 의존하여 행동들을 생성한다. 이것은 종종 표현적일 수 있는 로봇 행동으로 이어지지만, 인간의 선호도의 새로운 양식이나 변형으로 확장되지는 않는다. 반면에 데이터 기반 기술은 유연성과 변화에 적응할 수 있는 능력을 제공한다. 선행 연구에서는 표현적 움직임을 생성하는 데이터 기반 기법(Selig et al., 2019)을 연구하였으나(Selig et al., 2019), 이러한 기법들은 또한 특정 행동이 사용되는 각각의 사회적 상호작용에 대해 전문화된 데이터 세트가 종종 필요하기 때문에(예를 들어, 정의적 로봇 움직임(Selig et al., 2019; Selig et al., 2019)) 그들의 단점을 갖는다.\n' +
      '\n' +
      '우리의 목표는 로봇이 유연한 표현 행동을 생성할 수 있도록 하는 것이다: 인간의 다양한 선호에 대해 _adapt_할 수 있는 행동, 그리고 더 단순한 행동의 _composed_가 될 수 있다. 최근의 연구는 대형 언어 모델(LLM)이 가상(Selig et al., 2019) 및 체화된 에이전트(Selig et al., 2019; Goyal et al., 2018)를 제어하기 위해 코드를 합성할 수 있고, 보상 함수(Krishis et al., 2017; Goyal et al., 2018), 사회적 및 상식적 추론을 가능하게 하거나(Krishis et al., 2017), 또는 상황 내 학습을 통해 제어 및 순차적 의사 결정 작업을 수행(Krishis et al., 2017; Goyal et al., 2018; Goyal et al., 2018)함으로써 프롬프트에서 바람직한 입력 및 출력의 시퀀스를 제공함으로써 가상(Selig et al., 2019; Goyal et al., 2019) 및 체화된 에이전트(Selig et al., 2019; Goyal et al., 2018)를 설계할 수 있음을 보여준다. 우리의 핵심 통찰력은 LLM에서 사용할 수 있는 풍부한 사회적 맥락을 활용하여 적응 가능하고 구성 가능한 표현 행동을 생성하는 것이다. 예를 들어, LLM은 누군가에게 인사할 때 눈을 마주치는 것이 예의라는 것을 깨닫기에 충분한 맥락을 가지고 있다. 또한, LLM은 "팔을 조금 더 구부려!"와 같은 교정 언어 및 그러한 지시에 응답하여 모션을 생성하는 능력을 사용할 수 있게 한다. 이는 LLM을 인간-로봇 상호작용 설정에서 인간 피드백에 유연하게 반응하고 이를 통해 학습하는 표현적 행동을 자율적으로 생성하는 데 유용한 프레임워크로 만든다.\n' +
      '\n' +
      '본 논문에서는 LLM이 제공하는 힘과 유연성을 활용하여 표현 로봇 행동을 자율적으로 생성하기 위한 새로운 접근 방법인 Generative Expressive Motion(GenEM)을 제안한다. GenEM은 수 샷 프롬프트를 사용하고 원하는 표현 행동(또는 사회적 상황)을 언어 명령으로 취하고, 사회적 추론(akin to chain-of-thought(Selig et al., 2019))을 수행하고, 최종적으로 사용 가능한 로봇 API를 사용하여 로봇에 대한 제어 코드를 생성한다. GenEM은 로봇의 의도를 효과적으로 표현하기 위해 로봇의 가용 어포던스(예를 들어, 스피치, 신체 움직임, 및 라이트 스트립과 같은 다른 시각적 특징)를 활용하는 멀티모달 행동을 생성할 수 있다. GenEM의 주요 이점 중 하나는 살아있는 인간의 피드백에 반응한다는 것인데, 반복적인 수정에 적응하고 기존의 것을 구성하여 새로운 표현 행동을 생성한다는 것이다.\n' +
      '\n' +
      '온라인 사용자 연구에서, 우리는 사용자 피드백이 있거나 없는 두 가지 변형(HRI 행동 설계의 비전문가)을 사용하여 이동 로봇에서 생성된 행동을 전문 캐릭터 애니메이터(또는 _oracle animator_)가 설계한 행동 세트와 비교했다. 우리는 GenEM에 의해 생성되고 사용자 피드백에 추가로 적응된 행동이 사용자에 의해 긍정적으로 인식되었고, 어떤 경우에는 오라클 행동보다 더 잘 인식되었음을 보여준다.\n' +
      '\n' +
      '실험 결과, GenEM: (1) 언어 명령어가 코드로 직접 번역되는 버전보다 더 나은 성능을 보였으며, (2) 불가지론적인 행동의 생성을 허용하였으며, (3) 더 단순한 표현 행동을 기반으로 하는 구성 가능한 행동의 생성을 허용하였으며, 마지막으로 (4) 다양한 유형의 사용자 피드백에 적응하였다.\n' +
      '\n' +
      '##2. 관련업무\n' +
      '\n' +
      '**표현적 행동 생성.** 연구원들은 로봇과 가상 인간 모두에게 사회적으로 허용 가능한 행동을 생성하는 데 상당한 노력을 기울였다. 이들은 크게 _rule-based_, _template-based_ 및 _data-driven_(Goyal et al., 2018) 행동 생성 접근법으로 분류될 수 있다. 우리는 규칙 기반 접근 방식을 후속 로봇 동작을 생성하는 데 사용되는 공식화된 규칙 및 작업 세트(일반적으로 사람이 제공한다)를 필요로 하는 접근 방식으로 정의한다.\n' +
      '\n' +
      '규칙 기반 접근법들은 규칙들 및 연산들의 정형화된 세트들을 통해 행동 생성을 가능하게 한다(Krishis et al., 2017). 일부 방법은 사용자가 상호작용 규칙 및 로직을 수동으로 지정할 수 있게 하는 인터페이스를 포함한다(Krishis et al., 2017; Goyal et al., 2018; Goyal et al., 2018; Goyal et al., 2018). 다른 방법들은 인간을 관찰하고 모델링함으로써 작동한다(Goyal et al., 2018; Goyal et al., 2018; Goyal et al., 2018; Goyal et al., 2018). 그 사용에도 불구하고 규칙 기반 접근법은 공식 규칙의 요구로 인해 생성된 행동에 제한된 표현성과 모달리티의 수가 증가함에 따라 멀티모달 행동을 생성하는 능력이 감소하는 등 몇 가지 문제에 직면해 있다(Goyal et al., 2018). 템플릿 기반 방법들은 상호작용 데이터의 트레이스들로부터 학습에 의해 상호작용에 대한 제네릭 템플릿들을 공식화한다(Goyal et al., 2018; Goyal et al., 2018). 템플릿은 프로그램 합성을 통해 인간 흔적의 몇 가지 예를 재사용 가능한 프로그램으로 변환할 수 있다(Krishis et al., 2017; Goyal et al., 2018). 트레이스는 상호작용하는 인간(Selig et al., 2019; Selig et al., 2019)을 관찰함으로써 수집될 수 있거나(Selig et al., 2019), 스케치(Selig et al., 2019) 또는 테이블 상의 유형(Selig et al., 2019)과 같은 접근법을 통해 수집될 수 있다. 전반적으로, 사전 규칙 및 템플릿 기반 방법은 행동 생성을 가능하게 하기 위해 강력한 제약을 적용하지만 표현성이 제한된다. 대조적으로, GenEM은 라이브 사용자 피드백을 통한 반복적 개선뿐만 아니라 초기 행동 생성에서 증가된 표현성을 가능하게 한다.\n' +
      '\n' +
      '반면에 데이터 기반 접근법은 데이터에 대해 학습된 모델을 사용하여 행동을 생성한다. 일부 방법들은 데이터를 통해 _interaction logic_을 학습하고 이를 이용하여 고전적인 기계 학습 방법을 통해 멀티모달 거동을 생성한다(Krishis et al., 2017; Goyal et al., 2018; Goyal et al., 2018). 다른 방법은 생성 모델을 통해 손으로 만든 예를 훈련한다(Goyal et al., 2018; Goyal et al., 2018). 예를 들어, 백채널링 행동의 사용 시기를 예측하는 것(즉, nodding과 같은 대화 중에 피드백을 제공하는 것)은 배치 강화 학습(Goyal et al., 2018) 및 순환 신경망(Selig et al., 2019)을 통해 학습되었다. 마지막으로, 최근 연구는 사용자 피드백으로부터 목표 감정에 대한 비용 함수를 학습하는 방법(Krishis et al., 2017) 또는 많은 감정을 모델링하기 위해 감정적 잠재 공간을 학습하는 방법(Krishis et al., 2017)을 조사했다. 그러나 이러한 접근 방식은 데이터 비효율적이며 행동당 특화된 데이터 세트를 생성해야 하는 반면, GenEM은 상황 내 학습을 통해 몇 가지 예를 들어 다양한 표현 행동을 생성할 수 있다.\n' +
      '\n' +
      '**LMs for Robot Planning and Control.** 최근 작업은 컨텍스트에서 바람직한 입력-출력 쌍들의 시퀀스들을 제공함으로써 다운스트림 로봇 작업들에서 LLMs들을 구체적으로 활용함으로써 큰 성공을 달성하였다(Krishis et al., 2017; Goyal et al., 2018; Goyal et al., 2018). 또한, LLM들은 롱-호라이즌 태스크 계획(Krishis et al., 2017; Goyal et al., 2018)에 사용되어 왔으며, 환경 및 인간 피드백에 반응할 수 있다(Krishis et al., 2018). LLM은 강화 학습 에이전트 훈련에 대한 보상 기능을 설계하기 위해 활용되었다(Krishis et al., 2017; Goyal et al., 2018). 연구에 따르면 LLM은 인간과의 상호작용을 요약함으로써 사용자 선호도를 추론할 뿐만 아니라 사회적 및 상식적 추론(Krishis et al., 2017)을 가능하게 할 수 있다(Krishis et al., 2018). 우리의 접근법과 가장 관련이 있는 것은 LLM이 가상(Selig et al., 2019) 및 로봇 에이전트(Levy et al., 2017; Zhang et al., 2018)를 제어하기 위해 기존의 API를 사용하여 보다 복잡한 로봇 행동을 프로그램으로 구성하는 이전 작업이다. 우리는 또한 온라인에서 로봇 조작 행동을 교정하기 위해 언어를 사용할 수 있다는 것을 보여주는 작업에 의해 장려된다(Beng et al., 2019). 종합하면, 우리는 LLM에서 사용할 수 있는 풍부한 사회적 맥락과 사용자 명령에 적응하는 능력을 활용하여 표현 가능한 로봇 행동을 생성할 것을 제안한다. 우리가 아는 한, LLM은 이전에 사용자 피드백에 적응하는 표현적 로봇 행동을 생성하는 데 사용되지 않았다.\n' +
      '\n' +
      '##3. 생성적 표현동작\n' +
      '\n' +
      '**문제 진술.** 더 복잡한 행동이 더 단순한 행동을 기반으로 구축할 수 있도록 사용자 피드백에 적응적이고 구성 가능한 표현적 행동 생성 문제를 해결하는 것을 목표로 한다. 형식적으로, 우리는 애니메이터(또는 시연)에 의해 생성될 수 있는 어떤 전문가 표현 궤적과 로봇 궤적(\\tau\\) 사이의 거리로 _expressive_라고 정의한다. (\\text{dist}(\\tau,\\tau_{\\text{expert}})\\)는 동적 시간 워핑(DTW)과 같은 두 궤적 사이의 임의의 바람직한 거리 메트릭이 될 수 있다. GenEM은 거리\\(d^{*}=\\min\\text{dist}(\\tau,\\tau_{\\text{expert}})\\)를 최소화하는 것을 목표로 한다.\n' +
      '\n' +
      '우리의 접근법(그림 2)은 모듈 방식으로 여러 LLM을 사용하여 각 _LLM agent_가 뚜렷한 역할을 한다. 이후 실험을 통해 모듈식 접근법이 종단 간 접근법에 비해 더 나은 행동 품질을 산출한다는 것을 입증한다. GenEM은 사용자 언어 명령어 \\(l_{in}\\in}L\\)을 입력으로 하여 매개변수화된 코드 형태인 로봇 정책 \\(\\pi_{\\theta}\\)을 출력한다. 인간의 반복적 피드백(f_{i}\\in L\\)은 정책(\\pi_{\\theta}\\)을 갱신하는데 사용될 수 있다. 정책 매개변수는 피드백 \\(f_{i}\\)이 주어지면 한 번에 한 단계 업데이트되며, 여기서 \\(i\\in\\{1,\\dots,K\\}\\)이다. 이 정책은 S\\(s_{0}\\in S\\)의 초기 상태로부터 표현 로봇 행동의 궤적(\\tau=\\{s_{0},a_{0},\\dots,a_{N-1},s_{N}\\}) 또는 인스턴스화를 생성하기 위해 인스턴스화될 수 있다. 아래에서는 인간 피드백 \\(f_{i}\\)을 사용하여 하나의 샘플 반복을 설명한다. 전체 프롬프트는 **부록 A**를 참조하십시오.\n' +
      '\n' +
      '**표현적 지시 후속.** 우리의 접근법에 대한 입력은 언어 지시 \\(l_{in}\\in L\\)이며, 이는 로봇이 사회적 규범(예를 들어, "당신에게 파도를 타고 걷는 사람")을 따라 표현적 행동을 수행해야 하는 사회적 맥락에 대한 설명일 수 있다. _또는 생성되어야 하는 표현적 행동을 설명하는 지시(예를 들어, "고개를 끄덕여라"). 입력 프롬프트는 \\(u=[h_{pre},l_{in}]\\)의 형태로, \\(h_{pre}\\)은 LLM의 역할에 대한 컨텍스트를 추가하고 몇 개의 샷 예제를 포함하는 프롬프트 프리픽스이다. LLM 호출의 출력은 명령에 대한 응답으로 Chain-of-Thought 추론 \\(h_{cot}\\)(Levy et al., 2017)과 인간의 표현 동작 \\(h_{exp}\\)으로 구성된 \\(h=[h_{cot},h_{exp}]\\) 형태의 문자열이다. 예를 들어, \\(l_{in}=\\)_"사람이 지나가는 것을 승인한다. 말을 할 수 없다."_에 대해, _Expressive Instruction Following_ 모듈은 \\(h_{exp}=\\)_사람과 눈을 마주치게 할 것이다. 그들의 존재를 인정하기 위해 웃거나 비웃지 마라._ \\(h_{cot}\\)의 예는 다음과 같을 수 있다: _"그 사람이 지나가고 있고 그들의 존재를 인정하는 것은 예의 바른 일이다. 나는 말을 할 수 없기 때문에 비언어적 의사소통을 사용해야 한다. 끄덕임이나 미소는 인식의 보편적인 표시이다."\n' +
      '\n' +
      '**Human Expressive Motion에서 Robot Expressive Motion.** 다음 단계에서는 LLM을 이용하여 인간의 표현동작\\(h\\)을 로봇 표현동작\\(r\\)으로 변환한다. 프롬프트는 \\(u=[r_{pre},l_{in},h,r_{i-1_{opt}},f_{i-1_{opt}}]\\의 형태를 취하며, 여기서 \\(r_{pre}\\)은 LLM에 대한 프롬프트 프리픽스 설정 컨텍스트이고, 몇 개의 샷 예를 포함하고, 로봇의 능력들 중 일부는 미리 정의된 것(예를 들어, 머리를 말하거나 움직이는 능력) 및 다른 것들은 이전의 상호작용들(예를 들어, 고개를 끄덕이거나 사람에게 접근하는 것)로부터 학습된다. 선택적으로, 프롬프트는 이전 단계 \\(r_{i-1}\\)로부터의 응답 및 이전 단계 \\(f_{i-1}\\)로부터의 사용자 반복 피드백에 대한 응답을 포함할 수 있다. 출력은 LLM 추론과 표현 로봇 동작 생성 절차로 구성된 \\(r=[r_{cot},r_{exp}]\\)의 형태이다. 예제 응답 \\(r_{exp}\\)은 다음을 포함할 수 있다: _"I) 지나가는 사람을 대면하기 위해 헤드의 팬 및 틸트 능력을 사용한다. 2) 미소 또는 노드를 모방하는 미리 프로그래밍된 패턴을 디스플레이하기 위해 라이트 스트립을 사용한다."_. \\(r_{cot}\\)의 예는 다음과 같을 수 있다: _"로봇은 사람의 \'눈을 마주치기\' 위해 머리의 팬 및 틸트 능력을 사용할 수 있다. 로봇은 스마일 또는 노드를 모방하기 위해 라이트 스트립을 사용할 수 있다."_\n' +
      '\n' +
      '**로봇 표현 모션을 코드로 번역.** 다음 단계에서 LLM을 사용하여 표현 로봇 모션을 실행 가능한 코드로 생성하는 방법에 대한 단계별 절차를 번역한다. 본 논문에서는 기존의 로봇 스킬 프리미티브를 포함하는 Voyager(Voyager, 2017)와 유사한 방식으로 스킬 라이브러리를 제안하고, 미리 학습된 표현 동작을 나타내는 파라메트리징 로봇 코드 \\(\\pi_{\\theta}\\)를 제안한다. 이를 용이하게 하기 위해, 프롬프트는 표현 행동을 기술하는 더 복잡한 함수들을 생성하기 위해 문서 스트링들 및 명명된 인수와 함께 작고 재사용 가능한 함수들이 사용되는 예들을 제공함으로써 모듈식 코드 생성을 장려한다. 코드를 생성하려면 다음 프롬프트\n' +
      '\n' +
      '그림 2. 생성적 표현 동작. 언어명령어\\(l_{in}\\)이 주어지면, _Expressive Instruction Following_ module은 사회적 규범에 대한 이유를 제시하고 인간이 이러한 행동을 어떻게 표현할 수 있는지를 출력한다. 이것은 로봇의 기존 능력(\\(r_{pre}\\))과 학습된 표현 행동을 설명하는 프롬프트를 사용하여 로봇 표현 행동을 위한 절차로 변환된다. 그런 다음 이 절차를 사용하여 실행될 수 있는 매개변수 로봇 코드 \\(c\\)를 생성한다. 사용자는 로봇 행동 모듈을 먼저 실행한 후 코드 생성 모듈을 실행할 것인지 아니면 코드 생성 모듈만을 실행할 것인지를 결정하기 위해 처리되는 행동에 대해 반복적인 피드백 \\(f_{i}\\)을 제공할 수 있다. 참고: 모든 회색 모듈 위에 표시된 \\({}^{*}\\)은 냉동 LLM으로 표시된다.\n' +
      '\n' +
      'LLM은 \\(u=[c_{pre},l_{in},h_{exp},r_{exp,i-1_{opt}},c_{i-1_{opt},\\hat{f_{i-1}},\\)\\(r_{exp}]\\)의 형태를 취한다. 여기서, \\(c_{pre}\\)는 LLM에 코드 생성 에이전트로서의 역할에 대한 컨텍스트를 제공하고, 로봇의 현재 스킬 라이브러리를 포함하며, 몇 개의 샷 예를 포함한다. 선택적으로, 이전 단계의 표현 로봇 동작\\(r_{exp,i-1}\\), 코드\\(\\hat{c_{i-1}}\\)과 사용자 피드백\\(\\hat{f_{i-1}}\\)에 응답하는 LLM 출력\\(\\hat{f_{i-1}}\\)을 제공할 수 있다. 출력\\(c\\)은 표현 행동에 대한 정책\\(\\pi_{\\theta}\\)을 나타내는 매개변수 로봇 코드이다(샘플 출력은 그림 2 참조). 이후 생성된 코드는 로봇의 스킬 라이브러리에 통합되어 미래의 표현 행동 세대에 활용될 수 있다.\n' +
      '\n' +
      '**Propagating Human Feedback.** 최종(선택사항) 단계에서, 사용자가 생성된 행동에 만족하지 않는 경우, 인간 피드백 \\(\\hat{f_{i}}\\)에 응답하여 생성된 표현 행동을 갱신하기 위해 LLM을 사용한다. 프롬프트는 \\(u=[\\hat{f_{pre}},l_{in},r_{exp},c,f_{i}]\\의 형태이며, \\(\\hat{f_{pre}}\\)은 LLM에 문맥을 제공하며, 표현적 로봇의 동작을 위한 절차 \\(r_{exp}\\)과 생성된 코드 \\(c\\)을 모두 포함한다. 출력은 형태\\(f=[\\hat{f_{cont}},\\hat{f_{i}}]\\)이며, LLM 추론과 인간의 피드백을 기반으로 현재 표현 동작을 개선하는데 필요한 변화\\(\\hat{f_{i}}\\)을 포함한다. 출력은 또한 로봇의 표현 행동을 생성하기 위한 절차를 수정하기 위해 반복 호출이 필요한지 여부를 분류한 다음 생성된 코드를 코드(c\\)로 변환하거나 생성된 코드를 수정하기만 하면 된다.\n' +
      '\n' +
      '예를 들어, 사용자는 사람을 처음 볼 때 \\(\\hat{f_{i}}=\\)_"를 나타낼 수 있고, 출력 \\(\\hat{f_{i}}=\\)_는 다음과 같을 수 있다. _"[변화: 로봇이 무엇을 해야 하는가]...로봇이 사람을 보는 즉시, 로봇이 그들을 보지 않아야 한다. 고개를 끄덕인 후, 로봇은 자신의 라이트 스트립을 사용하여 미소 또는 노드..."를 모방하는 사전 프로그래밍된 패턴을 표시할 수 있다. 예를 들어, \\(f_{cot}\\)는 다음과 같이 말할 수 있다: _" 피드백은 사람을 인정하는 로봇의 행동이 올바르지 않음을 시사한다. 이는 로봇이 사람을 처음 볼 때 사람에게 있어서는 안 된다는 것을 의미한다."_\n' +
      '\n' +
      '##4. 사용자 연구\n' +
      '\n' +
      '우리는 두 가지 사용자 연구를 수행하여 우리의 접근 방식인 GenEM이 사람들이 인식할 수 있는 표현 행동을 생성하는 데 사용될 수 있는지 여부를 평가했다. 반복 피드백(또는 _GenEM++_)을 사용하여 GenEM과 GenEM의 두 가지 행동 버전을 생성했다. 두 연구 모두에서 모든 비교는 전문 애니메이터가 설계하고 소프트웨어 개발자가 구현한 행동에 대해 이루어졌으며, 이를 _oracle animator_라고 한다. 첫 번째 연구에서 우리의 목표는 GenEM 및 GenEM++를 사용하여 생성된 행동이 오라클 애니메이터를 사용하여 생성된 행동과 유사하게 인식되는지 여부를 평가하는 것이었다. 2차 연구에서, 우리는 오라클 애니메이터를 사용하여 생성된 행동과 유사한 GenEM 및 GenEM++를 사용하여 행동을 생성하려고 시도했다. 두 연구 모두 우리의 접근법이 인간의 피드백에 적응 가능하다는 것을 입증하는 것을 목표로 한다.\n' +
      '\n' +
      '**행동.** 모든 행동은 이동 로봇 플랫폼에서 생성되었다(전체 클립은 웹사이트 1 참조). 로봇은 팬과 틸트가 가능한 헤드, 포인트에서 포인트로 번역, 회전 및 탐색이 가능한 베이스, 서로 다른 색상과 패턴을 표시할 수 있는 라이트 스트립, 마지막으로 발화와 비언어적 효과를 생성할 수 있는 스피치 모듈을 포함하여 기존 API를 통해 행동을 생성하는 데 사용할 수 있는 몇 가지 기능을 가지고 있다. 오라클 애니메이터, GenEM 및 GenEM++의 세 가지 조건에서 생성된 행동을 비교할 수 있도록 각 행동의 비디오 클립을 녹화했다(그림 3 참조). 조건에 따른 일관성을 보장하기 위해 각 조건의 행동을 유사한 조명 조건에서 동일한 물리적 위치에 기록했다. GenEM 및 GenEM++ 동작은 텍스트 완성을 위한 OpenAI의 GPT-4 API(Shen et al., 2019)(gpt-4-0613)를 0으로 설정한 상태에서 샘플링하여 생성되었다.\n' +
      '\n' +
      '**연구 절차** 사전 동의를 제공한 후 참가자는 두 연구에서 로봇의 표현 행동을 평가하기 위해 온라인 조사를 완료했다. 설문조사는 3개의 섹션(행동 조건당 1개)으로 나뉘며 각 조건 내의 클립이 무작위로 나타났다. 순서 효과를 최소화하기 위해 균형 라틴 정사각형 설계(3 x 3)를 사용했다. 각 조건의 각 행동에 대해 참가자는 레이블이 지정되지 않은 비디오 클립 1을 시청한 다음 질문에 응답했다. 모든 참가자는 연구 후에 보수를 받았다.\n' +
      '\n' +
      '**측정.** 두 연구 모두에서 참가자는 각 행동을 평가하기 위한 설문조사를 완료하여 행동에 대한 이해, 로봇이 무엇을 하고 있는지 이해의 어려움 및 로봇의 행동에 대한 능력에 대한 자신감을 평가하는 3개의 7점 리커트 척도 질문에 응답했다. 참가자들은 또한 로봇이 어떤 행동을 표현하려고 했는지 설명하는 개방형 응답을 제공했다.\n' +
      '\n' +
      '**분석.** 본페로니 보정이 적용된 경우 유의한 차이가 있는 사후 쌍별 비교가 있는 데이터에 대해 일원 반복 측정 ANOVA를 수행했다. 조건 간의 비교를 보고할 때, 우리는 행동에 대해 질문된 3개의 리커트 척도 질문 중 적어도 하나에 대해 _instances_를 쌍별 유의 조건으로 정의한다.\n' +
      '\n' +
      '각주 1: [https://generative-expressive-motion.github.io/](https://generative-expressive-motion.github.io/]\n' +
      '\n' +
      '###연구 1 : 생성표현동작 벤치마킹\n' +
      '\n' +
      '우리의 접근법이 사람들이 지각할 수 있는 표현 행동을 생성하는지 여부를 결정하기 위해 18세에서 60세 사이의 30명의 참가자(여성 16명, 남성 14명)를 대상으로 피험자 내 사용자 연구를 수행했다. 한 참가자는 전체 조사를 완료하지 않았고 데이터가 누락되었다.\n' +
      '\n' +
      '**Behaviors.** 복잡도에서 10가지 표현 행동(도 3 참조)을 생성했다: _Nod_, shake head(_Shake_), wake up(_Wake_), excuse me(_Excuse_), recoveryable mistake(_Recoverable_), unrecoverable 실수(_Unrecoverable_), acknowledge person walking by(_Acknowledge_), follow person(_Follow_), approach person(_Approach_) 및 attention to person(_Attention_). 입력에는 원라인 명령어(예: _Respond to a person saying_, _"_Come here. You cannot speak_)가 포함되었다.\n' +
      '\n' +
      '**조건.** 오라클 애니메이터 조건은 스크립팅을 통해 로봇에 구현된 전문적으로 애니메이션된 행동으로 구성되었다. GenEM 행동을 생성하기 위해 접근 방식을 5번 샘플링하여 각 행동의 5가지 버전을 생성했다. 동작은 0의 온도로 샘플링되었기 때문에 그들 사이의 작은 변동과 상당한 중첩을 공유했다(GPT-4 출력의 비결정성으로 인해; 동일한 프롬프트를 사용하여 생성된 샘플에 대해 **부록 C** 참조). 그런 다음 로봇과 함께 작업한 경험이 있는 6명의 참가자에게 순위를 매기도록 요청했습니다. 각 행동에 대한 최상의 변이는 GenEM 행동의 일부로 포함되었다. GenEM++ 행동을 생성하기 위해 로봇 사용 경험이 있는 참가자(그러나 HRI 행동 설계가 미숙함)를 모집하고 각 행동의 가장 좋은 평가 버전에 대한 피드백을 제공하도록 요청했다. 피드백은 참가자가 결과에 만족할 때까지 또는 최대 피드백 라운드 수(n = 10)에 도달할 때까지 표현 행동을 반복적으로 수정하는 데 사용되었다. 우리는 참가자가 연구에서 행동을 평가했지만 행동 생성은 초기 피드백을 제공한 사용자에게 개인화되어 모든 잠재적 사용자(예: 연구 참가자)의 선호도를 반영하지 않을 수 있다.\n' +
      '\n' +
      '**가설.** GenEM++ 행동에 대한 인식은 오라클 애니메이터 행동(**H1**)과 크게 다르지 않을 것이라는 가설을 세웠다. 또한 GenEM 행동이 GenEM++ 및 오라클 애니메이터 행동(**H2**)에 비해 덜 호평을 받을 것이라는 가설을 세웠다.\n' +
      '\n' +
      '** 정량적 발견.** 그림 4는 각 행동에 대한 설문 질문에 대한 참가자의 응답을 요약한다. 그 결과 GenEM++ 행동이 2/10 인스턴스(_Shake_ 및 _Follow_)에서 오라클 애니메이터 행동보다 더 나쁜 것으로 나타났다. 대조적으로, GenEM++ 행동은 2/10 인스턴스(_Excuse_ 및 _Approach_)에서 오라클 애니메이터 행동보다 더 높은 점수를 받았다. 따라서, **H1**은 우리의 데이터에 의해 지원된다 - GenEM++ 행동은 잘 받아들여졌고 오라클 애니메이터 행동은 GenEM++ 행동보다 훨씬 더 잘 받아들여지지 않았다.\n' +
      '\n' +
      '2/10 인스턴스(_Acknowledge Walk_ and _Follow_)에서는 GenEM 행동이 오라클 애니메이터 행동에 비해 더 나쁜 반응을 보인 반면, 2/10 인스턴스(_Excuse_ and _Approach_)에서는 GenEM 행동이 오라클 애니메이터 행동보다 더 좋은 반응을 보였다. 이것은 이 조건에서 사용자 피드백이 행동 생성에 통합되지 않았기 때문에 놀라운 일이었다. 1/10 인스턴스(_Shake_) 외에도 GenEM 및 GenEM++ 행동에 대한 인식에는 유의한 차이가 없었다. 따라서 **H2**에 대한 지원을 찾지 못했습니다. 동등성 검정(동등성 한계: +/- 0.5 리커트 점)을 수행했지만 동등한 행동 집합을 찾지 못했습니다. 이러한 결과는 GenEM(학습되지 않은 사용자가 피드백을 제공하더라도)이 사용자가 유능하고 이해하기 쉬운 표현 로봇 행동을 생성한다는 것을 뒷받침한다.\n' +
      '\n' +
      '### 연구 2: 오라클 애니메이터 모방\n' +
      '\n' +
      '본 연구는 연령 18-60세(18-25:4, 26-30:3, 31-40:12, 41-50:4, 51-60:1)의 참가자 24명(남성 21명, 여성 2명, 선호하지 않음 1명)을 대상으로 피험자 내 사용자 연구를 추가로 수행하여 오라클 애니메이터와 유사한 행동을 생성하기 위해 GenEM을 사용하는 것이 다르게 인식되는지 여부를 평가했다. 한 참가자는 전체 조사를 완료하지 않았고 데이터가 누락되었다.\n' +
      '\n' +
      '**Behaviors.** 첫 번째 연구(도 3 참조)에서 8개의 2개의 행동이 겹치며 복잡도에 따라 10개의 표현 행동을 생성했으며, 노드(_Nod_), 흔들림 헤드(_Shake_), 웨이크 업(_Wake_), 변명 미(_Excuse_), 회복 가능한 실수(_Recoverable_), 회복 불가능한 실수(_Unrecoverable_), 회복 불가능한 실수(_Unrecoverable_), 회복 불가능한 사람 걷기(_Acknowledge Walking_), 회복 불가능한 사람 걷기(_Acknowledge Stop_), 팔로우 사람(_Follow_), 티칭 세션(_Teach_)이 있다. 첫 번째 연구와 다른 행동은 더 복잡성을 추가하기 위해 선택되었는데, 예를 들어 사람이 로봇을 걷거나 수업을 가르치는 것으로 시작하여 마지막으로 로봇이 사람의 지시를 이해한다는 것을 인정하는 _teaching_와 같은 더 긴 단일 회전 상호 작용이다. 첫 번째 연구와 달리 프롬프트는 더 다양했으며 때로는 더 복잡한 행동에 대한 추가 설명이 포함되었다(각 행동에 대한 전체 프롬프트는 **부록 B** 참조). 각 GenEM 동작을 생성하기 위해 10회 접근 방식을 샘플링한 후 실험자가 로봇에 배치할 때 동등한 오라클 애니메이터 동작과 가장 유사하게 나타나는 버전을 선택했다. 각 GenEM++ 행동을 생성하기 위해 실험자는 동일한 오라클 애니메이터 행동과 유사하게 나타날 때까지 또는 최대 피드백 라운드 수(n = 10) 1을 초과할 때까지 반복 피드백을 통해 GenEM 행동을 정제했다.\n' +
      '\n' +
      '각주 2: 두 번째 연구의 일부 행동은 첫 번째 연구에서 일관성을 유지하기 위해 유지한 단일 선 지시로 표현하기에는 너무 복잡하기 때문에 첫 번째 연구와 다르다. 대신, 첫 번째 연구에서, 이러한 복잡한 행동들은 더 단순한 행동들로 분해되었다 (예를 들어, 가르치는 것은 접근하고 주의를 기울이는 것과 같다).\n' +
      '\n' +
      '**가설.** GenEM++ 행동에 대한 사용자 인식은 오라클 애니메이터 행동(**H3**)과 비교할 때 크게 다르지 않을 것이라고 가정했다. 우리는 또한 GenEM 조건에서의 행동들이 GenEM++ 및 오라클 애니메이터 행동들(**H4**)보다 더 나쁜 것으로 인식될 것이라고 가정한다.\n' +
      '\n' +
      '2/10 인스턴스(_Acknowledge Walk_ and _Follow_)에서는 GenEM++ 행동이 오라클 애니메이터 행동보다 더 나쁜 것으로 나타난 반면, 2/10 인스턴스(_Excuse_ and _Teach_)에서는 GenEM++ 행동이 오라클 애니메이터 행동보다 더 긍정적인 것으로 나타났다. 따라서 우리의 가설은 데이터에 의해 뒷받침된다.\n' +
      '\n' +
      '그림 3. 녹색으로 표시된 행동은 첫 번째 연구에 고유한 행동을 나타내고 파란색으로 표시된 행동은 두 번째 연구에 고유한 행동을 나타내는 두 사용자 연구에서 테스트한 행동이다. 나머지 행동(8)은 두 연구에서 공통적이었다.\n' +
      '\n' +
      '**(H3)** - GenEM++ 행동과 오라클 애니메이터 행동은 유의하게 더 잘 인식되지 않았다. 오라클 애니메이터 행동과 GenEM 행동을 비교했을 때, GenEM 행동이 더 심하게 수신된 경우(_Wake_, _Acknowledge Walk_, _Acknowledge Stop_, _Follow_)가 4/10이었고, GenEM 행동이 더 긍정적으로 평가된 경우(_Excuse_)가 1/10이었다. 첫 번째 연구와 마찬가지로, GenEM 행동이 한 사례에서 기준선보다 더 잘 받아들여졌다는 것은 다소 놀라운 일이지만, 유사하지만 사용자 피드백이 제공되지 않기 때문에 오라클 애니메이터 행동에 존재하는 모든 뉘앙스를 포착하지 못한다. 마지막으로, GenEM 행동은 2/10 인스턴스(_Wake_ 및 _Teach_)에서 GenEM++ 행동보다 더 나쁜 평가를 받은 반면, 그 반대인 경우는 0/10 인스턴스였다. 따라서 우리는 마지막 가설 **(H4)에 대한 지지를 찾지 못했다.** 동등성 검정(동등성 한계: +/- 0.5 리커트 점)을 수행했을 때 동등할 행동 집합을 찾지 못했다. 전반적으로, 본 연구 결과를 통해 도출된 표현적 로봇 행동(사용자 피드백)은 사용자가 이해하기 쉽고 유능하다는 것을 알 수 있었다.\n' +
      '\n' +
      '## 5. Experiments\n' +
      '\n' +
      'GenEM의 다양한 측면을 주의 깊게 연구하기 위해 일련의 실험을 수행했다. 여기에는 프롬프트 구조와 다른 LLM 대 종단 간 접근법에 대한 모듈식 호출의 영향을 이해하기 위한 삭제가 포함된다. 또한 실험을 통해 GenEM이 모듈러 및 컴포저블 행동, 즉 서로 위에 쌓이는 행동을 생성할 수 있음을 보여준다. 텍스트에 대한 OpenAI의 GPT-4 API를 샘플링하여 행동을 생성했다.\n' +
      '\n' +
      '그림 4. 1차 사용자 연구에서 각 조건(3)에서 각 행동(10)에 대한 세 가지 질문에 대한 참가자의 설문 응답을 보여주는 그림이다. 상단의 막대는 유의한 차이를 나타내며, 여기서 (*)는 p<.05, (**)는 p<.001을 나타낸다. 오차 막대는 표준 오차를 나타낸다. 첫 번째 그림은 조건에 따른 각 질문에 대한 평균 점수를 보여줍니다. 화살표는 더 좋은 점수가 있는 방향을 반영한다.\n' +
      '\n' +
      '그림 5. 2차 사용자 연구에서 각 조건(3)에서 각 행동(10)에 대한 세 가지 질문에 대한 참가자의 설문 응답을 보여주는 그림. 상단의 막대는 유의한 차이를 나타내며, 여기서 (*)는 p<.05, (**)는 p<.001을 나타낸다. 오차 막대는 표준 오차를 나타낸다. 첫 번째 그림은 조건에 따른 각 질문에 대한 평균 점수를 보여줍니다. 화살표는 더 좋은 점수가 있는 방향을 반영한다.\n' +
      '\n' +
      '온도 0으로 설정한 [32] (gpt-4-0613) 완료. 사용자 연구 및 모바일 조작기에 대한 실험 외에도 ROS를 통해 Gazebo/Unity에서 시뮬레이션된 4족을 사용하여 추가 실험을 수행했다(도 6 참조).\n' +
      '\n' +
      '**Ablations.** GenEM을 언어 지침을 취하고 LLM에 한 번 전화를 걸어 표현 행동을 생성하는 엔드 투 엔드 접근법과 비교하기 위해 Ablations를 수행했다. 이동로봇을 위한 기존의 API를 이용하여 어블레이션을 수행하였다. 조사된 행동은 프롬프트와 함께 첫 번째 사용자 연구와 동일했다. 각 프롬프트를 5회 샘플링하여 동작을 생성하고 로봇에서 실행하여 _correctness_를 확인했다. 또한, 실험자는 인간의 사회적 규범을 설명하기 위해 행동 코드가 추론을 통합하는지 확인하기 위해 코드를 조사했다. 코드 정확성과 사회 규범 적절성에 대한 결과는 표 1에 나와 있으며, 전반적으로 제안된 방법은 2가지 행동(_Excuse_ 및 _Follow_)에 대해 성공적인 런이 생성되지 않은 절제된 변이에 비해 더 높은 성공률을 보였다. _Excuse_ 행동을 위해, 로봇은 사용자의 거리를 확인하고 그들이 방해하고 있다는 것을 사람에게 신호해야 한다. 그러나 절제된 변이의 경우 시도에서 거리를 확인하지 않았다. _Follow_ 동작에 대해, 코드는 이전에 정의되지 않은 함수들을 호출하고, 로봇 API들을 호출할 때 잘못된 입력 파라미터 타입을 사용함으로써, 제로 성공적인 시도들을 초래하였다. 또한, 생성된 거의 모든 함수에는 문서 문자열과 명명된 인수가 없어 더 복잡한 동작을 위해 모듈 방식으로 사용하기 어려울 수 있다(몇 개의 샷 코드 예제 제공에도 불구하고).\n' +
      '\n' +
      'GenEM에 의해 생성된 행동이 특히 더 복잡한 행동에 대해 사회적 규범을 반영하고 더 단순한 행동에 대해 유사하게 보인다는 것을 질적으로 관찰했다. 예를 들어, GenEM에 의해 생성된 _Excuse_ 행동은 음성 모듈을 사용하여 _"Excuse me"_라고 말했다. _Attention_ 행동의 경우, 절제된 변형은 사람을 보고 라이트 스트립을 켠 다음 끄는 반면, GenEM 변형은 "능동 청취"를 모방하기 위해 주기적인 노딩을 통합했다. _Approach_ 행동의 경우 GenEM 변이는 항상 사람을 향해 이동하기 전에 고개를 통합했지만 절제된 변이는 고개를 사용하지 않았으며 대신 조명은 두 가지 경우에 사용되었다.\n' +
      '\n' +
      '**Cross-Embolment Behavior Generation.** 시뮬레이션된 Spot 로봇에 대한 API를 사용하여 행동당 5회 첫 번째 사용자 연구에서 동일한 프롬프트를 샘플링했다. 표 2에 요약된 결과는 자체 어포던스와 API를 사용하여 다른 로봇 플랫폼을 사용하여 동일한 프롬프트를 사용하여 대부분의 표현 행동을 생성할 수 있음을 보여준다. 그러나, _Approach_와 같은 일부 생성된 행동들은 로봇이 그들 근처의 안전한 거리 대신에 인간의 위치로 항해하는 변화들을 포함했는데, 이는 (아마도 번역 API에서 거리 임계 파라미터가 없기 때문에) 사회적 규범 불일치로 간주될 수 있는 반면, 일부는 인간(예를 들어, 로봇이 _Attention_를 위해 인간을 향해 대신 임의의 각도를 회전시킴)을 고려하지 않았다. 전반적으로 성공률은 서로 다른 로봇 실시예에 대한 접근법의 일반성을 암시한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & \\multicolumn{2}{c}{**GenEM**} & \\multicolumn{2}{c}{**Ablated**} \\\\  & _Execution_ & _Norms_ & _Execution_ & _Norms_ \\\\ \\hline Nod & 5 & 0 & 5 & 2 \\\\ Shake & 5 & 0 & 5 & 2 \\\\ Wake & **4** & 2 & 3 & 0 \\\\ Excuse & 5 & 3 & 0 & - \\\\ Recoverable & 3 & 0 & 5 & 1 \\\\ Unrecoverable & 5 & 0 & 5 & 0 \\\\ Acknowledge & 5 & 1 & 5 & 0 \\\\ Follow & **3** & 1 & 0 & - \\\\ Approach & 5 & 1 & 5 & 3 \\\\ Attention & **4** & 0 & 1 & 0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1. 모바일 로봇 플랫폼 상의 예시들은 각각의 프롬프트를 5회 샘플링할 때 성공적인 행동 생성 시도를 보여주며, _Expressive Instruction Following_ 모듈이 없는 변화(피드백 없이)에 대한 우리의 접근법과 후속적으로 인간 표현 동작을 로봇 표현 동작으로 변환하는 모듈을 비교한다. _Execution_ column은 성공 시도 횟수를 나타낸다(5). _Norms_ 열은 사회적 규범이 적절하게 준수되지 않은(실험자에 의해 코딩된) 시도 횟수를 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & _Execution_ & _Norms_ \\\\ \\hline Nod & 5 & 0 \\\\ Shake & 5 & 0 \\\\ Wake & 5 & 0 \\\\ Excuse & 3 & 0 \\\\ Recoverable & 5 & 2 \\\\ Unrecoverable & 4 & 0 \\\\ Acknowledge & 4 & 1 \\\\ Follow & 2 & 2 \\\\ Approach & 5 & 5 \\\\ Attention & 1 & 0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2. 시뮬레이션에서 4족에서 생성된 행동은 각 프롬프트를 5회 샘플링할 때 성공적인 행동 생성 시도를 보여준다. _Execution_ column은 성공 시도 횟수를 나타낸다(5). _Norms_ 열은 사회적 규범이 제대로 관찰되지 않은(실험자에 의해 코딩된) 시도 횟수를 나타낸다.\n' +
      '\n' +
      '도 6. Gazebo에서 Quadruped simulated in performing the _Recoverable mistake_ behavior (top) and _Unrecoverable mistake_ (bottom) before feedback. GenEM. 로봇은 회복할 수 있는 실수를 한 뒤 뒤돌아서 다리를 내리고 빨간 불빛을 깜박여 아쉬움을 전하다가 다시 초기 위치로 돌아가 녹색 불빛을 깜빡이는 실수를 시연한다. 대조적으로, 복구할 수 없는 실수로 인해 로봇은 높이를 낮추고 짧은 기간 동안 빨간색 조명을 표시하고 앞으로 숙이고 이 자세를 유지한다.\n' +
      '\n' +
      '**복잡한 표현 행동 구성** 사용자 연구에서 모든 행동은 소수의 예제와 기존 로봇 API를 사용하여 처음부터 생성되었다. 우리는 이전 상호작용에서 학습된 표현 행동 세트를 사용하여 더 복잡한 행동을 생성하려고 시도했는데, 이러한 기술(도트 스트링이 있는 함수로 표현됨)은 로봇의 능력(접근법의 단계 2)과 로봇의 API(접근법의 단계 3)를 설명하는 프롬프트에 추가되었다. 프롬프트에 사용된 학습된 행동은 _nodding, eye contact, light strip 깜빡임, 둘러보기,_ 및 _shaking_이었다. 우리는 GenEM이 복잡도가 다양한 _Acknowledge Walk, Approach_ 및 혼동 표현(_Confusion_)의 세 가지 행동을 생성하도록 자극했다. 이러한 모든 행동은 원하는 행동에 대한 단일 선 설명이 포함된 지침을 사용하여 피드백을 제공하지 않고 4족에서 생성되었다. 출력된 프로그램에 학습된 행동이 포함될 빈도를 평가하기 위해 GenEM을 5회 샘플링했다. 성공 여부를 평가하기 위해, 실험자는 생성된 코드가 로봇 API와 학습된 API의 조합을 활용하는지 여부를 확인하였다(표 3 참조). 접근 행동의 경우, 깜박이는 조명이 항상 사용되는 반면 _nod head_ 행동은 결코 사용되지 않았다는 점에 주목하는 것은 놀라운 일이었다. 혼동을 표현하기 위해 4/5 인스턴스가 주위를 둘러보기 위한 코드를 생성했다는 것은 놀라운 일이었지만, 1/5 인스턴스만이 기존의 _looking around_ 행동을 사용했다.\n' +
      '\n' +
      '**Human Feedback에 대한 적응성** 사용자 연구에서 피드백은 생성된 행동에 대한 인식에 약간의 영향을 미쳤다. 또한 피드백이 다양한 방식으로 행동 생성을 조정할 수 있음을 질적으로 관찰했다. 우리는 두 가지 선행 연구인 _Excuse, Approach_ 및 _Acknowledge Stop_에서 세 가지 행동을 생성하는 실험에서 이를 연구했다. 각 동작은 이전과 같이 단일 라인 설명을 사용하여 생성되었으며 학습된 로봇 API가 없었다. 우리는 (1) 행동을 추가하고 다른 행동 이전에 발생해야 한다고 강제하는 것, (2) 행동의 순서를 바꾸는 것, (3) 행동을 반복하게 하는 것, (4) 대안을 제공하지 않고 기존 능력을 제거하는 것(예를 들어, 라이트 스트립을 사용하는 행동을 생성한 후 능력으로 라이트 스트립을 제거하는 것)의 네 가지 피드백을 통해 생성된 행동을 수정하려고 시도했다. 전반적으로 결과(표 4 참조)는 기능을 제거하면 정의되지 않은 기능을 호출하는 경우가 더 자주 발생하지만 제공된 피드백 유형에 따라 동작을 수정할 수 있음을 시사한다.\n' +
      '\n' +
      '## 6. Discussion\n' +
      '\n' +
      '**요약.** 이 작업에서 우리는 사용자 언어 명령어를 로봇 코드로 번역하여 큰 언어 모델을 사용하여 표현 가능한 로봇 동작을 생성하고 수정하는 접근 방식 GenEM을 제안했다. 사용자 연구와 실험을 통해, 본 프레임워크는 상황 내 학습과 소수의 프롬프트를 통해 표현 행동을 빠르게 생성할 수 있음을 보여주었다. 이는 이전 작업에서와 같이 특정 로봇 동작 또는 신중하게 조작된 규칙을 생성하기 위한 선별된 데이터 세트의 필요성을 감소시킨다. 사용자 연구에서 참가자들은 사용자 피드백이 유능하고 이해하기 쉬운 GenEM을 사용하여 생성된 행동을 발견했으며 어떤 경우에는 전문가 애니메이터가 생성한 행동보다 훨씬 더 긍정적으로 인식한다는 것을 보여주었다. 또한 다양한 유형의 사용자 피드백에 대한 적응성이 있으며, 더 단순하고 학습된 행동을 결합하여 더 복잡한 행동이 _composed_가 될 수 있음을 보여주었다. 그들은 함께 인간의 선호를 조건으로 한 표현적 로봇 행동의 신속한 생성을 위한 기초를 형성한다.\n' +
      '\n' +
      '**제한과 미래 작업** 우리의 접근 방식의 약속에도 불구하고 몇 가지 단점이 있습니다. 우리의 사용자 연구는 녹화된 비디오 클립을 통해 온라인으로 수행되었으며, 이는 유효한 방법론이지만(Han et al., 2018; Wang et al., 2018), 참가자가 로봇의 물리적 근접에 있을 때 어떻게 반응할지 반영하지 않을 수 있다(Wang et al., 2018). 따라서 로봇과의 상호 작용을 포함하는 추가 연구가 추구되어야 한다. 작은 컨텍스트 창과 텍스트 입력의 필요성을 포함하여 현재 LLM의 일부 고유한 한계가 주목되어야 한다.\n' +
      '\n' +
      '우리의 작업에서는 단일 회전 행동(예: 행인을 인정함)만을 평가하지만, 다중 회전이고 인간과 로봇 사이의 전후 상호작용을 수반하는 행동을 생성할 수 있는 기회가 있다. 향후 작업은 또한 조작기 및 그리퍼를 포함하여 더 큰 작동 공간으로 모션을 생성하는 것을 탐구해야 한다. 비록 우리의 접근법이 사용자 피드백과 그들의 선호도에 적응할 수 있다는 것을 보여주었지만, 현재 더 오랜 기간 동안 사용자 선호도를 학습할 수 있는 메커니즘은 없다. 실제로 우리는 사용자가 주어진 상황에서 로봇이 보여주기를 기대하는 행동에 대한 선호도에서 개인차를 나타낼 것으로 기대한다. 따라서, 상황 내 학습 선호도(Wang et al., 2018)는 표현 행동을 정제하는 강력한 메커니즘일 수 있다.\n' +
      '\n' +
      '이러한 한계에도 불구하고, 우리는 우리의 접근법이 대규모 언어 모델의 힘을 통해 적응 가능하고 구성 가능한 표현 동작을 생성하기 위한 유연한 프레임워크를 제시한다고 믿는다. 우리는 이것이 로봇이 사람들과 더 효과적으로 상호작용할 수 있도록 표현적 행동 생성을 위한 미래의 노력에 영감을 주기를 바란다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & _Insert_ & _Swap_ & _Loop_ & _Remove_ \\\\  & _actions_ & _actions_ & _actions_ & _capability_ \\\\ \\hline Excuse & 4 & 5 & 5 & 5 \\\\ Approach & 4 & 5 & 5 & 3 \\\\ Acknowledge Stop & 5 & 5 & 4 & 3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4. GenEM을 이용하여 생성된 행동들에 대해 상이한 타입들의 피드백을 제공할 때의 성공률들(시도들 중 5개)로서, 여기서: _Insert actions_다른 액션들보다 먼저 새로운 액션이 추가되도록 요청, _Swap actions_기존 액션들의 순서를 스왑하도록 요청, _Loop actions_반복 액션들에 루프를 추가하도록 요청, _Remove capability_기존 액션들을 대체 액션과 스왑하도록 요청.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & _Eye_ & _Blinking_ & _Look_ & _Shake_ & _Nod_ \\\\  & _contact_ & _lights_ & _around_ & _head_ & _head_ \\\\ \\hline Acknowledge Walk & 5 & - & - & - & 5 \\\\ Approach & 4 & 5 & - & - & 0 \\\\ Confusion & - & 4 & 1 & 5 & - \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3. GenEM을 이용하여 새로운 행동(행)을 구성할 때 이전에 학습된 행동(열)이 사용되는 횟수(시도 5회 중). 대시는 새로운 동작의 생성을 프롬프트할 때 주어진 학습된 동작 API가 제공되지 않음을 나타낸다.\n' +
      '\n' +
      '###### Acknowledgements.\n' +
      '\n' +
      ' 베이스라인 로봇 행동에 대한 애니메이션을 제공한 더그 둘리와 시스템에 대한 유용한 토론에 대해 에드워드 리에게 감사드린다. 리시 크리슈난, 디에고 레예스, 스푸르티 모어, 에이프릴 지트코비치, 로사리오 자우레귀가 로봇 접근 및 문제 해결에 도움을 준 데 대해 감사하고, 저스티스 카르바잘, 조딜린 페랄타, 조나단 벨라가 비디오 녹화를 지원하는 데 대해 감사한다. 마지막으로 사용자 연구와 데이터 수집 노력을 조율해준 벤 제니스와 UX 연구팀에 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* M. 안아한 브라운영 체보타르, 오 코테스, B. 데이비드, C. 핀, C. 푸, K. 고팔라크리쉬난 Hausmann, et al. (2023)Do as I can, Not as Is Say: grounding language in robotic affordances. In Conference on Robot Learning, PMLR, pp. 287-318. Cited by: SS1.\n' +
      '* A. A. Aly and A. Tapas (2013)A model for synthesizing combined verbal and nonverbal behavior based on personality traits in human-robot interaction. 2013년 제8회 ACM/IEEE International Conference on Human-Robot Interaction(HRI), pp. 325-332. Cited by: SS1.\n' +
      '*N. 버그스트롬 칸다, T 미야시타, H. 이시구로, N. Hagita (2008) Modeling of natural human-robot encounter in: 2008 ieee/ij international conference on intelligent robots and systems. 인용: SS1.\n' +
      '*N. 부치나 Kamel, E. Barakova(2016)는 로봇 프로그래밍을 위한 최종 사용자 친화적인 도구의 설계 및 평가이다. 2016년 55th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), pp. 185-191. Cited by: SS1.\n' +
      '* M. J. Chung, J. Huang, L. 타카야마 라우와 M Cakmak (2016) Iterative design of the system for programming social interactive service robots. 소셜 로봇 8차 국제 콘퍼런스에서, ICSR 2016, 캔자스 시티, MO, 미국, 2016년 11월 1-3일 Proceedings A S. Springer, pp. 919-929. Cited by: SS1.\n' +
      '*Y. 최승 카르메티, B. 팔레티, N. 시바쿠마르, P. 량, D. 새디(2023) 아니, 오른쪽: 공유 자율성을 통한 로봇 조작을 위한 온라인 언어 수정. In Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction, pp. 93-101. Cited by: SS1.\n' +
      '* P. David, M. Cakmak, A. Sauppe, A. Albarghouthi, and B. Mutlu(2022) Interaction Templates: a Data-Driven Approach for Authoring Robot Program. PLATAT에서 외부 링크: 인용된 링크: SS1입니다.\n' +
      '*R. Desai, F. Anderson, J. Matejka, S. 코로스, J. 맥칸, G. 피츠무리스, 그리고 T. Grossman (2019)Geppetto: 표현 로봇 행동의 의미론적 설계를 가능하게 한다. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pp. 1-14. Cited by: SS1.\n' +
      '* M. 도어링, D. F. Glas, H. Ishiguro (2019) Modeling interaction for robot imitation learning of human social behavior. IEEE Transactions on Human-Machine Systems49(3), pp. 219-231. Cited by: SS1.\n' +
      '*Q. 동락 이덕대 우병창 Sun, J. Xu, Z. Sui (2022) in-context learning을 위한 설문 조사. ArXiv:2301.02034. 인용: SS1.\n' +
      '* P. Ferrarelli, M. 라이자로와 L. locchi (2018)Design of robot teaching assistant through multi-modal human-robot interaction. Robotics in Education: Latent Results and Developments, pp. 274-286. Cited by: SS1.\n' +
      '* G. Hoffman and W. Jyl(2014) 움직임을 염두에 둔 로봇 설계. Journal of Human-Robot Interaction3 (1), pp. 91-122. Cited by: SS1.\n' +
      '* C. Huang and B. Mutlu(2013) 로봇 행동의 레퍼토리: 로봇이 사회적 행동을 통해 상호 작용 목표를 달성할 수 있게 한다. Journal of Human-Robot Interaction2(2), pp. 80-102. Cited by: SS1.\n' +
      '* C. Huang and B. Mutlu (2014) Learning-based modeling of multiimodal behavior for humanlike robots. In Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction, pp. 57-64. Cited by: SS1.\n' +
      '*W. 황포샤 매리스 Zhang, J. Jiang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al. (2023)Inner Monologae: 언어 모델들로 계획을 통해 체화된 추론. In Conference on Robot Learning, pp. 1769-1782. Cited by: SS1.\n' +
      '*N. 후세인, E. 에르진, T. M. 세진, Y. 예메즈(2022) 사회적으로 매력적인 로봇들을 훈련시키는 것: 두 강화 학습으로 백채널 행동을 모델링하는 것. IEEE Transactions on Affective Computing13 (4), pp. 1840-1853. Cited by: SS1.\n' +
      '*Y. 가토태 칸다, H. 이시구로(2015)입니다. 무엇을 도와드릴까요? 인간과 같은 경찰이 접근하는 행동의 설계. In Proceedings of the Tenth Annual ACMIEEE International Conference on Human-Robot Interaction, pp. 35-42. Cited by: SS1.\n' +
      '* A. 쿠보타, E. I. 피터슨, V. Rajendren, H. Kress-Gaif, and L. D. Riek (2020)Jessie: synthesizing social robot behavior for personalized neuromedikination and beyond. In Proceedings of the 2020 ACM/IEEE international conference on human-robot interaction, pp. 121-130. Cited by: SS1.\n' +
      '* M. Kwon, S. H. Huang, and A. D. Dragan (2018)Expressing robot capable. In Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, pp. 87-95. Cited by: SS1.\n' +
      '* M. 권성민 Bullard, and D. Sadigh (2023) Reward design with language models. ICLR(International Conference on Learning Representations)에서 인용된 SS1.\n' +
      '*N. 레오나디, M. Manca, F. Paternio, and C. Santoro(2019) Triggeration programming for individualizing humanoid robot behavior. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pp. 1-13. Cited by: SS1.\n' +
      '*Z. 리치커밍스, 케이 Sreenath(2020) Animated cassie: dynamic relatable robotic character. 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 379-374. Cited by: SS1.\n' +
      '* J. Liang, W. 황, F. 샤, P. 쉬, K Hausman, B. Ichter, P. Florence, and A. Zeng(2023)Code as policy: language model programs for embodied control. 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 9493-9500. Cited by: SS1.\n' +
      '*K. Lin, C. Jia, T. M. Parone, and J. Bohg(2023)Text2030. Cited by: SS1.\n' +
      '*Y. 류동근 Kanda, and H. Ishiguro (2016)Data-driven filt: learning social behavior by examples from human-human interaction. IEEE Transactions on Robotics32(4), pp. 988-1008. Cited by: SS1.\n' +
      '* M. Marronegen, A. Lim, T. S. Dahl, N. 헤니언(2019)은 가변 자동 인코더를 사용하여 언어에 의한 로봇 제어를 생성한다. 2019년 제8차 Affective Computing and Intelligent Interaction(ACIT)에서 pp. 545-551. Cited by: SS1.\n' +
      '* S. 민익 류아홀츠만 아르테텍세 루이스, 하지시르지, L. 제틀모이어(2022) 시연의 역할을 수행하는 것: 상황 내 학습이 작동하게 하는 것. ArXiv:2202.12837. 인용: SS1.\n' +
      '* S. Mirachandani, F. Xia, P. Florence, B. Ichter, D. Driess, M. A. Arenas, K. Rao, D. Sadigh, and A. Zeng (2023) Large language models as general pattern machine. The Proceedings of the 7th Conference on Robot Learning (CoRL), Cited by: SS1.\n' +
      '* M. 머레이 Walker, A. Nanavati, P. Alves-Oliveira, N. 필리포프 샘페, B. Mutlu, M. Cakmak (2022) Learning backchnecting behavior for robot via data augmentation from human-human conversation. In Conference on robot learning, PMLR, pp. 513-525. Cited by: SS1.\n' +
      '*O. Ozsa(2023)GFT-4. Note: [http://www.dmplayer.com/arit/Aly](http://www.dmplayer.com/arit/Aly), A. Sandrybaoui, T. Beplaene (2023) 데이터 기반 의사소통 행동 생성: 설문조사. 인간-로봇 상호작용에 대한 ACM 거래 인용: SS1.\n' +
      '* D. Porfirio, L. Fisher, A. Sauppe, A. Albarghouthi, 및 B. Mutlu(2019) 보디스톰핑 인간-로봇 상호작용. 제32회 연례 ACM 심포지엄의 사용자 인터페이스 소프트웨어 및 기술에 관한 절차에서, pp. 479-491. 인용: SS1.\n' +
      '* J. Porfirio, A. Sauppe, A. Albarghouthi, and B. Mutlu(2018)Authoring and verifying human-robot interaction. In Proceedings of the 31st annual annual symposium on user interface software and technology, pp. 75-86. Cited by: SS1.\n' +
      '* D. Porfirio, A. Sauppe, A. Albarghouthi, and B. Mutlu(2020) Transforming robot programs based on social context. In Proceedings of the 2020 CHI conference on human factors in computing systems, pp. 1-12. Cited by: SS1.\n' +
      '* D. Porfirio, L. 스테그너 Cakmak, A. Sauppe, A. Albarghouthi, and B. Mutlu(2023)Sjecting robot programs on the rj. In Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction, pp. 584-593. Cited by: SS1.\n' +
      '* D. Porfirio, L. 스테그너 Cakmak, A. Sauppe, A. Albarghouthi, 및 B. Mutlu(2021) Figaro: 인간-로봇 상호작용을 위한 탁상 저작 환경. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1-15. Cited by: SS1.\n' +
      '* I. Singh, V. Blukis, A. Moussavi, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomson, and A. Garg(2023)Programport: 대형 언어 모델을 이용하여 위치 로봇 작업 계획을 생성한다. 2023년 IEEE International Conference on Robotics and Automation (ICRA), pp. 11523-11530. Cited by: SS1.\n' +
      '* A. Sipurib, A. Bohoz, J. K. Sreenath, D. S. Brown, and A. Dragan (2022) Teaching robot to span of functional expressive motion. 2022년 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp.13406-13413. Cited by: SS1.\n' +
      '* M. 수기탄 Bretan, and G. Hoffman (2019) Affective robot movement generation using cyclegrams. 2019년 19th ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 534-535. Cited by: SS1.\n' +
      '* M. 수기탄 Gomez, and G. Hoffman(2020)MoveAE: modifying affective robot movements using classifying variational autoencoder. In Proceedings of the 2020 ACM/IEEE international conference on human-robot interaction, pp. 1-10. Cited by: SS1.\n' +
      '\n' +
      '481-489.\n' +
      '* Takayama et al. (2011) Leila Takayama, Doug Dooley, and Wendy Ju. 2011. 사고 표현: 애니메이션 원리로 로봇 가독성 향상. In _Proceedings of the 6th international conference on Human-robot interaction_. 69-76\n' +
      '* Wang et al. (2023) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Aminx Anandkumar. 2023. 보이저: 언어 모델이 큰 개방형 구체화 에이전트 _ arXiv preprint arXiv:2305.16291_(2023).\n' +
      '* Wei et al. (2022) Jason Wei, Xuehui Wang, Dale Schurmanns, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Demy Zhou, Lei. 2022. Chain-of-thought promptits reasoning in large language models. _ 신경 정보 처리 시스템_35(2022), 24824-24837에서의 발전.\n' +
      '* Woods et al. (2006) Sarah Woods, Michael Walters, Kheng Lee Koay, and Kerstin Dautenhahn. 2006. 라이브 및 비디오 기반 방법을 사용하여 인간 로봇 상호 작용 시나리오를 비교하는 것: 새로운 방법론적 접근법을 지향한다. In _9th IEEE International Workshop on Advanced Motion Control_, 2006. IEEE, 750-755.\n' +
      '* Wu et al. (2023) Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Syvomn Rusnikev, and Thomas Funkhouser. 2023. Tidy: 대규모 언어 모델을 사용한 개인화된 로봇 지원 후 In _2023 IEEE/RSJ International Conference on Intelligent Robots and Systems(IROS)_. 3546-3553. [https://doi.org/10.1109/IROS.355522.2023.104377](https://doi.org/10.1109/IROS.355522.2023.104377)\n' +
      '* Yu et al. (2023) Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasencleer, Jan Humplik, Brian Leit, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas Lates, Dorsa Sadigh, Jie Tan, Yuval Tassa, and Fei Xia. 2023. 로봇 스킬 합성을 위한 언어 투 리워드. In _Proceedings of the 7th Conference on Robot Learning (CoRL)_.\n' +
      '* Zhou and Dragan (2018) Allan Zhou and Anna D Dragan. 2018. 로봇 모션 스타일에 대한 비용 함수. In _2018 IEEE/RSJ International Conference on Intelligent Robots and Systems(IROS)_. IEEE, 3632-3639.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>