<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# GARField.\n' +
      '\n' +
      '정민김매매({}^{*1}\\)는 저스틴 커트({}^{*1}\\) 켄 골드버그\\({}^{*1}\\)\n' +
      '\n' +
      '매트 투키카({}^{2}\\) 앙주 카나자와({}^{1}\\).\n' +
      '\n' +
      '대등한 기여도(*\\)는 동등한 기여도(*\\)를 나타내는데, 이는 동등한 기여도(*\\)를 의미한다.\n' +
      '\n' +
      'AI 버클리({}^{2}\\)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '현장을 분해할 수 있는 여러 수준의 과립성으로 인해 집단화는 본질적으로 모호하며, 굴삭기의 바퀴는 전체적으로 분리되거나 일부라고 간주될 수 있는가? 3D 장면을 제안하는 이미지 입력에서 의미 있는 의미 있는 그룹의 계층으로 분해하기 위한 접근 방식인 라딩 필드(GARF필드)를 그룹 Anything를 제시한다. 이를 위해 물리적 척도를 통해 그룹 모호성을 수용하며, 스케일 조절된 3D 친화성 특징 필드를 최적화하여 세계의 한 점은 다양한 크기의 다른 그룹에 속할 수 있다. 우리는 다양한 관점에서 상충되는 마스크를 일관되게 융합하기 위해 척도를 사용하여 거친 계층 간 위계를 존중하는 방식으로 세그먼트 Anything(SAM)에 의해 제공되는 2D 마스크 세트로부터 이 분야를 최적화한다. 이 분야에서 우리는 자동 트리 구성 또는 사용자 상호작용을 통해 가능한 그룹화의 계층화를 도출할 수 있다. 우리는 다양한 실내 장면에서 GARF필드를 평가하고 객체, 객체 및 다양한 하위 부분의 군집이라는 많은 수준에서 그룹을 효과적으로 추출한다. GARF 필드는 본질적으로 다중 뷰 일관된 그룹을 나타내며 입력 SAM 마스크보다 더 높은 충실도 그룹을 생성한다. GARF필드의 계층적 그룹화는 3D 자산 추출 또는 동적 장면 이해와 같은 하류 응용 프로그램을 흥미롭게 할 수 있다. [https://www.garfield][https://www.garfield. intentio/] (https://www.dyio/)프로젝트 웹사이트를 참조하십시오.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '그림 1의 장면을 고려할 때 NeRF[20]과 같은 최근의 기술은 이 장면의 광학적 3D 재구성을 회복할 수 있으며 세계는 구조적 의미가 없는 단일 부피로 모델링된다. 인간으로서 우리는 현장을 재구성할 수 있을 뿐만 아니라 여러 수준의 과립도 - 최고 수준에서 _그룹_이 할 수 있는 능력을 가지고 있으며, 우리는 장면의 부분 _i.e_를 본다. 굴삭기, 덤불, 인도가 있지만 굴삭기를 바퀴, 크레인, 객실 등 부품으로 분해할 수도 있습니다. 여러 수준의 그룹화에서 장면을 인식하는 이러한 능력은 우리 장면 이해의 핵심 요소이며, 함께 속한 것을 이해함으로써 3D 세계와 상호 작용할 수 있게 한다. 그러나 이러한 다른 수준의 과립성은 그룹 내 모호성을 도입하여 일관된 3D 표현에서 표현하기 위한 도전이다. 이러한 모호성을 깨는 방법은 여러 가지가 있지만, 우리는 그룹을 _hierarchy_로 통합하기 위한 큐로서 기업의 물리적 규모에 초점을 맞추고 있다.\n' +
      '\n' +
      '이 작업에서 우리는 제안된 이미지를 감안할 때 장면이 그룹의 계층으로 분해될 수 있는 스케일-조건 친화력과 함께 3D 장면을 재구성하는 접근인 라디오수스 필드(GARF필드)가 있는 그룹 Anything를 소개한다. 예를 들어, GARF필드는 전체 굴삭기를 모두 추출할 수 있다(그림). 1탑 우드(Bottom Right) 뿐만 아니라 하위 부분(Bottom 오른쪽에)도 있습니다. 이 조밀한 계층적 3D 그룹화는 3D 자산 추출 및 상호 작용 분할과 같은 애플리케이션을 가능하게 한다.\n' +
      '\n' +
      'GARF 필드는 2D 분할 마스크 세트를 3D 부피 스케일 조절된 친화성 필드로 증류한다. 그룹화는 모호한 작업이기 때문에 이러한 2D 라벨은 중첩되거나 상충될 수 있다. 이러한 불일치는 일관된 3D 그룹으로 마스크를 증류하는 데 어려움을 초래한다. 우리는 _규모 조건_특성 분야를 활용함으로써 이 문제를 극복한다. 구체적으로 GARF필드는 특징 거리가 점들의 친화도를 반영하도록 감독되는 조밀한 3D 특징 필드를 최적화한다. 스케일 컨디셔닝은 두 점이 대규모로 더 높은 친화도를 가질 수 있지만 더 작은 규모(_i.e_같은 수박의 와지)에서 낮은 친화도를 가질 수 있다. 그림 2에 도시된 바와 같이, 그림 2와 같다.\n' +
      '\n' +
      '원칙적으로 GARF필드는 2D 마스크의 모든 출처를 증류할 수 있지만, 인간이 합리적인 그룹으로 생각하는 것과 잘 일치하기 때문에 세그먼트 Anything Model(SAM) [15]에서 마스크 후보를 도출한다. 우리는 입력 이미지를 SAM으로 처리하여 후보 분할 마스크 세트를 얻는다. 각 마스크에 대해 장면 기하학을 기반으로 물리적 척도를 계산합니다. GARField를 훈련시키기 위해 마스크 멤버십을 기반으로 대비 손실이 있는 후보 2D 마스크를 증류하여 조회수 또는 마스크 후보 간의 불일치를 해결하기 위해 3D 척도를 활용합니다.\n' +
      '\n' +
      '잘 포장된 친화성 필드는 1) _트랜스_가 있는데, 이는 두 점을 세 번째와 상호 그룹화하면 스스로 묶어야 하며, 2) _containment_는 두 점을 소규모로 그룹화하면 더 높은 스케일에서 함께 그룹화해야 한다는 것을 의미한다. GARF필드는 억제 보조 손실 외에도 조영제 손실을 사용하는 것은 이러한 두 가지 특성을 모두 장려한다.\n' +
      '\n' +
      '최적화된 스케일 조절된 친화성 필드로 GARF필드는 더 이상의 클러스터가 나타나지 않을 때까지 하강 스케일에서 재귀적으로 클러스터링하여 3D 장면 위계를 추출한다. 건설에 의해, 이 재귀적 군집링은 생성된 그룹이 거친 간 방식으로 이전 군집의 하위 부분임을 보장한다. 우리는 주석이 달린 계층적 그룹을 가진 다양한 실제 장면에서 GARF필드를 평가하고 객체 위계성을 포착하는 능력과 다양한 견해에 걸친 일관성을 평가한다. 다중 뷰를 활용함으로써 GARF필드는 상세한 그룹을 생성할 수 있으며, 종종 입력 2D 분할 마스크의 품질에 따라 개선될 수 있다. 더욱이, 이들 그룹은 설계에 의해 3D 일관성이 있는 반면, 2D 기저부는 뷰 일관성을 보장하지 않는다. 계층적 3D 자산 추출 및 클릭 기반 상호 작용 분할을 위한 GARField의 다운스트림 응용 프로그램을 보여준다. GARF필드의 장면 분해 능력을 감안할 때 로봇이 동적 재구성에 앞서와 상호작용하거나 상호 작용할 수 있는 것과 같은 다른 하류 응용 분야에서 잠재력을 얻기를 바랍니다. 출판 시 모드와 데이터가 출시됩니다. 더 많은 시각화를 위해 첨부 영상을 참조하십시오.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '계층적 그룹화 다중 수준 그룹화는 전경 세분화[28] 초기에는 2D 영상에서 오랫동안 연구되었다. 다중 수준 분할 [5] 및 [1, 25, 31]을 파싱하는 보다 복잡한 계층적 장면에 대한 스펙트럼 군집링의 이 개념에 대해 여러 방법이 구축된다. 이러한 접근법은 고전적인 텍스처 신호를 통해 윤곽을 추출하고 하향다운 [37] 또는 상향식 통합[1]을 통해 계층화를 생성하는 데 의존한다. 보다 최근의 딥러닝 접근법은 계층화를 생성하기 위해 여러 척도로 계산된 에지[36]를 사용하고 Ke _et al_[11][11]는 Ke _et al_를 사용한다. 고전적인 계층 세분화 [1]의 출력에 의해 유도되는 비지도 계층적 분할 접근법을 변환기 기반 비지도 계층적 분할 접근법을 제공한다.\n' +
      '\n' +
      '많은 작업은 인스턴스가 분할될 경우에 _i.e_인 범주 세트를 정의함으로써 그룹화의 모호성 문제를 우회한다. 파노피틱 분할 [10, 14]. 최근에 각 픽셀 다중 세그에서 이 모호성을 촉발하기 위해 이 모호성을 꺼낸다.\n' +
      '\n' +
      '그림 2: ** Scale의 수입 A 단일 포인트를 그룹화할 때 여러 그룹에 속할 수 있다. GARF 필드는 _규모 조건_를 사용하여 이러한 상충되는 신호를 하나의 친화성 필드로 조정한다.**.\n' +
      '\n' +
      '발효용 마스크를 제안할 수 있습니다. 그러나 SAM은 현장에서 일관된 계층적 그룹 세트를 회수하지 않으며, 이는 다중 규모의 3D 증류으로 가능하게 한다.\n' +
      '\n' +
      '계층적 부분 분해는 감독 [17, 21, 35] 또는 비지도 방식 [24]에서 3D 객체에서도 탐구되었다. 우리의 접근법은 2D 모델에서 정보를 왜곡하고 이러한 접근법은 3D 객체에 초점을 맞추는 동안 완전한 장면을 고려한다.\n' +
      '\n' +
      'NRF에서 분할을 위한 NeRF****의 발효 접근법은 보통 지상 진실 의미 라벨[29, 38], 일치하는 인스턴스 마스크[18], NeRF [34]에서 3D 분할 네트워크를 사용하여 3D로 분할 마스크를 증류한다. 그러나 이러한 기술은 계층적 그룹화를 고려하지 않으며, 사물이나 사례의 평평한 계층화에만 관심이 있다. Ren et al. [27]은 인간 상호작용을 이미지 스카운의 형태로 환산하여 상호 작용을 갖는 객체들을 분할한다. 보다 최근에, Cen et al. [3]은 사용자 프롬프트를 통해 이웃 뷰들 사이의 2D 마스크를 추적하여 SAM으로부터 3D 일관된 마스크를 회수하려고 노력한다. 텐 등은 SAM 인코더 특징을 3D로 증류하고 디코더를 쿼리함으로써 이를 시도한다. 이러한 접근법과 대조적으로, 우리의 접근 GARF필드는 사용자 입력을 필요로 하지 않으며, 자동으로 장면의 계층적 그룹화를 얻을 수 있으며, 더 나아가 복구된 그룹은 정의에 의해 시야에 부합한다.\n' +
      '\n' +
      '광전장(뷰 의존적 색상 및 밀도)과 함께 고차원 특징을 신경장에 베팅하는***3D 철관 필드**가 철저하게 탐색되었다. 낭만성 NeRF[38]와 같은 방법[38]이 채워진 철망 필드[16], 신경 신학적 융합 필드[33] 및 판포틱 라이프[29]는 체적 렌더링 후 2D 기능을 재구성하기 위해 3D 특징 필드를 최적화하여 3D로 증류한다. 이러한 특징은 DINO와 같은 전처리된 비전 모델 또는 의미적 세분화 모델로부터의 것일 수 있다. LERF[13]는 이 아이디어를 스케일 조절된 특징 분야로 확장하여 CLIP[26]과 같은 글로벌 이미지 임베딩에서 특징 필드의 훈련을 가능하게 한다. GARF 필드는 유사하게 3D에서 스케일 조절된 특징 필드를 최적화하는 것이지만, 다중 규모의 특징의 목적은 CLIP와 같은 명시적인 2D 특징을 재구성하는 대신 그룹화의 모호성을 해결하는 것이다. 또한 LERF는 공간 그룹화가 없고, 짧은 GARF필드 주소도 있다. 앞서 언급한 방법은 이미지 특징으로부터의 직접적인 감독을 기반으로 하는 반면, NeRF-SOS [8] 및 콘트롤리프트 [2]와 같은 다른 방법은 유사성에 기초한 광선 쌍 간의 대조적인 손실을 사용하여 단일 척도로 임의의 특징 필드를 최적화한다. GARF 필드는 마스크 라벨을 기반으로 포인트 간의 쌍별 관계를 정의할 수 있기 때문에 이 대조적인 접근법을 사용한다. 그러나 3D로 상충되는 마스크를 증류할 수 있는 스케일 조절된 대조 손실을 설계합니다. 또한 GARF필드에는 안정적인 훈련을 위해 Bhalgat et al. [2]의 느린 빠른 제형이 필요하지 않으며 아마도 스케일 조절된 훈련으로 가능하게 될 것이다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '2D 마스크 세대는.\n' +
      '\n' +
      'GARF필드는 제안된 이미지 세트를 입력함에 따라 취하여 표준 3D 체적 방사선 필드 및 스케일 조절된 친화성 필드와 함께 장면의 계층적 3D 그룹화를 생성한다. 이를 위해 먼저 SAM으로 입력 이미지를 전처리하여 마스크 후보를 얻는다. 다음으로, 우리는 단일 3D 위치와 유클리드 스케일을 취하는 친화성 필드와 함께 체적 방사계를 최적화하고 특징 벡터를 출력한다. 점들의 특징 벡터들의 쌍을 비교하여 친화도를 구한다. 최적화 후, 생성된 친화도 필드는 조대-미세 방식으로 하강 스케일에서 3D로 특징 임베딩을 재귀적으로 클러스터링하거나, 또는 특정된 사용자 쿼리를 분할하기 위해 장면이 분해되는 데 사용될 수 있다. 전체 파이프라인은 그림 3에 나와 있다.\n' +
      '\n' +
      'GARField를 훈련시키기 위해 먼저 이미지로부터 2D 마스크 후보를 채굴한 다음 3D 척도를 할당한다.\n' +
      '\n' +
      '그림 3: **GARF 현장 방법**: 입력 이미지 세트를 감안할 때 (Left) SAM을 조밀하게 질의함으로써 후보 그룹 세트를 추출하고, NeRF로부터 깊이를 상쇄하여 각각의 물리적 척도를 할당한다. 이러한 척도는 _규모 조건 친화도 필드_(그렇죠)를 훈련하는 데 사용된다. 훈련하는 동안 샘플링된 광선의 쌍은 다른 마스크에 상주하면 떨어져 있고, 같은 마스크에 착륙하면 함께 당겨진다. 각 마스크의 규모에서만 친화성을 감독하여 이들 간의 갈등을 해결하는 데 도움이 된다.\n' +
      '\n' +
      '마스크를 발라. 구체적으로, 우리는 SAM의 자동 마스크 생성기[15]를 사용하여 점들의 그리드에 SAM을 쿼리하고 질의 포인트당 3개의 후보 분할 마스크를 생성한다. 그런 다음 자신감과 중복으로 이 마스크를 필터링하여 서로 중첩되거나 포함할 수 있는 여러 크기의 마스크 후보 목록을 생성한다. 이 과정은 관점과는 독립적으로 수행되어 견해마다 일관성이 없을 수 있는 마스크를 생산한다. 이 연구에서 우리는 물체의 물리적 크기를 기반으로 그룹화의 계층화를 생성하기 위해 각 2D 마스크를 그림 3과 같이 물리적 3D 척도를 할당하고 이를 통해 각 훈련 카메라 포즈에서 깊이 이미지를 렌더링한다. 이 방법은 마스크의 3D 척도를 동일한 세계 공간에 갖추어 스케일 조절된 친화력을 가능하게 한다.\n' +
      '\n' +
      '세일-컨디션 앤피니티 필드.\n' +
      '\n' +
      '스케일-조건은 일관성이 없는 2D 마스크 후보를 통합할 수 있는 GARF필드의 핵심 구성 요소이며, 동일한 점은 원하는 그룹의 입도에 따라 여러 가지로 그룹화될 수 있다. 스케일-조건은 질의가 어느 그룹에 속해야 하는지에 대한 모호성을 해소하기 때문에 이러한 불일치를 완화한다. 규모 조건하에서 같은 지점의 상충되는 마스크는 훈련 중 더 이상 서로 싸우지 않고, 오히려 서로 다른 친화도 규모에서 같은 장면에서 공존할 수 있다.\n' +
      '\n' +
      '우리는 LERF [13]의 제형과 유사한 3D 포인트 \\(x\\) 및 유클리드 스케일 \\(s\\)에 걸쳐 스케일 조절된 친화성 필드 \\(F_{\\text{g}}(x,s)\\맵스트로 R^{d}\\)를 정의한다. 출력 특징은 단위 하이퍼 스피드로 제한되며, 스케일에서의 두 점 사이의 친화도는 \\(A(x_{1},x_{2},s)=-||F_{\\text{g}}(x_{1},s)-F_{\\text{g}}(x_{2},{2})||_{2}\\)에 의해 정의된다. 이러한 특징은 NRF 밀도를 기반으로 동일한 렌더링 가중치를 사용하여 가중 평균으로 체적 렌더링되어 X선 기준으로 값을 얻을 수 있다.\n' +
      '\n' +
      '3.2.1.1 세부 감독 감독##### 3.2.1.\n' +
      '\n' +
      '필드는 DrLIM[9]에서 제공하는 정의에 따라 마진 기반 대비 목표로 감독된다. 손실의 핵심 구성 요소는 주어진 규모에서 두 가지이며, 하나는 동일한 그룹 내의 특징을 닫도록 끌어당기고 다른 그룹은 서로 다른 그룹으로 특징을 밀어낸다.\n' +
      '\n' +
      '구체적으로, 동일한 훈련 이미지 내에서 마스크 \\(\\mathcal{M}_{A}_{A},\\mathcal{M}_{A},\\mathcal{M}_{B}\\)에서 샘플링된 두 개의 광선 \\(r_{A}},\\mathcal{M}_{B}\\)을 고려하여 해당 척도 \\(s_{A}\\) 및 \\(s_{B}\\)를 사용한다. 우리는 광선 수준의 특징 \\(F_{A}\\) 및 \\(F_{B}\\)를 얻기 위해 각 광선을 따라 스케일 조절된 친화성 특징을 볼륨으로 렌더링할 수 있다. HH(\\mathcal{M}_{A}=\\mathcal{M}_{B}_{B}\\)가 L2 거리(\\mathcal{L}_{\\text{pull}}=||F_{A}-{B}||\\)와 함께 기능이 당겨진다. \\(\\mathcal{M}_{A}\\neq\\mathcal{M}_{B}\\\\neq\\mathcal{M}_{B}\\)가 떨어져 있는 경우,\\(\\mathcal{L}_{\\text{push}}=\\text{ReLU}=m-||F_{A}-F_{B}||)\\(\\m\\)는 하위 결합 거리 또는 마진이다. 중요하게도, 이러한 손실은 서로 다른 관점에 걸친 마스크는 대응 관계가 없기 때문에 동일한 이미지에서 샘플링된 광선들 사이에서만 적용된다.\n' +
      '\n' +
      '규모 수퍼비전 3.2.2는 스코일 수퍼비전 3.2.######## 3.2로 스코일 수퍼비전 3.2를 지각하고 있다.\n' +
      '\n' +
      '앞의 대조적 손실만으로는 위계를 보존하기에 충분하지 않다. 예를 들어, 그림 1에 나와 있다. 4, 계란은 0.22 척도로 국물과 올바르게 그룹화되지만 더 큰 규모에서 조각이 떨어져 있다. 우리는 이 그룹화 불안정성을 가정한다. 우리는 다음과 같은 수정을 도입함으로써 여기에서 이러한 단점을 다룬다.\n' +
      '\n' +
      '3D 마스크 스케일을 사용하는 경우*** 연속 스케일 감독*** 그룹은 마스크를 선택하는 개별 값에서만 정의된다. 이것은 그림 9의 상단에서 볼 수 있듯이 규모의 큰 비지도 영역을 생성하며, 이는 그림 9의 상단에 도시된 바와 같이 규모의 큰 비지도 영역을 생성한다.\n' +
      '\n' +
      '그림 4: **Densified Scale 감독***: 군집 내의 2개의 포도를 컨사이더한다. 조영적 손실에 대한 척도를 사용하는 나니어_는 포도 및 포도 3차 수준에서만 친화도를 감독하여 전체 간격을 부적합하게 두었다. GARField에서, 우리는 마스크 유클리드 척도와 2) 사이의 척도를 증가시켜 더 큰 규모의 봉쇄에 보조 손실을 부과하는 것을 1) 농도화한다.\n' +
      '\n' +
      '인터액티브 선택***를 사용한 그림 5: **3D 자산 추출: Users는 클릭 포인트 및 스케일을 사용하여 GARF필드와 상호작용적으로 뷰-연관 3D 그룹을 선택할 수 있다.\n' +
      '\n' +
      '현재 마스크의 스케일과 다음 가장 작은 마스크의 스케일 사이에서 균일하게 규모 \\(s\\)를 증강하여 규모 감독을 수행한다. 레이의 마스크가 주어진 관점에 가장 작은 마스크인 경우 0과 \\(s_{0}\\) 사이를 보간한다. 이것은 비지도 지역을 남기지 않는 현장 전체에 걸쳐 지속적인 규모의 감독을 보장합니다.\n' +
      '\n' +
      '**도담도 Loss**: 2개의 광선 \\(r_{1}\\)과 \\(r_{2}\\)가 규모 \\(\\)로 동일한 마스크에 있으면 \\(s\\)보다 큰 모든 규모로 함께 당겨야 한다. 직관적으로 동일한 군집 내에 두 개의 포도가 있다(그림). 4)도 더 큰 규모(예를 들어, 전체 반점)로 함께 그룹화된다. 각 훈련 단계에서 스케일 \\(s\\)에서 함께 그룹화된 광선에 대해 또한 광선도 함께 당기는 더 큰 규모 \\(s^{\\prime}>\\)를 샘플한다. 이것은 더 작은 규모의 친화도가 더 큰 규모에서 손실되지 않도록 보장한다.\n' +
      '\n' +
      '3.2.3레이의 3.2.3레이와 마스크 샘플링의#######\n' +
      '\n' +
      '표준 NeRF 훈련과 마찬가지로 손실을 계산하는 광선을 샘플링합니다. GARF필드는 각 기차 이미지 내에서 대조적인 손실을 사용하기 때문에 훈련 중 균일하게 순진한 샘플링 픽셀은 광선의 각 미니부치에 훈련 신호를 제공하기에는 부적절하다. 각 열차 배치에서 충분한 쌍을 보장하기 위해 먼저 N개의 이미지 및 각 이미지 내의 샘플 M 광선을 샘플한다. 감리를 위한 점쌍의 수와 이미지 수를 균형을 맞추기 위해 이미지당 16개의 이미지와 256개의 포인트를 샘플링하여 기차 반복당 4096개의 샘플을 생성했다.\n' +
      '\n' +
      '샘플링된 각 광선에 대해, 우리는 또한 문제의 열차 단계에 대한 그룹 라벨로 사용할 마스크를 선택해야 한다. 이를 위해 훈련 전반에 걸쳐 픽셀에서 마스크 라벨에 대한 매핑을 유지하고 각 기차 단계에서 해당 마스크 목록에서 각 레이에 대한 마스크를 무작위로 선택한다. 이 샘플링 과정에서 두 개의 중요한 굴이 있는데, **1)*** 마스크의 확률은 마스크의 2D 화소 영역의 로그와 반비례하여 가중된다. 이것은 더 큰 마스크를 더 많은 픽셀을 통해 선택할 수 있기 때문에 샘플링 프로세스를 지배하는 큰 척도를 방지한다. **2)***링 마스크 선택은 동일한 이미지에서 광선을 가로질러 선택된 랜덤 스케일을 조정하여 포지티브 쌍의 확률을 증가시킨다. 이를 위해 이미지당 0에서 1 사이의 단일 값을 샘플링하고, 동일한 값을 갖는 각 픽셀의 마스크 확률 CDF로 인덱스를 검수하여 동일한 그룹 내의 토지가 동일한 마스크를 할당되는 픽셀을 보장한다. 그렇지 않으면 훈련을 불안정화하는 힘을 밀어붙여 손실이 지배적이다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '이 방법은 그룹화 필드에 대한 별도의 출력 헤드를 정의함으로써 Nerfacto 모델 위에 Nerft 스튜디오[32]에 구축된다. 그룹화 필드는 24개의 레이어와 층당 2개의 특징 차원을 갖는 해시그리드[23]와 해시그리드 기능과 연결된 추가 입력으로 스케일링을 하는 256개의 뉴런 및 ReLU 활성화를 갖는 4층 MLP로 표시된다. 카메라의 정도를 \\(2\\times\\)로 스케일 캡을 하고, 계산된 3D 마스크 스케일(Sec 3.1)의 분포에 스케이트렌의 분위변환을 사용하여 MLP에 입력된 스케일을 정규화한다. 출력 임베딩은 \\(d=256\\) 치수입니다. 친화성 특징의 기울기는 이러한 표현이 가중치 또는 구배를 공유하지 않기 때문에 NeRF의 RGB 출력에 영향을 미치지 않는다.\n' +
      '\n' +
      '우리는 2000 단계의 NeRF 최적화 후에 그룹화 필드를 훈련하기 시작하여 수렴하기 위한 기하학 시간을 제공한다. 또한, 속도 트레이닝을 위해 먼저 해시 값을 부피로 렌더링한 다음 MLP에 입력으로 사용하여 광선 특징을 얻는다. 이 지연 렌더링을 통해 동일한 광선은 하나의 추가 MLP 호출만으로 서로 다른 스케일에 질의될 수 있다. 우리는 MLP에 입력하기 전에 볼륨 렌더링의 결과를 단위 규범으로 정규화하고 점별 질의에 대해 개별 해시그리드 값이 정규화된다. 전처리 SAM 마스크는 3∼10분 정도 걸리는데 이어 GTX 4090 훈련을 위해 20분 정도가 소요된다.\n' +
      '\n' +
      '4번 수업.\n' +
      '\n' +
      '일단 스케일 조절된 친화도를 최적화한 후, GARF필드는 각 노드가 잠재적 하위 그룹으로 파손되도록 나무에 조직화된 3D 그룹의 계층화를 생성한다. 이를 위해 HDBSCAN[19]을 사용하여 친화도에 대한 척도를 감소시켜 군집을 재귀적으로 군집링하는데, 이는 클러스터의 수에 대해 이전에 필요하지 않은 밀도 기반 군집링 알고리즘을 사용한다. 이 클러스터링 과정은 체적계로 2D로 수행할 수 있는 영상에서 렌더링된다.\n' +
      '\n' +
      '그림 6: **3D 결정: GARField는 축소 규모에서 재귀적으로 질의하여 장면을 객체와 하위 부분으로 군집링할 수 있다.**3D 하위 부분으로 군집화할 수 있다.\n' +
      '\n' +
      '그림 7: ** 결과**: GARF필드에서 최고 수준의 군집을 선택하여 글로벌 현장에서 물체를 추출한 다음 감소 규모에서 지역 군집을 시각화한다. GARF 필드는 완전한 3D 객체 마스크를 생성할 수 있으며, 입력 마스크를 기반으로 이러한 물체를 의미 있는 하위 부분으로 분해할 수 있다. 우리는 가우시안 스플랫[12]을 사용하여 이러한 시각화를 3D로 생성한다. 더 많은 결과를 위해 보충 비디오를 참조하세요.\n' +
      '\n' +
      '마스크, 또는 포인트에 걸친 3D로 포인트 라우드를 산출합니다. 그림을 참조하세요. 장면 분해의 시각화를 위한 6.\n' +
      '\n' +
      '** 초기화**: 첫째, 계층화를 초기화하기 위해 우선 입력 카메라의 위치 범위에 해당하는 모든 실험에 대해 \\(s_{\\text{max}}\\)로 설정한 대규모(s_{\\text{max}\\)에서 전 세계 군집 특징이 있다. 이러한 클러스터는 장면 분해에서 최상위 노드를 형성한다.\n' +
      '\n' +
      '** 재귀 클루스터링**: Next는 장면 노드의 계층적 트리를 생성하기 위해 고정된 엡실론(우리는 0.05)에 의해 척도를 반복적으로 감소시켜 각 잎 노드에 HDBSCAN을 실행한다. HDBSCAN이 주어진 노드에 대해 하나 이상의 클러스터를 반환하면 해당 클러스터를 아동으로 추가하고 재발한다. 이는 절차가 종료되는 스케일 0에 도달할 때까지 계속되며 현재 트리를 되돌린다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '우리는 GARF필드에서 3D 장면을 크기와 의미론이 광범위하게 다양한 계층 그룹으로 분해하는 능력을 평가한다. 기존의 3D 스캔 데이터 세트는 객체 레벨 스캔(7, 22], 시뮬레이션된 [2], 주로 실내 가정용 장면[6]을 포함하는 경향이 있다. GARF필드 평가를 위해 대신 네르프 학습지와 LERF 데이터세트로부터 다양한 실내외 장면을 사용하고 이 논문에 대한 추가 포획을 사용한다. 우리는 GARField의 분해 능력을 테스트하여 중요한 객체 계층화를 갖는 장면을 실험한다. 우리는 그림 1에서 질적 결과를 제공한다. 3 및 그림. 6개는 지상 진리 마스크를 선택 장면에 주석하여 정량적으로 평가하고, 그 중 전체 목록은 보충에 있다.\n' +
      '\n' +
      '적합한 내용.\n' +
      '\n' +
      '가우시안 스플라팅[12]을 사용하여 가우시안 센터에서 GARF필드 친화도 분야에 질의하여 분해를 시각화한다. 우리는 가우시안 스플렛이 NeRF에 비해 3D에서 분할하기 쉽기 때문에 이것을 한다. 파이프라인에 대한 완전한 설명을 위한 보충제를 참조하세요. 모든 렌더링은 2D 이미지 뷰의 분절이 아닌 완전한 3D 모델이다.\n' +
      '\n' +
      '우리는 두 가지 유형의 계층적 군집화 결과를 시각화한다. 그림에서. 7은 전 세계적으로 손으로 선택된 거친 규모로 현장을 군집화한 다음, 이러한 장면 전체 군집에서 우리는 소수의 객체에 해당하는 그룹을 선택하고 더 나아가 하위 그룹으로 분해한다. 우리는 성공적으로 감소하는 척도로 얻은 군집을 시각화하여 그룹의 과립성을 증가시킨다. GARF 필드는 작은 규모로 각 열쇠가 그룹으로 간주되는 키보드와 같은 인공물부터 NERF 건과 레고 불도저 부품까지 다양한 장면과 물체에 걸쳐 높은 충실도 3D 그룹을 달성하며, 이는 개별 꽃과 잎뿐만 아니라 식물처럼 복잡한 자연물까지 달성된다. 규모를 달리함으로써, 화분에 비해 각 개별 잎(1열)에 대한 숙시네이트와 같은 다양한 수준에서 물체를 분리할 수 있거나 불도저 스카퍼의 번니 장난감을 식별할 수 있으며, 이는 셔츠, 귀, 헤드(5열, 우측)로 더 그룹화된다. 그림을 참조하세요. 선택한 장면 전체 클러스터 시각화를 위한 10개입니다.\n' +
      '\n' +
      '그림에서. 6은 Sec. 4에 설명된 방법으로 생성된 트리 분해를 시각화한다. 우리는 먼저 나무 분해를 설명하기 위해 중심 동상을 선택하는 최상위 노드에서 글로벌 군집링을 보여준다. 화살표는 위계에 있는 아이들을 나타내며, 동상이 모발, 다리, 몸통 등으로 갈수록 어떻게 분해되는지를 보여준다. 더 많은 트리 시각화를 위한 보충제를 참조하세요.\n' +
      '\n' +
      '### Quantitative Hierarchy\n' +
      '\n' +
      '우리는 두 가지 메트릭을 사용하여 주석이 달린 이미지에 대한 접근 방식을 정량적으로 평가하는데, 첫 번째 측정 시각은 여러 견해의 주석에 대한 일치성과 두 번째 측정 결과는 지상 진리 인간 주석에 대한 mIOU를 통해 다양한 계층적 마스크의 리콜이다.\n' +
      '\n' +
      '**3D 보완:** 다운스트림 태스크의 경우 그룹들은 전체 3D 객체, 예를 들어, 그 측면 중 하나만 포함하는 그룹들에 대응하는데 유용하다. GARF필드는 항상 건설에 의해 뷰-지속성 그룹을 생성하지만, 반드시 완전한 물체를 포함하는 것은 아닐 수 있다. 우리는 확인을 통해 완성도를 평가한다.\n' +
      '\n' +
      '그림 8: **세그먼트-모든 것 [15] 대 대. GARF필드**: SAM의 자동 마스크 생성기는 특히 소형 마스크의 클러스터가 있고 카메라가 물체로부터 멀리 떨어져 있을 때 주어진 관점에서 모든 마스크를 회상하는 데 어려움을 겪을 수 있다. 대조적으로, GARF필드의 스케일 조절된 친화성 필드는 3D에서 여러 관점에서 마스크를 통합한다.\n' +
      '\n' +
      '그림 9: **Ablation**: 밀집된 위계 감독 없이 포인트는 규모에 걸쳐 일관성 없는 친화성을 가질 수 있다. 1) 비지도 규모에서 잘못된 큰 친화도가 있을 수 있거나 2) 더 큰 규모에서 예상치 못한 친화도가 떨어질 수 있다.\n' +
      '\n' +
      '전체 3D 객체가 다양한 관점에 걸쳐 함께 그룹화되는 것이다. 이를 위해 5개의 장면에서는 3차원점을 선택해서 3가지 다른 시각으로 투사하고, 그 점을 거친, 중, 미세 수준에서 포함하는 3개의 대응하는 관점과 일치하는 그라운드 진리 마스크를 라벨링한다. 이 지점에서 우리는 0.05 증분으로 여러 스케일에서 GARF필드에서 여러 개의 마스크를 채굴하며, 각 스케일에서는 0.9에서 임계치된 특징 유사도를 기반으로 마스크를 얻는다. 또한 이미지 내 지점을 클릭하고 3개의 마스크를 모두 복용하여 SAM과 비교한다. 두 가지 방법에 대해 모든 후보 마스크에 대해 계산된 최대 mIOU를 보고한다.\n' +
      '\n' +
      '결과는 표 1에 나와 있으며, GARF필드는 관점을 통해 SAM보다 더 완전한 3D 마스크를 생성하여 물체의 다중 뷰 인간 주석을 갖는 mIOU가 더 높다. 이 효과는 특히 가장 세분화된 수준에서 명백하며, 특정 관점에서 SAM이 그림 8의 멀리서 키보드 키와 같이 미세 그룹을 생성하기 위해 투쟁한다. 지하 마스크의 비교 및 시각화 수치를 보충한다.\n' +
      '\n' +
      '*** 계층적 그룹화 기록:** 여기 우리는 여러 입도에서 그룹을 리콜하는 GARField의 능력을 측정한다. 5개의 장면에서 1개의 _novel_ 관점을 선택하고 1-2개의 대상에 대해 최대 3개의 지상 진리 계층 그룹을 레이블링한다. GARF 필드는 이미지 공간 특징을 클러스터링하여 트리 노드당 하나의 마스크를 출력함으로써 섹션 4에 설명된 대로 마스크 세트를 출력한다. 우리는 모든 출력 마스크를 보관하여 SAM의 자동 마스크 생성과 비교한다. 우리는 GARF필드(-스케일) 두 가지 방법으로 GARF필드를 제거하며, GARF필드(-계층)는 Sec 3.2.2.2에서 농도화된 감독을 제거한다.\n' +
      '\n' +
      '표 2에서 우리는 SAM 마스크 세트 또는 GARField에 의해 생성된 트리에서 가장 중첩이 높은 지상 진리 마스크의 mIOU를 보고한다. GARF필드는 여러 관점에서 그룹을 융합했기 때문에 SAM의 단일 뷰보다 충실도가 높아 주석과 함께 mIOU가 더 높다. 우리의 연마는 고품질 그룹을 위해 스케일 컨디셔닝과 스케일 조도가 필요하다는 것을 보여준다. 그림. 9는 순진한 감독을 통해 더 높은 규모로 친화력을 저하시키는 것을 보여준다.\n' +
      '\n' +
      '## 6 Limitations\n' +
      '\n' +
      '코어의GARF 필드는 2D 마스크 발생기에서 출력을 증류하고 있으므로 마스크가 원하는 그룹을 포함하지 않으면 3D에서는 나타나지 않는다. 고르지 않은 관점을 가진 영역은 인공 집단 경계로 고통받을 수 있으며, 예를 들어 물체가 닫혀서만 볼 경우 입력관이 완전히 포함되어 있지 않기 때문에 함께 그룹화할 수 없다. 우리는 물리적 크기를 사용하여 그룹 모호성을 처리하지만 단일 규모 내에서 여러 그룹이 있을 수 있다. 예컨대, 오브젝트가 있거나 없는 컨테이너가 동일한 스케일을 가질 수 있기 때문에 컨테이너에 포함된 오브젝트와 충돌이 발생할 수 있다. 향후 작업은 어포던스와 같은 모호성을 그룹화하는 다른 방법을 고려할 수 있다. 스케일-조건의 또 다른 결과는 서로 다른 크기의 객체 부분이 모든 것이 아니라 나무에서 따로 분기되는 것이며, 동일한 테이블 상의 다수의 오브젝트들은 나무의 상이한 레벨들에서 나타날 수 있다는 것이다. 이 작품의 나무 생성은 순진한 욕심 알고리즘으로 보충제의 나무에서 볼 수 있듯이 더 깊은 수준에서 어지러운 작은 그룹을 초래할 수 있다. 향후 작업은 계층적 군집링의 보다 정교한 방법을 탐색할 수 있다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '우리는 계층적 3D 장면 분해를 위한 밀집 스케일 조절된 친화성 필드에 다단계 마스크를 증류하는 방법인 GARF필드를 제시한다. 친화성 필드는 스케일 조건을 활용함으로써 상충되는 2D 그룹 입력에서 의미 있는 그룹을 배우고 여러 수준에서 장면을 탈피할 수 있으며, 이는 추출에 사용될 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r r r} \\hline \\hline  & \\multicolumn{2}{c}{Fine} & \\multicolumn{2}{c}{Medium} & \\multicolumn{2}{c}{Coarse} \\\\ Scene & SAM & Ours & SAM & Ours & SAM & Ours \\\\ \\hline \\hline \\multicolumn{1}{l}{teatime} & 81.6 & **92.7** & 97.3 & **97.9** & - & - \\\\ \\multicolumn{1}{l}{bouquet} & 17.4 & **76.0** & 73.5 & **81.6** & 76.1 & **85.4** \\\\ \\multicolumn{1}{l}{keyboard} & 65.3 & **88.8** & 73.6 & **98.4** & - & - \\\\ \\multicolumn{1}{l}{ramen} & 53.3 & **79.2** & 74.7 & **90.7** & 92.6 & **95.5** \\\\ \\multicolumn{1}{l}{living\\_room} & 85.3 & **90.5** & 74.2 & **80.7** & 88.6 & **94.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **3D 보완*** 최대 3단계 계층이 있는 단일 지점에 대한 현장 주석 mIOU를 보고한다. SAM은 GARF필드에 비해 시야와 일치하는 미세 그룹을 생산하기 위해 고군분투한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r} \\hline \\hline \\multirow{2}{*}{Scene} & \\multirow{2}{*}{SAM [15]} & \\multicolumn{2}{c}{Ours} & \\multicolumn{2}{c}{Ours} & \\multirow{2}{*}{Ours} \\\\  & & (-scale) & (-dense) & \\\\ \\hline \\hline \\multicolumn{1}{l}{ramen} & 74.9 & 64.1 & 74.1 & **85.6** \\\\ \\multicolumn{1}{l}{teatime} & 64.9 & 67.7 & 66.1 & **86.6** \\\\ \\multicolumn{1}{l}{keyboard} & 23.2 & 57.6 & 73.1 & **77.9** \\\\ \\multicolumn{1}{l}{bouquet} & 34.4 & 49.8 & 72.9 & **76.4** \\\\ \\multicolumn{1}{l}{living\\_room} & 59.6 & 49.7 & 62.1 & **76.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: ** 계층적 그룹화 기록:** 우리는 다양한 물체의 다중 규모 그룹의 인간 주석에 대해 mIOU를 보고한다.\n' +
      '\n' +
      '그림 7의 선택된 장면에 대한 그림 10: **Scene-Wide Clustering*** 시각화를 보여준다.\n' +
      '\n' +
      '다층 과립의 설비입니다. GARF 필드는 로봇 공학, 동적 장면 재구성 또는 장면 편집과 같은 다단계 그룹이 필요한 작업에 대한 애플리케이션을 가질 수 있다.\n' +
      '\n' +
      '## 8 Acknowledgements\n' +
      '\n' +
      '이 사업은 NSF:CNS-2235013 및 DARPA 계약 번호 HR001123C0021에 의해 부분적으로 지원되었으며 정민 및 저스틴은 부분적으로 Grant No DGE 2146752의 NSF 대학원 연구 펠로우십 프로그램에 의해 지원되며 이 자료에 표현된 의견, 소견 및 권고 사항은 저자(들)의 견해이며 반드시 국립 과학 재단의 견해를 반영하지 않는다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]P. 아벨라즈, M. 마이어, C. 푸이크, J. 말릭(2011) 콘텐츠 검출 및 계층적 이미지 세분화이다. IEEE 전환은 패턴 분석 및 기계 정보 33(5), pp 898-916: SS1에 의해 전환된다.\n' +
      '*[2]Y. Bhalgat, I. Laina, J. F. 헨리쿼스, A. Zisserman 및 A. Vedaldi(2023) 조영제 리프트: 느린 빠른 대조 융합에 의한 3d 객체 인스턴스 분할이다. 네르IPS. SS1: SS1로 받았습니다.\n' +
      '* [3]J. 센, Z. 주, 제이 포, C. 양, W. 선, L. Xie, X. 장, Q. 티안(2023)은 네프가 있는 3d에 모든 것을 분할한다. SS1: SS1로 받았습니다.\n' +
      '*[4]X. 텐, J. 탕, D.완, J.왕, G.Zeng (2023) 상호 작용 세그먼트는 특징인 모방과 함께 모든 nerf이다. arXiv 프리프린트 arXiv:2211.12368: SS1에 의해 계산된다.\n' +
      '*[5]T. 평균, F. 바네이트 및 J. 시(2005) 스펙트럼 분할은 다중 피규어 그래프 분해를 사용한다. 2005년 컴퓨터 비전 및 패턴 인식 관련 IEEE 컴퓨터 협회 회의(CVPR\'05), 2, pp. 1124-1131 vol.: SS1.\n' +
      '*[6]A. 다이, A. X. 창, M. 사바, M. 할버, T. 푸커, M. Niessner(2017) Scannet: 풍부한 실내 장면의 3d 재구성을 하지 않았습니다. 컴퓨터 비전 및 패턴 인식에 대한 IEEE 회의의 진행에서 pp는 5828-5839: SS1에 의해 발표되었다.\n' +
      '*[7]L. 다운, 프란치스코, N. 코에니히, B. 킨만, R. 하이만, K. 리만, T. 맥후 및 V. 반우케(2022) 구글은 3d 스캔된 생활용품의 고품질 데이터셋인 물체를 스캔했다. 2022년 로봇 및 자동화에 관한 국제 회의(ICRA)에서 pp 2553-2560: SS1이 게시했다.\n' +
      '* [8]Z. 팬, P. Wang, Y. 지앙, X. 공, D. 주, Z. 왕(2022) 네르프-소: 복잡한 장면에서 모든 뷰 자기 지도 객체 분할이다. arXiv 프리프린트 arXiv:2209.08776: SS1에 의해 계산된다.\n' +
      '*[9]R. 헤델, S. 초프라, Y. 리쿤(2006)차원 감소는 불변 매핑을 학습함으로써 감소된다. 2006년 컴퓨터 비전 및 패턴 인식(CVPR\'06)에 관한 IEEE 컴퓨터 사회 콘퍼런스에서 pp 1735-1742: SS1로 지정되었다.\n' +
      '*[10]K. 그는 G. 게옥사리, P. 달러 및 R. 기릭(2017) 마스크 r-cnn. 컴퓨터 비전에 관한 IEEE 국제 회의의 개최에서 pp. 2961-2969: SS1에 의해 발표되었다.\n' +
      '*[11]T. 케, J 황, Y. 구, X. 왕과 S. X. 유(2022)는 멀티뷰 분류 및 클러스터링 변환기를 사용하여 계층적 의미 세분화를 폐지했다. 컴퓨터 비전 및 패턴 인식 관련 IEEE/CVF 회의의 개시: SS1에 의해 계산된다.\n' +
      '*[12]B. 케라블, G. 코파나스, T. 리무클러와 그레타키스(2023) 3d 가우시안가 실시간 라디에이터 필드 렌더링을 위해 튀었다. ACM 그래픽42(4)에 대한 거래. SS1: SS1로 받았습니다.\n' +
      '*[13]J]. Kerr, C. M. 김모. K. 골드버그, A. 가나가와, M. Tancik(2023) Lerf: 언어 내장 영상의 필드입니다. 컴퓨터 비전 국제 회의(ICCV)에서: SS1이 가입했다.\n' +
      '*[14]A. 키릴로프, K. 그는 R. 기릭, C. 루테 및 P. 달러(2017) 판오피틱 분할. 컴퓨터 비전 및 패턴 인식_ 페이지 9404-9413에 대한 IEEE/CVF 회의의 개시.\n' +
      '* [15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. _ICCV_, 2023.\n' +
      '* [16] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing nerf for editing via feature field distillation. _NeurIPS_, 35:23311-23330, 2022.\n' +
      '* [17] Jun Li, Kai Xu, Siddhartha Chaudhuri, Ersin Yumer, Hao Zhang, and Leonidas Guibas. Grass: Generative recursive autoencoders for shape structures. _ACM Transactions on Graphics (TOG)_, 36(4):1-14, 2017.\n' +
      '* [18] Yichen Liu, Benran Hu, Junkai Huang, Yu-Wing Tai, and Chi-Keung Tang. Instance neural radiance field. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [19] Leland McInnes, John Healy, and Steve Astels. hdbscan: Hierarchical density based clustering. _J. Open Source Softw._, 2(11):205, 2017.\n' +
      '* [20] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In _ECCV_, 2020.\n' +
      '* [21] Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy Mitra, and Leonidas J Guibas. Structurenet: Hierarchical graph networks for 3d shape generation. _arXiv preprint arXiv:1908.00575_, 2019.\n' +
      '* [22] Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna Tripathi, Leonidas J Guibas, and Hao Su. Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 909-918, 2019.\n' +
      '* [23] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Transactions on Graphics (ToG)_, 41(4):1-15, 2022.\n' +
      '* [24] Despoina Paschalidou, Luc Van Gool, and Andreas Geiger. Learning unsupervised hierarchical part decomposition of 3d objects from a single rgb image. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1060-1070, 2020.\n' +
      '* [25] Jordi Pont-Tuset, Pablo Arbelaez, Jonathan T Barron, Ferran Marques, and Jitendra Malik. Multiscale combinatorial grouping for image segmentation and object proposal generation. _IEEE transactions on pattern analysis and machine intelligence_, 39(1):128-140, 2016.\n' +
      '* [26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [27] Zhongzheng Ren, Aseem Agarwala, Bryan Russell, Alexander G Schwing, and Oliver Wang. Neural volumetric object selection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6133-6142, 2022.\n' +
      '* [28] Jianbo Shi and J Malik. Normalized cuts and image segmentation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 22(8):888-905, 2000.\n' +
      '* [29] Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bulo, Norman Muller, Matthias Niessner, Angela Dai, and Peter Kontschieder. Panoptic lifting for 3d scene understanding with neural fields. _arXiv preprint arXiv:2212.09802_, 2022.\n' +
      '* [30] Piotr Skalski. Make Sense. [https://github.com/SkalskiP/make-sense/](https://github.com/SkalskiP/make-sense/), 2019.\n' +
      '* [31] Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. Parsing natural scenes and natural language with recursive neural networks. In _Proceedings of the 28th international conference on machine learning (ICML-11)_, pages 129-136, 2011.\n' +
      '* [32] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Justin Kerr, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, et al. Nerfstudio: A modular framework for neural radiance field development. _arXiv preprint arXiv:2302.04264_, 2023.\n' +
      '* [33] Vadim Tscherzki, Iro Laina, Diane Larlus, and Andrea Vedaldi. Neural Feature Fusion Fields: 3D distillation of self-supervised 2D image representations. In _3DV_, 2022.\n' +
      '* [34] Suhani Vora, Noha Radwan, Klaus Greff, Henning Meyer, Kyle Genova, Mehdi SM Sajjadi, Etienne Pot, Andrea Tagliasacchi, and Daniel Duckworth. Nsf: Neural semantic fields for generalizable semantic segmentation of 3d scenes. _arXiv preprint arXiv:2111.13260_, 2021.\n' +
      '* [35] Yanzhen Wang, Kai Xu, Jun Li, Hao Zhang, Ariel Shamir, Ligang Liu, Zhiquan Cheng, and Yueshan Xiong. Symmetry hierarchy of man-made objects. In _Computer graphics forum_, pages 287-296. Wiley Online Library, 2011.\n' +
      '* [36] Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In _Proceedings of the IEEE international conference on computer vision_, pages 1395-1403, 2015.\n' +
      '* [37] Stella X Yu. Segmentation using multiscale cues. In _Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004._, pages I-I. IEEE, 2004.\n' +
      '* [38] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and Andrew Davison. In-place scene labelling and understanding with implicit scene representation. In _ICCV_, 2021.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      '그림 11: **C 완전한 나무**: 그림 6의 나무에서 모든 층과 모든 노드의 완전한 시각화는 각 노드 내의 다른 군집을 보여주고 각 행은 크기에 따라 정렬된 나무에서 주어진 깊이에서 모든 노드를 시각화한다.\n' +
      '\n' +
      '선택된 지점과의 친화도. 여러 그룹을 검색하기 위해 다양한 스케일에 걸쳐 GARF필드에 질의하고 큰 중첩과 함께 그룹을 병합한다. 비디오에서 사용자는 그림 1에서 굴삭기, 크레인 및 스투퍼를 추출할 수 있다. 한 번의 클릭이 있는 1.\n' +
      '\n' +
      '신청자 B 실험 세부사항\n' +
      '\n' +
      '### Hierarchical Decomposition\n' +
      '\n' +
      '관심 군집을 선택하면 HDBSCAN과 재귀적으로 군집링하여 트리를 구성한다. 이 과정을 위해 모든 실험에 대해 고정된 0.1의 HDBSCAN 군집 엡실론과 40의 최소 군집 크기를 사용한다. 나무는 비잡음 군집에서 재간호 _만_를 통해 깊이 우선 검색으로 탐욕스럽게 구성된다. 잡음 군집을 건설 후 나무에 다시 추가하기 때문에 이는 다육성 장면의 저수준과 같이 작은 사라지고 있는 지역을 초래할 수 있다는 점에 유의한다. 이러한 유물들은 향후 작업에서 해결하기를 희망하는 비공백나무 건설로 더 잘 다루어질 것이다.\n' +
      '\n' +
      '트리 경합을 속도화하기 위해 먼저 Open3D의 복셀다운 샘플링으로 입력 가우시안 스플라트를 하위 샘플링하여 점들의 해상도를 \\(0.01\\\\)로 감소시켰으며, 예를 들어 0.1 스케일 감쇠 샘플의 친화도를 0.001 복셀 분해능으로 감소시킨다. 나무 건설 후, 그로 인한 트리는 한 명의 아이와 한 명의 부모와 함께 노드의 사슬을 제거하기 위해 프루닝된다.\n' +
      '\n' +
      '푸셔스 노이즈의 치료.\n' +
      '\n' +
      '한 가지 극복을 위한 도전은 HDBSCAN이 클러스터 라벨을 얻지 못하는 \'소음\' 클러스터를 출력할 수 있다는 사실이다. 이는 NeRF 기하학과 잘 정렬되지 않는 가우스, 두 그룹의 경계에 있기 때문에 시끄럽거나 훈련된 친화성 분야에서 소음을 일으키기 때문에 발생할 수 있다. 이러한 소음 군집을 처리하기 위해 특징 공간과 달리 유클리드 공간에서 계산된 가장 가까운 _ph 물리적_ 군집의 라벨이 있는 노이즈로 간주되는 가우스에게 라벨을 할당한다. 우리는 이것이 특징 공간 자체 내에서 부드러운 군집링보다 더 응집적인 결과를 생성하는 것을 발견한다. 글로벌 군집링 동안(그림 10, 7) 세 잡음 클러스터는 전체 장면에 걸쳐 코스터들에게 할당되고, 트리 분해 동안(그림 6)이 할당된다. 세 잡음 클러스터는 각 노드에서만 사용할 수 있는 클러스터로부터 국부적으로 할당된다.\n' +
      '\n' +
      '3D 보완 실험.\n' +
      '\n' +
      '2.3.1 지상진술자승소######.\n' +
      '\n' +
      '우리는 주석을 위한 다각형 모양을 사용하여 온라인 도구 \'메이크 센스\'(30)를 사용하여 무작위로 선택된 소설 뷰에 지상 진리 분할 마스크를 주석한다. 그림에서. 12, 데이터 주석 과정에서 상태에 대한 시각화를 제시한다.\n' +
      '\n' +
      '주석 과정은 주어진 관점 내에서 특정 라벨 포인트의 할당으로 시작된다. 관정성의 평가를 효과적으로 향상시키기 위해 줌아웃, 줌아웃 또는 각도를 변경하는 것을 포함하는 시야의 선택이 무작위화되어 있다는 점에 유의한다. 이러한 라벨 포인트는 다양한 정도의 과립도로 만들어진 후속 마스크 주석의 기초가 된다. 그림의 사례로서. 21, 꽃다발 장면에서 다른 각도에서 클릭 포인트를 고려할 때, 우리는 꽃의 꽃잎(미세 레벨), 개별 꽃(중간 수준), 전체 꽃다발(거친 수준)이라는 다양한 계층적 수준에서 마스크를 주석한다. 다른 장면에서 그라운드 진리 마스크의 경우, 대상물의 미세한 부분부터 거친 전체 대상에 이르기까지 의미적 의미에 기반한 마스크 위계를 구축하는 것과 유사한 규칙을 따른다. 그러나 현장의 복잡성과 자연 의미성에 따라 마스크 수준의 수가 달라질 수 있다는 점에 유의한다. 예를 들어, 테타임 장면에서 곰의 팔은 그림이다. 20은 왼손과 곰의 두 가지 수준의 계층으로만 주석을 달았다.\n' +
      '\n' +
      '3.3.2 C 완전소명자료####.\n' +
      '\n' +
      'GARField의 견해 일관성에 관한 평가 결과의 포괄적인 제시는 그림과 같다. 20, 21, 22, 23, 24는 본문에 표시되지 않은 모든 장면을 포함한다. 각 장면에 대해 서로 다른 계층적 수준에서 주석이 달린 랜덤하게 선택된 뷰, 그라운드 진리 마스크에 대한 클릭 라벨 포인트 및 SAM 및 GARF필드에서 얻은 가장 가까운 마스크 비교를 보여준다. 또한 더 나은 시각화를 위해 결과의 줌인 이미지를 제공합니다.\n' +
      '\n' +
      '그림 12: ** 3D 보완 실험**: 오버핑 마스크(_egg_, _noodles_, _ramen_ 마스크 내부의 _nori_ 마스크)가 원하는 계층적 그룹을 모델링한다. 우리는 마스크 주석을 위한 온라인 도구인 \'메이크 센스\'(30)를 사용하여 이러한 다각형 마스크를 표지했다.\n' +
      '\n' +
      '알려요.\n' +
      '\n' +
      '2.4.1 지상진술자 주석#####.\n' +
      '\n' +
      '이 실험에서 우리는 5개의 장면 각각에 대해 하나의 새로운 시야를 주석을 달았다. 각 새로운 견해에 대해, 우리는 풍부한 위계를 가진 하나 또는 여러 개 객체를 표시한다. 지상 진리 마스크는 인간이 집단으로 생각할 수 있는 장면의 모든 부분, 하위 부분 또는 전체 객체이다. 라면 장면을 취(그림 12, 25)한다. 예로서, 라벨링된 객체들의 부분들 또는 하위 부분들은 노리, 계란, 그그 노른자, 면 등을 포함한다. 또한 전체 국물과 라면 장 전체를 그룹으로 주석을 달았다. 3D 완성도에 대한 실험과 달리 이 실험은 모델이 풍부한 계층성을 포함하는 물체의 합리적인 마스크를 모두 추출할 수 있는지 여부를 테스트하는 것을 목표로 한다. 따라서 주석이 달린 마스크의 수준을 계층화하지 않았습니다.\n' +
      '\n' +
      '2.4.2 C 완전정시화 규범####.\n' +
      '\n' +
      '그림에서. 25, 우리는 그라운드 진리 마스크와 최고의 마스크에서 모든 방법을 보여줍니다. 모든 그라운드 진리 마스크는 사이즈의 하행순으로 배열되어 있다. 우리의 실험에서, 우리는 서로 다른 방법을 통해 주석이 달린 지상 진리에 해당하는 모든 마스크를 체계적으로 회수한다. SAM, GARF필드, 조밀한 감독이 없는 GARF필드, 지상 진리마스크 대응의 가장 높은 IOU 점수를 받는 마스크를 순차적으로 선보인다. 모든 실험에 대한 모든 근거 진실 주석을 출시할 것입니다.\n' +
      '\n' +
      '그림 13: ** 글로벌 클루스터링 결과("부켓")**: 더 작은 규모의 글로벌 클러스터((들=0)는 꽃다발의 서로 다른 섹션과 표의 두 반쪽을 구별한다. 더 큰 규모에서는 꽃다발과 테이블이 전체적으로 고려된다.\n' +
      '\n' +
      '그림 14: ** 글로벌 클루스터링 결과("데크")**: 더 큰 규모 \\(들=0.5)\\에서 책상에는 _e._ 키보드, 카드, 새 무화과와 함께 그룹화된다.\n' +
      '\n' +
      '그림 15: ** 글로벌 클루스터링 결과("돈츠")**: 매우 작은 규모((들=0.0)\\)에서 GAR필드는 현장 중간에 다른 조식 샌드위치를 구별할 수 있다. 규모가 증가함에 따라 그룹화는 상당히 비합리적으로 두 반으로 이동하거나 체크보드 포장이 있는 전체 샌드위치로 이동한다.\n' +
      '\n' +
      '그림 16: ** 글로벌 클루스터링 결과("표")**: 가장 작은 규모 \\((들=0.0)\\)에서 전 세계 군집은 초콜릿 조각인 물병에 있는 _e.g._라벨의 일부를 강조한다.\n' +
      '\n' +
      '그림 17: ** 글로벌 클루스터링 결과("Teatime)": 식품, 기구 및 표는 작은 규모에서 서로 다른 군집에 포함되며 더 큰 규모에서 동일한 클러스터가 포함된다. 표백된 동물의 일부(_e.g._양 호브, 곰 코)도 \\(s=0.0\\)에서 볼 수 있다.\n' +
      '\n' +
      '그림 18: ** 글로벌 클루스터링 결과("독성")**: 더 작은 규모의 글로벌 클러스터(\\(s=0.0\\))는 다육엽과 같은 미세한 특징을 구별하는 반면, 더 큰 규모(\\(s=1.0\\)에서 단일 그룹으로 간주된다.\n' +
      '\n' +
      '그림 19: ** 글로벌 클루스터링 결과("살아있는 방": 바닥의 개별 육각 기와는 별도로 그룹화될 수 있다(\\(s=0.0\\)) 또는 \\(들=0.5).\n' +
      '\n' +
      '그림 20: **뷰 컨슈머티즘 실험-Teatime**: 미세하고 중간인 두 개의 위계를 구성했다. 이는 곰의 왼손과 곰 전체의 의미적 의미에 각각 해당한다.\n' +
      '\n' +
      '그림 21: **뷰 컨센서스 실험-부케**: 미세한 매질과 거친 세 개의 위계를 구성했다. 이는 꽃잎, 개별 꽃, 꽃다발 전체의 의미적 의미에 각각 해당한다.\n' +
      '\n' +
      '그림 22: **뷰 컨센서스 실험-키보드**: 미세하고 중간인 두 개의 위계를 구성했다. 이는 각각 단일 키와 전체 키보드의 의미적 의미에 해당한다.\n' +
      '\n' +
      '그림 23: **뷰 컨센서스 실험-람엔**: 미세하고 중간이고 거친 3개의 위계를 구성했다. 이는 계란 노른자의 의미적 의미, 하나의 달걀과 전체 수프 영역에 각각 해당한다.\n' +
      '\n' +
      '그림 24: ** 시청 대응 실험-생활실**: 미세 매질과 거친 두 개의 계층을 구성했다. 이는 nerf총의 작은 주황색 부분, nerf총의 중간 블루 부분과 전체 nerf총의 의미적 의미에 해당한다.\n' +
      '\n' +
      '그림 25: ** 계층적 그룹화 기록 실험**: SAM 및 GARField의 절제 연구와 같은 방법에 중점을 둔다. GARF 내야는 더 미세한 더 작은 마스크(예: 키보드 장면에서 작은 키를 모두 캡처)를 얻는 데 SAM을 능가한다. 계층화 그룹화가 없는 GARF필드와 달리 GARF필드는 더 많은 계층화 그룹화 결과(예를 들어 라면에 있는 장면에서)를 달성하면 계층적 군집화를 통해 라면의 마스크 전체를 성공적으로 식별한다. 또한, 조밀한 감독이 없는 GARF필드와 비교하여 GARF필드는 보다 안정적이고 철저한 그룹화 결과(예: 티타임 장면에서 GARF필드는 쿠키 백의 작은 라벨을 더 포괄적으로 식별)를 제공한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>