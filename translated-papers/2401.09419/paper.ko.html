<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# GARField.\n' +
      '\n' +
      '정민김매매({}^{*1}\\)는 저스틴 커트({}^{*1}\\) 켄 골드버그\\({}^{*1}\\)\n' +
      '\n' +
      '매트 투키카({}^{2}\\) 앙주 카나자와({}^{1}\\).\n' +
      '\n' +
      '대등한 기여도(*\\)는 동등한 기여도(*\\)를 나타내는데, 이는 동등한 기여도(*\\)를 의미한다.\n' +
      '\n' +
      'AI 버클리({}^{2}\\)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Grouping is inherently ambiguous due to the multiple levels of granularity in which one can decompose a scene--should the wheels of an excavator be considered separate or part of the whole? We present Group Anything with Radiance Fields (GARField), an approach for decomposing 3D scenes into a hierarchy of semantically meaningful groups from posed image inputs. To do this we embrace group ambiguity through physical scale: by optimizing a scale-conditioned 3D affinity feature field, a point in the world can belong to different groups of different sizes. We optimize this field from a set of 2D masks provided by Segment Anything (SAM) in a way that respects coarse-to-fine hierarchy, using scale to consistently fuse conflicting masks from different viewpoints. From this field we can derive a hierarchy of possible groupings via automatic tree construction or user interaction. We evaluate GARField on a variety of in-the-wild scenes and find it effectively extracts groups at many levels: clusters of objects, objects, and various subparts. GARField inherently represents multi-view consistent groupings and produces higher fidelity groups than the input SAM masks. GARField\'s hierarchical grouping could have exciting downstream applications such as 3D asset extraction or dynamic scene understanding. See the project website at [https://www.garfield.studio/](https://www.garfield.studio/)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Consider the scene in Figure 1. Though recent technologies like NeRFs [20] can recover photorealistic 3D reconstructions of this scene, the world is modeled as a single volume with no structural meaning. As humans, not only can we reconstruct the scene, but we also have the ability to _group_ it at multiple levels of granularity -- at the highest level, we see the parts of the scene _i.e_. the excavator, bushes, and the sidewalk, but we are also able to decompose the excavator into its parts such as its wheels, crane, and the cabin. This ability to perceive the scene at multiple levels of groupings is a key component of our scene understanding, enabling us to interact with the 3D world by understanding what belongs together. However, these different levels of granularity introduce ambiguity in groups, making it a challenge to represent them in a coherent 3D representation. While there are multiple ways to break this ambiguity, we focus on the physical scale of entities as a cue to consolidate groups into a _hierarchy_.\n' +
      '\n' +
      'In this work we introduce Group Anything with Radiance Fields (GARField), an approach that, given posed images, reconstructs a 3D scene along with a scale-conditioned affinity field that enables decomposing the scene into a hierarchy of groups. For example, GARField can extract both the entire excavator (Fig. 1 Top Right) as well as its subparts (Bottom Right). This dense hierarchical 3D grouping enables applications such as 3D asset extraction and interactive segmentation.\n' +
      '\n' +
      'GARF 필드는 2D 분할 마스크 세트를 3D 부피 스케일 조절된 친화성 필드로 증류한다. 그룹화는 모호한 작업이기 때문에 이러한 2D 라벨은 중첩되거나 상충될 수 있다. 이러한 불일치는 일관된 3D 그룹으로 마스크를 증류하는 데 어려움을 초래한다. 우리는 _규모 조건_특성 분야를 활용함으로써 이 문제를 극복한다. 구체적으로 GARF필드는 특징 거리가 점들의 친화도를 반영하도록 감독되는 조밀한 3D 특징 필드를 최적화한다. 스케일 컨디셔닝은 두 점이 대규모로 더 높은 친화도를 가질 수 있지만 더 작은 규모(_i.e_같은 수박의 와지)에서 낮은 친화도를 가질 수 있다. 그림 2에 도시된 바와 같이, 그림 2와 같다.\n' +
      '\n' +
      'Though in principle GARField can distill any source of 2D masks, we derive mask candidates from Segment Anything Model (SAM) [15] because they align well with what humans consider as reasonable groups. We process input images with SAM to obtain a set of candidate segmentation masks. For each mask, we compute a physical scale based on the scene geometry. To train GARField, we distill candidate 2D masks with a contrastive loss based on mask membership, leveraging 3D scale to resolve inconsistencies between views or mask candidates.\n' +
      '\n' +
      '잘 포장된 친화성 필드는 1) _트랜스_가 있는데, 이는 두 점을 세 번째와 상호 그룹화하면 스스로 묶어야 하며, 2) _containment_는 두 점을 소규모로 그룹화하면 더 높은 스케일에서 함께 그룹화해야 한다는 것을 의미한다. GARF필드는 억제 보조 손실 외에도 조영제 손실을 사용하는 것은 이러한 두 가지 특성을 모두 장려한다.\n' +
      '\n' +
      '최적화된 스케일 조절된 친화성 필드로 GARF필드는 더 이상의 클러스터가 나타나지 않을 때까지 하강 스케일에서 재귀적으로 클러스터링하여 3D 장면 위계를 추출한다. 건설에 의해, 이 재귀적 군집링은 생성된 그룹이 거친 간 방식으로 이전 군집의 하위 부분임을 보장한다. 우리는 주석이 달린 계층적 그룹을 가진 다양한 실제 장면에서 GARF필드를 평가하고 객체 위계성을 포착하는 능력과 다양한 견해에 걸친 일관성을 평가한다. 다중 뷰를 활용함으로써 GARF필드는 상세한 그룹을 생성할 수 있으며, 종종 입력 2D 분할 마스크의 품질에 따라 개선될 수 있다. 더욱이, 이들 그룹은 설계에 의해 3D 일관성이 있는 반면, 2D 기저부는 뷰 일관성을 보장하지 않는다. 계층적 3D 자산 추출 및 클릭 기반 상호 작용 분할을 위한 GARField의 다운스트림 응용 프로그램을 보여준다. GARF필드의 장면 분해 능력을 감안할 때 로봇이 동적 재구성에 앞서와 상호작용하거나 상호 작용할 수 있는 것과 같은 다른 하류 응용 분야에서 잠재력을 얻기를 바랍니다. 출판 시 모드와 데이터가 출시됩니다. 더 많은 시각화를 위해 첨부 영상을 참조하십시오.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '계층적 그룹화 다중 수준 그룹화는 전경 세분화[28] 초기에는 2D 영상에서 오랫동안 연구되었다. 다중 수준 분할 [5] 및 [1, 25, 31]을 파싱하는 보다 복잡한 계층적 장면에 대한 스펙트럼 군집링의 이 개념에 대해 여러 방법이 구축된다. 이러한 접근법은 고전적인 텍스처 신호를 통해 윤곽을 추출하고 하향다운 [37] 또는 상향식 통합[1]을 통해 계층화를 생성하는 데 의존한다. 보다 최근의 딥러닝 접근법은 계층화를 생성하기 위해 여러 척도로 계산된 에지[36]를 사용하고 Ke _et al_[11][11]는 Ke _et al_를 사용한다. 고전적인 계층 세분화 [1]의 출력에 의해 유도되는 비지도 계층적 분할 접근법을 변환기 기반 비지도 계층적 분할 접근법을 제공한다.\n' +
      '\n' +
      'Many works circumvent the question of ambiguity in grouping by defining a set of categories within which instances are to be segmented, _i.e_. panoptic segmentation [10, 14]. Recently, Segment Anything (SAM) [15] off-loads this ambiguity into prompting, where at each pixel multiple seg\n' +
      '\n' +
      'Figure 2: **Importance of Scale When Grouping A single point may belong to multiple groups. GARField uses _scale-conditioning_ to reconcile these conflicting signals into one affinity field.**\n' +
      '\n' +
      'mentation masks can be proposed. However SAM does not recover a consistent set of hierarchical groups in the scene, which we enable by multi-scale 3D distillation.\n' +
      '\n' +
      'Hierarchical part decomposition has also been explored in 3D objects, either in a supervised [17, 21, 35], or unsupervised manner [24]. Our approach distills information from a 2D model, and we consider full scenes while these approaches focus on 3D objects.\n' +
      '\n' +
      'NRF에서 분할을 위한 NeRF****의 발효 접근법은 보통 지상 진실 의미 라벨[29, 38], 일치하는 인스턴스 마스크[18], NeRF [34]에서 3D 분할 네트워크를 사용하여 3D로 분할 마스크를 증류한다. 그러나 이러한 기술은 계층적 그룹화를 고려하지 않으며, 사물이나 사례의 평평한 계층화에만 관심이 있다. Ren et al. [27]은 인간 상호작용을 이미지 스카운의 형태로 환산하여 상호 작용을 갖는 객체들을 분할한다. 보다 최근에, Cen et al. [3]은 사용자 프롬프트를 통해 이웃 뷰들 사이의 2D 마스크를 추적하여 SAM으로부터 3D 일관된 마스크를 회수하려고 노력한다. 텐 등은 SAM 인코더 특징을 3D로 증류하고 디코더를 쿼리함으로써 이를 시도한다. 이러한 접근법과 대조적으로, 우리의 접근 GARF필드는 사용자 입력을 필요로 하지 않으며, 자동으로 장면의 계층적 그룹화를 얻을 수 있으며, 더 나아가 복구된 그룹은 정의에 의해 시야에 부합한다.\n' +
      '\n' +
      '광전장(뷰 의존적 색상 및 밀도)과 함께 고차원 특징을 신경장에 베팅하는***3D 철관 필드**가 철저하게 탐색되었다. 낭만성 NeRF[38]와 같은 방법[38]이 채워진 철망 필드[16], 신경 신학적 융합 필드[33] 및 판포틱 라이프[29]는 체적 렌더링 후 2D 기능을 재구성하기 위해 3D 특징 필드를 최적화하여 3D로 증류한다. 이러한 특징은 DINO와 같은 전처리된 비전 모델 또는 의미적 세분화 모델로부터의 것일 수 있다. LERF[13]는 이 아이디어를 스케일 조절된 특징 분야로 확장하여 CLIP[26]과 같은 글로벌 이미지 임베딩에서 특징 필드의 훈련을 가능하게 한다. GARF 필드는 유사하게 3D에서 스케일 조절된 특징 필드를 최적화하는 것이지만, 다중 규모의 특징의 목적은 CLIP와 같은 명시적인 2D 특징을 재구성하는 대신 그룹화의 모호성을 해결하는 것이다. 또한 LERF는 공간 그룹화가 없고, 짧은 GARF필드 주소도 있다. 앞서 언급한 방법은 이미지 특징으로부터의 직접적인 감독을 기반으로 하는 반면, NeRF-SOS [8] 및 콘트롤리프트 [2]와 같은 다른 방법은 유사성에 기초한 광선 쌍 간의 대조적인 손실을 사용하여 단일 척도로 임의의 특징 필드를 최적화한다. GARF 필드는 마스크 라벨을 기반으로 포인트 간의 쌍별 관계를 정의할 수 있기 때문에 이 대조적인 접근법을 사용한다. 그러나 3D로 상충되는 마스크를 증류할 수 있는 스케일 조절된 대조 손실을 설계합니다. 또한 GARF필드에는 안정적인 훈련을 위해 Bhalgat et al. [2]의 느린 빠른 제형이 필요하지 않으며 아마도 스케일 조절된 훈련으로 가능하게 될 것이다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '2D 마스크 세대는.\n' +
      '\n' +
      'GARField takes as input a set of posed images and produces a hierarchical 3D grouping of the scene, along with a standard 3D volumetric radiance field and a scale-conditioned affinity field. To do this, we first pre-process input images with SAM to obtain mask candidates. Next, we optimize a volumetric radiance field along with the affinity field which takes in a single 3D location and a euclidean scale, and outputs a feature vector. Affinity is obtained by comparing pairs of points\' feature vectors. After optimization, the resulting affinity field can be used to decompose a scene by recursively clustering the feature embeddings in 3D at descending scales in a coarse-to-fine manner, or for segmenting user specified queries. The overall pipeline is illustrated in Figure 3.\n' +
      '\n' +
      'GARField를 훈련시키기 위해 먼저 이미지로부터 2D 마스크 후보를 채굴한 다음 3D 척도를 할당한다.\n' +
      '\n' +
      'Figure 3: **GARField Method**: (Left) given an input image set, we extract a set of candidate groups by densely querying SAM, and assign each a physical scale by deprojecting depth from the NeRF. These scales are used to train a _scale-conditioned affinity field_ (Right). During training, pairs of sampled rays are pushed apart if they reside in different masks, and pulled together if they land in the same mask. Affinity is supervised only at the scale of each mask, which helps resolve conflicts between them.\n' +
      '\n' +
      '마스크를 발라. 구체적으로, 우리는 SAM의 자동 마스크 생성기[15]를 사용하여 점들의 그리드에 SAM을 쿼리하고 질의 포인트당 3개의 후보 분할 마스크를 생성한다. 그런 다음 자신감과 중복으로 이 마스크를 필터링하여 서로 중첩되거나 포함할 수 있는 여러 크기의 마스크 후보 목록을 생성한다. 이 과정은 관점과는 독립적으로 수행되어 견해마다 일관성이 없을 수 있는 마스크를 생산한다. 이 연구에서 우리는 물체의 물리적 크기를 기반으로 그룹화의 계층화를 생성하기 위해 각 2D 마스크를 그림 3과 같이 물리적 3D 척도를 할당하고 이를 통해 각 훈련 카메라 포즈에서 깊이 이미지를 렌더링한다. 이 방법은 마스크의 3D 척도를 동일한 세계 공간에 갖추어 스케일 조절된 친화력을 가능하게 한다.\n' +
      '\n' +
      '세일-컨디션 앤피니티 필드.\n' +
      '\n' +
      '스케일-조건은 일관성이 없는 2D 마스크 후보를 통합할 수 있는 GARF필드의 핵심 구성 요소이며, 동일한 점은 원하는 그룹의 입도에 따라 여러 가지로 그룹화될 수 있다. 스케일-조건은 질의가 어느 그룹에 속해야 하는지에 대한 모호성을 해소하기 때문에 이러한 불일치를 완화한다. 규모 조건하에서 같은 지점의 상충되는 마스크는 훈련 중 더 이상 서로 싸우지 않고, 오히려 서로 다른 친화도 규모에서 같은 장면에서 공존할 수 있다.\n' +
      '\n' +
      'We define the scale-conditioned affinity field \\(F_{\\text{g}}(x,s)\\mapsto R^{d}\\) over a 3D point \\(x\\) and euclidean scale \\(s\\), similar to the formulation in LERF [13]. Output features are constrained to a unit hyper-sphere, and the affinity between two points at a scale is defined by \\(A(x_{1},x_{2},s)=-||F_{\\text{g}}(x_{1},s)-F_{\\text{g}}(x_{2},s)||_{2}\\). These features can be volumetrically rendered with a weighted average using the same rendering weights based on NeRF density to obtain a value on a per-ray basis.\n' +
      '\n' +
      '3.2.1.1 세부 감독 감독##### 3.2.1.\n' +
      '\n' +
      '유충은 (F_{\\t 외{g} (x, s)\\mapsto R^{ d}\\) 뿌리 nLERF[13]에 대한 모빌라트 3 D포인트\\(x\\)를 재구성하고, 아피니트 ybetnentnast-I-at에서 사위다.\n' +
      '\n' +
      'Specifically, consider two rays \\(r_{A},r_{B}\\) sampled from masks \\(\\mathcal{M}_{A},\\mathcal{M}_{B}\\) within the same training image, with corresponding scales \\(s_{A}\\) and \\(s_{B}\\). We can volumetrically render the scale-conditioned affinity features along each ray to obtain ray-level features \\(F_{A}\\) and \\(F_{B}\\). If \\(\\mathcal{M}_{A}=\\mathcal{M}_{B}\\), the features are pulled together with L2 distance: \\(\\mathcal{L}_{\\text{pull}}=||F_{A}-F_{B}||\\). If \\(\\mathcal{M}_{A}\\neq\\mathcal{M}_{B}\\), the features are pushed apart: \\(\\mathcal{L}_{\\text{push}}=\\text{ReLU}(m-||F_{A}-F_{B}||)\\) where \\(m\\) is the lower bound distance, or margin. Importantly, this loss is only applied among rays sampled from the same image, since masks across different viewpoints have no correspondence.\n' +
      '\n' +
      '규모 수퍼비전 3.2.2는 스코일 수퍼비전 3.2.######## 3.2로 스코일 수퍼비전 3.2를 지각하고 있다.\n' +
      '\n' +
      'The supervision provided by the previous contrastive losses alone are not sufficient to preserve hierarchy. For example in Fig. 4, although the egg is correctly grouped with the soup at scale 0.22, at a larger scale it fragments apart. We hypothesize this grouping instability is because 1) scale supervision is defined sparsely only when a mask exists and 2) nothing imposes containment such that small scale groups remain at larger scales. We address these shortcomings here by introducing the following modifications:\n' +
      '\n' +
      '3D 마스크 비늘을 사용하여*** 연속 스케일 감독***.\n' +
      '\n' +
      '그림 4: **Densified Scale 감독***: 군집 내의 2개의 포도를 컨사이더한다. 조영적 손실에 대한 척도를 사용하는 나니어_는 포도 및 포도 3차 수준에서만 친화도를 감독하여 전체 간격을 부적합하게 두었다. GARField에서, 우리는 마스크 유클리드 척도와 2) 사이의 척도를 증가시켜 더 큰 규모의 봉쇄에 보조 손실을 부과하는 것을 1) 농도화한다.\n' +
      '\n' +
      'Figure 5: **3D Asset Extraction with Interactive Selection**: Users can interactively select view-consistent 3D groups with GARField using a click point and a scale.\n' +
      '\n' +
      'scale supervision by augmenting the scale \\(s\\) uniformly randomly between the current mask\'s scale and the next smallest mask\'s scale. When a ray\'s mask is the smallest mask for the given viewpoint, we interpolate between 0 and \\(s_{0}\\). This ensures continuous scale supervision throughout the field leaving no unsupervised regions.\n' +
      '\n' +
      '**Containment Auxiliary Loss**: If two rays \\(r_{1}\\) and \\(r_{2}\\) are in the same mask with scale \\(s\\), then they should also be pulled together at any scale larger than \\(s\\). Intuitively, two grapes within the same cluster (Fig. 4) are also grouped together at larger scales (e.g., the entire bunch). At each training step, for the rays grouped together at scale \\(s\\), we additionally sample a larger scale \\(s^{\\prime}>s\\) at which the rays are also pulled together. This ensures that affinities at smaller scales are not lost at larger scales.\n' +
      '\n' +
      '3.2.3레이의 3.2.3레이와 마스크 샘플링의#######\n' +
      '\n' +
      'Just like standard NeRF training, we sample rays over which to compute losses. Because GARField uses a contrastive loss within each train image, naively sampling pixels uniformly during training is inadequate to provide a training signal in each minibatch of rays. To ensure sufficient pairs in each train batch, we first sample N images, and sample M rays within each image. To balance the number of images as well as the number of point pairs for supervision, we sample 16 images and 256 points per image, resulting in 4096 samples per train iteration.\n' +
      '\n' +
      'For each ray sampled, we must also choose a mask to use as the group label for the train step in question. To do this, we retain a mapping from pixels to mask labels throughout training, and at each train step randomly select a mask for each ray from its corresponding list of masks. There are two important caveats in this sampling process: **1)** The probability a mask is chosen is weighted inversely with the log of the mask\'s 2D pixel area. This prevents large scales from dominating the sampling process, since larger masks can be chosen via more pixels. **2)** During mask selection we coordinate the random scale chosen across rays in the same image to increase the probability of positive pairs. To do this, we sample a single value between 0 and 1 per image, and index into each pixel\'s mask probability CDF with the same value, ensuring pixels which land within the same group are assigned the same mask. Otherwise, the loss is dominated by pushing forces which destabilize training.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'The method is built in Nerft studio [32] on top of the Nerfacto model by defining a separate output head for the grouping field. The grouping field is represented with a hashgrid [23] with 24 layers and a feature dimension of 2 per layer, and a 4-layer MLP with 256 neurons and ReLU activation which takes in scale as an extra input concatenated with hashgrid feature. We cap scale at \\(2\\times\\) the extent of cameras, and normalize the scale input to the MLP using sklearn\'s quantile transform on the distribution of computed 3D mask scales (Sec 3.1). Output embeddings are \\(d=256\\) dimensions. Gradients from the affinity features do not affect the RGB outputs from NeRF, as these representations share no weights or gradients.\n' +
      '\n' +
      'We begin training the grouping field after 2000 steps of NeRF optimization, giving geometry time to converge. In addition, to speed training we first volumetrically render the hash value, then use it as input to the MLP to obtain a ray feature. With this deferred rendering, the same ray can be queried at different scales with only one extra MLP call. We normalize the result of volume rendering to unit norm before inputting to the MLP, and for point-wise queries, the individual hashgrid value is normalized. Preprocessing SAM masks takes around 3-10 minutes, followed by about 20 minutes for training on a GTX 4090.\n' +
      '\n' +
      '4번 수업.\n' +
      '\n' +
      '그림 7:**의 경우 p-레벨클루스를 멈추고, 이때 np 로즈가 완전3도 벅마를 중단한다.\n' +
      '\n' +
      '그림 6: **3D 결정: GARField는 축소 규모에서 재귀적으로 질의하여 장면을 객체와 하위 부분으로 군집링할 수 있다.**3D 하위 부분으로 군집화할 수 있다.\n' +
      '\n' +
      'Figure 7: **Results**: From a GARField we extract objects from the global scene by selecting top-level clusters, then visualize their local clusters at decreasing scales. GARField can produce complete 3D object masks, and break these objects into meaningful subparts based on the input masks. We use Gaussian Splats [12] to produce these visualizations in 3D. See the Supplemental video for more results.\n' +
      '\n' +
      '마스크, 또는 포인트에 걸친 3D로 포인트 라우드를 산출합니다. 그림을 참조하세요. 장면 분해의 시각화를 위한 6.\n' +
      '\n' +
      '**Initialization**: First, to initialize the hierarchy, we first globally cluster features at a large scale \\(s_{\\text{max}}\\), which we set to \\(1.0\\) for all experiments, corresponding to the extent of the input cameras\' positions. These clusters form the top-level nodes in the scene decomposition.\n' +
      '\n' +
      '**Recursive Clustering**: Next, to produce a hierarchical tree of scene nodes, we iteratively reduce scale by a fixed epsilon (we use 0.05), running HDBSCAN on each leaf node. If HDBSCAN returns more than one cluster for a given node, we add those clusters as children and recurse. This continues until we reach scale 0, at which point the procedure terminates, returning the current tree.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '우리는 GARF필드에서 3D 장면을 크기와 의미론이 광범위하게 다양한 계층 그룹으로 분해하는 능력을 평가한다. 기존의 3D 스캔 데이터 세트는 객체 레벨 스캔(7, 22], 시뮬레이션된 [2], 주로 실내 가정용 장면[6]을 포함하는 경향이 있다. GARF필드 평가를 위해 대신 네르프 학습지와 LERF 데이터세트로부터 다양한 실내외 장면을 사용하고 이 논문에 대한 추가 포획을 사용한다. 우리는 GARField의 분해 능력을 테스트하여 중요한 객체 계층화를 갖는 장면을 실험한다. 우리는 그림 1에서 질적 결과를 제공한다. 3 및 그림. 6개는 지상 진리 마스크를 선택 장면에 주석하여 정량적으로 평가하고, 그 중 전체 목록은 보충에 있다.\n' +
      '\n' +
      '적합한 내용.\n' +
      '\n' +
      '가우시안 스플라팅[12]을 사용하여 가우시안 센터에서 GARF필드 친화도 분야에 질의하여 분해를 시각화한다. 우리는 가우시안 스플렛이 NeRF에 비해 3D에서 분할하기 쉽기 때문에 이것을 한다. 파이프라인에 대한 완전한 설명을 위한 보충제를 참조하세요. 모든 렌더링은 2D 이미지 뷰의 분절이 아닌 완전한 3D 모델이다.\n' +
      '\n' +
      '우리는 두 가지 유형의 계층적 군집화 결과를 시각화한다. 그림에서. 7은 전 세계적으로 손으로 선택된 거친 규모로 현장을 군집화한 다음, 이러한 장면 전체 군집에서 우리는 소수의 객체에 해당하는 그룹을 선택하고 더 나아가 하위 그룹으로 분해한다. 우리는 성공적으로 감소하는 척도로 얻은 군집을 시각화하여 그룹의 과립성을 증가시킨다. GARF 필드는 작은 규모로 각 열쇠가 그룹으로 간주되는 키보드와 같은 인공물부터 NERF 건과 레고 불도저 부품까지 다양한 장면과 물체에 걸쳐 높은 충실도 3D 그룹을 달성하며, 이는 개별 꽃과 잎뿐만 아니라 식물처럼 복잡한 자연물까지 달성된다. 규모를 달리함으로써, 화분에 비해 각 개별 잎(1열)에 대한 숙시네이트와 같은 다양한 수준에서 물체를 분리할 수 있거나 불도저 스카퍼의 번니 장난감을 식별할 수 있으며, 이는 셔츠, 귀, 헤드(5열, 우측)로 더 그룹화된다. 그림을 참조하세요. 선택한 장면 전체 클러스터 시각화를 위한 10개입니다.\n' +
      '\n' +
      '그림에서. 6은 Sec. 4에 설명된 방법으로 생성된 트리 분해를 시각화한다. 우리는 먼저 나무 분해를 설명하기 위해 중심 동상을 선택하는 최상위 노드에서 글로벌 군집링을 보여준다. 화살표는 위계에 있는 아이들을 나타내며, 동상이 모발, 다리, 몸통 등으로 갈수록 어떻게 분해되는지를 보여준다. 더 많은 트리 시각화를 위한 보충제를 참조하세요.\n' +
      '\n' +
      '### Quantitative Hierarchy\n' +
      '\n' +
      '*** 3DC의 완전성 :***의 경우 트레마타스 ksitis.\n' +
      '\n' +
      '**3D Completeness:** For downstream tasks it is useful for groups to correspond to complete 3D objects, for example groups that contain an entire object rather than just one of its sides. Though GARField always produces view-consistent groups by construction, it may not necessarily contain complete objects. We evaluate for completeness by checking\n' +
      '\n' +
      'Figure 8: **Segment-Anything [15] vs. GARField**: SAM’s automatic mask generator may encounter difficulty recalling all masks from a given viewpoint, especially when there are clusters of small masks and the camera is far away from the object. In contrast, GARField’s scale-conditioned affinity field incorporates masks from multiple viewpoints in 3D.\n' +
      '\n' +
      '배설물 밀 측면에서 갈마당 sl에서 SA7138634783 255837 48a 유틸리티를 잠식하는 데 도움이 됩니다.\n' +
      '\n' +
      '전체 3D 객체가 다양한 관점에 걸쳐 함께 그룹화되는 것이다. 이를 위해 5개의 장면에서는 3차원점을 선택해서 3가지 다른 시각으로 투사하고, 그 점을 거친, 중, 미세 수준에서 포함하는 3개의 대응하는 관점과 일치하는 그라운드 진리 마스크를 라벨링한다. 이 지점에서 우리는 0.05 증분으로 여러 스케일에서 GARF필드에서 여러 개의 마스크를 채굴하며, 각 스케일에서는 0.9에서 임계치된 특징 유사도를 기반으로 마스크를 얻는다. 또한 이미지 내 지점을 클릭하고 3개의 마스크를 모두 복용하여 SAM과 비교한다. 두 가지 방법에 대해 모든 후보 마스크에 대해 계산된 최대 mIOU를 보고한다.\n' +
      '\n' +
      'Results are shown in Table 1. GARField produces more complete 3D masks than SAM across viewpoints, resulting in higher mIOU with multi-view human annotations of objects. This effect is especially apparent at the most granular level, where from certain perspectives SAM struggles to produce fine groups, like the keyboard keys from afar in Fig. 8. See the Supplement for figures of comparisons and visualization of the groundtruth masks.\n' +
      '\n' +
      '**Hierarchical Grouping Recall:** Here we measure GARField\'s ability to recall groups at multiple granularities. Across 5 scenes, we choose one _novel_ viewpoint and label up to 3 ground truth hierarchical groups for 1-2 objects. GARField outputs a set of masks as described in Section 4 by clustering image-space features, outputting one mask per tree node. We compare against SAM\'s automatic mask generation by keeping all output masks. We ablate GARField in two ways: GARField (-scale) removes scale-conditioning; and GARField (-hierarchy) removes the densified supervision in Sec. 3.2.2.\n' +
      '\n' +
      '표 2에서 우리는 SAM 마스크 세트 또는 GARField에 의해 생성된 트리에서 가장 중첩이 높은 지상 진리 마스크의 mIOU를 보고한다. GARF필드는 여러 관점에서 그룹을 융합했기 때문에 SAM의 단일 뷰보다 충실도가 높아 주석과 함께 mIOU가 더 높다. 우리의 연마는 고품질 그룹을 위해 스케일 컨디셔닝과 스케일 조도가 필요하다는 것을 보여준다. 그림. 9는 순진한 감독을 통해 더 높은 규모로 친화력을 저하시키는 것을 보여준다.\n' +
      '\n' +
      '## 6 Limitations\n' +
      '\n' +
      '코어의GARF 필드는 2D 마스크 발생기에서 출력을 증류하고 있으므로 마스크가 원하는 그룹을 포함하지 않으면 3D에서는 나타나지 않는다. 고르지 않은 관점을 가진 영역은 인공 집단 경계로 고통받을 수 있으며, 예를 들어 물체가 닫혀서만 볼 경우 입력관이 완전히 포함되어 있지 않기 때문에 함께 그룹화할 수 없다. 우리는 물리적 크기를 사용하여 그룹 모호성을 처리하지만 단일 규모 내에서 여러 그룹이 있을 수 있다. 예컨대, 오브젝트가 있거나 없는 컨테이너가 동일한 스케일을 가질 수 있기 때문에 컨테이너에 포함된 오브젝트와 충돌이 발생할 수 있다. 향후 작업은 어포던스와 같은 모호성을 그룹화하는 다른 방법을 고려할 수 있다. 스케일-조건의 또 다른 결과는 서로 다른 크기의 객체 부분이 모든 것이 아니라 나무에서 따로 분기되는 것이며, 동일한 테이블 상의 다수의 오브젝트들은 나무의 상이한 레벨들에서 나타날 수 있다는 것이다. 이 작품의 나무 생성은 순진한 욕심 알고리즘으로 보충제의 나무에서 볼 수 있듯이 더 깊은 수준에서 어지러운 작은 그룹을 초래할 수 있다. 향후 작업은 계층적 군집링의 보다 정교한 방법을 탐색할 수 있다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '우리는 계층적 3D 장면 분해를 위한 밀집 스케일 조절된 친화성 필드에 다단계 마스크를 증류하는 방법인 GARF필드를 제시한다. 친화성 필드는 스케일 조건을 활용함으로써 상충되는 2D 그룹 입력에서 의미 있는 그룹을 배우고 여러 수준에서 장면을 탈피할 수 있으며, 이는 추출에 사용될 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r r r} \\hline \\hline  & \\multicolumn{2}{c}{Fine} & \\multicolumn{2}{c}{Medium} & \\multicolumn{2}{c}{Coarse} \\\\ Scene & SAM & Ours & SAM & Ours & SAM & Ours \\\\ \\hline \\hline \\multicolumn{1}{l}{teatime} & 81.6 & **92.7** & 97.3 & **97.9** & - & - \\\\ \\multicolumn{1}{l}{bouquet} & 17.4 & **76.0** & 73.5 & **81.6** & 76.1 & **85.4** \\\\ \\multicolumn{1}{l}{keyboard} & 65.3 & **88.8** & 73.6 & **98.4** & - & - \\\\ \\multicolumn{1}{l}{ramen} & 53.3 & **79.2** & 74.7 & **90.7** & 92.6 & **95.5** \\\\ \\multicolumn{1}{l}{living\\_room} & 85.3 & **90.5** & 74.2 & **80.7** & 88.6 & **94.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **3D 보완*** 최대 3단계 계층이 있는 단일 지점에 대한 현장 주석 mIOU를 보고한다. SAM은 GARF필드에 비해 시야와 일치하는 미세 그룹을 생산하기 위해 고군분투한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r} \\hline \\hline \\multirow{2}{*}{Scene} & \\multirow{2}{*}{SAM [15]} & \\multicolumn{2}{c}{Ours} & \\multicolumn{2}{c}{Ours} & \\multirow{2}{*}{Ours} \\\\  & & (-scale) & (-dense) & \\\\ \\hline \\hline \\multicolumn{1}{l}{ramen} & 74.9 & 64.1 & 74.1 & **85.6** \\\\ \\multicolumn{1}{l}{teatime} & 64.9 & 67.7 & 66.1 & **86.6** \\\\ \\multicolumn{1}{l}{keyboard} & 23.2 & 57.6 & 73.1 & **77.9** \\\\ \\multicolumn{1}{l}{bouquet} & 34.4 & 49.8 & 72.9 & **76.4** \\\\ \\multicolumn{1}{l}{living\\_room} & 59.6 & 49.7 & 62.1 & **76.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: ** 계층적 그룹화 기록:** 우리는 다양한 물체의 다중 규모 그룹의 인간 주석에 대해 mIOU를 보고한다.\n' +
      '\n' +
      'Figure 10: **Scene-Wide Clustering** visualizations for selected scenes from Fig. 7.\n' +
      '\n' +
      'assets at a multitude of granularities. GARField could have applications for tasks that require multi-level groupings like robotics, dynamic scene reconstruction, or scene editing.\n' +
      '\n' +
      '## 8 Acknowledgements\n' +
      '\n' +
      '이 사업은 NSF:CNS-2235013 및 DARPA 계약 번호 HR001123C0021에 의해 부분적으로 지원되었으며 정민 및 저스틴은 부분적으로 Grant No DGE 2146752의 NSF 대학원 연구 펠로우십 프로그램에 의해 지원되며 이 자료에 표현된 의견, 소견 및 권고 사항은 저자(들)의 견해이며 반드시 국립 과학 재단의 견해를 반영하지 않는다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]P. 아벨라즈, M. 마이어, C. 푸이크, J. 말릭(2011) 콘텐츠 검출 및 계층적 이미지 세분화이다. IEEE 전환은 패턴 분석 및 기계 정보 33(5), pp 898-916: SS1에 의해 전환된다.\n' +
      '*[2]Y. Bhalgat, I. Laina, J. F. 헨리쿼스, A. Zisserman 및 A. Vedaldi(2023) 조영제 리프트: 느린 빠른 대조 융합에 의한 3d 객체 인스턴스 분할이다. 네르IPS. SS1: SS1로 받았습니다.\n' +
      '* [3]J. Cen, Z. Zhou, J. Fang, C. Yang, W. Shen, L. Xie, X. Zhang, and Q. Tian (2023) Segment anything in 3d with nerfs. Cited by: SS1.\n' +
      '* [4]X. Chen, J. Tang, D. Wan, J. Wang, and G. Zeng (2023) Interactive segment anything nerf with feature imitation. arXiv preprint arXiv:2211.12368. Cited by: SS1.\n' +
      '* [5]T. Cour, F. Benezit, and J. Shi (2005) Spectral segmentation with multiscale graph decomposition. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\'05), Vol. 2, pp. 1124-1131 vol. 2. Cited by: SS1.\n' +
      '* [6]A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Niessner (2017) Scannet: richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5828-5839. Cited by: SS1.\n' +
      '* [7]L. Downs, A. Francis, N. Koenig, B. Kinman, R. Hickman, K. Reymann, T. B. McHugh, and V. Vanhoucke (2022) Google scanned objects: a high-quality dataset of 3d scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), pp. 2553-2560. Cited by: SS1.\n' +
      '* [8]Z. Fan, P. Wang, Y. Jiang, X. Gong, D. Xu, and Z. Wang (2022) Nerf-sos: any-view self-supervised object segmentation on complex scenes. arXiv preprint arXiv:2209.08776. Cited by: SS1.\n' +
      '* [9]R. Hadsell, S. Chopra, and Y. LeCun (2006) Dimensionality reduction by learning an invariant mapping. In 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR\'06), pp. 1735-1742. Cited by: SS1.\n' +
      '* [10]K. He, G. Gkioxari, P. Dollar, and R. Girshick (2017) Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 2961-2969. Cited by: SS1.\n' +
      '*[11]T. 케, J 황, Y. 구, X. 왕과 S. X. 유(2022)는 멀티뷰 분류 및 클러스터링 변환기를 사용하여 계층적 의미 세분화를 폐지했다. 컴퓨터 비전 및 패턴 인식 관련 IEEE/CVF 회의의 개시: SS1에 의해 계산된다.\n' +
      '* [12]B. Kerbl, G. Kopanas, T. Leimkuhler, and G. Drettakis (2023) 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics42 (4). Cited by: SS1.\n' +
      '* [13]J. Kerr, C. M. Kim, K. Goldberg, A. Kanazawa, and M. Tancik (2023) Lerf: language embedded radiance fields. In International Conference on Computer Vision (ICCV), Cited by: SS1.\n' +
      '* [14]A. Kirillov, K. He, R. Girshick, C. Rother, and P. Dollar (2017) Panoptic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9404-9413, 2019.\n' +
      '* [15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. _ICCV_, 2023.\n' +
      '* [16] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing nerf for editing via feature field distillation. _NeurIPS_, 35:23311-23330, 2022.\n' +
      '* [17] Jun Li, Kai Xu, Siddhartha Chaudhuri, Ersin Yumer, Hao Zhang, and Leonidas Guibas. Grass: Generative recursive autoencoders for shape structures. _ACM Transactions on Graphics (TOG)_, 36(4):1-14, 2017.\n' +
      '* [18] Yichen Liu, Benran Hu, Junkai Huang, Yu-Wing Tai, and Chi-Keung Tang. Instance neural radiance field. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [19] Leland McInnes, John Healy, and Steve Astels. hdbscan: Hierarchical density based clustering. _J. Open Source Softw._, 2(11):205, 2017.\n' +
      '* [20] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In _ECCV_, 2020.\n' +
      '* [21] Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy Mitra, and Leonidas J Guibas. Structurenet: Hierarchical graph networks for 3d shape generation. _arXiv preprint arXiv:1908.00575_, 2019.\n' +
      '* [22] Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna Tripathi, Leonidas J Guibas, and Hao Su. Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 909-918, 2019.\n' +
      '* [23] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Transactions on Graphics (ToG)_, 41(4):1-15, 2022.\n' +
      '* [24] Despoina Paschalidou, Luc Van Gool, and Andreas Geiger. Learning unsupervised hierarchical part decomposition of 3d objects from a single rgb image. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1060-1070, 2020.\n' +
      '* [25] Jordi Pont-Tuset, Pablo Arbelaez, Jonathan T Barron, Ferran Marques, and Jitendra Malik. Multiscale combinatorial grouping for image segmentation and object proposal generation. _IEEE transactions on pattern analysis and machine intelligence_, 39(1):128-140, 2016.\n' +
      '* [26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [27] Zhongzheng Ren, Aseem Agarwala, Bryan Russell, Alexander G Schwing, and Oliver Wang. Neural volumetric object selection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6133-6142, 2022.\n' +
      '* [28] Jianbo Shi and J Malik. Normalized cuts and image segmentation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 22(8):888-905, 2000.\n' +
      '* [29] Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bulo, Norman Muller, Matthias Niessner, Angela Dai, and Peter Kontschieder. Panoptic lifting for 3d scene understanding with neural fields. _arXiv preprint arXiv:2212.09802_, 2022.\n' +
      '* [30] Piotr Skalski. Make Sense. [https://github.com/SkalskiP/make-sense/](https://github.com/SkalskiP/make-sense/), 2019.\n' +
      '* [31] Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. Parsing natural scenes and natural language with recursive neural networks. In _Proceedings of the 28th international conference on machine learning (ICML-11)_, pages 129-136, 2011.\n' +
      '* [32] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Justin Kerr, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, et al. Nerfstudio: A modular framework for neural radiance field development. _arXiv preprint arXiv:2302.04264_, 2023.\n' +
      '* [33] Vadim Tscherzki, Iro Laina, Diane Larlus, and Andrea Vedaldi. Neural Feature Fusion Fields: 3D distillation of self-supervised 2D image representations. In _3DV_, 2022.\n' +
      '* [34] Suhani Vora, Noha Radwan, Klaus Greff, Henning Meyer, Kyle Genova, Mehdi SM Sajjadi, Etienne Pot, Andrea Tagliasacchi, and Daniel Duckworth. Nsf: Neural semantic fields for generalizable semantic segmentation of 3d scenes. _arXiv preprint arXiv:2111.13260_, 2021.\n' +
      '* [35] Yanzhen Wang, Kai Xu, Jun Li, Hao Zhang, Ariel Shamir, Ligang Liu, Zhiquan Cheng, and Yueshan Xiong. Symmetry hierarchy of man-made objects. In _Computer graphics forum_, pages 287-296. Wiley Online Library, 2011.\n' +
      '* [36] Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In _Proceedings of the IEEE international conference on computer vision_, pages 1395-1403, 2015.\n' +
      '* [37] Stella X Yu. Segmentation using multiscale cues. In _Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004._, pages I-I. IEEE, 2004.\n' +
      '* [38] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and Andrew Davison. In-place scene labelling and understanding with implicit scene representation. In _ICCV_, 2021.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      '그림 11: **C 완전한 나무**: 그림 6의 나무에서 모든 층과 모든 노드의 완전한 시각화는 각 노드 내의 다른 군집을 보여주고 각 행은 크기에 따라 정렬된 나무에서 주어진 깊이에서 모든 노드를 시각화한다.\n' +
      '\n' +
      '선택된 지점과의 친화도. 여러 그룹을 검색하기 위해 다양한 스케일에 걸쳐 GARF필드에 질의하고 큰 중첩과 함께 그룹을 병합한다. 비디오에서 사용자는 그림 1에서 굴삭기, 크레인 및 스투퍼를 추출할 수 있다. 한 번의 클릭이 있는 1.\n' +
      '\n' +
      '신청자 B 실험 세부사항\n' +
      '\n' +
      '### Hierarchical Decomposition\n' +
      '\n' +
      '관심 군집을 선택하면 HDBSCAN과 재귀적으로 군집링하여 트리를 구성한다. 이 과정을 위해 모든 실험에 대해 고정된 0.1의 HDBSCAN 군집 엡실론과 40의 최소 군집 크기를 사용한다. 나무는 비잡음 군집에서 재간호 _만_를 통해 깊이 우선 검색으로 탐욕스럽게 구성된다. 잡음 군집을 건설 후 나무에 다시 추가하기 때문에 이는 다육성 장면의 저수준과 같이 작은 사라지고 있는 지역을 초래할 수 있다는 점에 유의한다. 이러한 유물들은 향후 작업에서 해결하기를 희망하는 비공백나무 건설로 더 잘 다루어질 것이다.\n' +
      '\n' +
      '트리 경합을 속도화하기 위해 먼저 Open3D의 복셀다운 샘플링으로 입력 가우시안 스플라트를 하위 샘플링하여 점들의 해상도를 \\(0.01\\\\)로 감소시켰으며, 예를 들어 0.1 스케일 감쇠 샘플의 친화도를 0.001 복셀 분해능으로 감소시킨다. 나무 건설 후, 그로 인한 트리는 한 명의 아이와 한 명의 부모와 함께 노드의 사슬을 제거하기 위해 프루닝된다.\n' +
      '\n' +
      '푸셔스 노이즈의 치료.\n' +
      '\n' +
      '한 가지 극복을 위한 도전은 HDBSCAN이 클러스터 라벨을 얻지 못하는 \'소음\' 클러스터를 출력할 수 있다는 사실이다. 이는 NeRF 기하학과 잘 정렬되지 않는 가우스, 두 그룹의 경계에 있기 때문에 시끄럽거나 훈련된 친화성 분야에서 소음을 일으키기 때문에 발생할 수 있다. 이러한 소음 군집을 처리하기 위해 특징 공간과 달리 유클리드 공간에서 계산된 가장 가까운 _ph 물리적_ 군집의 라벨이 있는 노이즈로 간주되는 가우스에게 라벨을 할당한다. 우리는 이것이 특징 공간 자체 내에서 부드러운 군집링보다 더 응집적인 결과를 생성하는 것을 발견한다. 글로벌 군집링 동안(그림 10, 7) 세 잡음 클러스터는 전체 장면에 걸쳐 코스터들에게 할당되고, 트리 분해 동안(그림 6)이 할당된다. 세 잡음 클러스터는 각 노드에서만 사용할 수 있는 클러스터로부터 국부적으로 할당된다.\n' +
      '\n' +
      '### 3D Completeness Experiment\n' +
      '\n' +
      '#### b.3.1 Ground Truth Annotation\n' +
      '\n' +
      '우리는 주석을 위한 다각형 모양을 사용하여 온라인 도구 \'메이크 센스\'(30)를 사용하여 무작위로 선택된 소설 뷰에 지상 진리 분할 마스크를 주석한다. 그림에서. 12, 데이터 주석 과정에서 상태에 대한 시각화를 제시한다.\n' +
      '\n' +
      '주석 과정은 주어진 관점 내에서 특정 라벨 포인트의 할당으로 시작된다. 관정성의 평가를 효과적으로 향상시키기 위해 줌아웃, 줌아웃 또는 각도를 변경하는 것을 포함하는 시야의 선택이 무작위화되어 있다는 점에 유의한다. 이러한 라벨 포인트는 다양한 정도의 과립도로 만들어진 후속 마스크 주석의 기초가 된다. 그림의 사례로서. 21, 꽃다발 장면에서 다른 각도에서 클릭 포인트를 고려할 때, 우리는 꽃의 꽃잎(미세 레벨), 개별 꽃(중간 수준), 전체 꽃다발(거친 수준)이라는 다양한 계층적 수준에서 마스크를 주석한다. 다른 장면에서 그라운드 진리 마스크의 경우, 대상물의 미세한 부분부터 거친 전체 대상에 이르기까지 의미적 의미에 기반한 마스크 위계를 구축하는 것과 유사한 규칙을 따른다. 그러나 현장의 복잡성과 자연 의미성에 따라 마스크 수준의 수가 달라질 수 있다는 점에 유의한다. 예를 들어, 테타임 장면에서 곰의 팔은 그림이다. 20은 왼손과 곰의 두 가지 수준의 계층으로만 주석을 달았다.\n' +
      '\n' +
      '3.3.2 C 완전소명자료####.\n' +
      '\n' +
      'GARField의 견해 일관성에 관한 평가 결과의 포괄적인 제시는 그림과 같다. 20, 21, 22, 23, 24는 본문에 표시되지 않은 모든 장면을 포함한다. 각 장면에 대해 서로 다른 계층적 수준에서 주석이 달린 랜덤하게 선택된 뷰, 그라운드 진리 마스크에 대한 클릭 라벨 포인트 및 SAM 및 GARF필드에서 얻은 가장 가까운 마스크 비교를 보여준다. 또한 더 나은 시각화를 위해 결과의 줌인 이미지를 제공합니다.\n' +
      '\n' +
      '그림 12: ** 3D 보완 실험**: 오버핑 마스크(_egg_, _noodles_, _ramen_ 마스크 내부의 _nori_ 마스크)가 원하는 계층적 그룹을 모델링한다. 우리는 마스크 주석을 위한 온라인 도구인 \'메이크 센스\'(30)를 사용하여 이러한 다각형 마스크를 표지했다.\n' +
      '\n' +
      '### Hierarchical Grouping Recall Experiment\n' +
      '\n' +
      '2.4.1 지상진술자 주석#####.\n' +
      '\n' +
      '10대 남자의 에코 너트 리즈 오업을 재탄생시키는 T 에보즈 엑세스네. 그녀의 어메이크 장면(그림 1.1,25 )을 예로 들자면, 파 쯔가 우베를 썰어 나루드 노리, 그 g, 에그른른, 까마귀, 지느러미, 이디온아 리콤 피츠 에스오 업, 센티 리 라면이 다사그 로프를 먹었다. 페레이트가 entson3D 혜성 흉막 네 ss을 모방하는 것과 달리, 세이즈 xp 에피 멘티는 이코 델칸 xtane decrac가 0.T 이하 히에르 차 또는 e.T, 마운트마 스케이트의 i fythel evel에서 빗대지 않았다.\n' +
      '\n' +
      '2.4.2 C 완전정시화 규범####.\n' +
      '\n' +
      'In Fig. 25, We show the ground truth masks as well as all the methods masks at the finest masks. Note that all the ground truth masks are arranged in descending order of size. In our experiment, we systematically recover all the masks that corresponds to the annotated ground truth through different method. For each distinct method employed, which are SAM, GARField without scale condition, GARField without dense supervision, we sequentially showcase the masks that get the highest IOU score of the correspondence to the ground truth masks. We will release all the ground truth annotations for all experiments.\n' +
      '\n' +
      'Figure 13: **Global Clustering Results (“Bouquet”)**: Global clusters at smaller scales \\((s=0)\\) distinguish between different sections of the bouquet, as well as the two halves of the table. At a larger scale, the bouquet and table are considered whole.\n' +
      '\n' +
      'Figure 14: **Global Clustering Results (“Desk”)**: At larger scales \\((s=0.5)\\), the desk is grouped together with the clutter on it _e.g._ keyboard, card, bird figurine).\n' +
      '\n' +
      'Figure 15: **Global Clustering Results (“Donuts”)**: At a very small scale \\((s=0.0)\\), GARField can distinguish between different pieces of the breakfast sandwich in the middle of the scene. As scale increases, its grouping shifts quite noticably — into its two halves, or the full sandwich with the checkerboard packaging.\n' +
      '\n' +
      'Figure 16: **Global Clustering Results (“Table”)**: At the smallest scale \\((s=0.0)\\), the global clusters highlight parts of objects _e.g._ labels on water bottles, pieces of chocolate.\n' +
      '\n' +
      'Figure 17: **Global Clustering Results (“Teatime”): The food, utensils, and the table are included in different clusters at small scales, and the same cluster at larger scales. Parts of the stuffed animals (_e.g._ sheep hooves, bear nose) can also be seen at \\(s=0.0\\).**\n' +
      '\n' +
      'Figure 18: **Global Clustering Results (“Succulent”)**: Global clusters at smaller scales (\\(s=0.0\\)) distinguish between fine features like succulent leaves, while they are considered a single group at larger scales (\\(s=1.0\\)).\n' +
      '\n' +
      'Figure 19: **Global Clustering Results (“Living Room”: The individual hexagonal tiles on the floor may be grouped separately (\\(s=0.0\\)) or together \\((s=0.5)\\).**\n' +
      '\n' +
      '그림 20: **뷰 컨슈머티즘 실험-Teatime**: 미세하고 중간인 두 개의 위계를 구성했다. 이는 곰의 왼손과 곰 전체의 의미적 의미에 각각 해당한다.\n' +
      '\n' +
      'Figure 21: **View Consistency Experiment-Bouquet**: We constructed three hierarchies, which are fine medium and coarse. These correspond to the semantic meanings of the petal of the flower, the individual flower and the whole bouquet, respectively.\n' +
      '\n' +
      '그림 22: **뷰 컨센서스 실험-키보드**: 미세하고 중간인 두 개의 위계를 구성했다. 이는 각각 단일 키와 전체 키보드의 의미적 의미에 해당한다.\n' +
      '\n' +
      '그림 23: **뷰 컨센서스 실험-람엔**: 미세하고 중간이고 거친 3개의 위계를 구성했다. 이는 계란 노른자의 의미적 의미, 하나의 달걀과 전체 수프 영역에 각각 해당한다.\n' +
      '\n' +
      '그림 24: ** 시청 대응 실험-생활실**: 미세 매질과 거친 두 개의 계층을 구성했다. 이는 nerf총의 작은 주황색 부분, nerf총의 중간 블루 부분과 전체 nerf총의 의미적 의미에 해당한다.\n' +
      '\n' +
      '그림 25: ** 계층적 그룹화 기록 실험**: SAM 및 GARField의 절제 연구와 같은 방법에 중점을 둔다. GARF 내야는 더 미세한 더 작은 마스크(예: 키보드 장면에서 작은 키를 모두 캡처)를 얻는 데 SAM을 능가한다. 계층화 그룹화가 없는 GARF필드와 달리 GARF필드는 더 많은 계층화 그룹화 결과(예를 들어 라면에 있는 장면에서)를 달성하면 계층적 군집화를 통해 라면의 마스크 전체를 성공적으로 식별한다. 또한, 조밀한 감독이 없는 GARF필드와 비교하여 GARF필드는 보다 안정적이고 철저한 그룹화 결과(예: 티타임 장면에서 GARF필드는 쿠키 백의 작은 라벨을 더 포괄적으로 식별)를 제공한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>