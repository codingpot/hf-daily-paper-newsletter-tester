<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '유엔 통합 영상 세대# UniVG를 제공합니다.\n' +
      '\n' +
      ' 루단 루단 루안, 레이 톈, 초안웨이 황, 벼 장, Xu 장, 잔난 샤오샤오 장.\n' +
      '\n' +
      'Baidu Inc.\n' +
      '\n' +
      'Beijing, China\n' +
      '\n' +
      '{ruanludan, tian1ei09}@baidu.com\n' +
      '\n' +
      'huangcv21@gmail.com\n' +
      '\n' +
      '{zhangxu44, xiaoxinyan}@baidu.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '확산 기반 영상 생성은 학계와 산업계 모두에서 광범위한 관심을 받고 상당한 성공을 거두었다. 그러나 현재의 노력은 주로 텍스트에 의해 구동되는 생성, 이미지에 의해 또는 텍스트와 이미지의 조합과 같은 단일 주관적 또는 단일 태스크 비디오 생성에 집중되어 있다. 이는 사용자가 개별적으로 또는 조합하여 유연한 방식으로 이미지 및 텍스트 조건을 입력할 가능성이 높기 때문에 실제 응용 시나리오의 요구를 완전히 충족할 수 없다. 이를 해결하기 위해 텍스트 및 이미지 양식에 걸쳐 다중 비디오 생성 작업을 처리할 수 있는 ** 비조정***-모달 **V** 비디오 **G** 폭기 시스템을 제안한다. 이를 위해 우리는 생성자유의 관점에서 우리 체제 내에서 다양한 영상 생성 과제를 재방문하고, 이를 고자유와 저자유 영상 생성 범주로 분류한다. 고자유 영상 생성을 위해 입력 이미지 또는 텍스트의 의미론과 일치하는 비디오를 생성하기 위해 멀티 조건 크로스 어시스트를 사용한다. 저자유 영상 생성을 위해 순수 랜덤 가우시안 노이즈를 대체하기 위해 바이오머드 가우시안 노즈를 도입하는데, 이는 입력 조건의 내용을 더 잘 보존하는 데 도움이 된다. 우리의 방법은 공공 학술 벤치마크 MSR-VTT에 대한 가장 낮은 프로쉐트 비디오 거리(FVD)를 달성하여 인간 평가에서 현재 오픈 소스 방법을 능가하고 현재 긴밀한 소스 방법 Gen2와 일치하며 더 많은 샘플에서 [https://univg-baidu.github.(https://univg-baidu.github)를 방문한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 확산 기반 생성 모델[1; 2; 3]은 영상 생성[4; 5; 6; 8; 8; 9]에서 영상 생성[10; 11; 14;]으로 빠르게 확장된다. 영상 생성 모델의 대부분은 텍스트 설명을 조건부 입력[15; 16; 17; 18; 11; 19]으로 사용한다. 그러나 최근 연구에서는 생성된 영상 [20]의 세부사항을 개선하거나 [21; 22; 12; 23; 24]를 위한 영상 조건의 활용을 탐색하기 시작하였다. 또한, 생성된 비디오의 시간적 평활성과 공간적 해상도를 향상시키기 위해, 현재 접근법은 종종 프레임 보간 및 초해상도 [10; 11]을 위한 모듈을 통합한다. 그러나 기존 작업은 텍스트[10; 11; 9; 18], 이미지[13], 텍스트와 이미지[12; 24]의 조합으로 입력이 제한된 단일 주관적 또는 단일 태스크 비디오 생성에만 초점을 맞추고 있다. 이 단일 주관적 또는 단일 태스크 파이프라인은 모든 사용자 요구를 충족시키기 위해 필요한 유연성이 부족하다. 실무적으로, 사용자는 입력을 위한 필수 텍스트 또는 이미지 조건을 갖지 못하여 모델을 사용할 수 없게 할 수 있다. 대안적으로, 상충되는 텍스트 이미지 쌍의 도입은 급격한 전이([24]에서 유사 결론이 제안됨)를 갖는 정적 비디오 또는 비디오의 생성으로 이어질 수 있다.\n' +
      '\n' +
      '본질적으로, 비디오 생성에서 사용되는 모든 모델은 대응하는 비디오를 생성하기 위해 하나 이상의 조건을 수용하는 조건부 생성 모델이다. 이러한 조건은 텍스트, 이미지, 저해상도 비디오, 심지어 제어 신호일 수 있다. 다중 비디오 생성 작업을 처리할 수 있는 다재다능한 비디오 생성 시스템을 구축하기 위해 기존 방법을 재방문하고 작업 자체보다는 ** 생성 자유**를 기반으로 관련 방법을 분류한다. 우리가 제안하는 ** 생성 자유**의 개념은 특정 조건이 주어진 비디오 생성 모델에 대한 솔루션 공간의 범위에 해당한다. 본 논문에서는 다양한 영상 생성 작업을 자유도가 높거나 자유도가 낮은 영상 생성으로 분류한다. 구체적으로, 고자유 영상 생성은 입력 조건, 즉 의미 수준에서 약하게 제약되는 텍스트와 이미지를 특징으로 하여, 이 시나리오의 생성 모델이 더 큰 해결 공간을 가지므로 더 높은 자유도를 제공한다. 반대로, 저자유 영상 생성은 일반적으로 이미지 애니메이션 및 비디오 슈퍼해상도와 같은 저수준 정보(즉, 픽셀)에서 강하게 제약되는 조건을 포함한다. 이러한 제약은 생성 모델에 사용할 수 있는 해결 공간을 제한하여 자유도가 낮아진다.\n' +
      '\n' +
      'In order to better match the characteristics of various video generation tasks, different strategies with varying degrees of generative freedom should be taken for video generation. For high-freedom video generation, the standard diffusion _Generation Paradigm_ is appropriate and has been extensively utilized in existing research some refs should be provided @ludan. Specifically, during training stage, the diffusion model learns the added noise in the forward processing, and predicts the target distribution by reversing from a purely random Gaussian distribution during inference stage. Classifier guidance [4] and classifier free guidance [25] are employed to align the predicted distribution with the one specified by the input conditions. For low-freedom video generation, the _Editing Paradigm_ is more suitable. Taking image editing [26] as a case in point, a prevalent practice involves adding noise to the original image up to a certain level and then using text as the editing signal to steer the distribution toward the intended outcome. This approach, compared to generation from scratch, offers better retention of the original input\'s content. Video super-resolution has utilized a similar technique to that of image editing [23]. However, the _Editing Paradigm_ has a limitation in the form of a discrepancy between training stage and inference one. Specifically, the model is trained solely to approximate the target distribution without learning the transition from the conditional distribution to the target distribution. This discrepancy results in a trade-off-related issue, i.e., **the less noise that is introduced, the weaker the model\'s ability to edit, whereas the more noise that is added, the less capable the model is of preserving the input.** In extreme cases, when the noise level approaches that of a completely random Gaussian distribution, editing paradigm becomes analogous to generation one, significantly diminishing the model\'s capability to preserve the content of the original input. How to reconcile the training and inference stages of editing models to balance their editing capabilities while preserving the input is also a problem that needs to be addressed but has been overlooked in previous work.\n' +
      '\n' +
      'In this paper, we propose a unified system **Un**ified-modal **Video **G**eneration (i.e.**UniVG**), designed to support flexible video generation conditioned on the arbitrary combination of image and text. To achieve this, we categorize all models within the system into two groups: high-freedom video generation and low-freedom video generation. For high-freedom video generation, we present a base model that is capable of the requirements of handling arbitrary combinationsof text and image conditions. We accomplish this by enhancing the original cross-attention module of the UNet architecture with a multi-condition cross-attention module. With regard to low-freedom video generation, we propose two corresponding models that are individually tailored for image animation and video super-resolution task. These models utilize the editing paradigm, as opposed to the generation paradigm. To reconcile the differences between the training process based on generation paradigm and the inference process based on editing one, in this paper, we predict **B**iased **G**aussian **N**oise (shorted as **BGN**) that is directed towards the target distribution, instead of standard Gaussian noise, by refining the objective function during training stage.\n' +
      '\n' +
      '제안된 UniVG 시스템은 베이스 모델, 이미지 삽입 모델 및 슈퍼 해결 모델을 포함한다. 베이스 모델은 텍스트 및 이미지 조건의 임의의 조합을 처리하고, 입력 조건과 초당 8 프레임(fps)으로 의미적으로 정렬된 \\(24\\t 320\\ 시간 576\\)의 비디오 시퀀스를 출력할 수 있다. 이미지 연결의 추가 조건으로 베이스 모델에서 미세 조정되는 이미지 삽입 모델은 입력 이미지와 픽셀 정렬된 \\(24개 320개 점수 576\\)의 비디오 프레임을 생성한다. 슈퍼 해결 모델은 각 프레임의 \\(720\\t 1280\\) 픽셀의 해상도를 향상시킨다. 우리 UniVG는 이전 작품과 비교하여 영상 생성에 대한 더 나은 과제 적응력, 즉 통일된 시스템 내에서 다양한 비디오 생성 작업을 처리하지만 생성 세부 사항 및 프레임 일관성에 대한 상당한 개선도 보여준다. 실험은 우리의 방법의 효과를 입증했다. 객관적 메트릭에서 우리의 방법은 기존의 다른 방법을 크게 능가하며 수동 평가에서 우리의 접근법은 Gen2와 일치하며 다른 방법을 초과한다.\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약될 수 있다.\n' +
      '\n' +
      '1. 의미적으로 정렬된 텍스트/이미지 대 비디오 생성, 이미지 애니메이션과 같은 다수의 비디오 생성 작업을 처리할 수 있는 제1 비디오 생성 시스템인 UniVG를 제안한다.\n' +
      '2. 바이오페이지 가우시안 노이스를 소개하고 이미지 애니메이션, 슈퍼 해상도 등 저자유 영상 생성 과제에 대한 효과를 확인한다.\n' +
      '3.실험은 우리의 방법이 객관적인 메트릭 측면에서 기존 텍스트/이미지 대 비디오 생성 방법을 능가하고 주관적 평가에서 Gen2와 일치한다는 것을 보여준다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '### Text-to-Video Generation\n' +
      '\n' +
      '비디오 생성에 대한 초기 작업은 GAN[27; 28; 29], VQ-VAE[30; 31], 자동 억제 모델[30; 18], 변압기 구조[32]를 활용했지만 낮은 해상도와 최적이 아닌 시각적 품질에 의해 제한되었다. 영상 생성[4; 5; 6; 7], 오디오 생성[33; 34] 및 기타 도메인[36; 37; 38]에서 확산 모델이 성공한 후 비디오 생성에서 확산 모델의 첫 번째 적용을 나타냈다. 이어서, Make-A-V 비디오 [10]와 ImagenV 비디오[11]는 2D U-Net을 텍스트 대 이미지 생성에서 3D U-Nets로 확장시켜 영상 생성을 오픈 영역으로 확장시켰다. 그 이전까지는 대규모 GPU 메모리 소비와 높은 훈련 비용이 필요한 화소 공간에서 영상 모델링을 연구해 왔다. 이 문제를 해결하기 위해 많은 연구자들이 픽셀 공간[8; 15; 40; 16] 대신 잠재 공간에서 확산 과정을 수행하고 무학습 샘플링[2; 41; 42; 43] 또는 학습 기반 샘플링[44; 45]으로 샘플링 효율을 향상시키기 위해 초점을 옮겼다. 또한 일부 작업은 훈련 비용을 단일 비디오[46]로 줄이거나 전혀 훈련 비용이 없다[47]로 줄이는 데 집중했다.\n' +
      '\n' +
      '### Image-to-Video Generation\n' +
      '\n' +
      '문자에서 직접 영상을 생성하는 것은 복잡성이 높은 도전적 과제이다. 자연적인 생각은 이미지를 중간 다리로 사용하는 것이다. 비디오 생성과 유사하게 비디오 예측에 대한 초기 작업은 비확산 방법[48; 49; 50]을 사용했으며, 이는 종종 낮은 해상도 또는 특정 영역에서 제한적이었다. 텍스트 대 비디오 작업에서 확산 기반 방법의 상당한 발전으로 I2VGen-XL[23]은 우리가 아는 한 오픈 도메인 이미지 대 비디오 생성에 확산의 첫 번째 활용이다. 문자 CLIP 특징을 텍스트 대 비디오 프레임워크 내의 이미지 CLIP 특징들로 대체하여 입력 이미지와 의미적으로 정렬된 비디오 생성을 달성한다. 유사하게, SVD[13]는 또한 텍스트 대 비디오 모델에서 이미지 대 비디오 모델에 미세 조정되지만 이미지의 VAE 특징을 더 강한 제어 신호로 연결한다. 현재 비디오제[21], 비디오크래프트1[20], EMU 비디오[12]와 메이크픽셀 댄스[24]는 텍스트 대 영상 생성의 목표를 유지하고 있으나, 중간 단계로 텍스트 대 이미지 합성을 소개한다. 생성된 이미지는 연결된 또는 CLIP 특징에 의해 비디오 생성 프레임워크에 통합된다.\n' +
      '\n' +
      '위에서 유추할 수 있듯이 텍스트 대 비디오 생성 및 이미지 대 비디오 생성은 다양한 응용 프로그램을 제공하지만 기술적 접근에서 많은 유사성을 공유한다. 따라서 본 논문에서는 하나의 틀이 이 두 가지 목적을 통일할 수 있는지를 탐색한다. 우리 UniVG의 초기 작품과의 주요 구분은 동영상 생성에 포함된 다양한 모델을 과제보다는 생성적 자유 관점에서 구별한다는 것이다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '이 섹션은 탄력적으로 조건부 비디오 생성을 위해 제안된 ** 유니프**-모달 **V** 비디오 ******증가(즉, **UniVG*********)를 제시한다. 그런 다음 특정 설계로 다이빙하기 전에 Sec 3.1에서 전체 시스템 UniVG의 예비 지식을 간략하게 설명한다.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '확산 모델[1]은 가우시안 노이즈에서 반복적으로 변성시켜 샘플을 생성하기 위해 훈련된 생성 모델 그룹이다. (\\_\\overline{t}\\)은 노이즈 일정(\\_{t}{t}\\sqrt{t}\\)을 의미하고 \\(\\math{N})에서 샘플링된 노이즈(\\math{N}(\\math{N},\\math{0},\\math{I})는 트레이닝 동안 a\\(\\math{f{f{I})를 사용하여 원래 입력(\\math{N})에서 샘플링된 노이즈(\\math{f{t)를 통해 결정된 노이즈(\\math{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f})를 통해 결정된 노이즈(\\)를 추가)하는 것을 말하며,\\) 모델을 통해 추가된 노이즈를 예측(\\)을 예측하는 것을 말한다. 추론 동안 샘플은 반복적으로 변성함으로써 순수 노이즈 \\(x_{N}\\ason\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\에서 생성된다. 또한, Con\\_{t}(x_{t+1})+w_{\\ta}(x_{t+1})+wc}(p_{\\theta})에 의한 예측 분포를 편향시키기 위한 추가 조건부 신호를 도입하고(p_{\\theta}(x_{\\theta})(x_{\\theta}(x_{\\ta+1},c)-p_{\\ta},c)-p(x_{\\t+1}, 25) 조건에 대한 추가 조건부 신호를 도입하고(p_{\\t+1})+w_{\\ta}(x_{\\ta}(x_{\\ta}(x_{\\ta}(x_{\\ta}(x_{\\ta}, +1},c)-p_{\\t+1},c)-p_{\\+1},c)-p_{\\+1},c)-p_{\\+1},c)-p_{\\+1},c)-p_{\\+1},c)-p_{\\+1},c)-p(x_{ 또 다른 주류 채택 확산 모델은 바리티브 오토엔코코더(VAE) [51]로 구성된 라트렌트 디확산 모델(LDM)[8]과 잠복 숨겨진 공간에서 변성시키는 잠재 확산 모델이다. 이 접근법은 고해상도에서 피팅 분포의 복잡성을 감소시킨다. 본 논문에서는 UniVG의 각 단일모형이 무조건적 빈곤확산모형이다. 즉, 영상 \\(F\\) RGB 프레임은 먼저 잠재 공간 \\(X\\in\\mathbb{R}^{F\\tcer C\\t 기간에는 W}\\)로 압축된 후, 이미지 자동 인코더가 있는 UNet(텍스트 조건 \\(T\\), 이미지 조건 \\(I\\), 저 해상도 비디오 \\(V^{lr}\\)로 입력된다.\n' +
      '\n' +
      '### UniVG\n' +
      '\n' +
      '그림 2-(a)에 도시된 바와 같이, 우리의 전체 UniVG는 (1) A 베이스 모델 \\(\\mathcal{F}_{B}\\)의 세 가지 모델로 구성되며, 이는 자유도 높은 비디오 생성을 위한 텍스트와 이미지 조건의 조합을 수락한다. (2) An Image Animation \\(\\mathcal{F}_{A}\\) 모델은 텍스트 이미지 쌍을 수용하여 픽셀 레벨에서 입력 이미지와 정렬된 생성된 비디오를, (3) 공간 해상도를 개선하기 위한 슈퍼해상도 모델 \\(\\mathcal{F}_{SR}\\)을 제공한다. 각 모델은 공간 플레이어, 테스포랄 플레이어, 크로스 어텐션 플레이어로 구성된 3D UNet 아키텍처가 있는 잠재 확산 모델이다. 기존 작품[10; 13]에 이어 공간층에는 2D 협화층과 공간 변압기로 구성되어 있고, 테포털층에는 1D 시간적 협화층과 시간적 변압기로 구성되어 있다. 횡단 주의 모듈은 텍스트, 이미지 특징 등과 같은 의미론적 제어 신호를 처리하는 데 사용된다.\n' +
      '\n' +
      '(1) For the Base Model \\(\\mathcal{F}_{B}\\), we employ an image encoder that matches the text encoder of CLIP [52] inspired by VideoCrafter1 [20]. To fully utilize the global semantics and local details of input image and text, we utilize all\n' +
      '\n' +
      '그림 2: 제안된 **UniVG** 시스템의 오버뷰. (a)는 홀p ipelineo fU niVG,w hichi ncludest 헴B 무균M odel\\(\\mathcal{F}_{B}\\),t 헴알미덤 odel\\(\\mathcal{F}_{A}\\), ndt 헴S uperR esolutionm odel\\(\\mathcal{F}_{SR}_{SR}\\)를 표시한다. (b)i llustratest 헴-조건C rossA ttentioni nventioni n\\(\\mathcal{F}_{B}\\)a nd\\(\\mathcal{F}_{A}\\) 입니다.\n' +
      '\n' +
      'F_{I}(F_{I}) 시각적 토큰(F_{f_{i}}=\\{f_{i}}_{i =0}^{K_{I}}}}}}}}}}}}}) 및 모든 \\(K_{T}}}) 텍스트 토큰(F_{T}=F_{T}=F_{T}=F_{T}) 텍스트 토큰 \\)은 CLIP ViT의 마지막 층으로부터 CLIP ViT의 마지막 층부터 CLIP ViT의 마지막 층부터 CL. 1개 이상의 의미적 특징을 처리하는 능력을 가진 원래 크로스 어텐션인 Sec 3.3(2)에 대한 메커니즘을 확장하고 픽셀 수준에서 입력 이미지와 정렬된 비디오를 추가로 생성하기 위해, 우리는 이미지 삽입 모델 \\(\\mathcal{F}_{A}\\)을 훈련시키고 추가 조건으로 첫 번째 프레임의 숨겨진 공간 특징을 연결함으로써 이미지 삽입 모델(\\mathcal{F}_{A}\\)을 훈련시킨다. 추가 조건 때문에 초기 컨볼루션 레이어의 커널의 대응하는 채널 치수는 \\(C\\)에서 \\(2C\\)로 변한다. 우리는 원래 모델의 성능을 보존하기 위해 추가 매개변수를 0으로 초기화한다. \\(\\mathcal{F}_{B}\\) 또는 \\(\\mathcal{F}_{A}\\)를 사용하여 \\(24\\tcer 320\\i 576\\)의 비디오 프레임을 획득할 수 있다. (3) 생성된 비디오의 선명도를 높이기 위해, 우리는 \\(\\mathcal{F}_{B}\\)의 슈퍼 해결 모델 \\(\\mathcal{F}_{SR}\\)을 추가로 고용했다. 초해상도 작업은 이미지 조건이 없기 때문에, 다중 조건 교차 주의 모듈은 텍스트 조건만을 수용하는 정기적인 교차 의도 모듈로 되돌아간다. 훈련 중 \\(\\mathcal{F}_{SR}\\)는 라모도블러, 무작위화, JPEG 압축 등을 통해 고화질 영상을 파괴하여 얻은 저해상도 \\(V^{lr}\\)의 영상을 접수한다. 우리는 \\(\\mathcal{F}_{A}\\)와 \\(\\mathcal{F}_{SR}\\)에 해당하는 작업을 자유도가 낮은 세대로 분류함에 따라 표준 가우시안 노이즈를 Sec 3.4에 도입된 바이오세스 가우시안 노이즈(**BGN***)로 조절하여 조건부 분포에서 목표 분포로 비사회 역진 과정을 제시한다.\n' +
      '\n' +
      '멀티 크로스 의도.\n' +
      '\n' +
      '우리의 기본 모델 \\(\\mathcal{F}_{B}\\) 및 이미지 삽입 모델 \\(\\mathcal{F}_{A}\\)이 텍스트 및 이미지 CLIP 특징을 수용하기 때문에 표준 크로스 의도 대신 멀티 조건 크로스 어시스트를 사용한다. 이 모듈의 아키텍처는 주로 VideoCrafter[20], \\(F_{\\text{out}}})를 구성하는 VideoCrafter[20]을 따른다.\n' +
      '\n' +
      '}(K_{in}.^{{}}{Q_{I}^{{{d})\n' +
      '\n' +
      'W_{I}\\ V_{T}\\ V_{T}\\ V_{I}}\\ V_{I}}\\ K_{dot F_{I}=W_{V_{I}}\\ K_{dot F_{I}}.\n' +
      '\n' +
      'Hf(d_{k}\\)가 키/쿼리 벡터의 차원이고 \\(Q_{\\text{in}}\\)는 \\(F_{I}\\)와 \\(F_{T}\\) 사이에 공유된다. 가중치 \\(W_{K_{I}}\\) 및 \\(W_{V_{I}}}\\)는 각각 \\(W_{K_{T}}}\\) 및 \\(W_{V_{T}}\\)에서 초기화된다. 이미지를 추가 입력 강화로 취급하는 비디오 더 부드러운 래트와 달리, 우리는 이미지를 텍스트와 함께 동등하게 유의미한 제어 신호로 간주한다. 이는 훈련 과정 전반에 걸쳐 일정 비율의 이미지 드롭아웃을 적용하여 달성된다. 확장하면 MCA는 재교육 필요성(예: 더 강한 텍스트 특징) 없이 교차 의도 단위의 수를 증가시켜 두 가지 이상의 조건을 수용할 수 있다. 이러한 유연성은 새로운 조건을 처리하기 위해 모델의 훈련을 연장하는 비용을 크게 감소시킨다.\n' +
      '\n' +
      '가우시안 노이즈를 수상했어요.\n' +
      '\n' +
      '제안된 바이오페이지 가우시안 노이즈는 낮은 자유 비디오 생성을 위한 목표 분포로 조건 분포를 전달하는 데 사용된다. 그림 3-(a)에 도시된 바와 같이, 표준 순방향 확산 과정은 목표 분포 \\(v^{T}_{t}=v^{T}_{t}=\\sqrt{T}_{t}v^{T}+\\sqrt{1-\\alpha}_{t}_{t}<{t}_\\epsilon\\)를 통해 목표 분포 \\(v^{T}_\\epsilon\\)에서 표준 가우시안 분포 \\(v^{T}.{t}_\\sqrt{t}_\\sqrt{t}_\\sqrt{t}_\\sqrt{t}_\\sqrt{t}_{t}_\\he}_{t}_\\he}_{t}_\\epsilat}_{t}_\\epsilon\\)에서 표준 가우시안 분포)에서 표준 가우시안 분포)로 전환한다. 그러나 일반적으로 후방 과정에서 이러한 분포는 관련된 유일한 두 가지 분포이다. 이는 추론 중 조건 분포 \\(v^{C}\\)에서 샘플이 도입될 때 최적이 아닌 편집 결과를 초래할 수 있다. 전방 및 후방 과정 모두에서 조건 분포를 설명하기 위해 그림 3-(b)에 도시된 바와 같이 원래 확산을 세 부분으로 분할한다. t_{m}\\, \\(v_{t}\\)는 0 내지 \\(v_{t}|v^{T}},t) 사이의 타임스메프를 사용하여 표적 샘플에 의해 계산되며, 이는 \\(q_{t}|v{t^{T}=\\sqrt{\\alpha}_{t}_{t}_{t}_{t}_{t}_{t}}_{t}. H\\(t_{n}\\)에서 \\(N\\) 사이의 타임스탬(t_{n}\\)의 경우, \\(v_{t}|v^{C},t)=\\sqrt{\\alpha}_{t}_{t} +\\sqrt{1-\\alpha}_{n}\\leq t <N)\\로 조건 샘플에 의해 \\(v_{t}. 핵심 문제는 \\(v_{t}|v^{C},v^{T}}}\\)에서 \\(v_{t_{m}}\\)로 원활하게 전환할 수 있는 \\(q_{t_{n}}},t)\\을 설계하는 방법이다. 원래의 확산 일정을 보존하기 위해 \\(\\epsilon\\)로 표시된 노이즈 \\(\\epsilon^{\\prime}\\) 변수를 소개한다. t_{m}\\(t_{n}\\)와\\(t_{t}}) 사이의 타임스메프를 위해 우리는\\(v_{t}|v^{C},v^{t},{t})를 가지고 있다.\n' +
      '\n' +
      '그림 3: 무작위 가우시안 노이즈 및 바이오머드 가우시안 노이즈와의 전방 및 후방 확산 프로세스는 그림 3:이다.\n' +
      '\n' +
      '}}(v_\\qrt{t_{t_{m}}}} <{t_{t_{m}}}} <{t_{t_{m}}}}) 및 \\(v_{t_{n}=v_{t_{n})=\\sqrt{\\alpha_{t_{n}-{t_{t_{t_{t_{t_{t_{t_{t_{t_{m} <{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{n}<{t_{t_{n} <{t_{t_{n},{t_{n}:{t_{t_{n}}.{t_{n}}<{t_{n}\'{t_{n}} 따라서 해당 \\(\\epsilon^{\\prime}\\)는 타임스톤 \\(t_{m}\\) 및 \\(t_{n}\\)에서 다음 공식을 만족해야 한다.\n' +
      '\n' +
      '}} <표실론>{t_{m}}.\n' +
      '\n' +
      '이론적으로 \\(\\epsilon^{\\prime}\\)에 대한 해결책은 무한히 많다. 본 논문에서는 \\를 단순히 \\(\\epsilon^{\\prime}\\)로 정의하고 있다.\n' +
      '\n' +
      '}}{\\frt{t_{m}} 〈^{C}-v^^ {T}]\\(t_{m\\leq t<{n})\\.\n' +
      '\n' +
      'i\\(\\epsilon^{\\prime}\\)는 \\(v^{C}\\) 및 \\(v^{T}\\)의 가중 조합에 의해 평균값이 이동된 비차원 가우시안 분포로부터 샘플링된다. 이러한 편향은 조건 분포로부터 목표 분포로의 확산 과정을 가교하는 데 중요하다. 미래의 작업에서 \\에 대한 대체 솔루션을 탐색할 것이다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'DatasetOur training datasets include publicly available academic datasets such as WebVid-10M [53] and LIAIONCOCO [54], along with self-collected data. WebVid-10M is a large and diverse text-video dataset containing approximately 10 million open-domain videos with a resolution of \\(336\\times 596\\) pixels. LIAION-COCO is a substantial text-image dataset comprising 600 million high-quality images, filtered from LIAION-2B and scored using the Aesthetic and Semantic Estimate (ASE). To further enhance the quality of the generated videos and to address the issue of watermarks present in WebVid-10M, we continue training on our own curated datasets of videos and images, which contain high-quality visual content. We prepare the self-collected videos by first proportionally compressing them to 720p resolution along their shorter edge and then segmenting them into 10-second clips. This process yielded 5 million high-quality text-video pairs. Additionally, our self-curated image dataset includes 1.3 million high-quality text-image pairs, with a focus on artistic styles.\n' +
      '\n' +
      'TrainingOur \\(\\mathcal{F}_{B}\\) is trained with an image:video:video frame ratio of 1:1:1, where the training video frames were sampled with equal probability from 8 to 24 frames. We set the text dropout to 0.5 and the image dropout to 0.1. In addition, we utilize offset noise [55] with a strength of 0.1 and zero terminal Signal-to-Noise Ratio (SNR) [12]. Offset noise has been proven helpful to be helpful in generating extremely dark or bright images. Zero terminal-SNR has been shown to be beneficial for generating high-quality and high-resolution visual content by adding noise to pure Gaussian noise following a rescaled schedule. Both techniques have proven useful in our experiments. Subsequently, we continue finetuning \\(\\mathcal{F}_{B}\\) to obtain \\(\\mathcal{F}_{A}\\) and \\(\\mathcal{F}_{SR}\\), using Biased Gaussian Noise (BGN) on our self-curated video dataset only. For \\(\\mathcal{F}_{A}\\), we set the text dropout to 0.1 and the image dropout to 0.1, the BGN is experimentally set during timesteps \\(t_{m}=600\\) to \\(t_{n}=990\\) since the earlier steps determine the content [26]. For \\(\\mathcal{F}_{SR}\\), the text dropout is set to 0.1, and the BGN is applied during timesteps \\(t_{m}=0\\) to \\(t_{n}=700\\) since the later steps deciding the details [26]. We incorporate \\(\\epsilon\\)-prediction [1] for \\(\\mathcal{F}_{B}\\) and \\(\\mathcal{F}_{A}\\), \\(v-\\)prediction for \\(\\mathcal{F}_{SR}\\). The learning rate of all models is fixed at \\(1\\times 10^{-5}\\). We use DPM Solver [56] for accelerating sampling: 50 steps for \\(\\mathcal{F}_{B}\\) and \\(\\mathcal{F}_{A}\\), and 7 steps for \\(\\mathcal{F}_{SR}\\) since we set initial weight to 0.7.\n' +
      '\n' +
      '우리는 UniVG의 평가 기준으로 객관적인 메트릭과 인간 평가를 모두 사용한다. 객관적 측정치 측면에서 우리는 MSR-VTT [57]의 테스트 세트를 표준 벤치마크로 사용하기 위해 이전 작업 [21, 24]을 따른다. 이 테스트 세트는 \\(2,990\\) 테스트 비디오를 포함하며, 각각은 \\(20\\) 프롬프트에 해당하여 총 \\(59,800\\) 프롬프트를 포함한다. 절제 연구의 효율성을 위해 각 테스트 비디오에 대해 무작위로 한 프롬프트를 선택하여 궁극적으로 평가 세트로 \\(2,990\\) 프롬프트를 얻었다. 생성된 비디오와 해당 텍스트 사이의 CLIPSIM[30]과 생성된 비디오와 원본 비디오 사이의 FVD[58]를 비교 측정값으로 계산한다. 일부 연구[12]는 객관적인 메트릭이 항상 인간의 인식과 일치하지는 않을 수 있다고 지적했기 때문에 우리는 주로 인간의 평가를 사용한다. 구체적으로 **V*********품질(비주얼품질 포함)을 포함하는 EMU 비디오 [12]의 비디오 생성 메트릭의 범주화를 채택하는데, 이는 픽셀 선명도 및 인식 가능한 오브젝트/센), **M***움직임 *******************품질(프레임 일관성, 모션 평활도 및 모션량 포함), **T******************************************************************************************************************************************************************** 유니VG는 텍스트와 이미지의 조합에 대한 조건부 생성을 지원하기 때문에 이미지 **F**aithity(텍스트 공간 정렬 및 텍스트 시간 정렬 포함)를 추가로 도입하여 주어진 이미지와 생성된 비디오의 정렬 성능을 측정한다. 에발레이터는 또한 비교 중인 두 영상의 **O**verall **L***ikbps를 제공하여 앞서 언급한 서브 인디케이터에 대한 보체 역할을 한다. 인간 평가에 사용된 프롬프트는 이전 작품의 웹페이지[10, 11, 21, 24, 16]에서 수집되었다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:7]\n' +
      '\n' +
      'SVD[13] 및 폐쇄 소스 작업 Pika 베타[64] 및 Gen2[65]는 웹사이트 또는 불화로부터 결과를 얻을 수 있다. 이 모든 것이 최근 작품이며 텍스트/이미지-영상 생성에서 현재 최고 수준을 나타낸다. 공정한 비교를 위해 이미지 입력만 지원하는 SVD와 Pika 베타를 제외하고, 다른 모든 작품은 텍스트 및 이미지 입력(SDXL1.0 및 리바이너에 의해 텍스트 프롬프트에서 이미지가 생성됨) 측면에서 일치하게 유지되었다. 비교 결과는 그림 4와 표 2에 나와 있으며, 그림 4는 우리의 모델에 의해 생성된 비디오(\\(\\mathcal{F}_{A}+\\mathcal{F}_{SR}\\))와 다른 방법으로 생성된 비디오 간의 전체 Likity 비교를 보여준다. 우리는 우리의 방법으로 생성된 영상이 오픈 소스 텍스트/영상 모델 및 폐쇄 소스 방법 Pika 베타를 능가하고 폐쇄 소스 방법 Gen2와 일치한다는 것을 발견하며 표 2는 다른 하위 메트릭의 당첨률을 기록한다. GSB로부터 승률을 계산하는 공식은 \\(G-B)/(G+S+B)\\이다. 숫자\\(>\\)0은 우리의 방법이 더 낫다는 것을 나타내며, 숫자\\(<\\)0은 다른 방법이 더 낫다는 것을 나타낸다. 우리는 우리의 방법의 두드러진 이점이 **FC**에 있다는 것을 발견했는데, 이는 낮은 자유 비디오 생성을 위한 편집 패러다임을 채택하여 보다 안정적인 비디오를 생성하는 데 도움이 되는 \\(\\mathcal{F}_{A}\\) 때문입니다. 또한, 생성된 비디오는 유사한 해상도의 비디오(더 큰 해상도의 비디오를 생성하는 유전형 2를 제외)에 비해 우수한 **PS**를 나타낸다. 왜냐하면, 우리는 **BGN***를 사용하여 저해상도 비디오에서 고해상도 비디오를 직접 예측함으로써 훈련과 추론 사이의 일관성을 보장하기 때문이다. 생성된 비디오의 한 가지 중요한 단점은 훈련 데이터에서 정적 비디오에 대한 필터링이 현재 없기 때문에 **AM***이다. 이를 추가하는 것은 우리의 미래 작업의 일부가 될 것입니다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '베이스 모델의 훈련 프로세스(\\mathcal{F}_{B}\\)는 텍스트와 이미지의 임의의 조합으로 조건부 비디오 생성을 강조하기 때문에 기본 모델이 텍스트 대 비디오, 이미지 대 비디오 및 텍스트/영상 생성에서 능력을 동시에 유지할 수 있는지 여부가 핵심 질문이다. 따라서 우리는 \\(\\mathcal{F}_{B}\\)의 훈련 과정에서 체크포인트를 취하고, 텍스트 대 비디오, 이미지 대 비디오 및 텍스트&영상 생성에서 FVD로 성능을 테스트한다. 결과는 그림 5에 나타나 있어 3개의 곡선의 전체 경향이 하향되어 있음을 알 수 있으며, 이는 3개의 곡선의 전체 경향이 하향되어 있음을 나타낸다.\n' +
      '\n' +
      '그림 5: FVD 스코어는 \\(\\mathcal{F}_{B}\\)의 훈련 과정 동안 MSR-VTT에 대한 것이다.\n' +
      '\n' +
      '훈련 과정은 텍스트 또는 이미지로부터 비디오를 생성하는 베이스 모델의 능력을 향상시킨다. 이는 자유도 높은 영상 생성의 경우, 다조건 영상 생성이 하나의 단일 모델로 통합될 수 있음을 증명한다.\n' +
      '\n' +
      'Biased Gaussian NoiseTo demonstrate that Biased Gaussian Noise (BGN) better suits low-freedom video generation tasks, we conducted ablation studies on the Animation Model \\(\\mathcal{F}_{A}\\) and the Video Super Resolution model \\(\\mathcal{F}_{SR}\\). The results, shown in Table 3, indicate that BGN enhances video quality in both Image Animation and Super Resolution, as evidenced by lower FVDs. It proves more beneficial for Super Resolution, a task with less freedom than Image Animation. Figure 7 visualizes \\(\\mathcal{F}_{SR}\\)\'s performance with and without BGN. The first row shows the original, low-resolution input video. Rows 2 and 3 depict the outputs from \\(\\mathcal{F}_{SR}\\) without BGN, processed from the upscaled low-resolution input and subjected to \\(700\\) and \\(900\\) denoising steps, respectively. The fourth row presents the output from \\(\\mathcal{F}_{SR}\\) using BGN at timestep \\(t_{m}=700\\) to \\(t_{n}=0\\), illustrating how a low-resolution video upscaled to high-resolution can be denoised effectively after 700 steps. Each row\'s far right offers a magnified view to better showcase the detail in the model-generated content. Our observations indicate that absent BGN, a smaller initial noise step count results in less clarity (second row), while a larger count produces a clear yet inconsistent output due to noise overpowering the original content (third row). With BGN, the model directly predicts high-resolution videos from low-resolution inputs, achieving clarity and preserving original features (fourth row). We also acknowledge that BGN\'s application can extend to other low-freedom video generation tasks, such as frame interpolation and video editing, which we aim to explore in future work.\n' +
      '\n' +
      'Text&Image ConditionsSince our system is capable of generating videos that align both image and text flexibly, we explore the videos generated under different inference weights for these two conditions. Given text prompt \\(T\\) and image condition \\(I\\), the inference formula we use is \\(V_{out}=\\mathcal{F}_{B}(\\varnothing)+w_{T}(\\mathcal{F}_{B}(T)-\\mathcal{F}_{B} (\\varnothing))+w_{I}(\\mathcal{F}_{B}(I)-\\mathcal{F}_{B}(\\varnothing))\\). We adjust the classifier free guidance scale of text \\(w_{T}\\) and image \\(w_{I}\\), the generating videos are shown in Figure 6-(a), we find that adjusting the \\(w_{T}\\) and \\(w_{I}\\) can bias the generated video towards the text or image conditions. Figure 6-a shows that in row 1, \\(w_{I}=0\\), \\(\\mathcal{F}_{B}\\) generates a video that is almost unrelated to the input image, while in row 3, \\(w_{T}=0\\), \\(\\mathcal{F}_{B}\\) produces a video that is almost unrelated to the text. By adjusting both \\(w_{T}\\) and \\(w_{I}\\) to appropriate values, the second row\'s generated video retains the characteristics of the input image and is also aligned with the textual semantics. Based on this feature, our \\(\\mathcal{F}_{B}\\) can achieve different video generation with the same input image combined with different text prompts, as shown in Figure 6-(b). We have also explored whether \\(\\mathcal{F}_{A}\\) possesses similar properties. However, due to the concatenated image features having much more stronger constraints than text, the generated videos mainly rely on image semantics. Nevertheless, inputting consistent text helps to enhance the dynamic effects of the generated videos.\n' +
      '\n' +
      '그림 7: \\(\\mathcal{F}_{SR}\\) w/o 또는 w/BGN의 생성 사례.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:10]\n' +
      '\n' +
      '* [18] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In _ICLR_, 2022.\n' +
      '* [19] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. _arXiv preprint arXiv:2211.13221_, 2022.\n' +
      '* [20] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. _arXiv preprint arXiv:2310.19512_, 2023.\n' +
      '* [21] Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu, Qi Zhang, Fu Li, Haocheng Feng, Errui Ding, and Jingdong Wang. Videogen: A reference-guided latent diffusion approach for high definition text-to-video generation. _arXiv preprint arXiv:2309.00398_, 2023.\n' +
      '* [22] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. _arXiv preprint arXiv:2309.15818_, 2023.\n' +
      '* [23] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. Izygen-xl: High-quality image-to-video synthesis via cascaded diffusion models. _arXiv preprint arXiv:2311.04145_, 2023.\n' +
      '* [24] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: High-dynamic video generation. _arXiv preprint arXiv:2311.10982_, 2023.\n' +
      '* [25] Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, and Trevor Darrell. More control for free! image synthesis with semantic diffusion guidance. In _WACV_, 2023.\n' +
      '* [26] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In _ICLR_, 2022.\n' +
      '* [27] Yitong Li, Martin Renqiang Min, Dinghan Shen, David E. Carlson, and Lawrence Carin. Video generation from text. In _AAAI_, 2017.\n' +
      '* [28] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei. To create what you tell: Generating videos from captions. In _ACM MM_, 2017.\n' +
      '* [29] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. In _ICLR_, 2022.\n' +
      '* [30] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. _arXiv preprint arXiv:2104.14806_, 2021.\n' +
      '* [31] Gaurav Mittal, Tanya Marwah, and Vineeth N. Balasubramanian. Sync-draw: Automatic video generation using deep recurrent attentive architectures. In _ACM MM_, 2017.\n' +
      '* [32] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic VQGAN and time-sensitive transformer. In _ECCV_, 2022.\n' +
      '* [33] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In _ICLR_, 2021.\n' +
      '* [34] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In _ICLR_, 2021.\n' +
      '* [35] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail A. Kudinov. Grad-tts: A diffusion probabilistic model for text-to-speech. In _ICML_.\n' +
      '* [36] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M. Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi. Nerfdiff: Single-image view synthesis with nerf-guided distillation from 3d-aware diffusion. In _ICML_, 2023.\n' +
      '* [37] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In _CVPR_, 2021.\n' +
      '* [38] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In _CVPR_, 2023.\n' +
      '* [39] Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models. In _NeurIPS_, 2022.\n' +
      '* [40] Chin-Wei Huang, Milad Aghajohari, Joey Bose, Prakash Panangaden, and Aaron C Courville. Riemannian diffusion models. In _NeurIPS_, 2022.\n' +
      '\n' +
      '* [41] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _ICLR_, 2021.\n' +
      '* [42] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In _ICLR_, 2022.\n' +
      '* [43] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In _ICLR_, 2023.\n' +
      '* [44] Daniel Watson, William Chan, Jonathan Ho, and Mohammad Norouzi. Learning fast samplers for diffusion models by differentiating through sample quality. In _ICLR_, 2023.\n' +
      '* [45] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In _ICLR_, 2022.\n' +
      '* [46] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. _arXiv preprint arXiv:2212.11565_, 2022.\n' +
      '* [47] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. _arXiv preprint arXiv:2303.13439_, 2023.\n' +
      '* [48] Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, and Evgeny Burnaev. Latent video transformer. In Giovanni Maria Farinella, Petia Radeva, Jose Braz, and Kadi Bouatouch, editors, _VISIGRAPP_, 2021.\n' +
      '* [49] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using VQ-VAE and transformers. _arXiv preprint arXiv:2104.10157_, 2021.\n' +
      '* [50] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, and Sergey Tulyakov. A good image generator is what you need for high-resolution video synthesis. In _ICLR_, 2021.\n' +
      '* [51] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In _ICLR_, 2014.\n' +
      '* [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_. PMLR, 2021.\n' +
      '* [53] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In _ICCV_, 2021.\n' +
      '* [54] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* [55] Nicholas Guttenberg. Diffusion with offset noise, 1 2023.\n' +
      '* [56] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps. In _NeurIPS_, 2022.\n' +
      '* [57] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In _CVPR_, 2016.\n' +
      '* [58] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross B. Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning. In _CVPR_, 2021.\n' +
      '* [59] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. _arXiv preprint arXiv:2211.11018_, 2022.\n' +
      '* [60] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: A large-scale video-text dataset for multimodal understanding and generation. _arXiv preprint arXiv:2307.06942_, 2023.\n' +
      '* [61] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation. _arXiv preprint arXiv:2304.08477_, 2023.\n' +
      '* [62] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* [63] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In _CVPR_, 2017.\n' +
      '* [64] Pika labs. Accessed December 18, 2023. [Online]. Available: [https://www.pika.art/](https://www.pika.art/).\n' +
      '* [65] Gen-2. Accessed December 18, 2023. [Online]. Available: [https://research.runwayml.com/gen2](https://research.runwayml.com/gen2).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>