<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '유엔 통합 영상 세대# UniVG를 제공합니다.\n' +
      '\n' +
      ' 루단 루단 루안, 레이 톈, 초안웨이 황, 벼 장, Xu 장, 잔난 샤오샤오 장.\n' +
      '\n' +
      'Baidu Inc.\n' +
      '\n' +
      'Beijing, China\n' +
      '\n' +
      '{ruanludan, tian1ei09}@baidu.com\n' +
      '\n' +
      'huangcv21@gmail.com\n' +
      '\n' +
      '{zhangxu44, xiaoxinyan}@baidu.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '확산 기반 영상 생성은 학계와 산업계 모두에서 광범위한 관심을 받고 상당한 성공을 거두었다. 그러나 현재의 노력은 주로 텍스트에 의해 구동되는 생성, 이미지에 의해 또는 텍스트와 이미지의 조합과 같은 단일 주관적 또는 단일 태스크 비디오 생성에 집중되어 있다. 이는 사용자가 개별적으로 또는 조합하여 유연한 방식으로 이미지 및 텍스트 조건을 입력할 가능성이 높기 때문에 실제 응용 시나리오의 요구를 완전히 충족할 수 없다. 이를 해결하기 위해 텍스트 및 이미지 양식에 걸쳐 다중 비디오 생성 작업을 처리할 수 있는 ** 비조정***-모달 **V** 비디오 **G** 폭기 시스템을 제안한다. 이를 위해 우리는 생성자유의 관점에서 우리 체제 내에서 다양한 영상 생성 과제를 재방문하고, 이를 고자유와 저자유 영상 생성 범주로 분류한다. 고자유 영상 생성을 위해 입력 이미지 또는 텍스트의 의미론과 일치하는 비디오를 생성하기 위해 멀티 조건 크로스 어시스트를 사용한다. 저자유 영상 생성을 위해 순수 랜덤 가우시안 노이즈를 대체하기 위해 바이오머드 가우시안 노즈를 도입하는데, 이는 입력 조건의 내용을 더 잘 보존하는 데 도움이 된다. 우리의 방법은 공공 학술 벤치마크 MSR-VTT에 대한 가장 낮은 프로쉐트 비디오 거리(FVD)를 달성하여 인간 평가에서 현재 오픈 소스 방법을 능가하고 현재 긴밀한 소스 방법 Gen2와 일치하며 더 많은 샘플에서 [https://univg-baidu.github.(https://univg-baidu.github)를 방문한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 확산 기반 생성 모델[1; 2; 3]은 영상 생성[4; 5; 6; 8; 8; 9]에서 영상 생성[10; 11; 14;]으로 빠르게 확장된다. 영상 생성 모델의 대부분은 텍스트 설명을 조건부 입력[15; 16; 17; 18; 11; 19]으로 사용한다. 그러나 최근 연구에서는 생성된 영상 [20]의 세부사항을 개선하거나 [21; 22; 12; 23; 24]를 위한 영상 조건의 활용을 탐색하기 시작하였다. 또한, 생성된 비디오의 시간적 평활성과 공간적 해상도를 향상시키기 위해, 현재 접근법은 종종 프레임 보간 및 초해상도 [10; 11]을 위한 모듈을 통합한다. 그러나 기존 작업은 텍스트[10; 11; 9; 18], 이미지[13], 텍스트와 이미지[12; 24]의 조합으로 입력이 제한된 단일 주관적 또는 단일 태스크 비디오 생성에만 초점을 맞추고 있다. 이 단일 주관적 또는 단일 태스크 파이프라인은 모든 사용자 요구를 충족시키기 위해 필요한 유연성이 부족하다. 실무적으로, 사용자는 입력을 위한 필수 텍스트 또는 이미지 조건을 갖지 못하여 모델을 사용할 수 없게 할 수 있다. 대안적으로, 상충되는 텍스트 이미지 쌍의 도입은 급격한 전이([24]에서 유사 결론이 제안됨)를 갖는 정적 비디오 또는 비디오의 생성으로 이어질 수 있다.\n' +
      '\n' +
      '본질적으로, 비디오 생성에서 사용되는 모든 모델은 대응하는 비디오를 생성하기 위해 하나 이상의 조건을 수용하는 조건부 생성 모델이다. 이러한 조건은 텍스트, 이미지, 저해상도 비디오, 심지어 제어 신호일 수 있다. 다중 비디오 생성 작업을 처리할 수 있는 다재다능한 비디오 생성 시스템을 구축하기 위해 기존 방법을 재방문하고 작업 자체보다는 ** 생성 자유**를 기반으로 관련 방법을 분류한다. 우리가 제안하는 ** 생성 자유**의 개념은 특정 조건이 주어진 비디오 생성 모델에 대한 솔루션 공간의 범위에 해당한다. 본 논문에서는 다양한 영상 생성 작업을 자유도가 높거나 자유도가 낮은 영상 생성으로 분류한다. 구체적으로, 고자유 영상 생성은 입력 조건, 즉 의미 수준에서 약하게 제약되는 텍스트와 이미지를 특징으로 하여, 이 시나리오의 생성 모델이 더 큰 해결 공간을 가지므로 더 높은 자유도를 제공한다. 반대로, 저자유 영상 생성은 일반적으로 이미지 애니메이션 및 비디오 슈퍼해상도와 같은 저수준 정보(즉, 픽셀)에서 강하게 제약되는 조건을 포함한다. 이러한 제약은 생성 모델에 사용할 수 있는 해결 공간을 제한하여 자유도가 낮아진다.\n' +
      '\n' +
      '다양한 영상 생성 과제의 특성에 더 잘 부합하기 위해서는 영상 생성을 위해 다양한 수준의 생성 자유를 가진 다양한 전략을 취해야 한다. 고자유 영상 생성을 위해서는 표준 확산 _포트 파라다임_가 적절하며 기존의 일부 리피스에 광범위하게 활용되어 온 @ludan을 제공해야 한다. 구체적으로, 훈련 단계 동안 확산 모델은 순방향 처리에서 추가된 노이즈를 학습하고 추론 단계 동안 순전히 랜덤 가우시안 분포로부터 역전함으로써 목표 분포를 예측한다. 분류기 안내[4] 및 분류기 자유 안내[25]는 예측 분포를 입력 조건에 의해 지정된 분포와 정렬하기 위해 사용된다. 저자유 영상 생성의 경우 _Edridge Paradigm_가 더 적합하다. 이미지 편집 [26]을 시점의 사례로 하는 것은 일반적인 관행은 원본 이미지에 일정 수준까지 노이즈를 추가한 다음 텍스트를 편집 신호로 사용하여 의도된 결과를 향해 분포를 제거하는 것을 포함한다. 이 접근법은 처음부터 발생하는 생성과 비교하여 원본 입력의 콘텐츠를 더 잘 보유할 수 있다. 비디오 초해상도는 이미지 편집[23]과 유사한 기술을 활용했다. 그러나 _Edridge Paradigm_는 훈련 단계와 추론 단계 사이의 불일치의 형태로 한계가 있다. 구체적으로, 모델은 조건부 분포에서 목표 분포로의 전환을 학습하지 않고 목표 분포를 근사화하기 위해만 트레이닝된다. 이러한 불일치는 트레이드오프 관련 문제, 즉 도입되는 덜한 노이즈로 인해 모델의 편집 능력이 약해지는 반면, 추가되는 노이즈가 많을수록 입력이 보존될 수 있는 모델이 적을수록 노이즈 수준이 완전히 무작위 가우시안 분포에 접근하면 편집 패러다임이 생성과 유사해짐에 따라 원래 입력의 콘텐츠를 보존하는 모델의 능력이 크게 감소한다. 입력을 보존하면서 편집 능력을 균형을 맞추기 위해 모델 편집의 훈련 및 추론 단계를 어떻게 조정해야 하는지도 해결해야 할 문제이지만 이전 작업에서 간과되어 왔다.\n' +
      '\n' +
      '본 논문에서는 이미지와 텍스트의 임의의 조합으로 조건화된 유연한 비디오 생성을 지원하기 위해 설계된 통일된 시스템 ***미화-모달 **V 비디오 **G***증가(즉,***UniVG******)를 제안한다. 이를 달성하기 위해 시스템 내의 모든 모델을 고자유 영상 생성과 저자유 영상 생성의 두 그룹으로 분류한다. 고자유 영상 생성을 위해 임의의 조합소프 텍스트 및 이미지 조건을 처리할 수 있는 요구 사항이 가능한 기본 모델을 제시한다. 우리는 다중 조건 교차 의도 모듈로 UNet 아키텍처의 원래 교차 의도 모듈을 강화함으로써 이를 달성한다. 저자유 영상 생성과 관련하여 이미지 애니메이션 및 비디오 초해상도 작업에 개별적으로 맞춘 두 가지 해당 모델을 제안한다. 이러한 모델은 세대 패러다임이 아닌 편집 패러다임을 활용한다. 생성 패러다임에 기반한 훈련 과정과 편집을 기반으로 한 추론 과정의 차이를 조정하기 위해 본 논문에서는 훈련 단계에서 목표 함수를 정제하여 표준 가우시안 노이즈 대신 목표 분포를 향하는 **B***G****N*****************로 명명되는 **BGN***)를 예측한다.\n' +
      '\n' +
      '제안된 UniVG 시스템은 베이스 모델, 이미지 삽입 모델 및 슈퍼 해결 모델을 포함한다. 베이스 모델은 텍스트 및 이미지 조건의 임의의 조합을 처리하고, 입력 조건과 초당 8 프레임(fps)으로 의미적으로 정렬된 \\(24\\t 320\\ 시간 576\\)의 비디오 시퀀스를 출력할 수 있다. 이미지 연결의 추가 조건으로 베이스 모델에서 미세 조정되는 이미지 삽입 모델은 입력 이미지와 픽셀 정렬된 \\(24개 320개 점수 576\\)의 비디오 프레임을 생성한다. 슈퍼 해결 모델은 각 프레임의 \\(720\\t 1280\\) 픽셀의 해상도를 향상시킨다. 우리 UniVG는 이전 작품과 비교하여 영상 생성에 대한 더 나은 과제 적응력, 즉 통일된 시스템 내에서 다양한 비디오 생성 작업을 처리하지만 생성 세부 사항 및 프레임 일관성에 대한 상당한 개선도 보여준다. 실험은 우리의 방법의 효과를 입증했다. 객관적 메트릭에서 우리의 방법은 기존의 다른 방법을 크게 능가하며 수동 평가에서 우리의 접근법은 Gen2와 일치하며 다른 방법을 초과한다.\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약될 수 있다.\n' +
      '\n' +
      '1. 의미적으로 정렬된 텍스트/이미지 대 비디오 생성, 이미지 애니메이션과 같은 다수의 비디오 생성 작업을 처리할 수 있는 제1 비디오 생성 시스템인 UniVG를 제안한다.\n' +
      '2. 바이오페이지 가우시안 노이스를 소개하고 이미지 애니메이션, 슈퍼 해상도 등 저자유 영상 생성 과제에 대한 효과를 확인한다.\n' +
      '3.실험은 우리의 방법이 객관적인 메트릭 측면에서 기존 텍스트/이미지 대 비디오 생성 방법을 능가하고 주관적 평가에서 Gen2와 일치한다는 것을 보여준다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '### Text-to-Video Generation\n' +
      '\n' +
      '비디오 생성에 대한 초기 작업은 GAN[27; 28; 29], VQ-VAE[30; 31], 자동 억제 모델[30; 18], 변압기 구조[32]를 활용했지만 낮은 해상도와 최적이 아닌 시각적 품질에 의해 제한되었다. 영상 생성[4; 5; 6; 7], 오디오 생성[33; 34] 및 기타 도메인[36; 37; 38]에서 확산 모델이 성공한 후 비디오 생성에서 확산 모델의 첫 번째 적용을 나타냈다. 이어서, Make-A-V 비디오 [10]와 ImagenV 비디오[11]는 2D U-Net을 텍스트 대 이미지 생성에서 3D U-Nets로 확장시켜 영상 생성을 오픈 영역으로 확장시켰다. 그 이전까지는 대규모 GPU 메모리 소비와 높은 훈련 비용이 필요한 화소 공간에서 영상 모델링을 연구해 왔다. 이 문제를 해결하기 위해 많은 연구자들이 픽셀 공간[8; 15; 40; 16] 대신 잠재 공간에서 확산 과정을 수행하고 무학습 샘플링[2; 41; 42; 43] 또는 학습 기반 샘플링[44; 45]으로 샘플링 효율을 향상시키기 위해 초점을 옮겼다. 또한 일부 작업은 훈련 비용을 단일 비디오[46]로 줄이거나 전혀 훈련 비용이 없다[47]로 줄이는 데 집중했다.\n' +
      '\n' +
      '### Image-to-Video Generation\n' +
      '\n' +
      '문자에서 직접 영상을 생성하는 것은 복잡성이 높은 도전적 과제이다. 자연적인 생각은 이미지를 중간 다리로 사용하는 것이다. 비디오 생성과 유사하게 비디오 예측에 대한 초기 작업은 비확산 방법[48; 49; 50]을 사용했으며, 이는 종종 낮은 해상도 또는 특정 영역에서 제한적이었다. 텍스트 대 비디오 작업에서 확산 기반 방법의 상당한 발전으로 I2VGen-XL[23]은 우리가 아는 한 오픈 도메인 이미지 대 비디오 생성에 확산의 첫 번째 활용이다. 문자 CLIP 특징을 텍스트 대 비디오 프레임워크 내의 이미지 CLIP 특징들로 대체하여 입력 이미지와 의미적으로 정렬된 비디오 생성을 달성한다. 유사하게, SVD[13]는 또한 텍스트 대 비디오 모델에서 이미지 대 비디오 모델에 미세 조정되지만 이미지의 VAE 특징을 더 강한 제어 신호로 연결한다. 현재 비디오제[21], 비디오크래프트1[20], EMU 비디오[12]와 메이크픽셀 댄스[24]는 텍스트 대 영상 생성의 목표를 유지하고 있으나, 중간 단계로 텍스트 대 이미지 합성을 소개한다. 생성된 이미지는 연결된 또는 CLIP 특징에 의해 비디오 생성 프레임워크에 통합된다.\n' +
      '\n' +
      '위에서 유추할 수 있듯이 텍스트 대 비디오 생성 및 이미지 대 비디오 생성은 다양한 응용 프로그램을 제공하지만 기술적 접근에서 많은 유사성을 공유한다. 따라서 본 논문에서는 하나의 틀이 이 두 가지 목적을 통일할 수 있는지를 탐색한다. 우리 UniVG의 초기 작품과의 주요 구분은 동영상 생성에 포함된 다양한 모델을 과제보다는 생성적 자유 관점에서 구별한다는 것이다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '이 섹션은 탄력적으로 조건부 비디오 생성을 위해 제안된 ** 유니프**-모달 **V** 비디오 ******증가(즉, **UniVG*********)를 제시한다. 그런 다음 특정 설계로 다이빙하기 전에 Sec 3.1에서 전체 시스템 UniVG의 예비 지식을 간략하게 설명한다.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '확산 모델[1]은 가우시안 노이즈에서 반복적으로 변성시켜 샘플을 생성하기 위해 훈련된 생성 모델 그룹이다. (\\_\\overline{t}\\)은 노이즈 일정(\\_{t}{t}\\sqrt{t}\\)을 의미하고 \\(\\math{N})에서 샘플링된 노이즈(\\math{N}(\\math{N},\\math{0},\\math{I})는 트레이닝 동안 a\\(\\math{f{f{I})를 사용하여 원래 입력(\\math{N})에서 샘플링된 노이즈(\\math{f{t)를 통해 결정된 노이즈(\\math{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f{f})를 통해 결정된 노이즈(\\)를 추가)하는 것을 말하며,\\) 모델을 통해 추가된 노이즈를 예측(\\)을 예측하는 것을 말한다. 추론 동안 샘플은 반복적으로 변성함으로써 순수 노이즈 \\(x_{N}\\ason\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\에서 생성된다. 또한, Con\\_{t}(x_{t+1})+w_{\\ta}(x_{t+1})+wc}(p_{\\theta})에 의한 예측 분포를 편향시키기 위한 추가 조건부 신호를 도입하고(p_{\\theta}(x_{\\theta})(x_{\\theta}(x_{\\ta+1},c)-p_{\\ta},c)-p(x_{\\t+1}, 25) 조건에 대한 추가 조건부 신호를 도입하고(p_{\\t+1})+w_{\\ta}(x_{\\ta}(x_{\\ta}(x_{\\ta}(x_{\\ta}(x_{\\ta}, +1},c)-p_{\\t+1},c)-p_{\\+1},c)-p_{\\+1},c)-p_{\\+1},c)-p_{\\+1},c)-p_{\\+1},c)-p_{\\+1},c)-p(x_{ 또 다른 주류 채택 확산 모델은 바리티브 오토엔코코더(VAE) [51]로 구성된 라트렌트 디확산 모델(LDM)[8]과 잠복 숨겨진 공간에서 변성시키는 잠재 확산 모델이다. 이 접근법은 고해상도에서 피팅 분포의 복잡성을 감소시킨다. 본 논문에서는 UniVG의 각 단일모형이 무조건적 빈곤확산모형이다. 즉, 영상 \\(F\\) RGB 프레임은 먼저 잠재 공간 \\(X\\in\\mathbb{R}^{F\\tcer C\\t 기간에는 W}\\)로 압축된 후, 이미지 자동 인코더가 있는 UNet(텍스트 조건 \\(T\\), 이미지 조건 \\(I\\), 저 해상도 비디오 \\(V^{lr}\\)로 입력된다.\n' +
      '\n' +
      '### UniVG\n' +
      '\n' +
      '그림 2-(a)에 도시된 바와 같이, 우리의 전체 UniVG는 (1) A 베이스 모델 \\(\\mathcal{F}_{B}\\)의 세 가지 모델로 구성되며, 이는 자유도 높은 비디오 생성을 위한 텍스트와 이미지 조건의 조합을 수락한다. (2) An Image Animation \\(\\mathcal{F}_{A}\\) 모델은 텍스트 이미지 쌍을 수용하여 픽셀 레벨에서 입력 이미지와 정렬된 생성된 비디오를, (3) 공간 해상도를 개선하기 위한 슈퍼해상도 모델 \\(\\mathcal{F}_{SR}\\)을 제공한다. 각 모델은 공간 플레이어, 테스포랄 플레이어, 크로스 어텐션 플레이어로 구성된 3D UNet 아키텍처가 있는 잠재 확산 모델이다. 기존 작품[10; 13]에 이어 공간층에는 2D 협화층과 공간 변압기로 구성되어 있고, 테포털층에는 1D 시간적 협화층과 시간적 변압기로 구성되어 있다. 횡단 주의 모듈은 텍스트, 이미지 특징 등과 같은 의미론적 제어 신호를 처리하는 데 사용된다.\n' +
      '\n' +
      '(1) 베이스 모델 \\(\\mathcal{F}_{B}\\)의 경우 비디오 스키어1[20]에서 영감을 받은 CLIP[52]의 텍스트 인코더와 일치하는 이미지 인코더를 사용한다. 입력 이미지 및 텍스트의 글로벌 의미 및 로컬 세부 사항을 완전히 활용하기 위해 우리는 모든 것을 활용한다.\n' +
      '\n' +
      '그림 2: 제안된 **UniVG** 시스템의 오버뷰. (a)는 베이스 모델 \\(\\mathcal{F}_{B}\\), Animation 모델 \\(\\mathcal{F}_{A}\\), 슈퍼 결의 모델 \\(\\mathcal{F}_{SR}\\)을 포함하는 UniVG의 전체 파이프라인을 표시한다. (b)는 \\(\\mathcal{F}_{B}\\) 및 \\(\\mathcal{F}_{A}\\)에 관여하는 다중 조건 크로스 관계를 보여준다.\n' +
      '\n' +
      'F_{I}(F_{I}) 시각적 토큰(F_{f_{i}}=\\{f_{i}}_{i =0}^{K_{I}}}}}}}}}}}}}) 및 모든 \\(K_{T}}}) 텍스트 토큰(F_{T}=F_{T}=F_{T}=F_{T}) 텍스트 토큰 \\)은 CLIP ViT의 마지막 층으로부터 CLIP ViT의 마지막 층부터 CLIP ViT의 마지막 층부터 CL. 1개 이상의 의미적 특징을 처리하는 능력을 가진 원래 크로스 어텐션인 Sec 3.3(2)에 대한 메커니즘을 확장하고 픽셀 수준에서 입력 이미지와 정렬된 비디오를 추가로 생성하기 위해, 우리는 이미지 삽입 모델 \\(\\mathcal{F}_{A}\\)을 훈련시키고 추가 조건으로 첫 번째 프레임의 숨겨진 공간 특징을 연결함으로써 이미지 삽입 모델(\\mathcal{F}_{A}\\)을 훈련시킨다. 추가 조건 때문에 초기 컨볼루션 레이어의 커널의 대응하는 채널 치수는 \\(C\\)에서 \\(2C\\)로 변한다. 우리는 원래 모델의 성능을 보존하기 위해 추가 매개변수를 0으로 초기화한다. \\(\\mathcal{F}_{B}\\) 또는 \\(\\mathcal{F}_{A}\\)를 사용하여 \\(24\\tcer 320\\i 576\\)의 비디오 프레임을 획득할 수 있다. (3) 생성된 비디오의 선명도를 높이기 위해, 우리는 \\(\\mathcal{F}_{B}\\)의 슈퍼 해결 모델 \\(\\mathcal{F}_{SR}\\)을 추가로 고용했다. 초해상도 작업은 이미지 조건이 없기 때문에, 다중 조건 교차 주의 모듈은 텍스트 조건만을 수용하는 정기적인 교차 의도 모듈로 되돌아간다. 훈련 중 \\(\\mathcal{F}_{SR}\\)는 라모도블러, 무작위화, JPEG 압축 등을 통해 고화질 영상을 파괴하여 얻은 저해상도 \\(V^{lr}\\)의 영상을 접수한다. 우리는 \\(\\mathcal{F}_{A}\\)와 \\(\\mathcal{F}_{SR}\\)에 해당하는 작업을 자유도가 낮은 세대로 분류함에 따라 표준 가우시안 노이즈를 Sec 3.4에 도입된 바이오세스 가우시안 노이즈(**BGN***)로 조절하여 조건부 분포에서 목표 분포로 비사회 역진 과정을 제시한다.\n' +
      '\n' +
      '멀티 크로스 의도.\n' +
      '\n' +
      '우리의 기본 모델 \\(\\mathcal{F}_{B}\\) 및 이미지 삽입 모델 \\(\\mathcal{F}_{A}\\)이 텍스트 및 이미지 CLIP 특징을 수용하기 때문에 표준 크로스 의도 대신 멀티 조건 크로스 어시스트를 사용한다. 이 모듈의 아키텍처는 주로 VideoCrafter[20], \\(F_{\\text{out}}})를 구성하는 VideoCrafter[20]을 따른다.\n' +
      '\n' +
      '}(K_{in}.^{{}}{Q_{I}^{{{d})\n' +
      '\n' +
      'W_{I}\\ V_{T}\\ V_{T}\\ V_{I}}\\ V_{I}}\\ K_{dot F_{I}=W_{V_{I}}\\ K_{dot F_{I}}.\n' +
      '\n' +
      'Hf(d_{k}\\)가 키/쿼리 벡터의 차원이고 \\(Q_{\\text{in}}\\)는 \\(F_{I}\\)와 \\(F_{T}\\) 사이에 공유된다. 가중치 \\(W_{K_{I}}\\) 및 \\(W_{V_{I}}}\\)는 각각 \\(W_{K_{T}}}\\) 및 \\(W_{V_{T}}\\)에서 초기화된다. 이미지를 추가 입력 강화로 취급하는 비디오 더 부드러운 래트와 달리, 우리는 이미지를 텍스트와 함께 동등하게 유의미한 제어 신호로 간주한다. 이는 훈련 과정 전반에 걸쳐 일정 비율의 이미지 드롭아웃을 적용하여 달성된다. 확장하면 MCA는 재교육 필요성(예: 더 강한 텍스트 특징) 없이 교차 의도 단위의 수를 증가시켜 두 가지 이상의 조건을 수용할 수 있다. 이러한 유연성은 새로운 조건을 처리하기 위해 모델의 훈련을 연장하는 비용을 크게 감소시킨다.\n' +
      '\n' +
      '가우시안 노이즈를 수상했어요.\n' +
      '\n' +
      '제안된 바이오페이지 가우시안 노이즈는 낮은 자유 비디오 생성을 위한 목표 분포로 조건 분포를 전달하는 데 사용된다. 그림 3-(a)에 도시된 바와 같이, 표준 순방향 확산 과정은 목표 분포 \\(v^{T}_{t}=v^{T}_{t}=\\sqrt{T}_{t}v^{T}+\\sqrt{1-\\alpha}_{t}_{t}<{t}_\\epsilon\\)를 통해 목표 분포 \\(v^{T}_\\epsilon\\)에서 표준 가우시안 분포 \\(v^{T}.{t}_\\sqrt{t}_\\sqrt{t}_\\sqrt{t}_\\sqrt{t}_\\sqrt{t}_{t}_\\he}_{t}_\\he}_{t}_\\epsilat}_{t}_\\epsilon\\)에서 표준 가우시안 분포)에서 표준 가우시안 분포)로 전환한다. 그러나 일반적으로 후방 과정에서 이러한 분포는 관련된 유일한 두 가지 분포이다. 이는 추론 중 조건 분포 \\(v^{C}\\)에서 샘플이 도입될 때 최적이 아닌 편집 결과를 초래할 수 있다. 전방 및 후방 과정 모두에서 조건 분포를 설명하기 위해 그림 3-(b)에 도시된 바와 같이 원래 확산을 세 부분으로 분할한다. t_{m}\\, \\(v_{t}\\)는 0 내지 \\(v_{t}|v^{T}},t) 사이의 타임스메프를 사용하여 표적 샘플에 의해 계산되며, 이는 \\(q_{t}|v{t^{T}=\\sqrt{\\alpha}_{t}_{t}_{t}_{t}_{t}_{t}}_{t}. H\\(t_{n}\\)에서 \\(N\\) 사이의 타임스탬(t_{n}\\)의 경우, \\(v_{t}|v^{C},t)=\\sqrt{\\alpha}_{t}_{t} +\\sqrt{1-\\alpha}_{n}\\leq t <N)\\로 조건 샘플에 의해 \\(v_{t}. 핵심 문제는 \\(v_{t}|v^{C},v^{T}}}\\)에서 \\(v_{t_{m}}\\)로 원활하게 전환할 수 있는 \\(q_{t_{n}}},t)\\을 설계하는 방법이다. 원래의 확산 일정을 보존하기 위해 \\(\\epsilon\\)로 표시된 노이즈 \\(\\epsilon^{\\prime}\\) 변수를 소개한다. t_{m}\\(t_{n}\\)와\\(t_{t}}) 사이의 타임스메프를 위해 우리는\\(v_{t}|v^{C},v^{t},{t})를 가지고 있다.\n' +
      '\n' +
      '그림 3: 무작위 가우시안 노이즈 및 바이오머드 가우시안 노이즈와의 전방 및 후방 확산 프로세스는 그림 3:이다.\n' +
      '\n' +
      '}}(v_\\qrt{t_{t_{m}}}} <{t_{t_{m}}}} <{t_{t_{m}}}}) 및 \\(v_{t_{n}=v_{t_{n})=\\sqrt{\\alpha_{t_{n}-{t_{t_{t_{t_{t_{t_{t_{t_{t_{m} <{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{n}<{t_{t_{n} <{t_{t_{n},{t_{n}:{t_{t_{n}}.{t_{n}}<{t_{n}\'{t_{n}} 따라서 해당 \\(\\epsilon^{\\prime}\\)는 타임스톤 \\(t_{m}\\) 및 \\(t_{n}\\)에서 다음 공식을 만족해야 한다.\n' +
      '\n' +
      '}} <표실론>{t_{m}}.\n' +
      '\n' +
      '이론적으로 \\(\\epsilon^{\\prime}\\)에 대한 해결책은 무한히 많다. 본 논문에서는 \\를 단순히 \\(\\epsilon^{\\prime}\\)로 정의하고 있다.\n' +
      '\n' +
      '}}{\\frt{t_{m}} 〈^{C}-v^^ {T}]\\(t_{m\\leq t<{n})\\.\n' +
      '\n' +
      'i\\(\\epsilon^{\\prime}\\)는 \\(v^{C}\\) 및 \\(v^{T}\\)의 가중 조합에 의해 평균값이 이동된 비차원 가우시안 분포로부터 샘플링된다. 이러한 편향은 조건 분포로부터 목표 분포로의 확산 과정을 가교하는 데 중요하다. 미래의 작업에서 \\에 대한 대체 솔루션을 탐색할 것이다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '당사의 학습 데이터셋에는 자체 수집 데이터와 함께 웹Vid-10M[53] 및 LIAIONCOCO[54]와 같은 공개적으로 이용 가능한 학술 데이터셋이 포함됩니다. 웹바이드-10M은 \\(336\\ 556\\) 픽셀의 해상도로 약 1천만 개의 오픈 도메인 비디오를 포함하는 크고 다양한 텍스트 영상 데이터세트이다. LIAION-COCO는 6억 개의 고품질 이미지를 포함하는 실질적인 텍스트 이미지 데이터세트로서 LIAION-2B로부터 필터링되어 애스티메틱 에티컬(ASE)을 사용하여 점수를 매겼다. 생성된 비디오의 품질을 더욱 높이고 웹Vid-10M에 존재하는 워터마크의 문제를 해결하기 위해 고품질 시각적 콘텐츠를 포함하는 비디오 및 이미지의 자체 큐레이트 데이터셋에 대한 교육을 계속한다. 먼저 더 짧은 가장자리를 따라 720p 해상도로 비례적으로 압축한 다음 10초 클립으로 분할하여 자체 수집 영상을 준비합니다. 이 프로세스는 500만 개의 고품질 텍스트 비디오 쌍을 생성했다. 또한 자체 제작한 이미지 데이터 세트에는 130만 개의 고품질 텍스트 이미지 쌍이 포함되어 있으며 예술 스타일에 중점을 둡니다.\n' +
      '\n' +
      '우리의 \\(\\mathcal{F}_{B}\\)를 훈련하는 것은 이미지:비디오:비디오 프레임 비율이 1:1:1:1로 훈련되며, 여기서 훈련 비디오 프레임은 8-24 프레임에서 동일한 확률로 샘플링되었다. 텍스트 드롭아웃을 0.5로 설정하고 이미지 드롭아웃을 0.1로 설정했는데, 0.1의 강도 및 0의 말단 신호 대 잡음 라티오[12]의 강도로 오프셋 노이즈[55]를 활용한다. 오프셋 노이즈는 매우 어둡거나 밝은 이미지를 생성하는 데 도움이 되는 것으로 입증되었습니다. 제로 단자-SNR은 리칼링된 일정에 따라 순수 가우시안 노이즈에 소음을 추가하여 고품질 및 고해상도 시각 콘텐츠를 생성하는 데 유익한 것으로 나타났다. 두 기술 모두 실험에 유용한 것으로 입증되었다. 그 후, 우리는 자체 설계된 비디오 데이터셋에서만 바이오머드 가우시안 노이즈(BGN)를 사용하여 \\(\\mathcal{F}_{B}\\) 및 \\(\\mathcal{F}_{A}\\)를 얻기 위해 지느러미(\\mathcal{F}_{SR}\\)를 계속한다. I\\(\\mathcal{F}_{A}\\)의 경우, 우리는 텍스트 드롭아웃을 0.1로 설정하고 이미지 드롭아웃을 0.1로 설정했으며, BGN은 초기 단계(t_{m}=600\\)에서 \\(t_{n}=990\\)까지 타임스톰 동안 실험적으로 설정된다. I\\(\\mathcal{F}_{SR}\\)의 경우, 텍스트 드롭아웃은 0.1로 설정되고, BGN은 이후 세부 사항 [26]을 결정하기 때문에 타임스메프 \\(t_{m}=0\\) 내지 \\(t_{n}=700\\) 동안 적용된다. 우리는 \\(\\mathcal{F}_{B}\\) 및 \\(\\mathcal{F}_{A}\\)에 대한 \\(\\epsilon\\)-예측[1]과 \\(\\mathcal{F}_{A}\\), \\(v-\\) 예측(\\mathcal{F}_{SR}\\)을 통합한다. 모든 모델의 학습률은 \\(1\\t10^{-5}\\)로 고정된다. 샘플링을 가속화하기 위해 DPM 솔버[56]를 사용하는데 \\(\\mathcal{F}_{B}\\) 50단계와 \\(\\mathcal{F}_{A}\\) 7단계, 초기 가중치를 0.7로 설정했기 때문에\\(\\mathcal{F}_{SR}\\) 7단계를 사용한다.\n' +
      '\n' +
      '우리는 UniVG의 평가 기준으로 객관적인 메트릭과 인간 평가를 모두 사용한다. 객관적 측정치 측면에서 우리는 MSR-VTT [57]의 테스트 세트를 표준 벤치마크로 사용하기 위해 이전 작업 [21, 24]을 따른다. 이 테스트 세트는 \\(2,990\\) 테스트 비디오를 포함하며, 각각은 \\(20\\) 프롬프트에 해당하여 총 \\(59,800\\) 프롬프트를 포함한다. 절제 연구의 효율성을 위해 각 테스트 비디오에 대해 무작위로 한 프롬프트를 선택하여 궁극적으로 평가 세트로 \\(2,990\\) 프롬프트를 얻었다. 생성된 비디오와 해당 텍스트 사이의 CLIPSIM[30]과 생성된 비디오와 원본 비디오 사이의 FVD[58]를 비교 측정값으로 계산한다. 일부 연구[12]는 객관적인 메트릭이 항상 인간의 인식과 일치하지는 않을 수 있다고 지적했기 때문에 우리는 주로 인간의 평가를 사용한다. 구체적으로 **V*********품질(비주얼품질 포함)을 포함하는 EMU 비디오 [12]의 비디오 생성 메트릭의 범주화를 채택하는데, 이는 픽셀 선명도 및 인식 가능한 오브젝트/센), **M***움직임 *******************품질(프레임 일관성, 모션 평활도 및 모션량 포함), **T******************************************************************************************************************************************************************** 유니VG는 텍스트와 이미지의 조합에 대한 조건부 생성을 지원하기 때문에 이미지 **F**aithity(텍스트 공간 정렬 및 텍스트 시간 정렬 포함)를 추가로 도입하여 주어진 이미지와 생성된 비디오의 정렬 성능을 측정한다. 에발레이터는 또한 비교 중인 두 영상의 **O**verall **L***ikbps를 제공하여 앞서 언급한 서브 인디케이터에 대한 보체 역할을 한다. 인간 평가에 사용된 프롬프트는 이전 작품의 웹페이지[10, 11, 21, 24, 16]에서 수집되었다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:7]\n' +
      '\n' +
      'SVD[13] 및 폐쇄 소스 작업 Pika 베타[64] 및 Gen2[65]는 웹사이트 또는 불화로부터 결과를 얻을 수 있다. 이 모든 것이 최근 작품이며 텍스트/이미지-영상 생성에서 현재 최고 수준을 나타낸다. 공정한 비교를 위해 이미지 입력만 지원하는 SVD와 Pika 베타를 제외하고, 다른 모든 작품은 텍스트 및 이미지 입력(SDXL1.0 및 리바이너에 의해 텍스트 프롬프트에서 이미지가 생성됨) 측면에서 일치하게 유지되었다. 비교 결과는 그림 4와 표 2에 나와 있으며, 그림 4는 우리의 모델에 의해 생성된 비디오(\\(\\mathcal{F}_{A}+\\mathcal{F}_{SR}\\))와 다른 방법으로 생성된 비디오 간의 전체 Likity 비교를 보여준다. 우리는 우리의 방법으로 생성된 영상이 오픈 소스 텍스트/영상 모델 및 폐쇄 소스 방법 Pika 베타를 능가하고 폐쇄 소스 방법 Gen2와 일치한다는 것을 발견하며 표 2는 다른 하위 메트릭의 당첨률을 기록한다. GSB로부터 승률을 계산하는 공식은 \\(G-B)/(G+S+B)\\이다. 숫자\\(>\\)0은 우리의 방법이 더 낫다는 것을 나타내며, 숫자\\(<\\)0은 다른 방법이 더 낫다는 것을 나타낸다. 우리는 우리의 방법의 두드러진 이점이 **FC**에 있다는 것을 발견했는데, 이는 낮은 자유 비디오 생성을 위한 편집 패러다임을 채택하여 보다 안정적인 비디오를 생성하는 데 도움이 되는 \\(\\mathcal{F}_{A}\\) 때문입니다. 또한, 생성된 비디오는 유사한 해상도의 비디오(더 큰 해상도의 비디오를 생성하는 유전형 2를 제외)에 비해 우수한 **PS**를 나타낸다. 왜냐하면, 우리는 **BGN***를 사용하여 저해상도 비디오에서 고해상도 비디오를 직접 예측함으로써 훈련과 추론 사이의 일관성을 보장하기 때문이다. 생성된 비디오의 한 가지 중요한 단점은 훈련 데이터에서 정적 비디오에 대한 필터링이 현재 없기 때문에 **AM***이다. 이를 추가하는 것은 우리의 미래 작업의 일부가 될 것입니다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '베이스 모델의 훈련 프로세스(\\mathcal{F}_{B}\\)는 텍스트와 이미지의 임의의 조합으로 조건부 비디오 생성을 강조하기 때문에 기본 모델이 텍스트 대 비디오, 이미지 대 비디오 및 텍스트/영상 생성에서 능력을 동시에 유지할 수 있는지 여부가 핵심 질문이다. 따라서 우리는 \\(\\mathcal{F}_{B}\\)의 훈련 과정에서 체크포인트를 취하고, 텍스트 대 비디오, 이미지 대 비디오 및 텍스트&영상 생성에서 FVD로 성능을 테스트한다. 결과는 그림 5에 나타나 있어 3개의 곡선의 전체 경향이 하향되어 있음을 알 수 있으며, 이는 3개의 곡선의 전체 경향이 하향되어 있음을 나타낸다.\n' +
      '\n' +
      '그림 5: FVD 스코어는 \\(\\mathcal{F}_{B}\\)의 훈련 과정 동안 MSR-VTT에 대한 것이다.\n' +
      '\n' +
      '훈련 과정은 텍스트 또는 이미지로부터 비디오를 생성하는 베이스 모델의 능력을 향상시킨다. 이는 자유도 높은 영상 생성의 경우, 다조건 영상 생성이 하나의 단일 모델로 통합될 수 있음을 증명한다.\n' +
      '\n' +
      '바이오머스트 가우시안 노이즈(BGN)가 저자유 영상 생성 과제에 더 잘 부합한다는 것을 입증하기 위해 Animation Model \\(\\mathcal{F}_{A}\\)와 비디오 슈퍼 해결 모델 \\(\\mathcal{F}_{SR}\\)에 대한 절제 연구를 수행했다. 표 3에 표시된 결과는 BGN이 낮은 FVD에 의해 입증된 바와 같이 이미지 삽입 및 슈퍼 해결 모두에서 비디오 품질을 향상시킨다는 것을 나타낸다. 이미지애니메이션보다 자유가 적은 과제인 슈퍼진화에 더 유익함을 증명한다. 그림 7은 BGN이 있거나 없는 \\(\\mathcal{F}_{SR}\\)의 성능을 시각화한다. 첫 번째 행은 원본, 저해상도 입력 영상을 보여준다. Rows 2와 3은 BGN이 없는 \\(\\mathcal{F}_{SR}\\)의 출력을 묘사하고 업스케일링된 저해상도 입력에서 처리하고 각각 \\(700\\) 및 \\(900\\) 변성 단계를 거쳤다. 네 번째 행은 타임스메프 \\(t_{m}=700\\)에서 BGN을 사용하여 \\(\\mathcal{F}_{SR}_{SR}\\)의 출력을 \\(t_{m}=700\\)에서 \\(t_{n}=0\\)로 제시하며, 700단계 이후에 고해상도 업스케일링되는 저해상도 비디오가 효과적으로 변성될 수 있는 방법을 보여준다. 각 행의 먼 오른쪽은 모델 생성 콘텐츠에 대한 디테일을 더 잘 보여주기 위해 확대된 뷰를 제공한다. 우리의 관찰은 더 작은 초기 소음 단계 카운트인 BGN이 없는 경우 덜 명확성(2열)을 초래하는 반면, 더 큰 카운트는 원래 콘텐츠(3열)를 과파워링하는 노이즈로 인해 명확하지만 일관성이 없는 출력을 생성함을 나타낸다. BGN을 사용하여 모델은 저해상도 입력에서 고해상도의 영상을 직접 예측하여 선명도를 달성하고 원본 특징(4열)을 보존한다. 또한 BGN의 애플리케이션이 프레임 보간 및 비디오 편집과 같은 다른 저자유 영상 생성 과제로 확장될 수 있음을 인정하며, 이는 향후 작업에서 탐구하는 것을 목표로 한다.\n' +
      '\n' +
      '우리 시스템은 이미지와 텍스트를 유연하게 정렬하는 비디오를 생성할 수 있기 때문에 이 두 조건에 대해 서로 다른 추론 가중치 하에서 생성된 비디오를 탐색한다. 우리는 텍스트 프롬프트 \\(V_{out}) 및 이미지 조건 \\(I\\)을 감안할 때, 추론 공식은 \\(V_{B}_{B}(\\mathcal{F}_{T})이고(\\mathcal{F}_{B}(T)-\\mathcal{F}_{T}) 텍스트 프롬프트 \\(\\varnothing)+wB}(\\varnothing)+wB}(\\varnothing)+wB}(\\varnothing)+wB}(\\varnothing)+w_{T}(\\varnothing)+w_{T}(\\varnothing)+w_{T}(\\varnothing)+w_{T}(\\varnothing)+w_{T})+w_{T}(\\varnothing){T}(\\vathcal:{T}){T}){T}(\\vathcal{T}){T}(\\vathcal{T}){T}(\\vathcal{T}(\\ 텍스트 \\(w_{T}\\) 및 이미지 \\(w_{I}\\)의 분류기 자유 안내 척도를 조정하며, 생성 영상은 그림 6-(a), \\(w_{T}\\) 및 \\(w_{I}\\)를 조절하는 것이 텍스트 또는 이미지 조건에 대해 생성된 비디오를 편향시킬 수 있음을 발견했다. 그림 6-a는 행 1에서 \\(w_{I}=0\\), \\(\\mathcal{F}_{B}_{B}\\)가 입력 이미지와 거의 관련이 없는 영상을 생성하는 반면, 행 3에서는 \\(w_{T}=0\\), \\(\\mathcal{F}_{B}_{B}\\)는 텍스트와 거의 관련이 없는 비디오를 생성함을 보여준다. i\\(w_{T}\\)와 \\(w_{I}\\)를 모두 적절한 값으로 조정함으로써, 제2 행의 생성된 비디오는 입력 이미지의 특성을 유지하고 또한 텍스트 의미론과 정렬된다. 이러한 특징에 기초하여, 우리의 \\(\\mathcal{F}_{B}\\)는 그림 6-(b)와 같이 서로 다른 텍스트 프롬프트와 결합된 동일한 입력 이미지로 서로 다른 비디오 생성을 달성할 수 있다. 우리는 또한 \\(\\mathcal{F}_{A}\\)가 유사한 특성을 가지고 있는지 여부를 조사했다. 그러나 텍스트보다 제약 조건이 훨씬 더 강한 연결된 이미지 특징으로 인해 생성된 비디오는 주로 이미지 의미론에 의존한다. 그럼에도 불구하고 일관된 텍스트를 입력하는 것은 생성된 비디오의 동적 효과를 향상시키는 데 도움이 된다.\n' +
      '\n' +
      '그림 7: \\(\\mathcal{F}_{SR}\\) w/o 또는 w/BGN의 생성 사례.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:10]\n' +
      '\n' +
      '* [18] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In _ICLR_, 2022.\n' +
      '* [19] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. _arXiv preprint arXiv:2211.13221_, 2022.\n' +
      '* [20] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. _arXiv preprint arXiv:2310.19512_, 2023.\n' +
      '* [21] Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu, Qi Zhang, Fu Li, Haocheng Feng, Errui Ding, and Jingdong Wang. Videogen: A reference-guided latent diffusion approach for high definition text-to-video generation. _arXiv preprint arXiv:2309.00398_, 2023.\n' +
      '* [22] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. _arXiv preprint arXiv:2309.15818_, 2023.\n' +
      '* [23] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. Izygen-xl: High-quality image-to-video synthesis via cascaded diffusion models. _arXiv preprint arXiv:2311.04145_, 2023.\n' +
      '* [24] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: High-dynamic video generation. _arXiv preprint arXiv:2311.10982_, 2023.\n' +
      '* [25] Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, and Trevor Darrell. More control for free! image synthesis with semantic diffusion guidance. In _WACV_, 2023.\n' +
      '* [26] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In _ICLR_, 2022.\n' +
      '* [27] Yitong Li, Martin Renqiang Min, Dinghan Shen, David E. Carlson, and Lawrence Carin. Video generation from text. In _AAAI_, 2017.\n' +
      '* [28] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei. To create what you tell: Generating videos from captions. In _ACM MM_, 2017.\n' +
      '* [29] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. In _ICLR_, 2022.\n' +
      '* [30] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. _arXiv preprint arXiv:2104.14806_, 2021.\n' +
      '* [31] Gaurav Mittal, Tanya Marwah, and Vineeth N. Balasubramanian. Sync-draw: Automatic video generation using deep recurrent attentive architectures. In _ACM MM_, 2017.\n' +
      '* [32] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic VQGAN and time-sensitive transformer. In _ECCV_, 2022.\n' +
      '* [33] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In _ICLR_, 2021.\n' +
      '* [34] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In _ICLR_, 2021.\n' +
      '* [35] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail A. Kudinov. Grad-tts: A diffusion probabilistic model for text-to-speech. In _ICML_.\n' +
      '* [36] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M. Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi. Nerfdiff: Single-image view synthesis with nerf-guided distillation from 3d-aware diffusion. In _ICML_, 2023.\n' +
      '* [37] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In _CVPR_, 2021.\n' +
      '* [38] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In _CVPR_, 2023.\n' +
      '* [39] Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models. In _NeurIPS_, 2022.\n' +
      '* [40] Chin-Wei Huang, Milad Aghajohari, Joey Bose, Prakash Panangaden, and Aaron C Courville. Riemannian diffusion models. In _NeurIPS_, 2022.\n' +
      '\n' +
      '* [41] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _ICLR_, 2021.\n' +
      '* [42] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In _ICLR_, 2022.\n' +
      '* [43] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In _ICLR_, 2023.\n' +
      '* [44] Daniel Watson, William Chan, Jonathan Ho, and Mohammad Norouzi. Learning fast samplers for diffusion models by differentiating through sample quality. In _ICLR_, 2023.\n' +
      '* [45] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In _ICLR_, 2022.\n' +
      '* [46] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. _arXiv preprint arXiv:2212.11565_, 2022.\n' +
      '* [47] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. _arXiv preprint arXiv:2303.13439_, 2023.\n' +
      '* [48] Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, and Evgeny Burnaev. Latent video transformer. In Giovanni Maria Farinella, Petia Radeva, Jose Braz, and Kadi Bouatouch, editors, _VISIGRAPP_, 2021.\n' +
      '* [49] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using VQ-VAE and transformers. _arXiv preprint arXiv:2104.10157_, 2021.\n' +
      '* [50] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, and Sergey Tulyakov. A good image generator is what you need for high-resolution video synthesis. In _ICLR_, 2021.\n' +
      '* [51] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In _ICLR_, 2014.\n' +
      '* [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_. PMLR, 2021.\n' +
      '* [53] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In _ICCV_, 2021.\n' +
      '* [54] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* [55] Nicholas Guttenberg. Diffusion with offset noise, 1 2023.\n' +
      '* [56] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps. In _NeurIPS_, 2022.\n' +
      '* [57] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In _CVPR_, 2016.\n' +
      '* [58] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross B. Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning. In _CVPR_, 2021.\n' +
      '* [59] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. _arXiv preprint arXiv:2211.11018_, 2022.\n' +
      '* [60] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: A large-scale video-text dataset for multimodal understanding and generation. _arXiv preprint arXiv:2307.06942_, 2023.\n' +
      '* [61] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation. _arXiv preprint arXiv:2304.08477_, 2023.\n' +
      '* [62] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* [63] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In _CVPR_, 2017.\n' +
      '* [64] Pika labs. Accessed December 18, 2023. [Online]. Available: [https://www.pika.art/](https://www.pika.art/).\n' +
      '* [65] Gen-2. Accessed December 18, 2023. [Online]. Available: [https://research.runwayml.com/gen2](https://research.runwayml.com/gen2).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>