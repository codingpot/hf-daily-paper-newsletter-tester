<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '유엔 통합 영상 세대# UniVG를 제공합니다.\n' +
      '\n' +
      ' 루단 루단 루안, 레이 톈, 초안웨이 황, 벼 장, Xu 장, 잔난 샤오샤오 장.\n' +
      '\n' +
      'Baidu Inc.\n' +
      '\n' +
      'Beijing, China\n' +
      '\n' +
      '{ruanludan, tian1ei09}@baidu.com\n' +
      '\n' +
      'huangcv21@gmail.com\n' +
      '\n' +
      '{zhangxu44, xiaoxinyan}@baidu.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '확산 기반 영상 생성은 학계와 산업계 모두에서 광범위한 관심을 받고 상당한 성공을 거두었다. 그러나 현재의 노력은 주로 텍스트에 의해 구동되는 생성, 이미지에 의해 또는 텍스트와 이미지의 조합과 같은 단일 주관적 또는 단일 태스크 비디오 생성에 집중되어 있다. 이는 사용자가 개별적으로 또는 조합하여 유연한 방식으로 이미지 및 텍스트 조건을 입력할 가능성이 높기 때문에 실제 응용 시나리오의 요구를 완전히 충족할 수 없다. 이를 해결하기 위해 텍스트 및 이미지 양식에 걸쳐 다중 비디오 생성 작업을 처리할 수 있는 ** 비조정***-모달 **V** 비디오 **G** 폭기 시스템을 제안한다. 이를 위해 우리는 생성자유의 관점에서 우리 체제 내에서 다양한 영상 생성 과제를 재방문하고, 이를 고자유와 저자유 영상 생성 범주로 분류한다. 고자유 영상 생성을 위해 입력 이미지 또는 텍스트의 의미론과 일치하는 비디오를 생성하기 위해 멀티 조건 크로스 어시스트를 사용한다. 저자유 영상 생성을 위해 순수 랜덤 가우시안 노이즈를 대체하기 위해 바이오머드 가우시안 노즈를 도입하는데, 이는 입력 조건의 내용을 더 잘 보존하는 데 도움이 된다. 우리의 방법은 공공 학술 벤치마크 MSR-VTT에 대한 가장 낮은 프로쉐트 비디오 거리(FVD)를 달성하여 인간 평가에서 현재 오픈 소스 방법을 능가하고 현재 긴밀한 소스 방법 Gen2와 일치하며 더 많은 샘플에서 [https://univg-baidu.github.(https://univg-baidu.github)를 방문한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'In recent years, diffusion-based generative models [1; 2; 3] have significant progress in image generation [4; 5; 6; 7; 8; 9] with applications rapidly expanding to video generation [10; 11; 12; 13; 14]. The majority of video generation models employ textual descriptions as conditional inputs [15; 16; 17; 18; 10; 11; 19]. However, recent studies have begun to explore the use of image conditions to improve the detail of generated videos [20] or for pixel-level controlling [21; 22; 13; 12; 23; 24]. Additionally, to enhance the temporal smoothness and spatial resolution of the generated videos, current approaches often incorporate modules for frame interpolation and super-resolution [10; 11]. However, existing works focus exclusively on single-objective or single-task video generation, where the input is limited to text [10; 11; 9; 18], an image [13], or a combination of text and image [12; 24]. This single-objective or single-task pipeline lacks the necessary flexibility to satisfy all user needs. In practice, users may not have the requisite text or image conditions for input, rendering the model unusable. Alternatively, the introduction of conflicting text-image pairs may lead to the generation of static videos or videos with abrupt transitions (similar conclusion is proposed in [24]).\n' +
      '\n' +
      'In essence, all models used in video generation are conditional generative models that accept one or more conditions to produce a corresponding video. These conditions can be text, images, low-resolution videos, even control signals. In order to construct a versatile video generation system capable of handling multiple video generation tasks, we revisit existing methods and categorize the relevant methods based on **generative freedom** rather than the task itself. The concept of **generative freedom** that we propose corresponds to the range of solution space for video generation models given certain conditions. In this paper, we categorize various video generation tasks as either high-freedom or low-freedom video generation. Specifically, high-freedom video generation is characterized by input conditions, i.e., text and image, that are weakly constrained at the semantic level, so that the generative model in this scenario has a larger solution space, providing a higher degree of freedom. Conversely, low-freedom video generation typically involves strongly constrained conditions at the low-level information (i.e., pixel), such as in image animation and video super-resolution. These constraints limit the solution space available to the generative model, resulting in a lower degree of freedom.\n' +
      '\n' +
      '다양한 영상 생성 과제의 특성에 더 잘 부합하기 위해서는 영상 생성을 위해 다양한 수준의 생성 자유를 가진 다양한 전략을 취해야 한다. 고자유 영상 생성을 위해서는 표준 확산 _포트 파라다임_가 적절하며 기존의 일부 리피스에 광범위하게 활용되어 온 @ludan을 제공해야 한다. 구체적으로, 훈련 단계 동안 확산 모델은 순방향 처리에서 추가된 노이즈를 학습하고 추론 단계 동안 순전히 랜덤 가우시안 분포로부터 역전함으로써 목표 분포를 예측한다. 분류기 안내[4] 및 분류기 자유 안내[25]는 예측 분포를 입력 조건에 의해 지정된 분포와 정렬하기 위해 사용된다. 저자유 영상 생성의 경우 _Edridge Paradigm_가 더 적합하다. 이미지 편집 [26]을 시점의 사례로 하는 것은 일반적인 관행은 원본 이미지에 일정 수준까지 노이즈를 추가한 다음 텍스트를 편집 신호로 사용하여 의도된 결과를 향해 분포를 제거하는 것을 포함한다. 이 접근법은 처음부터 발생하는 생성과 비교하여 원본 입력의 콘텐츠를 더 잘 보유할 수 있다. 비디오 초해상도는 이미지 편집[23]과 유사한 기술을 활용했다. 그러나 _Edridge Paradigm_는 훈련 단계와 추론 단계 사이의 불일치의 형태로 한계가 있다. 구체적으로, 모델은 조건부 분포에서 목표 분포로의 전환을 학습하지 않고 목표 분포를 근사화하기 위해만 트레이닝된다. 이러한 불일치는 트레이드오프 관련 문제, 즉 도입되는 덜한 노이즈로 인해 모델의 편집 능력이 약해지는 반면, 추가되는 노이즈가 많을수록 입력이 보존될 수 있는 모델이 적을수록 노이즈 수준이 완전히 무작위 가우시안 분포에 접근하면 편집 패러다임이 생성과 유사해짐에 따라 원래 입력의 콘텐츠를 보존하는 모델의 능력이 크게 감소한다. 입력을 보존하면서 편집 능력을 균형을 맞추기 위해 모델 편집의 훈련 및 추론 단계를 어떻게 조정해야 하는지도 해결해야 할 문제이지만 이전 작업에서 간과되어 왔다.\n' +
      '\n' +
      'In this paper, we propose a unified system **Un**ified-modal **Video **G**eneration (i.e.**UniVG**), designed to support flexible video generation conditioned on the arbitrary combination of image and text. To achieve this, we categorize all models within the system into two groups: high-freedom video generation and low-freedom video generation. For high-freedom video generation, we present a base model that is capable of the requirements of handling arbitrary combinationsof text and image conditions. We accomplish this by enhancing the original cross-attention module of the UNet architecture with a multi-condition cross-attention module. With regard to low-freedom video generation, we propose two corresponding models that are individually tailored for image animation and video super-resolution task. These models utilize the editing paradigm, as opposed to the generation paradigm. To reconcile the differences between the training process based on generation paradigm and the inference process based on editing one, in this paper, we predict **B**iased **G**aussian **N**oise (shorted as **BGN**) that is directed towards the target distribution, instead of standard Gaussian noise, by refining the objective function during training stage.\n' +
      '\n' +
      'The proposed UniVG system comprises a Base model, an Image Animation model and a Super Resolution model. The Base model is capable of handling arbitrary combinations of text and image conditions and outputs a video sequences of \\(24\\times 320\\times 576\\) that are semantically aligned with the input conditions at 8 frames per second (fps). The Image Animation model that fine-tuned from the Base model with the additional condition of image concatenation, generates video frames of \\(24\\times 320\\times 576\\) that are pixel-aligned with the input image. The Super Resolution model enhances the resolution of each frame to \\(720\\times 1280\\) pixels. Compared to previous works, Our UniVG demonstrates better tasks adaptability for video generation, i.e., handling various video generation tasks within an unified system, but also significantly improvements on the generation details and frame consistency. Experiments have proven the effectiveness of our method. On objective metrics, our method significantly surpasses other existing methods, and in manual evaluations, our approach is on par with Gen2 and exceeds the other methods.\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약될 수 있다.\n' +
      '\n' +
      '1. 의미적으로 정렬된 텍스트/이미지 대 비디오 생성, 이미지 애니메이션과 같은 다수의 비디오 생성 작업을 처리할 수 있는 제1 비디오 생성 시스템인 UniVG를 제안한다.\n' +
      '2. We introduce Biased Gaussian Noise and confirm its effectiveness for low-freedom video generation tasks, such as image animation and super-resolution.\n' +
      '3. 실험 결과에 따르면 우리의 방법은 비스제크 ti vemet 메트릭과ison par wi th Gen 2ins ubje ct iveevaluat 이온의 기존 gtext/im 연령 대 혈관 내 이온 방법 사이에 있다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '### Text-to-Video Generation\n' +
      '\n' +
      '비디오 생성에 대한 초기 작업은 GAN[27; 28; 29], VQ-VAE[30; 31], 자동 억제 모델[30; 18], 변압기 구조[32]를 활용했지만 낮은 해상도와 최적이 아닌 시각적 품질에 의해 제한되었다. 영상 생성[4; 5; 6; 7], 오디오 생성[33; 34] 및 기타 도메인[36; 37; 38]에서 확산 모델이 성공한 후 비디오 생성에서 확산 모델의 첫 번째 적용을 나타냈다. 이어서, Make-A-V 비디오 [10]와 ImagenV 비디오[11]는 2D U-Net을 텍스트 대 이미지 생성에서 3D U-Nets로 확장시켜 영상 생성을 오픈 영역으로 확장시켰다. 그 이전까지는 대규모 GPU 메모리 소비와 높은 훈련 비용이 필요한 화소 공간에서 영상 모델링을 연구해 왔다. 이 문제를 해결하기 위해 많은 연구자들이 픽셀 공간[8; 15; 40; 16] 대신 잠재 공간에서 확산 과정을 수행하고 무학습 샘플링[2; 41; 42; 43] 또는 학습 기반 샘플링[44; 45]으로 샘플링 효율을 향상시키기 위해 초점을 옮겼다. 또한 일부 작업은 훈련 비용을 단일 비디오[46]로 줄이거나 전혀 훈련 비용이 없다[47]로 줄이는 데 집중했다.\n' +
      '\n' +
      '### Image-to-Video Generation\n' +
      '\n' +
      '기그니프 기반 sinTks, IVGe n-XL[2VGe n-XL[4.5]의 비-데노 다이렉트 에프토-레디메데데에 있는 이스트-레프-레프-레프-레프-레프-레프-레프-레프-레프-레프-데오-레프-데노-레프-레프-레프-레프-레프-레프-레프-레프-레프-레디프-레시네이션-아이콘-데데데데데데데데데데데데데데데데데데데데데노-레디페스-레디테일-레프-레프-레프-레프-레프-레프-레프-레프-레프-레프-레프-레프-레프-레프-레디프-레프-레프-레프-레프-레프-레프-레프-레프-레프-레프-레프-레프-레프-레프-레프-레프- 1[2 0],EMUV 이상 후 비디오Cr[12] 및 M.\n' +
      '\n' +
      'As can be inferred from the above, although text-to-video generation and image-to-video generation serve different applications, they share many similarities in their technical approaches. Therefore, this paper explores whether a single framework can unify these two objectives. The primary distinction of our UniVG from earlier works is that we differentiate various models included in video generation from the perspective of generative freedom rather than task.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '이 섹션은 탄력적으로 조건부 비디오 생성을 위해 제안된 ** 유니프**-모달 **V** 비디오 ******증가(즉, **UniVG*********)를 제시한다. 그런 다음 특정 설계로 다이빙하기 전에 Sec 3.1에서 전체 시스템 UniVG의 예비 지식을 간략하게 설명한다.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '(x.{t},\\<<\\f>) 염색에서 x.{t 및\\-rf. {R}^{F\\t Hangi sH\\tim es W}\\) R B\\t 부모가 ntsp ace를 인코딩한다.\n' +
      '\n' +
      '### UniVG\n' +
      '\n' +
      'As illustrated in Figure 2-(a), our entire UniVG consists of three models: (1) A Base model \\(\\mathcal{F}_{B}\\) accepts any combination of text and image conditions for high-freedom video generation. (2) An Image Animation \\(\\mathcal{F}_{A}\\) model accepts text-image pairs to generated video aligned with input image in pixel level, and (3) a Super-resolution model \\(\\mathcal{F}_{SR}\\) for improving spatial resolution. Each model is a latent diffusion model with 3D UNet architecture composed of Spatial Layers, Temporal Layers, and Cross Attention Layers. Following previous works [10; 13], the Spatial Layer consists of 2D Convolution layer and spatial transformers, while the Temporal Layer consists of 1D temporal Convolution layer and temporal transformers. The cross attention module is used to process semantic control signals, such as text and image feature.\n' +
      '\n' +
      '(1) 베이스 모델 \\(\\mathcal{F}_{B}\\)의 경우 비디오 스키어1[20]에서 영감을 받은 CLIP[52]의 텍스트 인코더와 일치하는 이미지 인코더를 사용한다. 입력 이미지 및 텍스트의 글로벌 의미 및 로컬 세부 사항을 완전히 활용하기 위해 우리는 모든 것을 활용한다.\n' +
      '\n' +
      '그림 2: 제안된 **UniVG** 시스템의 오버뷰. (a)는 베이스 모델 \\(\\mathcal{F}_{B}\\), Animation 모델 \\(\\mathcal{F}_{A}\\), 슈퍼 결의 모델 \\(\\mathcal{F}_{SR}\\)을 포함하는 UniVG의 전체 파이프라인을 표시한다. (b)는 \\(\\mathcal{F}_{B}\\) 및 \\(\\mathcal{F}_{A}\\)에 관여하는 다중 조건 크로스 관계를 보여준다.\n' +
      '\n' +
      'F_{I}(F_{I}) 시각적 토큰(F_{f_{i}}=\\{f_{i}}_{i =0}^{K_{I}}}}}}}}}}}}}) 및 모든 \\(K_{T}}}) 텍스트 토큰(F_{T}=F_{T}=F_{T}=F_{T}) 텍스트 토큰 \\)은 CLIP ViT의 마지막 층으로부터 CLIP ViT의 마지막 층부터 CLIP ViT의 마지막 층부터 CL. 1개 이상의 의미적 특징을 처리하는 능력을 가진 원래 크로스 어텐션인 Sec 3.3(2)에 대한 메커니즘을 확장하고 픽셀 수준에서 입력 이미지와 정렬된 비디오를 추가로 생성하기 위해, 우리는 이미지 삽입 모델 \\(\\mathcal{F}_{A}\\)을 훈련시키고 추가 조건으로 첫 번째 프레임의 숨겨진 공간 특징을 연결함으로써 이미지 삽입 모델(\\mathcal{F}_{A}\\)을 훈련시킨다. 추가 조건 때문에 초기 컨볼루션 레이어의 커널의 대응하는 채널 치수는 \\(C\\)에서 \\(2C\\)로 변한다. 우리는 원래 모델의 성능을 보존하기 위해 추가 매개변수를 0으로 초기화한다. \\(\\mathcal{F}_{B}\\) 또는 \\(\\mathcal{F}_{A}\\)를 사용하여 \\(24\\tcer 320\\i 576\\)의 비디오 프레임을 획득할 수 있다. (3) 생성된 비디오의 선명도를 높이기 위해, 우리는 \\(\\mathcal{F}_{B}\\)의 슈퍼 해결 모델 \\(\\mathcal{F}_{SR}\\)을 추가로 고용했다. 초해상도 작업은 이미지 조건이 없기 때문에, 다중 조건 교차 주의 모듈은 텍스트 조건만을 수용하는 정기적인 교차 의도 모듈로 되돌아간다. 훈련 중 \\(\\mathcal{F}_{SR}\\)는 라모도블러, 무작위화, JPEG 압축 등을 통해 고화질 영상을 파괴하여 얻은 저해상도 \\(V^{lr}\\)의 영상을 접수한다. 우리는 \\(\\mathcal{F}_{A}\\)와 \\(\\mathcal{F}_{SR}\\)에 해당하는 작업을 자유도가 낮은 세대로 분류함에 따라 표준 가우시안 노이즈를 Sec 3.4에 도입된 바이오세스 가우시안 노이즈(**BGN***)로 조절하여 조건부 분포에서 목표 분포로 비사회 역진 과정을 제시한다.\n' +
      '\n' +
      '멀티 크로스 의도.\n' +
      '\n' +
      '우리의 기본 모델 \\(\\mathcal{F}_{B}\\) 및 이미지 삽입 모델 \\(\\mathcal{F}_{A}\\)이 텍스트 및 이미지 CLIP 특징을 수용하기 때문에 표준 크로스 의도 대신 멀티 조건 크로스 어시스트를 사용한다. 이 모듈의 아키텍처는 주로 VideoCrafter[20], \\(F_{\\text{out}}})를 구성하는 VideoCrafter[20]을 따른다.\n' +
      '\n' +
      '}(K_{in}.^{{}}{Q_{I}^{{{d})\n' +
      '\n' +
      'W_{I}\\V_{I}}\\V_{I}\\cotF_{I}}\\cotF_{I}=W_{I}\\cotF_{I}]W_{dotF_{I}}.\n' +
      '\n' +
      'Hf(d_{k}\\)가 키/쿼리 벡터의 차원이고 \\(Q_{\\text{in}}\\)는 \\(F_{I}\\)와 \\(F_{T}\\) 사이에 공유된다. 가중치 \\(W_{K_{I}}\\) 및 \\(W_{V_{I}}}\\)는 각각 \\(W_{K_{T}}}\\) 및 \\(W_{V_{T}}\\)에서 초기화된다. 이미지를 추가 입력 강화로 취급하는 비디오 더 부드러운 래트와 달리, 우리는 이미지를 텍스트와 함께 동등하게 유의미한 제어 신호로 간주한다. 이는 훈련 과정 전반에 걸쳐 일정 비율의 이미지 드롭아웃을 적용하여 달성된다. 확장하면 MCA는 재교육 필요성(예: 더 강한 텍스트 특징) 없이 교차 의도 단위의 수를 증가시켜 두 가지 이상의 조건을 수용할 수 있다. 이러한 유연성은 새로운 조건을 처리하기 위해 모델의 훈련을 연장하는 비용을 크게 감소시킨다.\n' +
      '\n' +
      '### Biased Gaussian Noise\n' +
      '\n' +
      '제안된 바이오페이지 가우시안 노이즈는 낮은 자유 비디오 생성을 위한 목표 분포로 조건 분포를 전달하는 데 사용된다. 그림 3-(a)에 도시된 바와 같이, 표준 순방향 확산 과정은 목표 분포 \\(v^{T}_{t}=v^{T}_{t}=\\sqrt{T}_{t}v^{T}+\\sqrt{1-\\alpha}_{t}_{t}<{t}_\\epsilon\\)를 통해 목표 분포 \\(v^{T}_\\epsilon\\)에서 표준 가우시안 분포 \\(v^{T}.{t}_\\sqrt{t}_\\sqrt{t}_\\sqrt{t}_\\sqrt{t}_\\sqrt{t}_{t}_\\he}_{t}_\\he}_{t}_\\epsilat}_{t}_\\epsilon\\)에서 표준 가우시안 분포)에서 표준 가우시안 분포)로 전환한다. 그러나 일반적으로 후방 과정에서 이러한 분포는 관련된 유일한 두 가지 분포이다. 이는 추론 중 조건 분포 \\(v^{C}\\)에서 샘플이 도입될 때 최적이 아닌 편집 결과를 초래할 수 있다. 전방 및 후방 과정 모두에서 조건 분포를 설명하기 위해 그림 3-(b)에 도시된 바와 같이 원래 확산을 세 부분으로 분할한다. t_{m}\\, \\(v_{t}\\)는 0 내지 \\(v_{t}|v^{T}},t) 사이의 타임스메프를 사용하여 표적 샘플에 의해 계산되며, 이는 \\(q_{t}|v{t^{T}=\\sqrt{\\alpha}_{t}_{t}_{t}_{t}_{t}_{t}}_{t}. H\\(t_{n}\\)에서 \\(N\\) 사이의 타임스탬(t_{n}\\)의 경우, \\(v_{t}|v^{C},t)=\\sqrt{\\alpha}_{t}_{t} +\\sqrt{1-\\alpha}_{n}\\leq t <N)\\로 조건 샘플에 의해 \\(v_{t}. 핵심 문제는 \\(v_{t}|v^{C},v^{T}}}\\)에서 \\(v_{t_{m}}\\)로 원활하게 전환할 수 있는 \\(q_{t_{n}}},t)\\을 설계하는 방법이다. 원래의 확산 일정을 보존하기 위해 \\(\\epsilon\\)로 표시된 노이즈 \\(\\epsilon^{\\prime}\\) 변수를 소개한다. t_{m}\\(t_{n}\\)와\\(t_{t}}) 사이의 타임스메프를 위해 우리는\\(v_{t}|v^{C},v^{t},{t})를 가지고 있다.\n' +
      '\n' +
      '그림 3: 무작위 가우시안 노이즈 및 바이오머드 가우시안 노이즈와의 전방 및 후방 확산 프로세스는 그림 3:이다.\n' +
      '\n' +
      '}}(v_\\qrt{t_{t_{m}}}} <{t_{t_{m}}}} <{t_{t_{m}}}}) 및 \\(v_{t_{n}=v_{t_{n})=\\sqrt{\\alpha_{t_{n}-{t_{t_{t_{t_{t_{t_{t_{t_{t_{m} <{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{t_{n}<{t_{t_{n} <{t_{t_{n},{t_{n}:{t_{t_{n}}.{t_{n}}<{t_{n}\'{t_{n}} 따라서 해당 \\(\\epsilon^{\\prime}\\)는 타임스톤 \\(t_{m}\\) 및 \\(t_{n}\\)에서 다음 공식을 만족해야 한다.\n' +
      '\n' +
      '\\[\\epsilon^{\\prime}_{t_{m}}=\\epsilon,\\ \\ \\epsilon^{\\prime}_{t_{n}}=\\epsilon+ \\frac{\\sqrt{\\alpha_{t_{n}}}}{\\sqrt{1-\\overline{\\alpha}_{t_{n}}}}\\times\\left(v^ {C}-v^{T}\\right)\\]\n' +
      '\n' +
      'In theory, there are an infinite number of solutions to \\(\\epsilon^{\\prime}\\). In this paper, we simply define \\(\\epsilon^{\\prime}\\) as a linear transformation following\n' +
      '\n' +
      '\\[\\epsilon^{\\prime}_{t}=\\epsilon+\\frac{\\sqrt{\\overline{\\alpha}_{t}}}{\\sqrt{1- \\overline{\\alpha}_{t}}}\\times\\frac{t-t_{m}}{t_{n}-t_{m}}\\times\\left(v^{C}-v^ {T}\\right),\\ (t_{m}\\leq t<t_{n})\\]\n' +
      '\n' +
      'i\\(\\epsilon^{\\prime}\\)는 \\(v^{C}\\) 및 \\(v^{T}\\)의 가중 조합에 의해 평균값이 이동된 비차원 가우시안 분포로부터 샘플링된다. 이러한 편향은 조건 분포로부터 목표 분포로의 확산 과정을 가교하는 데 중요하다. 미래의 작업에서 \\에 대한 대체 솔루션을 탐색할 것이다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'DatasetOur training datasets include publicly available academic datasets such as WebVid-10M [53] and LIAIONCOCO [54], along with self-collected data. WebVid-10M is a large and diverse text-video dataset containing approximately 10 million open-domain videos with a resolution of \\(336\\times 596\\) pixels. LIAION-COCO is a substantial text-image dataset comprising 600 million high-quality images, filtered from LIAION-2B and scored using the Aesthetic and Semantic Estimate (ASE). To further enhance the quality of the generated videos and to address the issue of watermarks present in WebVid-10M, we continue training on our own curated datasets of videos and images, which contain high-quality visual content. We prepare the self-collected videos by first proportionally compressing them to 720p resolution along their shorter edge and then segmenting them into 10-second clips. This process yielded 5 million high-quality text-video pairs. Additionally, our self-curated image dataset includes 1.3 million high-quality text-image pairs, with a focus on artistic styles.\n' +
      '\n' +
      'TrainingOur \\(\\mathcal{F}_{B}\\) is trained with an image:video:video frame ratio of 1:1:1, where the training video frames were sampled with equal probability from 8 to 24 frames. We set the text dropout to 0.5 and the image dropout to 0.1. In addition, we utilize offset noise [55] with a strength of 0.1 and zero terminal Signal-to-Noise Ratio (SNR) [12]. Offset noise has been proven helpful to be helpful in generating extremely dark or bright images. Zero terminal-SNR has been shown to be beneficial for generating high-quality and high-resolution visual content by adding noise to pure Gaussian noise following a rescaled schedule. Both techniques have proven useful in our experiments. Subsequently, we continue finetuning \\(\\mathcal{F}_{B}\\) to obtain \\(\\mathcal{F}_{A}\\) and \\(\\mathcal{F}_{SR}\\), using Biased Gaussian Noise (BGN) on our self-curated video dataset only. For \\(\\mathcal{F}_{A}\\), we set the text dropout to 0.1 and the image dropout to 0.1, the BGN is experimentally set during timesteps \\(t_{m}=600\\) to \\(t_{n}=990\\) since the earlier steps determine the content [26]. For \\(\\mathcal{F}_{SR}\\), the text dropout is set to 0.1, and the BGN is applied during timesteps \\(t_{m}=0\\) to \\(t_{n}=700\\) since the later steps deciding the details [26]. We incorporate \\(\\epsilon\\)-prediction [1] for \\(\\mathcal{F}_{B}\\) and \\(\\mathcal{F}_{A}\\), \\(v-\\)prediction for \\(\\mathcal{F}_{SR}\\). The learning rate of all models is fixed at \\(1\\times 10^{-5}\\). We use DPM Solver [56] for accelerating sampling: 50 steps for \\(\\mathcal{F}_{B}\\) and \\(\\mathcal{F}_{A}\\), and 7 steps for \\(\\mathcal{F}_{SR}\\) since we set initial weight to 0.7.\n' +
      '\n' +
      '우리는 UniVG의 평가 기준으로 객관적인 메트릭과 인간 평가를 모두 사용한다. 객관적 측정치 측면에서 우리는 MSR-VTT [57]의 테스트 세트를 표준 벤치마크로 사용하기 위해 이전 작업 [21, 24]을 따른다. 이 테스트 세트는 \\(2,990\\) 테스트 비디오를 포함하며, 각각은 \\(20\\) 프롬프트에 해당하여 총 \\(59,800\\) 프롬프트를 포함한다. 절제 연구의 효율성을 위해 각 테스트 비디오에 대해 무작위로 한 프롬프트를 선택하여 궁극적으로 평가 세트로 \\(2,990\\) 프롬프트를 얻었다. 생성된 비디오와 해당 텍스트 사이의 CLIPSIM[30]과 생성된 비디오와 원본 비디오 사이의 FVD[58]를 비교 측정값으로 계산한다. 일부 연구[12]는 객관적인 메트릭이 항상 인간의 인식과 일치하지는 않을 수 있다고 지적했기 때문에 우리는 주로 인간의 평가를 사용한다. 구체적으로 **V*********품질(비주얼품질 포함)을 포함하는 EMU 비디오 [12]의 비디오 생성 메트릭의 범주화를 채택하는데, 이는 픽셀 선명도 및 인식 가능한 오브젝트/센), **M***움직임 *******************품질(프레임 일관성, 모션 평활도 및 모션량 포함), **T******************************************************************************************************************************************************************** 유니VG는 텍스트와 이미지의 조합에 대한 조건부 생성을 지원하기 때문에 이미지 **F**aithity(텍스트 공간 정렬 및 텍스트 시간 정렬 포함)를 추가로 도입하여 주어진 이미지와 생성된 비디오의 정렬 성능을 측정한다. 에발레이터는 또한 비교 중인 두 영상의 **O**verall **L***ikbps를 제공하여 앞서 언급한 서브 인디케이터에 대한 보체 역할을 한다. 인간 평가에 사용된 프롬프트는 이전 작품의 웹페이지[10, 11, 21, 24, 16]에서 수집되었다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:7]\n' +
      '\n' +
      'SVD[13] 및 폐쇄 소스 작업 Pika 베타[64] 및 Gen2[65]는 웹사이트 또는 불화로부터 결과를 얻을 수 있다. 이 모든 것이 최근 작품이며 텍스트/이미지-영상 생성에서 현재 최고 수준을 나타낸다. 공정한 비교를 위해 이미지 입력만 지원하는 SVD와 Pika 베타를 제외하고, 다른 모든 작품은 텍스트 및 이미지 입력(SDXL1.0 및 리바이너에 의해 텍스트 프롬프트에서 이미지가 생성됨) 측면에서 일치하게 유지되었다. 비교 결과는 그림 4와 표 2에 나와 있으며, 그림 4는 우리의 모델에 의해 생성된 비디오(\\(\\mathcal{F}_{A}+\\mathcal{F}_{SR}\\))와 다른 방법으로 생성된 비디오 간의 전체 Likity 비교를 보여준다. 우리는 우리의 방법으로 생성된 영상이 오픈 소스 텍스트/영상 모델 및 폐쇄 소스 방법 Pika 베타를 능가하고 폐쇄 소스 방법 Gen2와 일치한다는 것을 발견하며 표 2는 다른 하위 메트릭의 당첨률을 기록한다. GSB로부터 승률을 계산하는 공식은 \\(G-B)/(G+S+B)\\이다. 숫자\\(>\\)0은 우리의 방법이 더 낫다는 것을 나타내며, 숫자\\(<\\)0은 다른 방법이 더 낫다는 것을 나타낸다. 우리는 우리의 방법의 두드러진 이점이 **FC**에 있다는 것을 발견했는데, 이는 낮은 자유 비디오 생성을 위한 편집 패러다임을 채택하여 보다 안정적인 비디오를 생성하는 데 도움이 되는 \\(\\mathcal{F}_{A}\\) 때문입니다. 또한, 생성된 비디오는 유사한 해상도의 비디오(더 큰 해상도의 비디오를 생성하는 유전형 2를 제외)에 비해 우수한 **PS**를 나타낸다. 왜냐하면, 우리는 **BGN***를 사용하여 저해상도 비디오에서 고해상도 비디오를 직접 예측함으로써 훈련과 추론 사이의 일관성을 보장하기 때문이다. 생성된 비디오의 한 가지 중요한 단점은 훈련 데이터에서 정적 비디오에 대한 필터링이 현재 없기 때문에 **AM***이다. 이를 추가하는 것은 우리의 미래 작업의 일부가 될 것입니다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '베이스 모델의 훈련 프로세스(\\mathcal{F}_{B}\\)는 텍스트와 이미지의 임의의 조합으로 조건부 비디오 생성을 강조하기 때문에 기본 모델이 텍스트 대 비디오, 이미지 대 비디오 및 텍스트/영상 생성에서 능력을 동시에 유지할 수 있는지 여부가 핵심 질문이다. 따라서 우리는 \\(\\mathcal{F}_{B}\\)의 훈련 과정에서 체크포인트를 취하고, 텍스트 대 비디오, 이미지 대 비디오 및 텍스트&영상 생성에서 FVD로 성능을 테스트한다. 결과는 그림 5에 나타나 있어 3개의 곡선의 전체 경향이 하향되어 있음을 알 수 있으며, 이는 3개의 곡선의 전체 경향이 하향되어 있음을 나타낸다.\n' +
      '\n' +
      '그림 5: FVD 스코어는 \\(\\mathcal{F}_{B}\\)의 훈련 과정 동안 MSR-VTT에 대한 것이다.\n' +
      '\n' +
      '훈련 과정은 텍스트 또는 이미지로부터 비디오를 생성하는 베이스 모델의 능력을 향상시킨다. 이는 자유도 높은 영상 생성의 경우, 다조건 영상 생성이 하나의 단일 모델로 통합될 수 있음을 증명한다.\n' +
      '\n' +
      'Biased Gaussian NoiseTo demonstrate that Biased Gaussian Noise (BGN) better suits low-freedom video generation tasks, we conducted ablation studies on the Animation Model \\(\\mathcal{F}_{A}\\) and the Video Super Resolution model \\(\\mathcal{F}_{SR}\\). The results, shown in Table 3, indicate that BGN enhances video quality in both Image Animation and Super Resolution, as evidenced by lower FVDs. It proves more beneficial for Super Resolution, a task with less freedom than Image Animation. Figure 7 visualizes \\(\\mathcal{F}_{SR}\\)\'s performance with and without BGN. The first row shows the original, low-resolution input video. Rows 2 and 3 depict the outputs from \\(\\mathcal{F}_{SR}\\) without BGN, processed from the upscaled low-resolution input and subjected to \\(700\\) and \\(900\\) denoising steps, respectively. The fourth row presents the output from \\(\\mathcal{F}_{SR}\\) using BGN at timestep \\(t_{m}=700\\) to \\(t_{n}=0\\), illustrating how a low-resolution video upscaled to high-resolution can be denoised effectively after 700 steps. Each row\'s far right offers a magnified view to better showcase the detail in the model-generated content. Our observations indicate that absent BGN, a smaller initial noise step count results in less clarity (second row), while a larger count produces a clear yet inconsistent output due to noise overpowering the original content (third row). With BGN, the model directly predicts high-resolution videos from low-resolution inputs, achieving clarity and preserving original features (fourth row). We also acknowledge that BGN\'s application can extend to other low-freedom video generation tasks, such as frame interpolation and video editing, which we aim to explore in future work.\n' +
      '\n' +
      'Text&Image ConditionsSince our system is capable of generating videos that align both image and text flexibly, we explore the videos generated under different inference weights for these two conditions. Given text prompt \\(T\\) and image condition \\(I\\), the inference formula we use is \\(V_{out}=\\mathcal{F}_{B}(\\varnothing)+w_{T}(\\mathcal{F}_{B}(T)-\\mathcal{F}_{B} (\\varnothing))+w_{I}(\\mathcal{F}_{B}(I)-\\mathcal{F}_{B}(\\varnothing))\\). We adjust the classifier free guidance scale of text \\(w_{T}\\) and image \\(w_{I}\\), the generating videos are shown in Figure 6-(a), we find that adjusting the \\(w_{T}\\) and \\(w_{I}\\) can bias the generated video towards the text or image conditions. Figure 6-a shows that in row 1, \\(w_{I}=0\\), \\(\\mathcal{F}_{B}\\) generates a video that is almost unrelated to the input image, while in row 3, \\(w_{T}=0\\), \\(\\mathcal{F}_{B}\\) produces a video that is almost unrelated to the text. By adjusting both \\(w_{T}\\) and \\(w_{I}\\) to appropriate values, the second row\'s generated video retains the characteristics of the input image and is also aligned with the textual semantics. Based on this feature, our \\(\\mathcal{F}_{B}\\) can achieve different video generation with the same input image combined with different text prompts, as shown in Figure 6-(b). We have also explored whether \\(\\mathcal{F}_{A}\\) possesses similar properties. However, due to the concatenated image features having much more stronger constraints than text, the generated videos mainly rely on image semantics. Nevertheless, inputting consistent text helps to enhance the dynamic effects of the generated videos.\n' +
      '\n' +
      '그림 7: \\(\\mathcal{F}_{SR}\\) w/o 또는 w/BGN의 생성 사례.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:10]\n' +
      '\n' +
      '* [18] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In _ICLR_, 2022.\n' +
      '* [19] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. _arXiv preprint arXiv:2211.13221_, 2022.\n' +
      '* [20] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. _arXiv preprint arXiv:2310.19512_, 2023.\n' +
      '* [21] Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu, Qi Zhang, Fu Li, Haocheng Feng, Errui Ding, and Jingdong Wang. Videogen: A reference-guided latent diffusion approach for high definition text-to-video generation. _arXiv preprint arXiv:2309.00398_, 2023.\n' +
      '* [22] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. _arXiv preprint arXiv:2309.15818_, 2023.\n' +
      '* [23] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. Izygen-xl: High-quality image-to-video synthesis via cascaded diffusion models. _arXiv preprint arXiv:2311.04145_, 2023.\n' +
      '* [24] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: High-dynamic video generation. _arXiv preprint arXiv:2311.10982_, 2023.\n' +
      '* [25] Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, and Trevor Darrell. More control for free! image synthesis with semantic diffusion guidance. In _WACV_, 2023.\n' +
      '* [26] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In _ICLR_, 2022.\n' +
      '* [27] Yitong Li, Martin Renqiang Min, Dinghan Shen, David E. Carlson, and Lawrence Carin. Video generation from text. In _AAAI_, 2017.\n' +
      '* [28] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei. To create what you tell: Generating videos from captions. In _ACM MM_, 2017.\n' +
      '* [29] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. In _ICLR_, 2022.\n' +
      '* [30] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. _arXiv preprint arXiv:2104.14806_, 2021.\n' +
      '* [31] Gaurav Mittal, Tanya Marwah, and Vineeth N. Balasubramanian. Sync-draw: Automatic video generation using deep recurrent attentive architectures. In _ACM MM_, 2017.\n' +
      '* [32] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic VQGAN and time-sensitive transformer. In _ECCV_, 2022.\n' +
      '* [33] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In _ICLR_, 2021.\n' +
      '* [34] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In _ICLR_, 2021.\n' +
      '* [35] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail A. Kudinov. Grad-tts: A diffusion probabilistic model for text-to-speech. In _ICML_.\n' +
      '* [36] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M. Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi. Nerfdiff: Single-image view synthesis with nerf-guided distillation from 3d-aware diffusion. In _ICML_, 2023.\n' +
      '* [37] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In _CVPR_, 2021.\n' +
      '* [38] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In _CVPR_, 2023.\n' +
      '* [39] Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models. In _NeurIPS_, 2022.\n' +
      '* [40] Chin-Wei Huang, Milad Aghajohari, Joey Bose, Prakash Panangaden, and Aaron C Courville. Riemannian diffusion models. In _NeurIPS_, 2022.\n' +
      '\n' +
      '* [41] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _ICLR_, 2021.\n' +
      '* [42] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In _ICLR_, 2022.\n' +
      '* [43] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In _ICLR_, 2023.\n' +
      '* [44] Daniel Watson, William Chan, Jonathan Ho, and Mohammad Norouzi. Learning fast samplers for diffusion models by differentiating through sample quality. In _ICLR_, 2023.\n' +
      '* [45] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In _ICLR_, 2022.\n' +
      '* [46] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. _arXiv preprint arXiv:2212.11565_, 2022.\n' +
      '* [47] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. _arXiv preprint arXiv:2303.13439_, 2023.\n' +
      '* [48] Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, and Evgeny Burnaev. Latent video transformer. In Giovanni Maria Farinella, Petia Radeva, Jose Braz, and Kadi Bouatouch, editors, _VISIGRAPP_, 2021.\n' +
      '* [49] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using VQ-VAE and transformers. _arXiv preprint arXiv:2104.10157_, 2021.\n' +
      '* [50] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, and Sergey Tulyakov. A good image generator is what you need for high-resolution video synthesis. In _ICLR_, 2021.\n' +
      '* [51] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In _ICLR_, 2014.\n' +
      '* [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_. PMLR, 2021.\n' +
      '* [53] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In _ICCV_, 2021.\n' +
      '* [54] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* [55] Nicholas Guttenberg. Diffusion with offset noise, 1 2023.\n' +
      '* [56] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps. In _NeurIPS_, 2022.\n' +
      '* [57] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In _CVPR_, 2016.\n' +
      '* [58] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross B. Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning. In _CVPR_, 2021.\n' +
      '* [59] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. _arXiv preprint arXiv:2211.11018_, 2022.\n' +
      '* [60] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: A large-scale video-text dataset for multimodal understanding and generation. _arXiv preprint arXiv:2307.06942_, 2023.\n' +
      '* [61] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation. _arXiv preprint arXiv:2304.08477_, 2023.\n' +
      '* [62] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* [63] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In _CVPR_, 2017.\n' +
      '* [64] Pika labs. Accessed December 18, 2023. [Online]. Available: [https://www.pika.art/](https://www.pika.art/).\n' +
      '* [65] Gen-2. Accessed December 18, 2023. [Online]. Available: [https://research.runwayml.com/gen2](https://research.runwayml.com/gen2).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>