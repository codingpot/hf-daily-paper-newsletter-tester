<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '비디오, 멀티 프로젝트 맞춤형 영상\n' +
      '\n' +
      '자오 왕\\({}^{1}\\), 아수스 리\\({}^{2}\\), 엔즈 제이\\({}^{2}\\), 라이팅 주\\({}^{3}\\), 용 구오\\({}^{2}\\), Qi Dou\\({}^{1}\\), 쾅구오 리\\({}^{2}\\).\n' +
      '\n' +
      '홍콩대학({}^{1}\\)은 중국 홍콩대학, \\({}^{2}\\) 노르웨이 노아 아크의 랩, \\({}^{2}\\) 홍콩대학({}^{3}\\)이다.\n' +
      '\n' +
      '구용cs@gmail.com.hk, 구용cs@gmail.com.hk, 루제우.{zwang21@cn, 리케시.hk. qax@pk.com,@cuhk}@cuhk.@cuhect.\n' +
      '\n' +
      '[https://kyfafyd.wang/projects/customvideo](https://kyfafyd.wang/projects/customvideo)\n' +
      '\n' +
      'Corresponding author.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '맞춤형 텍스트 대 비디오 생성은 텍스트 프롬프트 및 주제 참조에 의해 가이드되는 고품질 비디오를 생성하는 것을 목표로 한다. 단일 교과를 위해 설계된 현재 접근법은 더 도전적이고 실용적인 시나리오인 여러 교과를 다루는 데 어려움을 겪고 있다. 이 작업에서 우리는 다중 피험자 유도 텍스트 대 비디오 맞춤화를 촉진하는 것을 목표로 한다. 여러 피험자의 안내로 신원보존 영상을 생성할 수 있는 새로운 틀인 고객 영상물을 제안한다. 구체적으로, 첫째, 우리는 단일 이미지로 구성함으로써 여러 피험자의 동시 발생을 장려한다. 또한, 기본 텍스트 대 비디오 확산 모델에 따라 확산 모델의 잠재 공간에서 다른 주제를 무시하기 위한 단순하면서도 효과적인 주의 제어 전략을 설계한다. 더욱이, 모델이 특정 객체 영역에 초점을 맞추도록 돕기 위해 주어진 참조 이미지로부터 객체를 분할하고 주의 학습을 위해 대응하는 객체 마스크를 제공한다. 또한, 우리는 69명의 개별 피험자와 57개의 의미 있는 쌍으로 포괄적인 벤치마크로서 다중 대상 텍스트 대 비디오 생성 데이터 세트를 수집한다. 광범위한 질적, 정량적 및 사용자 연구 결과는 이전 최첨단 접근법과 비교하여 우리의 방법의 우월성을 보여준다.\n' +
      '\n' +
      'Introduction\n' +
      '\n' +
      '텍스트 대 비디오(T2V) 세대[22, 14, 15]는 확산 모델[11, 23, 24, 25]의 장점을 가지고 환상적인 발전을 이루었다. 최근 작가들은 맞춤형 T2V 세대라는 새로운 연구 방향을 지휘하는 자신의 소지품인 _e.g._ 애완동물과 함께 동영상을 생성하는 것을 꿈꿨다. 기존 방법[15, 22, 23]이 단일 객체로부터 동영상을 생성하기 위해 제안되었지만, 복수의 객체들을 다루는 것은 여전히 어려운 시나리오로 남아 있다. 키 챌린지는 생성된 비디오에서 다수의 객체들의 동시 발생을 보장하고 그에 상응하는 정체성을 유지하는 것이다.\n' +
      '\n' +
      '최근 작품인 영상드림러[1]는 여러 피험자로부터 동영상을 생성하는 것을 목표로 하는 스톤블 디퓨전[13]을 기반으로 하는 디엔믹스 핀셋링 및 인간-인-루프 리피네이션 전략을 제안한다. 그러나 비디오드림러는 일관성 없는 객체 혼합 전략으로 인해 여러 과목의 동시 발생 보장과 유사한 과목의 무력화에 짧다. 그림 3에서 보는 바와 같이, 고양이는 추론 동안 모든 프레임에서 일관되게 생성될 수 없다. 더욱이 영상드림러는 자동차, 바렌(그림 3 참조)과 같은 배경 장면으로 전경 객체로 시나리오를 다룰 수 없다.\n' +
      '\n' +
      '대조적으로, 우리의 접근법에서 모델 훈련 동안 여러 물체의 동시 발생을 보장하며, 이는 모델이 서로 다른 피험자의 존재를 동시에 포착하도록 장려하여 추론 동안 동시 발생을 촉진한다. 또한, 훈련 중 여러 교과를 무시하는 주의 제어 메커니즘을 제안하고, 이미지의 관련 없는 부분을 무시하면서 해당 교과 영역에 초점을 맞추도록 모델을 효과적으로 안내한다. 이 과정을 용이하게 하기 위해 SAM[11]과 같은 모델에서 분할하거나 최적화 동안 감독자로서 인간 주석을 통해 제공한 지상 진리 객체 마스크를 통합한다. 우리의 주의 메커니즘은 피험자의 불중심에 기여하는 두 가지 주요 설계로 구성된다. 첫째, 우리는 교차 의도 지도 상의 지상 진리 객체 마스크를 사용하여 해당 주제 영역을 강조하여 학습 가능한 텍스트 토큰을 교과 동일성과 정렬한다. 두 번째로, 우리는 원하는 주제를 제외하고, 교차 의도 맵을 약간의 음의 값으로 최적화하고 입력 영상에서 관련 없는 영역의 영향력을 완화한다. 제안된 접근법을 종합적으로 평가하기 위해 애완 동물, 사람, 장난감, 운송, 장면, 식물, 웨어러블 아이템 등을 포함한 광범위한 범주를 덮는 다양한 데이터 세트를 선별했다. 12개의 주제 쌍을 포함하는 멀티-스튜디오벤치[1]를 넘어서, 우리의 데이터세트 고객 연구소는 69개의 개별 피험자와 57개의 주제 쌍으로 구성되며, 특히 시각적으로 유사한 대상을 포함하는 몇 가지 도전 시나리오를 포함한다. 이 벤치마크 데이터셋에 대한 광범위한 실험을 통해 맞춤형 피험자와 고품질 비디오를 생성하는 데 있어 우리의 방법의 우월성을 입증하는 질적, 정량적, 사용자 연구 결과를 제공한다. 요약하면, 우리의 기여금은 다음과 같습니다.\n' +
      '\n' +
      '* 우리는 단순하면서도 효과적인 동시 발생 및 주의력 조절 메커니즘으로 구동되는 새로운 다중 객체 구동 T2V 생성 프레임워크인 고객 비디오를 제안한다.\n' +
      '* 다주제 T2V 데이터셋을 수집하여 종합 벤치마크로 구축합니다. 우리는 벤치마크가 광범위한 주제 범주 및 그 위에 다양한 주제 쌍을 포괄한다는 것을 강조한다.\n' +
      '* 우리 방법은 _Textual Alignment_, _Image Alignment_ 및 _탭 컨센서스_를 포함한 다양한 메트릭 측면에서 이전 최첨단 접근법을 일관되게 능가한다. 보다 비판적으로, 사용자 연구에서 우리는 이러한 방법보다 높은 점수 \\(3\\\\)로 상당한 개선을 얻는다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '### Text-to-Video Generation\n' +
      '\n' +
      '텍스트 대 비디오 생성은 최근 몇 년 동안 [1, 13, 14]에서 상당한 발전을 이루었다. 초기 접근법은 GAN[1, 15] 및 VQVAE[11]을 사용한 반면, 보다 최근의 작업은 고품질 비디오[12, 13, 14]를 생성하기 위해 확산 모델을 탐구했다. 플레이어-A-비디오[15]는 추가적인 시간적 주의력 융통으로 사전 훈련된 영상 확산 모델을 활용한다. 비디오LDM[1]은 잠재 공간에서 다단계 정렬 접근법을 도입하여 고해상도 및 시간적으로 일관된 비디오를 생성한다. 다른 방법[1, 13, 14]은 이미지를 제1 프레임으로 하여 동영상을 생성하고 무작위로 초기화된 후속 프레임을 생성한다. 제어 가능성을 높이기 위해 비디오 컴포넌트(13])는 깊이 맵과 같은 추가 안내 신호를 통합하여 텍스트 입력과 함께 원하는 비디오를 생성한다. 토네-A-비디오[12]는 사전 훈련된 영상 확산 모델을 미세 조정하여 특정 텍스트 안내로 영상의 성공적인 생성을 달성하는 시간적 자기 의도 모듈을 제안한다. 또한, 확산 기반 비디오 대 비디오 편집 접근법[14, 15]도 실제 우리에게 제안되었습니다.\n' +
      '\n' +
      '### Subject-driven Customization\n' +
      '\n' +
      '개인화된 생성을 위한 사전 훈련된 이미지 및 비디오 확산 모델을 맞춤화하는 데 관심이 증가하고 있다. 맞춤화는 일반적으로 몇 가지 참조 이미지를 기반으로 특정 피사체를 가진 이미지 및 비디오를 생성하는 것을 포함한다. 이미지 확산 커스터마이징을 위해, 텍스트 인버전(1])은 몇 개의 참조 이미지만을 사용하여 학습 가능한 텍스트 토큰으로서 특정 객체를 나타낸다. 이 학습된 텍스트 토큰은 추론 단계 동안 개인화된 이미지를 생성하기 위해 문장으로 통합될 수 있다. 또한 드림보스[14]는 확산 모델의 가중치를 미세 조정하여 이미지 생성의 충실도를 향상시킨다. 여러 작품[11, 13, 12]은 파라미터 효율적인 핀셋링 및 텍스트 임베딩 학습을 중심으로 여러 교과목과의 개인화된 이미지 확산을 탐색하였다. 교과 정체성을 보존하면서 영상 확산 모델을 개인화하기 위해 참조 영상을 이용하는 영상물(23)과 영상보(11) 등 영상 확산을 커스터마이징하려는 초기 시도가 있었지만, 교과와 모션 커스터마이징을 위한 학습 과정을 디커플링하는 드림비디오[24]는 하나의 대상에 설계돼 주어졌을 때 여러 과목을 다룰 수 없다. 최근 작품인 비디오드림어[2]는 LoRA[15]와 함께 디믹스 핀셋링 전략을 통해 다중 객체 구동 영상 맞춤화를 제안한다. 그러나 생성된 비디오는 여러 피험자의 동시 발생 또는 다른 피험자의 불응을 보장하지 않는다. 이 연구에서 우리는 생성된 비디오에서 피험자의 동시 발생을 보존하면서 마스크를 지도로 사용하여 여러 주제를 무시하는 단순하면서도 효과적인 동시 발생 및 주의 메커니즘을 제안한다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Preliminary: Text-to-Video\n' +
      '\n' +
      '비디오 확산 모델(VDM)[25, 14, 15]는 고정된 길이의 마르코프 체인의 역 과정과 유사한 반복적 변성 프로세스에 따라 무작위로 샘플링된 가우시안 노이즈 \\(\\epsilon\\)를 점진적으로 변성시켜 비디오를 생성한다. 이러한 반복적 데모징은 VDM이 비디오 데이터에 제시된 시간적 의존성을 캡처할 수 있게 한다. 구체적으로, 비디오 확산 모델 \\(\\theta\\)는 \\(t\\in\\{1,2,\\cdots,T\\}\\)가 있는 텍스트 조건 \\(c\\)을 감안할 때 각 타임스팟 \\(t\\)에서 추가된 노이즈를 예측한다. 따라서 이 과정에 대한 훈련 목표는 재구성 손실로 나타낼 수 있다.\n' +
      '\n' +
      '{\\bf{z}}\\mathbf{z}}\\mathf{z}_{t}\\mathcal{T}(c),\\right)\n' +
      '\n' +
      '\\(\\mathbf{z}\\in\\mathbb{R}^{B\\tcer L\\t 기간에는 D\\)는 배치 크기 \\(B\\), 비디오 길이 \\(L\\), 높이 \\(H\\), 폭 \\(W\\), 잠재 치수 \\(D\\)로 입력된 동영상의 잠재 코드이다. 모형으로부터의 노이즈 예측(\\epsilon_{\\theta}\\)은 모델로부터의 노이즈 예측이다. \\(\\epsilon_{\\theta}\\)이다. (\\mathcal{T}\\)은 미리 학습된 텍스트 인코더이다. \\(\\mathcal{T}\\)는 미리 학습된 텍스트 인코더이다. (\\mathbf{z}_{t})은 지반 진리 \\(\\mathbf{z}_{t}_{t}=\\alpha_{t}=\\alpha_{t}\\mathbf{z}_{0}_{0}+\\sqrt{1-\\alpha_{t}}^{2}}\\)를 갖는 그라운드 진리(\\mathbf{z}_{t}:\\mathbf{z}_{t}:\\mathbf{z}_{t}:\\mathbf{z}_{t} <\\mathbf{z}_{t}_{t}_{t}_{t}_{t}_{t}{t}_\\sf{z}_\\sf{z}_\\alf{z}_{t}:\\mathbf{z}_{t}_{t}_{t}_{t}_{t}_{t}_{t}_{t}_{t} 이 작업에서 우리는 3D UNet에 구축된 기본 모델로 제로스코프1 T2V 모델을 활용하며 고품질 비디오를 생성하기 위한 공간 및 시간적 모델링을 사용한다.\n' +
      '\n' +
      '폐지 1: [대보강 표면.코/서식/저서화_v2_576w](대보강/서식/저서화_v2_576w)\n' +
      '\n' +
      '멀티 오브젝트가 있는 서비스입니다.\n' +
      '\n' +
      '그림 2는 다중 객체 구동 T2V 맞춤화를 가능하게 하는 제안된 고객 비디오 프레임워크에 대한 개요를 제공한다. 훈련 단계에서 우리는 여러 피험자의 동시 발생을 보장하기 위해 연결 기술을 사용한다. 구체적으로, 우리는 이러한 주제를 단일 이미지로 결합하여 모델에 의한 다중 대상 패턴의 학습을 용이하게 한다. 매우 유사한 피험자 사이에서 얽힘의 문제를 해결하기 위해 주의 제어 메커니즘을 제안한다. 이 메커니즘은 학습 가능한 단어 토큰이 교차 의도 맵 상의 해당 영역과 정렬되도록 보장한다. 이 정렬을 달성함으로써 모델을 통해 서로 다른 주제를 무시하고 생성된 비디오의 품질을 향상시킬 수 있습니다. 훈련 과정은 교과 관련 학습 가능한 단어 토큰을 훈련하고, UNet 아키텍처의 교차 의도 계층 내의 키 및 값 가중치를 트레이닝하여 파라미터 효율적인 핀셋링 접근법을 채택하는 데 중점을 둔다. 추론 단계 동안, 사용자는 그들의 선호도와 정렬된 고품질 비디오를 생성하기 위해 해당 학습된 단어 토큰과 통합된 텍스트 프롬프트 설명만을 제공할 필요가 있다. 이하에서는 고객 비디오의 세부 작업에 대해 설명하겠습니다.\n' +
      '\n' +
      '다주제 구동 T2V 생성의 맥락에서***은 다중주제 구동 T2V 생성의 맥락에서 모델을 일관되게 일관되게 발생 조절*** 동시 발생 제어**를 보장하도록 한다.\n' +
      '\n' +
      '그림 2: ** 제안된 고객 비디오의 개요입니다. 다중 피험자 구동 텍스트 대 비디오 생성을 위한 피험자의 충실도를 보존하기 위해 마스크 안내와 단순하면서도 효과적인 동시 발생 및 주의 관리 메커니즘을 제안한다. 훈련 단계 동안 교차 주의 레이어의 키 및 값 가중치만 미세 조정된다. 추론 단계에서는 학습된 텍스트 토큰과 통합되는 텍스트 프롬프트가 주어지면 특정 피험자와의 양질의 영상을 쉽게 얻을 수 있다.**.\n' +
      '\n' +
      '여러 피험자가 있는 비디오를 생성하는 것이 중요합니다. 이전 작업에서 비디오드림러[3] 저자는 단일 주제 이미지와 다중 피험자의 연결된 이미지를 모두 사용하여 모델을 미세 조정함으로써 다중 피험자 구동 생성을 위한 디스믹스 전략을 제안했다. 그러나 우리는 이 혼합 전략이 단일 이미지에 존재하는 피험자의 일관성이 없는 수로 인해 모델을 혼동할 수 있음을 확인했다. 결과적으로, 다중 주제(그림 5의 \'단일 및 대화\' 라인 참조)가 있는 비디오 생성이 불안정하게 된다. 우리의 접근법에서 우리는 여러 피험자의 연결된 이미지만을 사용하여 모델을 미세 조정하면 생성된 비디오에서 여러 피험자의 동시 발생을 보장하기에 충분하다는 것을 발견했다. 또한 배경 없이 명확한 교과를 제공하는 것이 대상자의 구체적인 특성을 보다 효과적으로 학습하는 데 모델을 보조한다는 것을 관찰했다. 이를 달성하기 위해 교과 이미지에 대한 배경 제거를 수행한다. 이는 수동으로 또는 SAM 모델[11]과 같은 자동 도구를 사용하여 달성될 수 있다.\n' +
      '\n' +
      '생성된 비디오에서 여러 피험자의 동시 발생을 방지하는 관심은 이미지 연결을 통해 달성된다. 다만, 더 어려운 과제는 간섭 없이 교과별 뚜렷한 특성을 보존하고 있다. 모델을 연결된 이미지로 미세 조정하기만 하면 대상 특성 간의 혼란을 초래할 수 있다. 그림 5(온라인 \'w/o fin. attn)\'에서 볼 수 있듯이 생성된 고양이는 제공된 고양이와 유사한 색상 질감을 가지고 있음에도 불구하고 제공되는 개 샘플의 모양을 주로 닮았다. 따라서, 복수의 주제를 무시하는 것은 각 주체의 특성을 충실히 나타내는 고품질 영상을 생성하는 데 중요해진다.\n' +
      '\n' +
      '우리의 접근법에서 우리는 확산 모델의 교차 의도 계층에서 작동하는 각각의 특정 주제를 나타내기 위해 학습 가능한 텍스트 토큰을 사용한다. 교과 학습 과정을 효과적으로 조절하기 위해 교차의도 지도를 직접 활용할 수 있다. 그림 2에 도시된 바와 같이, 우리는 자동 세그먼터를 사용하여 각 주제에 대한 지상 진리 마스크 \\(\\mathbf{\\mathcal{M}}^{p}\\)를 얻었으며, 이는 피험자의 공간적 위치를 나타낸다. 훈련 단계 동안, 주어진 텍스트 프롬프트에서 각 단어의 액티베이션을 사용하여 교차 의도 계층에서 교차 선택 지도 \\(\\mathbf{\\mathcal{A}\\)를 추출한다. \'\\(<\\) 뉴1\\(>\\)\'와 같은 각각의 학습 가능한 단어 토큰에 대하여, 다음과 같이 손실 기능을 가진 대상 영역과 지상 진실 주제 마스크의 정렬을 장려하여 교차 의도 지도 \\(\\mathbf{\\mathcal{A}\\)에 해당하는 영역을 향상시킨다.\n' +
      '\n' +
      '}}\\mathbf{{i}}^{f{{i}^{{2}\\\\\\{2입니다.\n' +
      '\n' +
      'H\\(N\\)가 피험자 수인 경우, 마스크 \\(\\mathbf{\\mathcal{M}}^{p}\\)의 해당 교과 면적이 값 1로 채워지고 나머지 영역은 0으로 채워지며, 이 긍정적인 주의 메커니즘을 사용하여 모델이 정확한 주제 영역에 더 많은 관심을 할당하여 해당 교과 특성에 대한 효과적인 학습이 이루어지게 된다.\n' +
      '\n' +
      '위에서 언급한 긍정식 주의 메커니즘은 교과별 특정 특성 학습을 효과적으로 향상시킨다. 그러나 특히 관련 없는 영역이 존재할 때 생성된 피험자에 대한 문제가 여전히 있을 수 있다. 예를 들어, 그림 5(온라인 \'w/o 부정. attn)\'에서 생성된 개의 다리가 주어진 고양이의 색상 정보에 영향을 받는다는 것을 관찰할 수 있으며, 이는 바람직하지 않다. 이 문제를 해결하기 위해 대상 외부의 영역을 고려하여 부정적인 안내를 소개합니다. 지상 진리 주제 마스크 내의 긍정적인 지침 외에도 \\(\\eta\\)로 표시되는 작은 음의 값을 마스크 \\(\\mathbf{\\mathcal{M}}^{p}\\) 내의 피험자 외부의 영역에 통합한다. 그런 다음 \\(\\mathbf{\\mathcal{M}}^{[p,n]}\\)로 표시되는 이 변형된 마스크를 Eq에 사용한다. (2) 교과 학습 과정을 규제하기 위한 것이다. 부정적인 지도를 통합하여 관련 없는 영역의 문제를 완화하고 생성된 대상자의 충실도를 향상시킬 수 있다.\n' +
      '\n' +
      '모범훈련과 인솔.\n' +
      '\n' +
      '훈련 전략 동안, 우리는 모든 교차 주의 계층에서 키와 값의 가중치를 훈련 가능하게 함으로써 매개변수 효율적인 미세 조정 전략을 사용한다. 이전 텍스트 대 이미지 개인화[13, 14]에 의해 영감을 받아 생성된 비디오의 다양성을 개선하고 언어 드리프트 문제를 완화하기 위해 수업별 사전 보존을 수행한다. 사전 보존에 대한 손실은 다음 각 호와 같이 수립된다.\n' +
      '\n' +
      '}\\math{{}\\math{{{.\n' +
      '\n' +
      '\\(\\mathbf{z}^{\\prime}\\)는 입력 클래스 이미지의 잠재 코드이다. \\(\\mathbf{z}^{\\prime}\\)이다. (\\epsilon^{\\prime}_{\\theta}\\) 모델은 노이즈 예측(\\theta\\)이고 \\(\\epsilon^{\\prime}\\)는 무작위로 샘플링된 가우시안 노이즈이다. (c^{\\prime}\\)은 클래스 이미지의 텍스트 조건이고 \\(t^{\\prime}\\)는 샘플링 타임스탬프이다. 이를 위해, 우리는 다음과 같은 전반적인 훈련 목표와 함께 종단 간 방식으로 고객 영상을 훈련한다.\n' +
      '\n' +
      '메스칼{L}.\n' +
      '\n' +
      'Ex(\\alpha\\)와 \\(\\beta\\)는 각각 주의 조절 및 사전 보존의 가중치를 제어하기 위해 두 개의 초모수이다.\n' +
      '\n' +
      '추론 과정에서 고객 영상에는 필요한 동영상을 생성하기 위해 통합된 대응하는 학습된 단어 토큰이 있는 특정 텍스트 프롬프트만 필요하다. 주체의 근거 진실 마스크는 추론에서 요구되지 않는다는 점에 유의한다. 입력으로서 텍스트 프롬프트만 있으면, 제안된 고객 비디오는 피험자의 충실도와 운동 평활도를 잃지 않고 다양성이 높은 비디오를 생성할 수 있다.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '다타셋은 69명의 개별 피험자가 있는 데이터셋 고객스튜디오를 수집하고 다중 피험자 구동 T2V 생성을 위한 57개의 의미 있는 쌍을 구성한다. 이 과목들은 드림보트[13], 고객확산[11] 및 인터넷으로 조정됩니다. 물체는 애완동물, 사람, 장난감, 교통, 장면, 식물, 웨어러블 아이템 등을 포함한 광범위한 범주를 포괄한다. 각 한 쌍의 피험자는 10개의 상이한 텍스트 프롬프트가 있으며, 이는 상이한 컨텍스트, 액션 등으로 설계된다.\n' +
      '\n' +
      '실행 디테일을 구현합니다. 애덤W[11] 최적화기(배치 크기 2, 학습률 4e-5, 체중 붕괴 1e-2)로 500단계로 맞춤형 비디오를 훈련합니다. 클래스별 사전 보존을 위해 각 주제에 대해 LAION-400M[12]에서 200개의 클래스 이미지를 수집한다. 해당 클래스 이미지도 훈련 단계에서 연결된다는 점에 유의한다. 마스크(\\mathbf{\\mathcal{M}}\\)의 음의 값 \\(\\eta\\)은 -1e-8로 설정되며, Eq의 중량 매개변수 \\(\\alpha\\)와 \\(\\beta\\)로 설정된다. (4)는 각각 0.1 및 1.0으로 설정된다. 추론 과정에서 DDIM[23] 샘플러와 분류기가 없는 안내[14]로 데노징하는 50단계를 수행한다. 생성된 24프레임 비디오의 해상도는 8fps의 \\(576\\번호 320\\)이다. 흥미로운 사실은 낮은 해상도 제로 스코프 T2V 모델에서 학습된 학습된 가중치 및 단어 토큰이 추가로 훈련 연산 비용이 필요하지 않은 \\(1024\\t iter 576\\) 해상도로 개인화된 비디오를 생성하기 위해 고해상도 12에 직접 로드될 수 있음을 발견했다. 저희 고객 영상은 디퓨저[22]를 기반으로 구현됩니다. 1 RTX 3090 GPU에서 피사체 쌍은 훈련 단계가 약 8분 걸린다. 한편, 1 RTX 3090 GPU에서 각각 저해상도 동영상 생성에는 약 1분 3분이 소요된다.\n' +
      '\n' +
      '폐지 2: [대보강 표면.코/서식/저서식_v2_XL][대보강 표면.코/서식/저서식/제보경_v2_XL)].\n' +
      '\n' +
      '비디오드림러[1]를 제외한 비교 방법은 또한 이전 SOTA 이미지 기반 다중 피험자 구동 방법을 드림보스[15] 및 고객확산 [12]를 포함한 비교를 위해 비디오 시나리오에 적응하는 것을 고려한다. 드림보드는 확산 모델에서 모든 가중치를 훈련시켜 여러 피험자로부터 생성을 개인화시킨다. 맞춤 확산 모델은 여러 피험자에 대한 공동 훈련을 통해 확산 모델을 미세 조정했다. 비디오드림러는 T2V 모델의 다중 객체 맞춤화를 위한 디스믹스와 인간-인-루프 미세 조정 전략을 제안한다. 공정 비교를 위해 제노스코프 모델을 기반으로 하는 모든 비교 방법과 고객 영상을 구현합니다.\n' +
      '\n' +
      '이전 작품 [24, CLIP [16] ViT-B/32[17] 이미지와 텍스트 모델 간의 생성된 프레임과 텍스트 프롬프트_의 평균 코사인 유사성을 계산하고 CLIP [17] 이미지 및 텍스트 모델 사이의 텍스트 프롬프트__CLIP 이미지 정렬_은 CLIP ViT-B/32 모델 사이의 평균 코사인 유사성을 계산하며, 2) CLIP 이미지[1] ViT-B/32 이미지와의 평균 코사인 유사성을 계산하며, 2) CLIP 이미지 및 주제 이미지 사이의 평균 코사인 유사성을 계산하며, 2) CLIP 이미지 및 텍스트 프롬프트_CLIP 이미지 및 텍스트 프롬프트_CLIP 이미지 및 텍스트 프롬프트_CLIP 이미지와의 평균 코사인 유사성을 계산하며, 2) CLIP 이미지[20]은 생성된 프레임과 텍스트 프롬프트_CLIP 이미지 사이의 평균 코사인 유사성을 계산하며, 2) CLIP 이미지 및 기준 이미지 사이의 평균 코사인 유사성을 CLIP 이미지 및 주제 이미지 사이의 평균 코사인 유사성을 계산하며, 2) CLIP 이미지 및 기준 이미지 사이의 평균 코사인 유사성을 CLIP 이미지(2) CLIP 이미지 및 기준 이미지 사이의 평균 코사인 유사성을 CLIP 이미지[2) CLIP\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Method & CLIP-T \\(\\uparrow\\) & CLIP-I \\(\\uparrow\\) & DINO-I \\(\\uparrow\\) & T. Cons. \\(\\uparrow\\) \\\\ \\hline \\hline DreamBooth & 0.6476 & 0.6015 & 0.3098 & 0.7209 \\\\ CustomDiffusion & 0.6581 & 0.6218 & 0.3186 & 0.7548 \\\\ VideoDreamer & 0.6649 & 0.6314 & 0.3471 & 0.7362 \\\\ \\hline CustomVideo (ours) & **0.7051** & **0.6749** & **0.3955** & **0.8142** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '드림보스, 고객확산 및 비디오꿈터를 포함한 SOTA 방법***와 비교하여 고객 비디오의 표 1: ** 정량적 결과는 다음과 같다. 제안된 고객 비디오는 4가지 평가 메트릭 모두에 대해 이전 SOTA 방법을 일관되게 능가한다.\n' +
      '\n' +
      '그림 3: ** 당사의 고객 비디오의 정성적 결과는 드림보드와 고객확산 및 비디오드림어를 포함한 SOTA 방법**와 비교된다. 제1 라인은 주어진 피사체를 나타내는 반면, 각 라인은 해당 방법으로 생성된 프레임을 나타낸다. 하단은 추론 중에 사용되는 텍스트 프롬프트입니다. 우리는 고객 영상이 이전 SOTA 방법에 비해 피험자의 충실도가 훨씬 더 나은 비디오를 생성할 수 있음을 관찰할 수 있다.\n' +
      '\n' +
      '생성된 비디오에서 모든 연속 프레임 쌍이 생성된다.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '** 정량적 결과** 수집된 데이터 세트에 대한 정량적 실험을 수행한다. 철저한 분석을 위해 각 피험자 쌍에 대해 10개의 개별 프롬프트와 4개의 랜덤 종자를 사용하여 비디오를 생성하여 각 방법에 대해 총 2,280개의 생성된 비디오를 생성한다. 그런 다음 4개의 메트릭을 사용하여 생성된 비디오의 품질을 평가하고 결과는 표 1에 나와 있으며, 표는 제안된 방법이 주어진 피험자와 더 잘 정렬된 비디오를 생성할 수 있음을 분명히 보여준다. 이러한 개선은 특별히 설계된 동시 발생 및 주의 조절 메커니즘으로 인해 피험자의 충실도를 효과적으로 방해하고 보존하기 때문일 수 있다. 특히, 당사의 고객 비디오는 _CLIP 텍스트 정렬_, _CLIP 이미지 정렬_ 및 _DINO 이미지 정렬_ 측면에서 가장 최근의 방법인 비디오 스트리밍기를 각각 6.05%, 6.89% 및 13.94% 능가한다. 또한, 고객 비디오는 표 1에 표시된 바와 같이 최첨단 방법에 비해 시간적 일관성이 상당히 높은 비디오를 생성하며, 예를 들어 맞춤형 비디오 비디오는 _시간적 컨시스트리티_ 측면에서 비디오 다이버를 10.59% 능가한다.\n' +
      '\n' +
      '*** 정성적 결과***의 질적 비교 결과를 그림 3에 제시했는데, 이러한 결과는 고객 비디오 접근법이 여러 피험자의 동시 발생을 효과적으로 보장하고 다양한 피험자를 성공적으로 무시한다는 것을 관찰했다. 그러나 드림보드와 고객확산 모두 실험에 제공된 자동차의 구조적 색상 정보를 포착하지 못한다. 자동차 창에서 우세한 블랙 컬러가 자동차 전체를 가려져 충실도가 낮아진다. 더욱이, 비디오드림러의 생성된 프레임들은 일부 프레임들이 두 대의 자동차를 묘사하는 반면 다른 프레임들은 하나의 차만을 나타내기 때문에 일관성이 부족하다. 또한 영상드림러는 자동차 도어의 색상과 같은 정확한 색상 정보를 포착하지 못한다. 대조적으로, 당사의 고객 비디오 방법은 이러한 도전 장면 및 전경 주제 시나리오를 처리하는 데 탁월하여 비디오 생성을 위해 제공된 자동차의 복잡한 구조적 세부 사항을 효과적으로 캡처한다. 마찬가지로 \'캣\'과 \'독\'의 경우를 고려할 때 우리의 접근 방식은 고품질의 비디오를 생성하는 데 탁월한 능력을 보여준다.\n' +
      '\n' +
      '*** 인간 준비 연구*** 방법을 추가로 검증하기 위해 3가지 SOTA 방법과 비교하여 고객 비디오에 대한 인간 평가를 수행한다. 이 연구에서 우리는 다음과 같은 질문으로 25명의 독립적인 인간 비준자로부터 900개의 답을 수집했으며, 1) 텍스트에 가장 잘 정렬되는 답변을 수집합니까? 2) 어떤 것을 주제 이미지에 가장 잘 정렬합니까? 3) 전체 품질이 가장 좋은가요? 결과는 그림 4에 나와 있으며, 제안된 고객 비디오는 이 모든 사람의 인간 측면에서 가장 선호하는 것임을 알 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Method & CLIP-T \\(\\uparrow\\) & CLIP-I \\(\\uparrow\\) & DINO-I \\(\\uparrow\\) & T. Cons. \\(\\uparrow\\) \\\\ \\hline \\hline w/o remove bg & 0.6676 & 0.6248 & 0.3498 & 0.7891 \\\\ w/o concat & 0.6189 & 0.6371 & 0.3572 & 0.7936 \\\\ both single and concat & 0.6148 & 0.6319 & 0.3208 & 0.7983 \\\\ w/o pos. attn. & 0.6858 & 0.6402 & 0.3569 & 0.8015 \\\\ w/o neg. attn. & 0.6926 & 0.6392 & 0.3694 & 0.8075 \\\\ \\hline CustomVideo (ours) & **0.7051** & **0.6749** & **0.3955** & **0.8142** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '제안된 고객 비디오의 구성 요소 분석***에 대한 표 2: ** 정량적 결과는 다음과 같다. 우리는 피험자가 연결되거나 배경 제거를 제거할 때 상당한 성능 저하를 관찰한다. 실제로 우리는 가장 좋은 결과를 얻기 위해 긍정적이고 부정적인 주의 메커니즘을 동시에 사용하는 것을 제안한다.\n' +
      '\n' +
      '그림 4: ** 사용자 연구***입니다. 우리의 고객 비디오는 _Textual Alignment_, _Image Alignment_ 및 _전반적으로 품질_ 측면에서 3개의 SOTA 비교 방법과 비교하여 최고의 인간 선호도를 달성한다.\n' +
      '\n' +
      '제안된 고객 비디오의 구성 요소 분석***에 대한 그림 5: ** 정성적 결과는 다음과 같다. 우리는 훈련 중 연결 과목을 보장하는 것이 생성된 영상에서 동시 발생을 보장하는 데 효과적이라는 것을 발견했다. 또한, 우리의 주의 메커니즘은 다른 주제를 무시할 수 있다.\n' +
      '\n' +
      '3차원 평가.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '우리는 2가지 측면: 1)에서 우리의 방법에 대한 절제 분석을 수행하며, 우리 방법에서 각 구성요소의 효과; 2) 주의 조절 메커니즘에서 하이퍼 파라미터의 효과를 수행한다.\n' +
      '\n' +
      '구성 요소 분석에서는 그림 5와 표 2에서 정성적 및 정량적 결과를 모두 제시하여 고객 비디오에서 각 구성 요소에 대한 철저한 분석을 수행하며, 이 연구 결과는 각 구성요소의 중요성과 효과에 대한 귀중한 통찰력을 제공한다. 한 가지 중요한 관찰은 미세 조정 과정에서 동시 발생을 보장하는 것의 중요성이다. 여러 과목이 하나의 이미지(표 2의 \'w/o 콘트라트\', \'맞춤 비디오\'로 연결되지 않을 때 성능이 크게 감소한 것을 알 수 있다. 피험자 연결의 부재는 미세 조정 시 하나의 단일 주체의 지배로 이어지며, 이는 용납할 수 없는 것으로 판단된다(그림 5의 라인 \'w/o 콘타트\'(온라인 \'w/o 콘트라스트). 더욱이 단일 교과와 연결 피험자 모두 확산 모델을 미세 조정하여 단일 교과가 해당 특성을 배우는 데 도움이 될 수 있는지 조사하기 위해 절제 연구를 수행한다. 놀랍게도, 결과는 훈련 과정의 단일 대상자를 추가하는 것이 해롭다는 것을 입증하여 추론 중 일관성이 없는 생성(그림 5의 \'단일 및 대화\' 라인)을 초래한다는 것을 보여준다. 또한, 주어진 주제 이미지로부터 배경을 제거하는 것이 고품질 비디오의 생성을 크게 향상시킨다는 것을 관측한다. 배경을 제거함으로써 확산 모형은 주어진 교과목의 특성 학습에만 집중할 수 있다. 배경 제거 유무에 관계없이 생성된 비디오 간의 비교는 이 구성요소가 부족한 비디오의 단조로운 특성(그림 5의 라인 \'w/o 제거 bg)\'을 명확하게 보여준다. 우리의 주의 제어 메커니즘은 생성된 비디오를 주어진 피험자와 정렬하는 데 중요한 역할을 한다. 특히 긍정적인 주의 안내는 표 2에서 알 수 있듯이 _CLIP 이미지 정렬_ 메트릭을 5.42% 개선하며, 부정적인 주의 안내는 더 나은 이미지 정렬을 촉진하는 데 도움이 되는 것으로 입증되었다. 더욱이, 주의 제어 메커니즘은 비디오 생성의 중요한 요소인 생성된 비디오의 시간적 일관성을 상당히 향상시킨다. 부정적인 주의 안내 없이 생성된 비디오와 부정적 주의 안내 없이 생성된 비디오의 비교는 두 구성 요소(w/o fin. attn)와 \'w/o 부정\'의 긍정적인 영향을 분명히 보여준다. 긍정적인 지도는 고양이와 강아지를 구별하는 것과 같은 주체의 고유한 특성을 구체적으로 보존하는 반면, 부정적 지도는 다른 과목이 특정 주제에 미치는 영향을 약화시킨다. 긍정적인 안내와 부정적인 안내를 모두 통합함으로써, 우리의 고객 비디오는 높은 주제 충실도와 놀라운 시간적 일관성을 가진 비디오를 생성하는 데 탁월합니다.\n' +
      '\n' +
      '의도 통제에서 하이퍼-파라미터의 효과 외에도 구성 요소 분석 외에도 주의력 감소의 무게(\\(\\alpha\\))와 안내 마스크(\\(\\mathbf{\\mathcal{M}}\\)에 사용된 음의 값(\\(\\(\\)의 두 가지 중요한 매개변수의 효과에 대한 조사도 수행한다. 이러한 조사의 정량적 결과는 <표 3>과 <표 4>에 제시되어 있으며, 주의력 감소의 가중치를 고려할 때 \\(\\alpha\\)가 0.1로 설정되었을 때 최상의 성능을 달성한다는 것을 관찰하며, 이 값은 생성된 비디오와 주어진 피험자 간의 최적 정렬을 초래하여 훈련 중 주의 지도를 적절하게 균형을 맞추는 것의 중요성을 나타낸다. 안내 마스크의 음의 값에 대해, 우리의 연구 결과는 약간의 음의 값이라도 T2V 생성의 품질을 향상시키기에 충분하다는 것을 보여준다. 이는 부정적인 주의 지도를 통합하는 것이 특정 주제에 대한 다른 과목의 영향력을 효과적으로 억제할 수 있어 생성 품질을 향상시킬 수 있음을 의미한다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '그림 6에서 우리는 실험 중에 발생하는 일부 실패 사례를 제시한다. 초과된 디테일을 포착하는 것이 어려워지는 천과 테디곰과 같이 매우 유사한 주제를 생성할 때 한 가지 문제가 발생한다. 또한, 우리의 방법은 기본 모델의 능력에 의존하기 때문에 베이스 모델이 작은 얼굴과 같이 생성할 수 없다면 실패할 것이다. 고장 사례를 해결하기 위해 Stable Viffusion[1]과 같이 더 강한 T2V 베이스 모델을 사용하여 맞춤형 비디오를 개선할 수 있다. 또한, 훈련 중 주어진 이미지에서 누운 특성을 우선시하고 강조하기 위한 메커니즘을 구현할 수 있다.\n' +
      '\n' +
      '## 5 Conclusions\n' +
      '\n' +
      '본 논문은 단순하면서도 효과적인 동시 발생 및 주의력 조절 메커니즘으로 구동되는 다중 대상 구동 T2V 생성을 위한 새로운 프레임워크 고객 비디오를 제공한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\(\\eta\\) & CLIP-T \\(\\uparrow\\) & CLIP-I \\(\\uparrow\\) & DINO-I \\(\\uparrow\\) & T. Cons. \\(\\uparrow\\) \\\\ \\hline \\hline -1e-5 & 0.6891 & 0.6582 & 0.3917 & 0.7996 \\\\ \\(\\downarrow\\)-1e-8 & **0.7051** & **0.6749** & **0.3955** & **0.8142** \\\\ -1e-11 & 0.6918 & 0.6627 & 0.3895 & 0.8079 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 음의 값의 정량적 결과를 보여준다. 흥미롭게도 \\(\\eta\\)=-1e-8은 고려된 모든 메트릭 측면에서 최상의 결과를 얻을 수 있다. 실제로, 우리는 모든 실험에서 이 설정을 채택한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\(\\alpha\\) & CLIP-T \\(\\uparrow\\) & CLIP-I \\(\\uparrow\\) & DINO-I \\(\\uparrow\\) & T. Cons. \\(\\uparrow\\) \\\\ \\hline \\hline\n' +
      '0.6784 & 0.6651 & 0.8015 \\\\ 0.6784 & 0.6651 & 0.8015 \\\\ & 0.6784 & 0.6651 & 0.8015 \\\\ & 0.6651 & 0.8015.\n' +
      '0.3955** & **0.8142*** 및 **0.6751** & **0.7051**0.1 & **0.0.0.\n' +
      '0.01 & 0.6892 & 0.6584 & 0.3817 & 0.8092 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 주의력 감소의 중량에 대한 정량적 결과는 표 3이다. 실제로 \\(\\alpha\\)=0.1은 최상의 결과를 산출하며 모든 실험에서 채택된다.\n' +
      '\n' +
      '그림 6: ** 저희 고객 비디오의 실패 사례입니다. 우리의 고객 비디오는 \'슬롯\', \'티디베어\'(첫 번째 라인)와 같이 매우 유사한 주제의 쌍을 만날 때 피험자의 어떤 미묘한 세부 사항을 포착하지 못할 수 있다. 또한, 우리의 접근법은 \'사람\'과 \'바이크\'(2선)**(2선)와 같은 세계적인 시각으로 생생한 얼굴 콘텐츠를 생성하지 못한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:8]\n' +
      '\n' +
      '아멜랜드 자울린. 자기 지도 비전 변압기의 출현 특성입니다. _ICCV_에서 2021년에는 9650-9660쪽이 있다.\n' +
      '*[Chen _et al._2023a] 해독소 첸, 메나건샤, 예킹허, 용장, 샤오동쿤, 샤오슈 양, 진보싱, 야포앙 류, Qifeng Chen, Xintao 왕 등 양질의 영상 생성을 위한 오픈 확산 모델. arXiv_, 2023.\n' +
      '*[Chen _et al._2023b] 홍텐, 신장 왕, 구닝생, 예펑장, 유웨이 주, 필린 한, 원우 주 등이 있다. Videodreamer: 이젠 믹스 핀셋링으로 맞춤형 다중 대상 텍스트 대 비디오 생성._ix-mix 핀셋링이 가능합니다. arXiv_, 2023.\n' +
      '*[도보비츠키_et al._2021] 알렉시 도소비츠키, 루카스 베이어, 알렉산더 칼레스니코프, 디르코우 바이세노프, 샤오하아 지하이, 토머스 유니터너, 마토스타파 데헤하니, 마토스타인 민더러, 게르그 허폴드, 실베인 젤리, 작보우즈키트, 닐 하울스비 및 네오일 호울스비. 이미지는 16x16 단어, 즉 스케일에서의 이미지 인식을 위한 트랜스포머이다. 2021년 _ICLR_에서.\n' +
      '*[도안 _et al._2023] 중지 도안, 리주 유, 선규 왕, 선 선우, 지정 우, 위닝 시안, 준황, 파이 차오, 루롱 지 등이 있다. 디퍼시네트: 현실감 있는 비디오 합성을 위한 라텐트 인 톨레이션 디클릭. __ 현실감 있는 비디오 합성을 위한 라텐트 인티브 디클릭입니다. arXiv_, 2023.\n' +
      '*[Esser _et al._2023] 패트릭 에셔, 존타난 치루, 파미다 아티게히히안, 조나단 젠프코그 및 아나스타시스 독일디스이다. 확산 모델을 사용한 구조 및 콘텐츠 유도 비디오 합성입니다. _ICCV_에서 7346-7356 페이지는 2023년이다.\n' +
      '*[Gal _et al._2023] 리논 갈, 유발 알알루프, 유발 아츠몬, 오르 파타슈니크, 아미트 하임 버마노, 갈 체치크, 다니엘 코헨 등이 있다. 이미지는 텍스트 역전을 사용하여 텍스트 대 이미지 생성을 개인화하는 한 마디의 가치가 있다. 1974년 _ICLR_에서.\n' +
      '* [Geyer _et al._2023] 미칼 게이어, Omer Bar-Tal, Shai Bagon 및 탈리 데켈. 토큰 흐름: 일관된 비디오 편집을 위한 지속적인 확산 특징. __토큰 흐름: 일관된 비디오 편집을 위한 지속적인 확산 특징. arXiv_, 2023.\n' +
      '*[Gugger _et al._2022] 실바인 구거, Lysandre Deger, 토머스 볼프, 필리 슈미트, 자차 뮬러, Sourab 망룰카르, Marc Sun, 벤자민 보산 등이다. 규모의 훈련과 추론은 단순하고 효율적이고 적응 가능하게 하였다. (https://github.com/huggingface/accelerate)\n' +
      '*[구_et al._2023] 유웨이 구오, 세위안 양, 안이 라오, 야의왕, 유샤오, 다후린, 보다이가 있다. 애니메이션: _Mimatediff: 특정 튜닝 없이 개인화된 텍스트 대 이미지 확산 모델을 활용하십시오. arXiv_, 2023.\n' +
      '*[He _et al._2022] 예칭 하이, 톈유 양, 용장, 예잉 샹, 키펑 첸. 임의의 길이를 갖는 고충성 비디오 생성을 위한 중간 비디오 확산 모델 __의 최적 비디오 확산 모델은 임의의 길이를 갖는 고충성 비디오 생성을 위한 것이다. arXiv_, 2022.\n' +
      '*[호, 살리만2022] 조나단 호와 팀 살리만스. 고전기가 없는 확산 안내. _클래스 무확산 안내. _클래스-프리 확산 안내. __클래스-프리 확산 안내. arXiv_, 2022.\n' +
      '*[호 _et al._2020] Jonathan Ho, 아주야 Jain, Pieter Abbeel. 덴노징 확산 확률적 모델 __데노징 확산 확률적 모델. _<덴도징 확산 확률적 모델. NeurIPS_, 33:6840-6851, 2020.\n' +
      '*[홍 _et al._2022] 위니 홍, 명딩, 위디 정, 쉬한 류, 지 탕. 프로덕션 비디오: 변압기를 통한 텍스트 대 비디오 생성을 위한 대규모 전처리: __영상 생성이다. arXiv_, 2022.\n' +
      '*[Hu _et al._2022] 에드워드 J Hu, 윔드 사이언, 필립 월리스, 제유안 알렌 주, 원히 리, 시안 왕, 루왕, 위즈후 첸 등이다. LoRA: 대형 언어 모델의 낮은 순위 적응입니다. 2022년 _ICLR_에서.\n' +
      '*[지앙 _et al._2023] 장씨, 톈싱우, 샤이양, 선양시, 다후이린, 유샤오, 첸 변화 로이, 지웨이 류. 이미지 프롬프트가 있는 __ideobooth: Diffusion 기반 비디오 생성. _ideobooth: Diffusion 기반 비디오 생성. arXiv_, 2023.\n' +
      '* [Khachatryan _et al_2023] Levon Khachatryan, Andranik Movsisan, Vahram Tadevosan, 로베르토 Henschel, Zhangyang 왕, Shant Navasardyan 및 Humphrey Shi. 텍스트2video-제로: 텍스트-투-이미지 확산 모델은 제로 샷 비디오 생성기이다. __ 텍스트-투-이미지 확산 모델. ICCV_, 2023.\n' +
      '*[키릴로프 _2023] 알렉산더 키릴로프, 에릭 민틀런, 니켈라 라비, 한지 마오, 차에 롤란드, 로라 구스타프슨, 테타 샤오, 스피서 화이트헤드, 알렉산더 크릭, 완예엔 Lo, 푸엇 데일리, 로스 기르슈크 등이 있다. 뭐든지 물어봐. __분담. _E. ICCV_, 2023.\n' +
      '*[당다육 _et al_2023] 단돈다르트육, 리준유, 시례구, 호세 레자마, 조나단황, 라첼 호둥, 하트위그 아담, 하산 아크리리, 야어 알론, 비하네스 비로다르 등 0샷 영상 생성을 위한 대형 언어 모델. arXiv_, 2023.\n' +
      '*[금리 _et al._2023] 누푸라 쿠아리, 빙리랑 장, 리처드 장, 에리 셰흐트만, 준유안 주. 텍스트 대 이미지 확산의 다중 개념 관습화이다. _CVPR_에서 1931-1941 페이지는 2023년이다.\n' +
      '*[Liu _et al._2023a] 자이정 류, 루실리 펑, 카이 주, 요피 장, 케청 정, 유 류, 델리 자오, 진렌 주, 양 카오. 맞춤형 생성을 위한 확산 모델의 체스: 개념 뉴런은 맞춤형 생성을 위한 확산 모델의 개념 뉴런이다. __ ICML_, 2023.\n' +
      '*[Liu _et al._2023b] 지청 류, 야피 장, 유준 선, 케청 정, 카이 주, 루실리 펑, 유 류, 델리 자오, 진렌 주, 양 카오. 블록 2: 다중 피험자가 있는 맞춤형 이미지 합성: __톤 2: 다중 피험자가 있는 맞춤형 이미지 합성. NeurIPS_, 2023.\n' +
      '* [로쉬칠로프와 후터2017] 이라이아 로쉬칠로프와 프랑크 홉터. 감소된 체중 붕괴 규칙화 __ 감소 체중 붕괴 규칙화. __ 감소 체중 붕괴 규칙화. 2017년arXiv_.\n' +
      '*[루 _et al._2022] 청루, 유하오 주, 판바오, 지안페이 첸, 충수안 리, 준주 등이 있다. Dpm-솔버: 약 10 단계에서 확산 확률 모델 샘플링을 위한 빠른 오드 솔버: 약 10 단계에서 확산 확률 모델 샘플링을 위한 빠른 오드 솔버입니다. NeurIPS_, 2022년 35:5775-5787입니다.\n' +
      '*[Qi _et al._2023] 첸양키, 샤오동쿤, 용장, 선양리, 신타오왕, 예잉샹, 키펑첸 등이다. 페이트제로: 제로 샷 텍스트 기반 비디오 편집에 대한 표시를 사용한다. __ 영샷 텍스트 기반 비디오 편집에 대해 언급한다. ICCV_, 2023.\n' +
      '*[라드포드 _et al_2021] 알레크 라드포드, 종욱 김, 크리스 홀리스, 아디아 레즈, 가브리엘 고, 샌히니 아가왈, 기리시 사스트리, 아미다 아셀, 파멜라 미슈킨, 잭 클락 등 자연 언어 감독으로부터 시각적 모델을 전수할 수 있다. _ICML_에서 2021년 페이지 8748-8763.\n' +
      '\n' +
      '*[Rombach _et al._2022] 로빈 라이바흐, 안드레아스 블라트만, 도미니크 로렌츠, 패트릭 에셔 및 보존 오머입니다. 잠재 확산 모델을 사용한 고해상도 이미지 합성입니다. _CVPR_에서 10684-10695 페이지는 2022년이다.\n' +
      '*[Ruiz _et al._2023] 나타니엘 루즈, 원안전 리, 바룬 탬파니, 야엘 프릿치, 마이클 루비타틴, Kfir Aberman. 드림보스: 주제 중심 생성을 위한 텍스트 대 이미지 확산 모델입니다. _CVPR_에서 22500-22510 페이지는 2023년이다.\n' +
      '* [슈하만 _et al._2021] 크리스토프 슈하만, 리처드 베르누, 로메인 베아몬트, 로베르트 카카마키크, 클레이튼 풀리스, 아루시 카타, 테오 코바스, 제니아 지트세프, 아란 코마쓰자키 등이 있다. 라온-400m: 클립 필터링된 4억 이미지-텍스트 쌍의 오픈 데이터셋. __ 클립 필터링된 4억 이미지-텍스트 쌍. 2021년arXiv_, 2021.\n' +
      '*[가수 _et al._2023] 우리엘 싱어, 아담 폴리악, 토마스 헤이즈, 시인, 지안, 송양 장, 기위안 후, 해리 양, 오론 애시얼, 오리 가프니, 데비 파라크, 시날 구파타, 옌비 타이그만 등이 있다. 비디오: 텍스트 비디오 데이터가 없는 텍스트 대 비디오 생성입니다. 1974년 _ICLR_에서.\n' +
      '* [소코르코도프 _et al._2022] 이반 스소록호도프, 세르게이 투르사코프, 모하메드 엘호선 등이 있다. Stylegan-v: 스타간2의 가격, 이미지 품질 및 포크를 가진 연속적인 비디오 생성기. _CVPR_ 페이지 3626-3636, 2022.\n' +
      '*[송 _et al._2020] 송, 첸린 멍, 스테파노 에르몬을 지밍한다. 덴노징 확산 암묵 모델 __데노징 확산 암묵 모델. _다노징 확산 암묵 모델. arXiv_, 2020.\n' +
      '*[빌레바스 _et al._2023] 루벤 빌레바스, 모하마드 바비자데, 피에테르-잔 커더만스, 호르난 모랄도, 한장, 모하마드 태기 사파르, 산티아고 카스트로, 줄리우스 쿤제, 두미트루 에르한 등이 있다. 페나키: 오픈 도메인 텍스트 설명에서 주기적인 길이 비디오 생성입니다. 1974년 _ICLR_에서.\n' +
      '*[빈 플라텐 _et al._2022] 패트릭 파틸, 수라즈 파틸, 안톤 로즈호코프, 페드로 쿠넥사, 나탄 램버트, 카시프 로즐, 미시그 다바도르즈, 토머스 볼프 등이다. 첨단 확산 모델: 최첨단 확산 모델[플랫폼://github.com/huggingface/확산자]]. 2022년 (https://github.com/huggingface/diffusers)\n' +
      '*[왕 _et al._2023a] 지운이우 왕, 항지위안, 데이우 첸, 빙야 장, 샤앙 왕, 시웨이 장 등이 있다. 모델 스코프 텍스트-영상 기술 보고서 __모델 스코프 텍스트-영상 기술 보고서. _모델 스코프 텍스트-영상 기술 보고서. arXiv_, 2023.\n' +
      '*[왕 _et al._2023b] 샤앙 왕, 항지위안, 시웨이 장, 데이우 첸, 지누 왕, 유야 장, 유준 선, 델리 자오, 진렌 주. 보데코메이저: 움직임 제어성을 갖는 위치 비디오 합성: 동작 제어성을 갖는 구성 비디오 합성. __ arXiv_, 2023.\n' +
      '*[왕 _et al_2023c] 야의왕, 신유안 첸, 신마, 샹첸 저우, 지창황, 이왕, 세위안양, 진안허, 지아슈오 유, 페이킹양 등 __음음 잠재 확산 모델이 있는 고급 영상 생성. arXiv_, 2023.\n' +
      '*[위_et al._2023] 유지웨이 장, 시웨이 장, 지후 청, 항지위안, 지청 류, 유류, 유류, 유잉야 장, 진렌 주, 홍밍 샨 등이다. 드림비디오. _드림비디오: 맞춤형 주제와 운동으로 꿈의 영상을 작곡합니다. arXiv_, 2023.\n' +
      '*[Wu _et al._2023] 제이 장지 우, 요시아오 거, 신타오 왕, 스탠 웨시안 리, 유차오 구, 유피 시, 웨인 후수, 유잉 샹, 샤오우, 마이크 정슈 등이 있다. 테네-아-비디오: 텍스트 대 비디오 생성을 위한 이미지 확산 모델의 원샷 튜닝입니다. 1998년에 _ICCV_에서.\n' +
      '*[Zeng _et al._2023] 옌멍, 구치앙위이, 지안이정, 자이스신주, 양웨이, 유첸장, 항리 등이 있다. 픽셀 댄스를 만드는 픽셀 댄스: 하이-동적 비디오 생성. __하이-동적 비디오 생성. arXiv_, 2023.\n' +
      '*[장 _et al._2023a] 데이비드 주하오 장, 제이 장지 우, 자아위 류, 리의 자오, 링민 러, 유차오 구, 다피 가오, 마이크 정슈 등이 있다. 쇼-1: 텍스트 대 비디오 생성을 위한 픽셀과 잠재 확산 모델. __ 마르잉 픽셀 및 잠재 확산 모델. arXiv_, 2023.\n' +
      '*[장 _et al._2023b] 시웨이 장, 지유왕, 예야 장, 강자오, 항지위안, 지후 진, 시왕, 델리 자오, 진렌 주 등이 있다. I2vgen-xl: 캐스케이드 확산 모델을 통한 고품질 이미지 대 비디오 합성. arXiv_, 2023.\n' +
      '*[자오 _et al._2023] 하오유 자오, 톈이 루, 자오시 구, 쉬밍 장, 자쿠안 우, 항 주, 유강 장. 비디오 조립기: 확산 모델을 사용하여 참조 엔티티와의 식별-지속 비디오 생성: 확산 모델을 사용하여 참조 엔티티와의 식별-지속 비디오 생성이다. arXiv_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>