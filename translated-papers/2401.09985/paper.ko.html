<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '비디오 개발용 일반 세계 모델\n' +
      '\n' +
      '탈출된 도컨스.\n' +
      '\n' +
      '지웬 루섬 왕1 정주111, 과안 왕1.\n' +
      '\n' +
      '>2개.\n' +
      '\n' +
      '프로젝트 페이지: [https://월드-꿈치-꿈치.githubio](https://월드-꿈치-dithub.githubio)\n' +
      '\n' +
      '부타주 1: 그 저자들은 이 작업에 동등하게 기여했으며, 그 저자들은 이 작업에 동등하게 기여했다. 자청주, 정주, 정주, 지청주, 지청주, 자정주({}^{8888}}\\) 대응 저자:({}^{88}}\\) 동조 저자.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '세계 모델은 영상 생성에 필수적인 세계의 역학을 이해하고 예측하는 데 중요한 역할을 한다. 그러나 기존 세계 모델은 게임이나 운전과 같은 특정 시나리오에 국한되어 일반 세계 동적 환경의 복잡성을 포착하는 능력을 제한한다. 따라서 일반적인 세계 물리학과 운동에 대한 종합적인 이해를 함양하기 위한 선구적인 세계 모델인 월드드림러를 소개하여 영상 생성의 역량을 대폭 향상시킵니다. 큰 언어 모델의 성공으로부터 영감을 얻은 월드드림어 프레임 세계 모델링은 비지도 시각적 서열 모델링 도전으로 구성된다. 이는 시각 입력을 이산 토큰에 매핑하고 마스킹된 토큰들을 예측함으로써 달성된다. 이 과정에서 우리는 세계 모델 내에서 상호작용을 용이하게 하기 위해 다중 모달 프롬프트를 통합한다. 우리의 실험은 월드드림러가 자연 장면 및 운전 환경을 포함한 다양한 시나리오에 걸쳐 비디오를 생성하는 데 탁월함을 보여준다. 월드드림러는 텍스트 대 비디오 변환, 이미지 대 비디오 합성 및 비디오 편집과 같은 작업을 실행하는 데 다양한 기능을 보여준다. 이러한 결과는 다양한 일반 세계 환경 내에서 동적 요소를 포착하는 데 있어 월드드림러의 효과를 강조한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '인공 지능의 다음 중요한 도약은 동적 시각적 세계에 대한 깊은 이해를 가진 시스템에서 나올 것으로 예상된다. 이러한 진보의 핵심은 세계 모델이며, 우리 세계의 역동적인 특성을 이해하고 예측하는 데 중요하다. 세계 모델은 영상 생성에 필수적인 일반 세계에서의 운동 및 물리학 학습에 큰 약속을 갖고 있다.\n' +
      '\n' +
      '세계 모델[19]의 초기 탐색은 주로 게임 시나리오에 초점을 맞추고 있으며, 이는 게임 환경 내에서 공간적 및 시간적 역학의 압축된 표현을 학습할 수 있는 생성 신경망 모델을 제안한다. 꿈거 시리즈 [21, 22, 23]의 후속 연구는 다양한 게임 시나리오에 걸쳐 세계 모델의 효능을 추가로 검증했다. 구조화된 성격과 가장 중요한 중요성을 고려할 때 자율주행은 세계 모델의 실질적인 적용을 위한 최전방 영역이 되었다. 자율 주행 시나리오에서 세계 모델의 효능을 탐색하기 위해 다양한 접근법[31, 32, 49, 50]이 소개된다. 나아가 데이드림어[52]는 실제 로봇 환경을 포괄하기 위해 세계 모델의 적용을 확장했지만, 현재 세계 모델은 주로 게임, 로봇 및 자율주행에 국한되어 일반 세계의 움직임과 물리학을 포착할 수 있는 능력이 부족하다. 또한, 세계 모델의 관련 연구는 시각적 역학을 모델링하기 위해 주로 신장 신경네트웍스(RNN) [20, 21, 22, 23, 35, 42, 52] 및 확산 기반 방법[32, 49, 50]에 의존한다. 이러한 접근법은 비디오 생성에서 약간의 성공을 거두었지만 일반 세계 장면에서 운동과 물리학을 효과적으로 포착하는 데 어려움을 겪고 있다.\n' +
      '\n' +
      '본 논문에서는 영상 생성을 위한 일반 세계 모델 구축을 개척하는 _WorldDreamer_를 소개한다. 대형 언어 모델(LLM)[5, 12, 37, 38]의 성공에서 영감을 얻은 우리는 마스크된 시각적 토큰을 예측하여 시각적 신호에 내장된 모션 및 물리학의 복잡한 역학을 효과적으로 모델링한다. 구체적으로, _WorldDreamer_는 VQGAN[15]을 사용하여 이미지를 이산 토큰으로 인코딩하는 것을 포함한다. 그런 다음 이러한 토큰의 일부를 무작위로 가리고 마스크되지 않은 토큰을 사용하여 마스크되지 않은 토큰을 예측하며, 이는 시각적 데이터에서 기본 동작과 물리학을 캡처하는 데 필수적인 과정입니다. 월드드림러_는 트랜스포머 아키텍처[46]에 구성되어 있다. 영상 신호에 내재된 공간-시간적 우선순위와 관련하여 시간적-공간적 창 내에서 국소화된 패치에 집중하여 시각적 신호 역학의 학습이 촉진되고 훈련 과정의 수렴이 가속화될 수 있는 공간-공간적 테포털 파치적 트랜스폼러(STPT)를 제안한다. 또한 _WorldDreamer_는 교차 의사를 통해 언어와 액션 신호를 통합하여 세계 모델 내의 상호 작용에 대한 다중 모달 프롬프트를 구성한다. 특히, 확산 기반 방법과 비교하여 _WorldDreamer_는 LLM 인프라의 재사용 및 모델 스케일링 학습 레시피를 포함하여 LLM에 대해 수년 동안 개발된 최적화의 이점을 자본화한다. 또한 _WorldDreamer_는 확산 기반 방법[3, 10, 48]보다 더 빠른 \\(\\ason\\)3\\(\\tot\\)인 몇 번의 반복만으로 놀라운 속도 우위, 병렬 디코딩 비디오를 나타낸다. 따라서 _WorldDreamer_는 시각적 신호로부터 일반적인 세계 모델을 구성할 수 있는 큰 약속을 가지고 있다.\n' +
      '\n' +
      '본 논문의 주요 기여는 다음과 같이 요약할 수 있는데, (1) 일반적인 세계운동과 물리학을 학습하는 영상생성 최초의 일반세계모델인 _WorldDreamer_을 소개한다. (2) 시간 공간 창 내의 국소 패치에 대한 관심의 초점을 향상시키는 공간 테스포라 페치온 트랜스폼러(STPT)를 제안한다. 이는 시각적 신호 역학에 대한 보다 쉬운 학습을 용이하게 하고 훈련 과정을 신속하게 수행할 수 있다. (3) 자연 장면 및 운전 환경을 포함한 다양한 시나리오에 걸쳐 비디오를 생성하는 데 _WorldDreamer_ 엑셀이 영향을 미친다는 것을 확인하기 위해 광범위한 실험을 수행한다. 월드드림러_는 텍스트 대 비디오 변환, 이미지 대 비디오 합성, 비디오 편집 및 액션 대 비디오 생성과 같은 작업을 실행하는 데 다양한 기능을 보여준다(그림 1 참조).\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '### Video Generation\n' +
      '\n' +
      '현재 최첨단 영상 생성 모델은 주로 트랜스포머 기반 방법과 확산 기반 방법의 두 가지로 분류된다.\n' +
      '\n' +
      '** 트랜스포머 기반 방법.**트랜스포머 기반 비디오 생성 방법은 LLM[5, 12, 37, 38]의 일반 계열에서 파생된다. 일반적으로 이러한 방법은 비디오 생성을 위해 다음 토큰의 자기회귀 예측 또는 마스킹된 토큰의 병렬 디코딩을 사용한다. 이미지 생성 기술[11, 13, 40, 54], 비디오GPT[53]에서 영감을 얻은 것은 VQVAE[45]를 트랜스폼 기반 토큰 예측과 통합하여 비디오 생성을 위한 시각적 토큰을 자동 예측할 수 있게 한다. 나아가 GAIA-1[31]은 텍스트 설명, 이미지, 운전 행위 등 다양한 양식이 통합되어 자율주행 시나리오 동영상이 생성된다. 이러한 자기회귀 방식과 달리 일부 트랜스포머 기반 접근 방식[29, 47]은 [8, 9, 14, 55]에서 영감을 이끌어내어 병렬 디코딩을 통해 영상 생성을 가속화한다. 이러한 방법 외에도 비디오팟[33]은 비디오 토큰화기[56]를 채택하고 병렬 디코딩을 기반으로 예외적으로 고품질 비디오를 생성한다. 트랜스포머 모델을 비디오 언어 모델에 통합하는 것은 사전 조작 동안 다양한 작업을 처리하는 데 있어 만만치 않은 제로 샷 능력을 보여준다. 따라서, 일반 세계 모델의 기반으로서 트랜스포머 기반의 마스크 이미지 모델을 사용하는 것이 유망한 수단으로서 나타난다.\n' +
      '\n' +
      '***D확산 기반 방법.**트랜스포머 기반 모델에 따라 비디오 생성을 위한 확산 기반 모델을 사용하는 광범위한 연구가 있었다. 비디오LDM[4]은 2D 확산 모델의 잠재 공간에 시간적 차원을 도입하고 동영상을 사용하여 미세 조정하여 영상 생성기를 효과적으로 비디오 생성기로 변환하고 고해상도 비디오 합성을 가능하게 한다. 유사하게 LVDM[26]은 경량 비디오 확산 모델을 탐색하여 저차원 3D 잠재 공간을 사용한다. 메이크-A-비디오 [43]도 사전 훈련된 텍스트 대 이미지 모델을 사용하여 대규모 비디오 트레이닝의 필요성을 제거한다. 더욱이, 이젠 비디오[27]에서는 전처리된 2D 확산 모델[27]에 캐스케이딩 비디오 확산 모델이 구축된다. 디프T[25]와 W.A.L.T[18]는 트랜스포머 기반 디퓨전 네트워크를 활용하여 영상 생성을 향상시킨다. 최근 Emu Video[17]과 PixelDance[57]은 텍스트 대 영상 생성을 위한 2단계 요인화 접근법을 제안하는데, 이 과정은 처음에 텍스트 대 이미지 변환으로 분해되고 이미지 대 비디오 합성이 뒤따른다. 이 방법론은 현대 텍스트 대 이미지 모델의 효과에 대해 자본화되어 모션 동학의 학습을 향한 비디오 확산 모델 훈련의 초점을 전략적으로 연출한다. 그러나 확산 기반 방법은 단일 모델 내에서 여러 가지 방식을 통합하는 데 어려움이 있다. 또한, 이러한 확산 기반 접근법은 역학 및 운동을 정확하게 포착하는 결과를 생성하기 위한 투쟁이다.\n' +
      '\n' +
      '### World Models\n' +
      '\n' +
      '세계 모델은 우리의 환경의 역동적인 특성을 이해하고 예측하는 데 중추적인 역할을 하며, 전 지구적 규모로 운동과 물리학에 대한 통찰력을 얻을 수 있는 엄청난 잠재력을 가지고 있다. 당초 세계 모델[19]의 탐색은 주로 게임 시나리오에 초점을 맞추고 있어 게임 환경 내에서 공간적, 시간적 역학에 대한 응축된 표현을 학습할 수 있는 생성 신경망 모델을 제시한다. 꿈거 시리즈[21, 22, 23] 내의 후속 연구는 다양한 게임 시나리오에 걸쳐 세계 모델의 효과를 긍정했다. 그 구조화된 성격과 비판적 의의를 감안할 때 자율 주행의 영역은 세계 모델의 최전방 적용 영역으로 부상하였다. 자율 주행 시나리오에서 세계 모델의 효능을 평가하기 위해 수많은 접근법[31, 32, 49, 50]이 도입되었다. 또한, 데이-드림어[52]는 실제 로봇 환경을 포괄하기 위해 세계 모델의 범위를 확장했다. 그러나 현재 세계 모델은 주로 게임, 로봇 공학, 자율 주행의 현실 내에서 작동하여 일반 세계의 운동과 물리학을 종합적으로 포착할 수 있는 능력이 결여되어 있다는 점은 주목할 만하다.\n' +
      '\n' +
      '## 3 WorldDreamer\n' +
      '\n' +
      '### Overall Framework\n' +
      '\n' +
      'E_WorldDreamer_의 전체 프레임워크는 그림 2에 나와 있으며 초기 단계는 시각적 신호(_i.e_: 이미지 및 비디오)를 시각적 토큰을 사용하여 이산 토큰으로 인코딩하는 것을 포함한다. 이러한 토큰은 STPT에 의해 처리되기 전에 조심스럽게 고안된 마스킹 전략을 거친다. 한편, 텍스트 및 액션 신호는 임베딩으로 별도로 인코딩되어 다중 모드 프롬프트 역할을 한다. STPT.\n' +
      '\n' +
      '그림 2: _WorldDreamer_ _WorldDreamer_ _WorldDreamer_ _tall 프레임워크 1/2. 월드드림러_는 먼저 이미지와 비디오를 시각적 토큰으로 변환한 후 토큰-태스킹 동작이 뒤따른다. 텍스트 및 액션 입력은 멀티모달 프롬프트로 작용하는 임베딩으로 별도로 인코딩된다. 이어서, STPT는 다양한 시나리오에서 비디오 생성 및 편집이 가능하도록 시각 디코더에 의해 처리되는 마스킹된 시각적 토큰을 예측한다.\n' +
      '\n' +
      ' 그런 다음 시각적 디코더에 의해 디코딩되는 마스킹된 시각적 토큰을 예측하는 중추적인 작업의 장치는 비디오 생성 및 여러 맥락에서 편집을 용이하게 한다.\n' +
      '\n' +
      'i_WorldDreamer_를 훈련시키기 위해, 우리는 교육 감독이 추가 감독 신호 없이 마스크 처리된 시각적 토큰을 예측하는 것만을 포함하는 _시각적 텍스트-Action_ 데이터의 삼중선을 구성한다. 월드드림러_는 또한 텍스트나 액션 데이터 없이 훈련을 지원하는데, 이는 데이터 수집의 어려움을 줄일 뿐만 아니라 _월드드림러_가 무조건적이거나 단일 조건 비디오 생성을 학습할 수 있게 한다. 추론 시간에 _WorldDreamer_는 다양한 비디오 생성 및 비디오 편집 작업을 수행할 수 있으며, (1) 이미지 대 비디오의 경우 나머지 프레임을 마스킹으로 간주하여 단일 이미지 입력만이 필요하다. 월드드림러_는 또한 단일 이미지 조건과 텍스트 조건을 기반으로 미래 프레임을 예측할 수 있다. (2) 비디오 스타일화는 특정 픽셀의 랜덤 마스킹으로 비디오 세그먼트를 입력할 수 있다. 월드드림러_는 입력 언어를 기반으로 가을 테마 효과를 만드는 등 비디오 스타일을 변경할 수 있다. (3) 텍스트 대 비디오의 경우, 언어 입력을 제공하는 것은 모든 시각적 토큰이 마스킹된다고 가정하여 _WorldDreamer_가 해당 비디오를 예측할 수 있게 한다. (4) 비디오 인포팅의 경우, 수동으로 마스킹된 관심 영역으로 비디오 세그먼트를 입력할 수 있다. 월드드림러_는 입력 언어 및 마스크되지 않은 시각적 신호에 기초하여 마스킹된 부분을 채울 수 있다. (5) 액션 대 비디오에 대해, 미래 주행 명령과 함께 구동 장면의 초기 프레임을 입력하면 _WorldDreamer_가 미래 프레임을 예측할 수 있다.\n' +
      '\n' +
      '후속 하위 섹션은 모델 아키텍처 및 마스킹 전략에 대해 자세히 설명한다.\n' +
      '\n' +
      '### Model Architecture\n' +
      '\n' +
      '**Preliminery**_WorldDreamer_는 시각적 신호를 토큰화하기 위해 VQGAN[16]을 활용한다.\n' +
      '\n' +
      '\\[V=\\mathcal{F}_{v}(I), \\tag{1}\\]\n' +
      '\n' +
      'I\\(I\\in\\mathcal{R}^{N\\tcer H\\tcer 3}\\)는 시각적 입력의\\(N\\) 프레임이다. VQGAN \\(\\mathcal{F}_{v}\\)은 시각적 토큰(T_{\\text{V}\\in\\mathcal{V}}\\in\\mathcal{R}^{N\\tross w}\\)을 생산하는 16\\(\\tep\\)의 해상도를 샘플링한다(h=\\frac{H}{4}{W}{4}{4}\\). VQGAN은 어휘 크기가 8192이며 수십억 개의 이미지[41]로 훈련된다. 텍스트 입력을 위해 전처리된 T5 [39]를 사용하여 고차원 임베딩(E_{\\text{T}}\\in\\mathcal{R}^{K\\times C_{\\text{T}}}\\)으로 매핑하고, 여기서 \\(K\\)는 서열 길이이고 \\(C_{\\text{T}}\\)는 임베딩 채널이다. STPT의 특징 학습과 호환되기 위해, 텍스트 임베딩은 \\(N\\) 프레임에 대해 반복되고 임베딩 채널은 \\(C_{\\text{V}}\\)에 매핑된다. 또한, 멀티 플레이어 퍼셉션(MLP)은 액션 입력들을 인코딩하기 위해 활용되며, 이는 액션 임베딩(E_{\\text{A}\\in\\mathcal{R}^{N\\times C_{\\text{V}}\\)을 생성한다. 텍스트 임베딩 및 액션 임베딩은 연결되어 다중 모드 프롬프트 임베딩(E_{\\text{M}}\\in\\mathcal{R}^{N\\times(K+1)\\times C_{\\text{V}}\\)을 생성한다. 텍스트나 액션 임베딩 중 하나가 비어 있어 무조건 학습이 가능하다.\n' +
      '\n' +
      '훈련 중 최적화된 목표는 마스크되지 않은 토큰과 멀티모달 프롬프트에서 조건화된 마스크형 시각적 토큰을 예측하는 것이다.\n' +
      '\n' +
      '}=\\log p(\\hat{T}:{\\text{T},E_{\\text{M})\n' +
      '\n' +
      'H\\(\\hat{T}_{\\text{V}}\\)가 마스킹된 시각적 토큰이고, \\(\\widetilde{T}_{\\text{V}}\\)는 마스크되지 않은 시각적 토큰이다.\n' +
      '\n' +
      '**STPT** STPT** STPT는 U-ViT [30]의 기초를 받아들이는 동시에 비디오 데이터에서 공간-시간 역학의 복잡한 부분을 더 잘 포착하기 위해 아키텍처를 전략적으로 향상시켰다. 구체적으로 STPT는 공간-시간 패치 내에서 주의 메커니즘을 혼동한다. 또한 다중 모드 정보를 원활하게 통합하기 위해 다중 모드 임베딩을 통합하기 위해 공간적인 교차 의도를 사용한다. 입력 토큰 \\(\\hat{T}_{\\text{V}}\\)의 경우 STPT는 학습 가능한 코드북을 참조하여 시각적 임베딩(E_{\\text{V}}\\in\\mathcal{R}^{N\\tep w\\times C_{\\text{V}}}\\)으로 변환한다. 이 코드북의 크기는 8193개로 설정되어 VQGAN의 코드북 크기를 1만큼 초과하여 마스킹된 토큰과의 호환성을 가능하게 한다. STPT의 각 층에서 그림 1에 도시된 바와 같이. 3, 시각적 임베딩들은 먼저 3D 컨볼루션 네트워크를 통해 처리된다. 그런 다음 이러한 임베딩은 공간적으로 여러 패치(E_{\\text{P}}\\in\\mathcal{R}^{N\\times{R}^{N\\t 시간 w/\\tcer C_{\\{V}}})로 분할되며, 결과적으로 각 패치 임베딩은 공간적으로 패치 스트라이드(\\)를 경험적으로 설정하는 공간적으로 플랫하게 설정된다.\n' +
      '\n' +
      'MS(E_{\\text{P})는 카바칼{G}(이하스칼{F}_{{{+}(E_{{ \\text{P}))}(\\mathcal{G}, E_{ \\text{P}))\n' +
      '\n' +
      '아이티(\\mathcal{G}_{s}\\)는 임베딩 차원을 \\(\\mathcal{R}^{Nhw/s^{2}\\times C_{\\text{V}}}\\)로 매핑하는 평탄한 조작이며,\\(\\mathcal{G}_{s}^{G}_{s}^{-1}\\)는 역동작이다. (\\mathcal{F}_{s}\\)은 표준 자기 의도이다. 이 패치들은\n' +
      '\n' +
      '그림 3: STPT의 모든 아키텍처. STPT는 먼저 3D 컨볼루션을 사용하여 시각적 임베딩을 집계한다. 그런 다음 이러한 임베딩을 공간-시간 패치적 자기 의사를 위해 여러 패치로 분할하여 다음에서는 시각적 임베딩과 다중 모드 임베딩 사이의 특징 상호작용을 용이하게 하기 위해 공간-외 교차 주의를 적용한다.\n' +
      '\n' +
      '그 다음 연결되고 다시 원래 차원으로 재구성됩니다. 이하에서는 시각 임베딩과 복합 임베딩 간의 특징 상호 작용을 용이하게 하는 공간별 교차 주의가 적용된다.\n' +
      '\n' +
      '\\[E_{\\text{V}}=\\mathcal{F}_{\\text{c}}(E_{\\text{V}},E_{\\text{M}}), \\tag{4}\\]\n' +
      '\n' +
      'HH(\\mathcal{F}_{\\text{c}}\\)는 프레임 번호를 배치 크기로 간주하는 교차 의도 동작이다. STPT의\\(L\\) 층을 통해 처리된 후, VQGAN의 코드북 크기에 \\(E_{\\text{V}\\)의 특징 차원을 매핑한다. 이는 소프트맥스의 활용이 각 토큰의 확률을 계산할 수 있게 하여 마스킹된 시각적 토큰의 예측을 용이하게 한다. 마지막으로 제안된 STPT를 최적화하기 위해 교차 보상 손실을 사용한다.\n' +
      '\n' +
      'MS(\\tildcal{L})}(\\tilde{T}{\\text{V},\\mathcal{P}.{\\text{T}(E_{\\text{V}),\\tag{5}\\\\\\})\n' +
      '\n' +
      'HH(\\mathcal{P}_{\\text{ STPT}}(알테데틸테{T}_{\\text{V}},E_{\\text{M}})\\)은 STPT에 의해 예측된 시각적 토큰 확률이다.\n' +
      '\n' +
      '특히, 제안된 STPT는 비디오 및 이미지와 공동으로 트레이닝될 수 있다. 이미지 입력의 경우, 우리는 대각 매트릭스 [18]로서 \\(\\mathcal{F}_{\\text{s}\\)의 주의 가중치를 간단하게 대체한다. 비디오 및 이미지 데이터 세트 모두에 대한 동시 훈련은 트레이닝 샘플의 상당한 증가를 제공하여 광범위한 이미지 데이터 세트를 보다 효율적으로 이용할 수 있다. 또한, 공동 훈련 전략은 시각적 신호 내에서 시간적, 공간적 측면을 이해하는 _WorldDreamer_\'s 능력을 상당히 향상시켰다.\n' +
      '\n' +
      '### Mask Strategy\n' +
      '\n' +
      '마스크 전략은 [8]에 이어 _WorldDreamer_ 교육을 위해 중요하며, 우리는 코사인 스케줄링을 기반으로 동적 마스킹 속도를 사용하여 _WorldDreamer_를 훈련시킨다. 구체적으로, 각 반복에서 무작위 마스크 속도 \\(r\\in[0,1]\\)를 샘플링하고, 각 프레임에서 완전히 \\(\\frac{2hw}{\\pi}(1-r^{2})^{\\frac{-1}{2}}\\) 토큰을 가리고 있다. 다른 프레임에 걸쳐 동일한 토큰 마스크를 사용한다는 점에 유의하세요. 이 결정은 인접한 프레임 간의 시각적 신호의 유사성에 근거한다. 다른 토큰 마스크를 사용하면 잠재적으로 학습 과정에서 정보 유출로 이어질 수 있다. 자가회귀 마스크 스케줄러와 비교하여 우리의 접근법에 사용된 동적 마스크 일정은 추론 시간에 병렬 샘플링에 중요하며, 이는 단일 전방 패스에서 다중 출력 토큰의 예측을 가능하게 한다. 이 전략은 많은 토큰이 다른 토큰[8]을 통해 조건적으로 독립화되는 마르코비안 재산의 가정에 자본화된다. 추론 과정은 또한 코사인 마스크 일정을 따르며, 각 단계에서 예측을 위한 최고 자신감 마스킹 토큰의 고정된 부분을 선택한다. 이어서, 이러한 토큰들은 나머지 단계들에 대해 마스크되지 않고, 마스크화된 토큰들의 세트를 효과적으로 감소시킨다. 그림과 같이. 4, 확산 기반 방법은 보통 노이즈를 줄이기 위해 \\(\\ason\\)30 단계를 필요로 하며, 자가회귀 방법은 다음 토큰을 반복적으로 예측하기 위해 \\(\\ Res\\)200 단계가 필요하다. 대조적으로, _WorldDreamer_ 평행은 약 10단계로 마스킹된 토큰을 예측하여 확산 기반 또는 자기회귀 방법에 비해 \\(3\\t 기간에는 20\\) 가속도를 나타낸다.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      '우리는 _WorldDreamer_를 훈련시키기 위해 다양한 이미지 및 비디오를 사용하여 시각적 역학에 대한 이해도를 향상시킨다. 본 교육에 활용된 구체적인 자료는 포함한다.\n' +
      '\n' +
      '그림 4: 확산 기반 방법의 추론 일정, 자기회귀 방법 및 _WorldDreamer_ 간의 비교이다. 확산 기반 방법은 보통 노이즈를 줄이기 위해 \\(\\ason\\)30 단계를 필요로 하며, 자기회귀 방법은 다음 토큰을 반복적으로 예측하기 위해 \\(\\ Res\\)200 단계가 필요하다. 대조적으로, _WorldDreamer_ 평행은 마스킹된 토큰을 예측하여 약 10단계로 비디오 생성을 달성한다.\n' +
      '\n' +
      '**Deduselfated LAION-2B[34]** 원래 LAION 데이터세트[41]는 텍스트 설명과 수반되는 이미지 간의 데이터 복제 및 불일치 등의 과제를 제시하였다. 우리는 이러한 문제를 해결하기 위해 [36]를 따른다. 구체적으로, 우리는 _WorldDreamer_ 교육을 위해 중복된 LAION-2B 데이터세트[34]를 활용하기로 결정했다. 이 정제된 데이터 세트는 워터마크 확률이 50%를 초과하거나 NSFW 확률이 45%를 초과하는 이미지를 배제한다. 중복화된 LAION 데이터 세트는 [51]에 도입된 방법론에 따라 [41]에서 사용할 수 있었다.\n' +
      '\n' +
      '**WebVid-10M[1]** WebVid-10M은 약 1천만 개의 짧은 비디오로 구성되며, 각각은 평균 18초 동안 지속되며 주로 \\(336년 596\\)의 해상도로 표시된다. 각 영상은 시각적 콘텐츠와 상관관계가 있는 연관된 텍스트와 페어링된다. 웹바이드-10M이 제기하는 과제는 모든 비디오에 워터마크가 존재하여 생성된 모든 비디오 콘텐츠에서 워터마크가 볼 수 있다. 따라서 고품질 자체 수집 비디오 텍스트 쌍을 활용하는 _WorldDreamer_를 추가로 정제하기로 결정했습니다.\n' +
      '\n' +
      '** 자기 수집 비디오 텍스트 쌍** 우리는 인터넷으로부터 공개적으로 이용 가능한 비디오 데이터를 획득하고, 획득한 비디오를 전처리하기 위해 [3]에서 상세한 절차를 적용한다. 구체적으로, 우리는 장면 전환의 순간을 감지하고 단일 연속 장면의 비디오를 얻기 위해 PyScenelockect[7]을 사용한다. 그런 다음 광학 흐름을 계산하여 느린 운동으로 클립을 걸러냅니다. 결과적으로 500K 고품질 비디오 클립이 교육을 위해 획득됩니다. 비디오 캡션을 위해 비디오의 10번째, 50번째 및 90번째 백분위수 프레임을 키프레임으로 추출한다. 이러한 키 프레임은 각 키프레임에 대한 캡처를 생성하기 위해 게미니[44]에 의해 처리된다. 또한, 게미니는 이러한 개별 이미지 캡션을 전체 비디오에 대한 전체 캡션으로 집계하도록 지시받는다. 고기술적 캡션이 생성 모델[2]의 훈련을 향상시키는 것과 관련하여 우리는 게미니가 가능한 한 많은 세부 사항을 가진 캡처를 생성하도록 지시했다. 자세한 캡션은 _WorldDreamer_가 보다 미세한 텍스트-시각적 대응 관계를 배울 수 있도록 한다.\n' +
      '\n' +
      '**NuScenes[6]** NuScenes]은 자율주행을 위한 인기 데이터셋으로 총 700개의 훈련 영상과 150개의 검증 영상을 포함한다. 각 비디오는 12Hz의 프레임 레이트에서 약 20초를 포함한다. 월드드림러_는 프레임 간격이 6 프레임인 트레이닝 세트의 전면 영상을 활용한다. 총 28K 가량의 운전 장면 영상이 훈련하고 있다. 비디오 캡션을 위해 게미니에게 날씨, 하루 시간, 도로 구조 및 중요한 교통 요소를 포함하여 각 프레임에 대한 자세한 설명을 생성할 것을 지시했다. 그런 다음 게미니는 이러한 이미지 캡션을 각 비디오에 대한 전체 캡션으로 집계하도록 지시받는다. 또한, 우리는 액션 메타데이터로서 자아차의 요각과 속도를 추출한다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '**Train 디테일**_WorldDreamer_는 웹바이드와 LAION 데이터셋의 조합으로 먼저 훈련된다. 웹바이드 동영상의 경우 16개의 프레임을 훈련 샘플로 추출한다. LAION 데이터셋의 경우 16개의 독립적인 이미지가 학습 샘플로 선택된다. 각 샘플은 \\(256\\tco 256\\)의 입력 해상도로 재구성되고 크롭된다. 월드드림러_는 배치 크기 64로 2M 반복으로 훈련되며, 훈련 과정은 AdamW로 최적화와 \\(5\\t10^{-5}\\), 체중 감소 0.01을 포함하며, 훈련은 향상되고 데이터 범위를 확장하기 위해, _WorldDreamer_는 자기 수집 데이터 세트 및 nuScenes 데이터에 더 적합하며, 여기서 STPT의 모든 (1B) 매개변수를 훈련할 수 있다. 핀셋링 단계 동안, 입력 해상도는 \\(192\\t 320\\)이고, 각 샘플에는 24 프레임이 있다. 월드드림러_는 배치 크기가 32인 20K 반복 이상 피니스트로 학습률은 \\(1\\t10^{-5}\\)이다.\n' +
      '\n' +
      '** 인딩 디테일** 추론 시간에서 분류기 프리 가이드스(CFG) [28]를 사용하여 생성 품질을 향상시킵니다. 구체적으로, 우리는 훈련 샘플의 10%에 대한 다중 모드 임베딩을 무작위로 제거한다. 추론하는 동안, 우리는 각 마스킹 토큰에 대해 조건부 로짓 \\(c\\) 및 무조건 로짓 \\(u\\)을 계산한다. 그런 다음 지침 척도라고 하는 \\(\\beta\\)의 요인으로 무조건적인 로그에서 벗어나서 최종 로그(g\\)를 도출한다.\n' +
      '\n' +
      '유사태그[g=(1+\\베타)c-\\베타 u. \\태그{6}\\]](1+\\베타)c-\\베타 u. \\태그{6}\\]](1+\\베타)c-\\베타 u.\n' +
      '\n' +
      '예측된 시각적 토큰을 위해 전처리된 VQGAN 디코더를 사용하여 비디오를 직접 출력한다. 특히 _WorldDreamer_는 단일 A800에서 3초밖에 걸리지 않는 \\(192\\t 320\\)의 해상도로 24 프레임으로 구성된 영상을 생성할 수 있다.\n' +
      '\n' +
      '### Visualizations\n' +
      '\n' +
      '우리는 _WorldDreamer_가 일반 세계의 일반적인 시각적 역학에 대한 심오한 이해를 획득했음을 입증하기 위해 포괄적인 시각적 실험을 수행했다. 자세한 시각화와 결과를 통해 다양한 시나리오에 걸쳐 비디오 생성 및 비디오 편집을 달성하는 _월드드림러_\'의 능력을 보여주는 강력한 증거를 제시한다.\n' +
      '\n' +
      '*** 비디오**_월드드림러_ 엑셀은 다양한 시나리오에 걸쳐 높은 충실도 이미지 대 비디오 생성에서 뛰어납니다. 그림과 같이. 5는 초기 이미지 입력에 기초하여 _월드드림러_는 고품질 영화 경관 영상을 생성할 수 있는 능력을 가지고 있다. 생성된 비디오는 실제 필름에서 볼 수 있는 부드러운 카메라 동작과 유사한 심리스 프레임 대 프레임 운동을 나타낸다. 더욱이, 이러한 비디오들은 원본 이미지에 의해 부과된 제약들에 꼼꼼하게 부착하여 프레임 구성의 현저한 일관성을 보장한다. 초기 이미지의 제약 조건에 부착된 후속 프레임을 생성하여 놀라운 프레임 일관성을 보장합니다.\n' +
      '\n' +
      '*** 비디오** 그림. 비디오 기반 비디오 기반 기반 비디오 기반 비디오 기반 비디오 기반 비디오 기반 비디오 기반 비디오 기반 비디오 기반 비디오 기반 비디오 기반 비디오 기반 비디오 기반 비디오 기반 비디오 기반 비디오 기반 비디오 기반 기반 비디오 기반 기반 비디오 기반 기반 비디오 기반 기반 비디오 기반 기반 비디오 기반 아키텍처이다.\n' +
      '\n' +
      '** 비디오 그림**에 도시된 바와 같다. 7, _WorldDreamer_는 고품질 비디오 인포팅에 탁월한 능력을 나타낸다. 특정 관심 영역을 능가하는 마스크와 원하는 수정을 지정하는 텍스트 프롬프트를 제공함으로써 _WorldDreamer_는 원본 비디오를 복잡하게 변경하여 인포팅 과정에서 현저하게 현실적인 결과를 낳는다.\n' +
      '\n' +
      '** 비디오 Stylization*** 그림. 8은 _WorldDreamer_ 엑셀이 고품질 비디오 스타일링을 전달하는 데 탁월함을 보여준다. 무작위로 생성된 시각적 토큰 마스크와 원하는 변형을 나타내는 스타일 프롬프트를 공급함으로써 월드드림러는 원본 비디오를 설득력 있게 변형시켜 스타일화 과정에서 진정한 현실적 결과를 달성했다.\n' +
      '\n' +
      '비디오***_월드드림러_에 대한***Action은 자율 주행의 맥락에서 행동을 기반으로 영상을 생성하는 능력을 보여준다. 그림과 같이. 동일한 초기 프레임과 상이한 주행 액션들을 감안할 때, _WorldDreamer_는 서로 다른 주행 액션(예를 들어, 차를 제어하여 좌회전 또는 우회전)에 대응하는 별개의 미래 비디오를 생성할 수 있다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '결론적으로 _WorldDreamer_는 영상 생성을 위한 세계 모델링에서 주목할 만한 발전을 나타낸다. 세계크림러는 특정 시나리오에 제약을 받은 전통적인 모델과 달리 일반 세계 동적 환경의 복잡성을 포착한다. 세계드림러_프레임 세계 모델링은 시각적 토큰 예측 도전으로 일반 세계 물리학과 운동에 대한 포괄적인 이해를 함양하여 영상 생성의 역량을 크게 향상시켰다. 실험에서 _WorldDreamer_는 자연 장면 및 주행 환경과 같은 시나리오 전반에 걸친 탁월한 성능을 보여주며 텍스트 대 비디오 변환, 이미지 대 비디오 합성 및 비디오 편집과 같은 작업에 대한 적응력을 보여준다.\n' +
      '\n' +
      '그림 8: _WorldDreamer_ 엑셀은 고품질 비디오 스타일링 기능을 제공하는 데 탁월합니다.\n' +
      '\n' +
      '그림 7: _WorldDreamer_는 고품질 비디오 인포팅을 달성할 수 있는 탁월한 능력을 가지고 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]M. 베인, A. 나가란이, G. 바롤 및 A. 지스머만(2021)은 엔드 투 엔드 검색을 위한 공동 비디오 및 이미지 인코더이다. ICCV에서 SS1, SS2가 계산합니다.\n' +
      '*[2]J. 베커, G. 고, L. 제잉, T. 브룩스, 조 왕, L. 리, L. 오양, 주강, J. 이, Y. 구, 등(2023)은 더 나은 캡션으로 이미지 생성을 개선합니다. 컴퓨터 과학. 벚꽃. com/ Model/dall-e-3. pdf. SS1: SS1로 받았습니다.\n' +
      '*[3]A. 블라트만, T. 도호렌, S. 칼, D. 멘델라비치, M. 킬리안, D. 로렌츠, Y. 레비, Z. 영어, V. Voleti, A. L. Letts, et al.(2023) 케이블 비디오 확산: 잠재 비디오 확산 모델을 대형 데이터셋으로 스케일링한다. arXiv 프리프린트 arXiv:2311.15127: SS1에 의해 계산된다.\n' +
      '*[4]A. 블라트만, R. Rombach, H. Ling, T. 도쇼렌, S. W. 김, S. 피들러, K. 크리스(2023)는 래치들: 잠재 확산 모델을 사용한 고해상도 비디오 합성. CVPR에서 SS1, SS2가 Cited한다.\n' +
      '*[5]T. 브라운, B. Mann, N. 리더, M. 하위피아, J D Kaplan, P. D. 카플란, P. 다라리왈, A. 네이만탄, P. Shyam, G. Sastry, A. Askell, et al.2020) 언어 모델은 소수의 샷 학습자들이다. 네르IPS. SS1: SS1로 받았습니다.\n' +
      '*[6]H. Caesar, V. 뱅크티, A. H. 랑, S. 보라, V. E. Liong, Q. Xu, A. 키슈난, Y. 패널, G. 발단 및 O. Beijbom(2020) 나스켄: 자율주행을 위한 복합 데이터셋입니다. CVPR에서 SS1, SS2가 Cited한다.\n' +
      '*[7]B. 카스텔라노(2020) PyScenedetect. 기투브 저장소가요. SS1: SS1로 받았습니다.\n' +
      '* [8]H. 창, 장, L. 지앙, C. 류, W. T. 프로만(2022) 탈색 생성 이미지 변압기. CVPR에서 SS1, SS2가 Cited한다.\n' +
      '* [9]H. 텐, M. 샤, Y. >야, 야. 장, X. 스쿱, S. 양, J.Xing, Y. 리, Q. 텐, X. 왕, 예를 들어 (2023) Videocrafter1: 고품질 비디오 생성을 위한 오픈 확산 모델입니다. arXiv 프리프린트 arXiv:2310.19512: SS1, SS2에 의해 계산된다.\n' +
      '*[10]H. 텐, A. 라드포드, R. 아동, J. 우, H. 준, P. Dhariwal, D. Luan 및 I. Sutskever(2020) 유전 전술을 픽셀에서 투여한다. SS1, SS2로 제공됩니다.\n' +
      '*[11]M. 텐, A. 라드포드, R. 아동, J. 우, H. 준, P. Dhariwal, D. Luan 및 I. Sutskever(2020) 유전 전술을 픽셀에서 투여한다. SS1, SS2로 제공됩니다.\n' +
      '*[12]J. 데블린, M. 창, K. 이씨, K씨. 투타노바(2018) 베르트: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련이다. arXiv 프리프린트 arXiv:1810.04805: SS1, SS2에 의해 계산된다.\n' +
      '*[13]M. 드잉, Z. 양, W. 홍, W. 정, C. 저우, D. Yin, J. Lin, X. Z. 샤오, H. 양씨 등은 (2021) 코뷰: 변압기를 통한 텍스트 대 이미지 생성을 마스터한다. 네르IPS. SS1, SS2로 제공됩니다.\n' +
      '*[14]M. 드잉, W. 정, W. 홍과 J. 탕(2022) 코뷰2는 계층적 변압기를 통해 더 빠르고 나은 텍스트 대 이미지 생성이다. NIPS. SS1, SS2로 제공됩니다.\n' +
      '*[15]P. 어저, R. 레바치, 고해상도 이미지 합성을 위한 B. Ommer(2021) 게이밍 변압기입니다. CVPR에서 SS1, SS2가 Cited한다.\n' +
      '*[16]R. 새하, M. 싱, A. 브라운, Q. 두발, S. 아자디, S. 사크스 람바틀라, A. 샤, X.\n' +
      '\n' +
      '그림 9: _WorldDreamer_ 엑셀은 자율 주행의 맥락에서 행동을 기반으로 영상을 생성할 수 있는 능력을 구현하는 데 탁월하다.\n' +
      '\n' +
      '이인, 데비 파라크, 이산 미스라. 이무 영상: 명시적 이미지 컨디셔닝에 의한 텍스트 대 비디오 생성 인자화: 명시적 이미지 컨디셔닝에 의한 텍스트-영상 생성이다. arXiv 프리프린트 arXiv:2311.10709_, 2023.\n' +
      '*[18]가림 구파, 리준 유, 기혁 손, 시례구, 메라 하안, 리피-피, 어판 에사, 루장, 호세 레자마 등이다. 확산 모델을 가진 __광학적 영상 생성  _:광학적 영상 생성은 확산 모델을 가지고 있다. arXiv 프리프린트 arXiv:2312.06662_, 2023.\n' +
      '* [19] David Ha and Jurgen Schmidhuber. Recurrent world models facilitate policy evolution. _NeurIPS_, 2018.\n' +
      '* [20] Danijar Hafner, Kuang-Huei Lee, Ian Fischer, and Pieter Abbeel. Deep hierarchical planning from pixels. _NeurIPS_, 2022.\n' +
      '* [21] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. _arXiv preprint arXiv:1912.01603_, 2019.\n' +
      '* [22] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. _arXiv preprint arXiv:2010.02193_, 2020.\n' +
      '* [23] Danijar Hafner, Jurgis Paskonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. _arXiv preprint arXiv:2301.04104_, 2023.\n' +
      '* [24] Danijar Hafner, Jurgis Paskonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. _arXiv preprint arXiv:2301.04104_, 2023.\n' +
      '* [25] Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. Difit: Diffusion vision transformers for image generation. _arXiv preprint arXiv:2312.02139_, 2023.\n' +
      '* [26] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. _arXiv preprint arXiv:2211.13221_, 2022.\n' +
      '* [27] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_, 2022.\n' +
      '* [28] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [29] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. _arXiv preprint arXiv:2205.15868_, 2022.\n' +
      '* [30] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. _arXiv preprint arXiv:2301.11093_, 2023.\n' +
      '* [31] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: A generative world model for autonomous driving. _arXiv preprint arXiv:2309.17080_, 2023.\n' +
      '* [32] Fan Jia, Weixin Mao, Yingfei Liu, Yucheng Zhao, Yuqing Wen, Chi Zhang, Xiangyu Zhang, and Tiancai Wang. Adriver-i: A general world model for autonomous driving. _arXiv preprint arXiv:2311.13549_, 2023.\n' +
      '* [33] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vignhesh Birodkar, et al. Videopoet: A large language model for zero-shot video generation. _arXiv preprint arXiv:2312.14125_, 2023.\n' +
      '* [34] Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. Obelisc: An open web-scale filtered dataset of interleaved image-text documents. _arXiv preprint arXiv:2306.16527_, 2023.\n' +
      '* [35] Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter Abbeel, Dan Klein, and Anca Dragan. Learning to model the world with language. _arXiv preprint arXiv:2308.01399_, 2023.\n' +
      '* [36] Suraj Patil, William Berman, Robin Rombach, and Patrick von Platen. amused: An open muse reproduction. _arXiv preprint arXiv:2401.01808_, 2024.\n' +
      '* [37] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. _OpenAI_, 2018.\n' +
      '* [38] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI_, 2019.\n' +
      '* [39] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 2020.\n' +
      '* [40] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _ICML_, 2021.\n' +
      '* [41] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training nerst generation image-text models. _NIPS_, 2022.\n' +
      '* [42] Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter Abbeel. Masked world models for visual control. In _CoRL_, 2023.\n' +
      '* [43] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. _arXiv preprint arXiv:2209.14792_, 2022.\n' +
      '* [44] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* [45] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. _NeurIPS_, 2017.\n' +
      '* [46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _NIPS_, 2017.\n' +
      '* [47] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. _ICLR_, 2023.\n' +
      '\n' +
      '* [48] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. _arXiv preprint arXiv:2308.06571_, 2023.\n' +
      '* [49] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, and Jiwen Lu. Driverdeamer: Towards real-world-driven world models for autonomous driving. _arXiv preprint arXiv:2309.09777_, 2023.\n' +
      '* [50] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. _arXiv preprint arXiv:2311.17918_, 2023.\n' +
      '* [51] Ryan Webster, Julien Rabin, Loic Simon, and Frederic Jurie. On the de-duplication of laion-2b. _arXiv preprint arXiv:2303.12733_, 2023.\n' +
      '* [52] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Daydreamer: World models for physical robot learning. In _CoRL_, 2023.\n' +
      '* [53] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. _arXiv preprint arXiv:2104.10157_, 2021.\n' +
      '* [54] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. _arXiv preprint arXiv:2206.10789_, 2022.\n' +
      '* [55] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In _CVPR_, 2023.\n' +
      '* [56] Lijun Yu, Jose Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, et al. Language model beats diffusion-tokenizer is key to visual generation. _arXiv preprint arXiv:2310.05737_, 2023.\n' +
      '* [57] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: High-dynamic video generation. _arXiv preprint arXiv:2311.10982_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>