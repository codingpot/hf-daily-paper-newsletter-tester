<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '카탈로그에 대한 데이터 임탁\n' +
      '\n' +
      '오케이.\n' +
      '\n' +
      ' 샤오동 쿡은 마메도니아샤오신 첸({}^{\\dagger}})과 용장성(神門)이다.\n' +
      '\n' +
      '샤인타오 차오.\n' +
      '\n' +
      'Tencent AI Lab\n' +
      '\n' +
      'Homepage: [https://ailab-cvc.github.io/videocrafter](https://ailab-cvc.github.io/videocrafter)\n' +
      '\n' +
      'Github: [https://github.com/AILab-CVC/VideoCrafter](https://github.com/AILab-CVC/VideoCrafter)\n' +
      '\n' +
      'Discord: [https://discord.gg/RQENrunu92](https://discord.gg/RQENrunu92)\n' +
      '\n' +
      ' _사이버 정크, 네온패크 스타일, 궁후판다, 점프, 킥. 시네마틱 사진은 원뿔 아래로 피스타치오 아이스크림을 녹여줍니다. 35mm 사진, 영화, 보케. 나비로 둘러싸인 큰 운동은 소녀가 울창한 정원을 산책하고 있다._나비로 둘러쌓여 있다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '텍스트 대 비디오 생성은 주어진 프롬프트에 기초하여 비디오를 생성하는 것을 목표로 한다. 최근 여러 상용 비디오 모델은 소음, 우수한 세부 사항 및 높은 심미적 점수로 그럴듯한 비디오를 생성할 수 있게 되었다. 그러나 이러한 모델은 커뮤니티에 접근할 수 없는 대규모, 잘 여과된 고품질 영상에 의존한다. 저품질 웹Vid-10M 데이터셋을 사용하여 모델을 훈련시키는 기존 많은 연구 작업은 모델이 웹Vid-10M에 적합하도록 최적화되기 때문에 고품질 비디오를 생성하기 위해 어려움을 겪고 있다. 본 연구에서는 스테이블 디퓨전으로부터 확장된 비디오 모델의 훈련 방식을 탐색하고, 양질의 영상 및 합성 고품질 영상을 레버리징하여 고품질 비디오 모델을 얻을 수 있는 가능성을 조사한다. 먼저 비디오 모델의 공간적 모듈과 시간적 모듈 간의 연결과 낮은 품질의 비디오로의 분포 이동을 분석한다. 우리는 모든 모듈의 전체 훈련이 트레이닝 시간 모듈만 하는 것보다 공간적 모듈과 시간적 모듈 간의 더 강한 결합을 초래한다는 것을 관찰한다. 이 더 강한 결합을 기반으로 고품질 이미지로 공간 모듈을 미세화하여 움직임 저하 없이 분포를 더 높은 품질로 전환하여 일반적인 고품질 비디오 모델을 생성했다. 특히 화질, 운동, 개념 구성에서 제안된 방법의 우월성을 입증하기 위해 평가를 실시한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Benefiting from the development of diffusion models [25, 43], video generation has achieved breakthroughs, particularly in basic text-to-video (T2V) generation models. Most existing methods [14, 23, 26, 30, 48, 63] follow a logic to obtain video models, _i.e.,_ extending a text-to-image (T2I) backbone to a video model by adding temporal modules and then training it with videos. Several models train video models from scratch, while most start from a pre-trained T2I model, typically Stable Diffusion (SD) [38]. Models can also be categorized into two groups based on the space modeled by diffusion models, _i.e.,_ pixel-space models [26, 30, 60] and latent-space models [14, 23, 48, 63]. The latter is the dominant approach. Picture quality, motion consistency, and concept composition are essential dimensions for evaluating a video model. Picture quality refers to aspects such as sharpness, noise, distortion, aesthetic score, and more. Motion consistency refers to the appearance consistency between frames and motion smoothness. Concept composition represents the ability to combine different concepts that might not appear simultaneously in real videos.\n' +
      '\n' +
      'Recently, a few commercial startups have released their T2V models [5, 6, 8, 9] that can produce plausible videos with minimal noise, excellent details, and high aesthetic scores. However, they are trained on a large-scale and well-filtered high-quality video dataset, which is not accessible to the community and academia. Collecting millions of high-quality videos is challenging due to copyright restrictions and post-filtering processing. Though there are a few open-source video datasets collected from the Internet for video understanding, such as HowTo100M [33], HD-VILA-100M [56], and InterVid [51], there exist many issues for video generation, _e.g.,_ poor picture quality and caption, multiple clips in one video, and static frames or slides. WebVid-10M [12] is the most widely used dataset to train video generation models in academia. The clips are well-segmented, and the diversity is good. However, the picture quality is unsatisfactory, and most videos have a resolution of about 320p. The lack of high-quality datasets poses a significant obstacle to training high-quality video models in academia.\n' +
      '\n' +
      '이 작품에서는 고품질의 영상을 사용하지 않고 상당히 어려운 문제, 즉,_i,_훈련 수준 높은 비디오 모델을 대상으로 한다. 우리는 SD 기반 비디오 모델의 훈련 과정에 다이빙하여 다양한 훈련 전략하에서 공간 모듈과 시간 모듈 간의 연결을 분석하고 저품질 비디오로의 분포 이동을 조사한다. 우리는 모든 모듈의 전체 훈련이 단순한 훈련 시간 모듈보다 외관과 움직임 사이의 더 강한 결합을 초래한다는 의미 있는 관찰을 한다. 풀 트레이닝은 보다 자연스러운 동작을 달성하고 더 많은 후속적인 공간 모듈의 변형을 견딜 수 있으며, 이는 생성된 비디오의 품질을 향상시키는 열쇠이다. 연결의 관찰을 바탕으로 데이터 수준에서의 외관으로부터 모션을 무력화시켜 데이터 한계를 극복할 수 있는 방법을 제안한다. 구체적으로 양질의 영상 대신 저품질 영상을 활용하여 동작 일관성을 보장하고 고품질 이미지를 사용하여 화질 및 개념 구성 능력을 보장합니다. SDXL, 미들즈니와 같은 성공적인 T2I 모델들로부터 학습하는 것은 고해상도 및 복잡한 개념 구성을 가진 큰 이미지 세트를 얻는 것이 편리하다. 분석의 지침에 따라 SD에서 확장된 비디오 모델을 완전히 훈련시키기 위해 파이프라인을 설계한다. 그런 다음 합성된 이미지로 완전히 훈련된 모델의 공간적 및 시간적 모듈을 수정하는 다양한 방법을 탐색함으로써, 우리는 공간 가중치가 다른 방법보다 더 우수하고 직접 핀셋링이 LORA[29]보다 낫다는 것을 식별한다. 그림. 1은 우리의 방법으로 생성된 시각적 예를 보여준다.\n' +
      '\n' +
      'Our main contributions are summarized as follows:\n' +
      '\n' +
      '* We propose a method to overcome the data for training high-quality video models by disentangling motion from appearance at the data level.\n' +
      '* 우리는 공간 모듈과 시간 모듈 간의 연결과 분포 이동을 조사한다. 우리는 고품질 비디오 모델을 얻기 위한 키를 식별한다.\n' +
      '* We는 관측치를 기반으로 효과적인 파이프라인을 설계하며, _i.e.__는 완전히 훈련된 비디오 모델을 먼저 획득하고 합성된 고품질 이미지로 공간 모듈을 튜닝한다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '영상 생성 기술의 진화는 생성 모델의 개발과 함께 진행된다. 생성적 적대 네트워크[17] 및 가변적 자동 암호화 네트워크[18]는 비디오 생성 초기 연구에서 일반적으로 사용되는 백본으로, _e,_VGAN[47], TGAN[40], MoCoGAN[44], GODIA[52], StyleGAN-V[41], MCVD[46]이다. 그런 다음 변압기가 다양한 분야에서 성공적으로 적용되었기 때문에 비디오 합성, _e,_CogV 비디오 [28], 비디오GPT[57], NUVA-인피니티[53], TATS[19], MAGVIT[58], 페나키[45]에도 도입된다.\n' +
      '\n' +
      '최근 확산 모델(DM)[25, 42, 43]은 생성 모델에서 유명 스타로서 특히 텍스트 대 이미지(T2I) 세대[13, 21, 24, 34, 36, 37, 38, 39, 62]에서 유명하다. 비디오 생성을 위해 비디오의 분포를 모델링하기 위해 비디오 디퓨전 모델(VDM)이 제안된다. VDM[27]은 무조건적인 영상 생성을 위해 픽셀 공간에서 영상을 모델링하기 위해 시공간적 요인화된 U-Net을 활용하는 것이 처음이다. 그것은 개념을 잊는 것을 피하기 위해 이미지 영상 공동 훈련 전략을 사용한다. 이미젠 비디오[26]와 메이크-a-비디오[30]는 픽셀 공간에서 텍스트 대 비디오 생성을 목표로 하는 2개의 캐스케이드 모델이다. Show-1 [60]은 IF [1]을 기본 모델로 사용하는 또 다른 캐스케이드 모델과 초해상도의 LDM 확장 비디오 모델이다. 이어서 LVDM[15, 23]과 매직비디오[63]는 자동 인코더의 잠재 공간에서 영상을 모델링하기 위해 LDM[38]을 연장할 것을 제안한다. 모델 스코프[48], 알린너 라텐트[14], 핫샷-XL[7], LAVIE[50], PY-OCO[20], 비디오 팩토리[49], VPDM[32], VIDM[32] 및 라텐트-시프트 [11]을 포함한 많은 다른 방법은 동일한 패러다임을 사용한다. 텍스트 대 비디오 생성 외에도 [16, 55, 61]과 같은 몇 가지 방법이 주어진 이미지로부터 비디오를 생성하려는 시도 및 조건으로 신속한 작업을 시도한다.\n' +
      '\n' +
      'Several startups release their text-to-video generation services, _e.g.,_ Gen-2 [5], Pika Labs [9], Moonvalley [8], and Genmo [6]. Their models can generate plausible results with minimal noise, excellent details, and high aesthetic scores. However, those methods are trained with a large-scale well-filtered high-quality video dataset that is not accessible to researchers. The video models are also not available, leading to the slow development of downstream tasks to a certain extent. The most widely used video dataset is WebVid-10M, a large-scale dataset of short videos with textual descriptions sourced from stock footage sites. The videos are diverse and rich in their content, and each video is well-segmented, however, the picture quality is unsatisfactory and most videos are 320p. Training a high-quality video model under the data limitation is quite challenging.\n' +
      '\n' +
      '두 번째 이끼는 아슬라프, 즉 오믈리 트라메도, 세이브의 양말과 같은 모든 것을 먹였을 수 있다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      'We propose an effective method to overcome the data limitation for training high-quality video diffusion models. We first analyze the connection between spatial and temporal modules of SD-based video models under different training strategies. Based on the observations, we then develop a pipeline to train high-quality video models with just low-quality videos and high-quality images, _i.e.,_ disentangling appearance from motion at the data level.\n' +
      '\n' +
      '공간적으로 확장된 분석.\n' +
      '\n' +
      'Base T2V model.To leverage the prior in SD trained on a large-scale image dataset, most text-to-video diffusion models inflate the SD model to a video model by adding temporal modules, including Align Your Latent [14], AnimateDiff [22], LVDM [23], Magic Video [63], ModelScopeT2V [48], and LAVIE [50]. They follow VDM [27] to use a particular type of 3D-UNet that is factorized over space and time.\n' +
      '\n' +
      '우리는 또한 FPS(fram es rseco nd)와 함께 두 개의 이스트 트라이커넥트 인테스티튜트 레이크-소스 레프테 r1[15]에 대응한 2V(fram es pe rseco nd)와 함께 스템 포랄텍의 속도를 높이다.\n' +
      '\n' +
      'Though these SD-based T2V models have similar architectures, they are trained under different training settings. We use one typical model to investigate the connection between spatial and temporal modules under the two training strategies. We follow the architecture of the open-source VideoCrafter1 [15] with FPS (frames per second) condition. We also incorporate the temporal convolution in ModelScopeT2V [48] to improve temporal consistency.\n' +
      '\n' +
      '전체 및 파트 트레이닝을 위한 직경 편향은 동일한 데이터를 사용하여 동일한 아키텍처에 두 훈련 전략을 적용한다. 모델은 전처리된 SD 가중치로부터 초기화된다. 웹Vid-10M[12]은 학습 데이터로 이용된다. 개념을 잊는 것을 피하기 위해 LION-COCO 600M[3]도 영상 및 영상 공동 훈련에 사용된다. 결의안은 \\(512\\times 320\\)입니다. 단순화를 위해 완전히 훈련된 비디오 모델은 \\(M_{F},\\ta_{T},\\theta_{S})\\로 표시되고 부분적으로 훈련된 비디오 모델은 \\(M_{P}(\\theta_{T},\\theta_{T},\\ta_{S},^{0})\\로 표시되며, 여기서 \\(\\_{T})는 각각 시간적 및 공간적 모듈의 학습된 파라미터이다. (\\theta_{S}^{0}\\)은 SD의 원래 공간 매개변수이다.\n' +
      '\n' +
      '__S 파시알Pe rtu rtu rbation.__S 파시알Pe rtu rtu rbation._S 파시알Pe rtu rtu rbation. 두 개의 바오모드 라자의 탁한 비늘적인 오트 모델(M _{F }\\)ca 네베드 에페시아 래퍼는 테리마 지오 데이터라고 부른다.\n' +
      '\n' +
      '_Spatial Perturbation._ We first perturb the spatial parameters of the two video models using the image dataset. The temporal parameters are frozen. The perturbation process of the fully trained base model \\(M_{F}\\) can be denoted as:\n' +
      '\n' +
      'MS(M_{F},\\ta_{T},\\ta_{S}),\\{{F}(\\mathcal{D})\n' +
      '\n' +
      'where \\(\\text{PERTB}_{\\theta_{S}}^{\\text{LORA}}\\) denote finetuning \\(M_{F}\\) with respect to \\(\\theta_{S}\\) on the image dataset \\(\\mathcal{D}_{I}\\) using LORA. \\(\\Delta_{\\theta_{S}}\\) represents the parameters of the LORA branch. Similarly, we can obtain the perturbed model of the partially trained video model:\n' +
      '\n' +
      '\\[M_{P}^{{}^{\\prime}}(\\theta_{T},\\theta_{S}^{0}+\\Delta_{\\theta_{S}})\\leftarrow \\text{PERTB}_{\\theta_{S}}^{\\text{LORA}}(M_{P}(\\theta_{T},\\theta_{S}^{0}), \\mathcal{D}_{I}).\\]\n' +
      '\n' +
      '쉽게 이해하기 위해 모델 \\(M_{F}^{{{{}^{\\prime}}\\) 및 \\(M_{P}^{{{}^{\\prime}}\\)에 대한 \'P-Spa-LORA\'를 나타내기 위해 \'F-Spa-LORA\'라는 이름도 사용한다. \'F\'는 완전히 훈련된 기본 모델을 나타내는 반면 \'P\'는 부분적으로 훈련된 모델을 나타낸다. \'Spa\'와 \'템프\'는 각각 미세한 공간 모듈과 시간적 모듈을 의미한다. \'LORA\'는 파이밍을 위해 LORA를 사용하는 반면, \'DIR\'은 LORA가 없는 직접 핀셋링을 의미한다. 예를 들어, \'F-공간-LORA\'는 LORA를 사용하여 완전히 훈련된 T2V 모델의 교란 공간 모듈을 나타낸다.\n' +
      '\n' +
      'Comparing the synthesized videos of the two resulting models, we have the following observations. First, the motion quality of F-Spa-LORA is more stable than that of P-Spa-LORA (see user study in Table 4). The motion of P-Spa-LORA becomes worse quickly during the finetuning process. The more finetuning steps, the video tends to be more still with local flicker (see Fig. 2). While the motion of F-Spa-LORA slightly degenerates compared to the fully trained base model. Second, P-Spa-LORA achieves much better visual quality than F-Spa-LORA (see Fig. 2). The picture quality and aesthetic score of F-Spa-LORA are greatly improved compared to the partially trained base model (see Table 3). Surprisingly, the watermark is also removed. While F-Spa-LORA obtains a slight improvement in picture quality and aesthetic score, the generated videos are still noisy.\n' +
      '\n' +
      'From the two observations, we can conclude that the coupling strength between spatial and temporal modules of the fully trained model is stronger than that of the partially trained model. Because the spatial-temporal coupling of the partially trained model can be easily broken, leading to quick motion degeneration and picture quality shift. A stronger connection can tolerate parameter perturbation more than a weak one. Our observation can be used to explain the quality improvement and motion degeneration of AnimateDiff. AnimateDiff is not a generic model and only works for selected personalized SD models. The reason is that its motion modules are obtained with the partially training strategy, and they cannot tolerate large parameter perturbations. When the personalized model does not match the temporal modules, both picture and motion quality will degenerate.\n' +
      '\n' +
      '__시간적 편향__시간적 반복__시간적 변절._시간적 교란. 부분적으로 훈련된 모델은 시간 모듈만 업데이트되지만, 화질은 WebVid-10M의 품질로 이동된다. 따라서 시간 모듈은 동작뿐만 아니라 화질에도 책임을 진다. 이미지 데이터셋으로 공간 모듈을 고정하면서 시간 모듈을 교란시켰다. 변조 과정은 그대로 나타낼 수 있다.\n' +
      '\n' +
      '\\[M_{F}^{{}^{\\prime\\prime}}(\\theta_{T}+\\Delta_{\\theta_{T}},\\theta _{S}) \\leftarrow\\text{PERTB}^{\\text{LORA}}_{\\theta_{T}}(M_{F}(\\theta_{T},\\theta_{S}),\\mathcal{D}_{I}),\\] \\[M_{P}^{{}^{\\prime\\prime}}(\\theta_{T}+\\Delta_{\\theta_{T}},\\theta _{S}^{0}) \\leftarrow\\text{PERTB}^{\\text{LORA}}_{\\theta_{T}}(M_{P}(\\theta_{T},\\theta_{S}^{0}),\\mathcal{D}_{I}).\\]\n' +
      '\n' +
      'We observe that the picture quality of P-Temp-LORA (\\(M_{P}^{{}^{\\prime\\prime}}\\)) is better than F-Temp-LORA (\\(M_{P}^{{}^{\\prime\\prime}}\\)). However, the foreground and background of the videos are more shaky, _i.e.,_ the temporal consistency becomes worse (see Fig. 3). The picture of F-Temp-LORA is improved, but the watermark is still there. Its motion is close to the base model and much better than P-Temp-LORA (see Table 4). Those observations also support the conclusion obtained from spatial perturbation.\n' +
      '\n' +
      '그림 3: LORA를 이용한 흡입 시간 모듈 _<그림 3>. _Perturbing 시간적 모듈. 아크로밭 판독기로 가장 잘 보았습니다. 영상을 클릭하여 비디오 클립을 재생합니다.___ 비디오를 재생합니다.\n' +
      '\n' +
      '중요성 및 움직임의 중요도 수준 설명요.\n' +
      '\n' +
      '저작권 문제로 다양성이 높은 대규모 고품질 비디오 데이터셋 획득이 어려워 질 높은 영상을 사용하지 않고 수준 높은 비디오 모델을 학습할 수 있는 가능성을 모색한다. 웹비드-10M과 같은 저품질 비디오와 JDB와 같은 고품질 이미지에 액세스할 수 있습니다. 우리는 고품질 영상에서 화질과 미학을 학습하면서 저품질 영상으로부터 데이터 수준, _i.e._학습운동에서 외모에 의한 미관을 단절할 것을 제안한다. 우리는 먼저 비디오로 비디오 모델을 훈련한 다음 영상으로 비디오 모델을 미세 조정 할 수 있다. _ 키들은 비디오 모델을 훈련하는 방법과 이미지들로 어떻게 미세 조정해야 하는지에 놓여 있다.___의 이미지들로 나열한다.\n' +
      '\n' +
      '공간 모듈과 시간 모듈 간의 연결 연구에 따르면, 완전히 훈련된 모델은 고품질 이미지로 후속 핀셋링에 더 적합하다. 강력한 공간-시간적 커플링은 명백한 움직임 변성이 없이 공간 모듈과 시간 모듈 모두에 대한 매개변수 섭동을 견딜 수 있기 때문이다.\n' +
      '\n' +
      'Next, we need to investigate how to fine-tune the base model with images. In both spatial and temporal perturbation (Sec. 3.1), the picture quality can be improved but not very significantly. To obtain a greater quality improvement, we evaluate two strategies. One is to involve more parameters, _i.e.,_ finetuning both spatial and temporal modules with images. The other is to change the finetuning method, _i.e.,_ using direct finetuning without LORA. We can evaluate the following four cases:\n' +
      '\n' +
      'Hff}(\\ta_\\{T}),\\_{{S}(\\ta_\\{S}),\\_{{F}(\\ta_\\{D}),\\_{{S})\n' +
      '\n' +
      'Where \\(M_{F}^{A}\\) (F-Spa&Temp-LORA) is obtained by following the first strategy, \\(M_{F}^{B}\\), \\(M_{F}^{C}\\), and \\(M_{F}^{D}\\) are obtained using the second strategy. \\(M_{F}^{B}\\) (F-Spa-DIR) and \\(M_{F}^{C}\\) (F-Temp-DIR) represent directly finetuning the spatial and temporal modules, respectively. \\(M_{F}^{D}\\) (F-Spa&Temp-DIR) represents directly finetuning all modules.\n' +
      '\n' +
      '4개 모델의 생성된 비디오를 비교해보면 다음과 같은 관찰이 있다. 먼저 F-Spa&Temp-LORA는 F-Spa-LORA의 화질을 더욱 향상시킬 수 있지만, 품질은 여전히 만족스럽지 못하고 있다. 대부분의 생성된 비디오에는 워터마크가 존재하며 소음은 분명하다. 둘째, F-Temp-DIR은 F-Temp-LORA보다 더 나은 화질을 달성한다. 또한 F-Spa&Temp-LORA보다 좋습니다. 수박은 비디오의 절반으로 제거되거나 가볍습니다. 셋째, F-Spa&DIR과 F-Spa&Temp-DIR은 미세 조정 모델 중 최고의 화질을 달성한다. 그러나 F-Spa-DIR의 움직임은 더 낫다(그림 4 및 표 4 참조). F-Spa&Temp-DIR의 전경 및 배경은 특히 로컬 텍스처인 \\(M_{F}^{D}\\)에 의해 생성된 비디오에서 플러싱된다.\n' +
      '\n' +
      '핀셋링 전략과 다른 모듈을 탐색함으로써, 우리는 직접 핀셋링 전략 및 상이한 모듈로 공간 모듈을 조달하는 것을 식별한다.\n' +
      '\n' +
      '완전 훈련된 T2V 모델을 기반으로 한 모듈 선택 _그림 4: 모듈 선택은 완전히 훈련된 T2V 모델을 기반으로 한다. 아크로밭 판독기로 가장 잘 보았습니다. 영상을 클릭하여 비디오 클립을 재생합니다.___ 비디오를 재생합니다.\n' +
      '\n' +
      'Figure 5: Influence of image data on concept composition. ‘F-Spa-DIR-LAION’ uses the LAION aesthetics V2 as the image data while ‘F-Spa-DIR’ uses JDB. _Best viewed with Acrobat Reader. Click the images to play the video clips._high-quality images is the best way to improve the picture quality without marginal loss of motion quality. At this point, our data-level disentanglement pipeline can be summarized as follows: fully training a video model with low-quality videos first and then directly finetuning the spatial modules only with high-quality images.\n' +
      '\n' +
      '문서 구성 촉진\n' +
      '\n' +
      'To improve the concept composition ability of video models, we propose to use synthesized images with complex concepts instead of using real images at the partial finetuning stage. The success of T2I models such as SDXL and Midjornery is built upon large-scale high-quality images. They have the ability to composite concepts that do not appear in the real world. Rather than using their training images, we propose transferring their concept composition ability to video models by synthesizing a set of images with complex concepts. In this way, we can alleviate the burden of capturing both concept and motion well at the same time.\n' +
      '\n' +
      '합성된 이미지의 효과를 검증하기 위해 JDB와 LAION-에스테이션 V2[2]를 제2 핀셋링 단계의 이미지 데이터로 사용한다. LAION-에스티섹션 V2는 웹 수집 이미지로 구성되며 JDB에는 미도즈니가 합성한 이미지가 포함되어 있다. JDB로 훈련된 모델이 훨씬 더 나은 개념 구성 능력을 가지고 있음을 관찰한다(그림 5 및 표 3 참조). 추가 자료에 더 많은 결과가 있습니다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Settings\n' +
      '\n' +
      'Data.To overcome data limitations, we utilized WebVid-10M [12] as the source of low-quality video data and JDB [35] for high-quality image data. WebVid-10M is a large-scale, diverse video dataset comprising approximately 10 million text-video pairs. The resolution of most videos is \\(336\\times 596\\), and each video consists of a single shot. During training, we sample from videos at varying frame rates. JDB is a large-scale image dataset featuring around 4 million high-resolution images from Midjourney, each annotated with a corresponding text prompt. To prevent concept forgetting during the training of the base T2V model, we also employ LAION-COCO[3], a dataset comprising 600 million generated high-quality captions for publicly available web images, for both image and video training.\n' +
      '\n' +
      'Metrics.We exploit EvalCrafter [31] for quantitative evaluation. EvalCrafter is a benchmark to evaluate text-to-video generation models, which contains around 18 objective metrics for visual quality, content quality, motion quality, and text-caption alignment. It provides about 512 prompts. The objective metrics are aligned to user opinions from five subjective studies, i.e., motion quality, text-video alignment, temporal consistency, visual quality, and user favor. The motion quality considers three metrics: action recognition, average flow, amplitude classification score, while temporal consistency considers warping error, semantic consistency, face consistency. The technical and aesthetic scores in EvalCrafter are adapted from DOVER [54]. Besides, we conduct user studies of human preference since there still lacks a comprehensive objective metric to measure motion quality.\n' +
      '\n' +
      'Training Details.In Sec 3.1, the two based models share the same architecture, adapted from the open-source VideoCrafter1 [15], and incorporate temporal convolution from ModelScopeT2V [48]. The spatial modules are initialized with weights from SD 2.1, and the outputs of the temporal modules are initialized to zeros. The training resolution is set at \\(512\\times 320\\). For joint image and video training, we utilize the low-quality WebVid-10M and LAION-COCO datasets. The models are trained on 32 NVIDIA A100 GPUs for \\(270K\\) iterations with a batch size of 128. The learning rate is set at \\(5\\times 10^{-5}\\) for all training tasks. When employing LORA for the perturbation of temporal or spatial modules, we exclusively use JDB for tuning. The finetuning is conducted on 8 A100 GPUs for \\(30K\\) iterations with a batch size of 256. Given that the images from JDB have a square resolution, we adjust the finetuning resolution to \\(512\\times 512\\).\n' +
      '\n' +
      'S2V 모델은 주와 비교합니다.\n' +
      '\n' +
      '우리는 유전자-2 [5] 및 피카랩스[9]와 같은 인기 상업 모델뿐만 아니라 쇼-1[60], 비디오 스키드1[15] 및 AnimateDiff [22]와 같은 오픈 소스 모델을 포함한 여러 최첨단 T2V 모델과 접근법을 비교한다. 유전자-2, Pika Labs 및 VideoCrafter1은 모두 T2V 모델을 트레이닝하기 위해 고품질 비디오를 사용한다. 아메리칸디프와 우리 모델은 웹바이드-10M의 비디오만을 사용한다는 점은 주목할 만하다. 쇼-1은 웹Vid-10M의 워터마크를 제거하기 위해 핀셋을 위해 추가 고품질 비디오를 사용한다. 안티펜디프는 일반적인 T2V 모델이 아니며 LORA SD 모델이 시간 모듈과 호환되어야 작동한다. 우리의 비교를 위해 SD v1.5를 기반으로 한 시간 모듈(제2 버전)을 사용하고 해당 LORA 모델로 리얼리즘 비전 V2.0 [10]을 사용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline  & Visual & Text-Video & Motion & Temporal \\\\  & Quality & Alignment & Quality & Consistency \\\\ \\hline Pikal\\(\\text{ab}^{*}\\) & 63.52 & 54.11 & 57.74 & 69.35 \\\\ Gen2\\({}^{*}\\) & 67.35 & 52.30 & 62.53 & 69.71 \\\\ \\hline VideoCrafter1 & 61.64 & 66.76 & 56.06 & 60.36 \\\\ Show-1 & 52.19 & 62.07 & 53.74 & 60.83 \\\\ AnimateDiff & 58.89 & 74.79 & 51.38 & 56.61 \\\\ Ours & 63.28 & 64.67 & 53.95 & 62.02 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Comparison on the EvalCrafter benchmark. Higher score indicates better performance. * commercial models.\n' +
      '\n' +
      '정량적 평가는 표 1에 EvalCraub를 사용하여 얻은 정량적 결과가 제시되어 있으며, 이 방법은 트레이닝을 위해 고품질 비디오를 사용하는 VideoCrafter1 및 Pika Labs와 유사한 시각적 품질을 달성한다. 이것은 고품질 이미지를 사용하여 화질과 심미성을 향상시키는 효과를 강조한다.\n' +
      '\n' +
      '그림 6: 다양한 텍스트 대 세대 아이오온 모델 s.__ 생성 아토온 모델 s의 비교이다. 크라우드 이발 r._ 크로브 이음대 r.Cim 연령 정지 yt 헵비 데오 클립을 클릭한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:8]\n' +
      '\n' +
      'LAION 임상 V2 시각 품질에 대한 정량적 평가는 표 3에 나와 있으며, F-Spa-DIR은 미학적 및 기술적 점수 모두에서 F-Spa-DIR-LAION보다 훨씬 우수하다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '데이터 한계를 극복하기 위해 고품질 비디오를 사용하지 않고 고품질 비디오 확산 모델을 훈련하는 방법을 제안한다. SD 기반 비디오 모델의 훈련 계획에 대해 설명하고 공간 차원 및 시간적 차원 간의 결합 강도를 조사한다. 완전히 훈련된 T2V 모델이 부분적으로 훈련된 모델보다 더 강한 공간-시간 결합을 나타냄을 관찰한다. 이러한 관찰을 바탕으로 모션 러닝을 위한 저품질 동영상과 외관 학습을 위한 고품질 이미지를 활용함으로써 데이터 수준 _i.__에서의 동작으로부터 불협화음을 제안한다. 또한, 우리는 실제 이미지보다는 핀셋링을 위해 복잡한 컨셉이 있는 합성 이미지를 사용하는 것을 제안한다. 제안된 방법의 효과를 입증하기 위해 정량적, 질적 평가가 이루어진다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]S. 지, T. 하예, H. 양, X. Yin, G. Pang, D. Jacobs, J. Huang 및 D. Parikh(2022) 롱 비디오 생성은 시간 진단 vqgan 및 시간에 민감한 변압기를 가지고 있다. ECCV에서 pp. 102-118.: SS1에 의해 계산된다.\n' +
      '* [2]H. Chen, M. Xia, Y. He, Y. Zhang, X. Cun, S. Yang, J. Xing, Y. Liu, Q. Chen, X. Wang, et al. (2023) VideoCarfter1: open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512. Cited by: SS1.\n' +
      '* [6]S. 지,S. Na h,G. Li u,T.P oon,A. Ta o,Ba Ctzar o, D.Jac ob s,J. Hu ang,M.L iu,an dY.Ba laji(2 021)Preserveyo urow ncorrelat 이온: ois 에피프리포비드 에오디프 융합 모델 s.I CCV,Cite dby:SS1.\n' +
      '* [4]M. Chen, Y. Wang, L. Zhang, S. Zhuang, X. Ma, J. Yu, Y. Wang, D. Lin, Y. Qiao, and Z. Liu (2023) Seine: short-to-long video diffusion model for generative transition and prediction. arXiv preprint arXiv:2310.20700. Cited by: SS1.\n' +
      '* [5]C. Doersch (2016) Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908. Cited by: SS1.\n' +
      '* [6]S. Ge, S. Nah, G. Liu, T. Poon, A. Tao, B. Catanzaro, D. Jacobs, J. Huang, M. Liu, and Y. Balaji (2021) Preserve your own correlation: a noise prior for video diffusion models. In ICCV, Cited by: SS1.\n' +
      '* [7]S. Ge, D. Chen, J. Bao, F. Wen, B. Zhang, D. Chen, L. Yuan, and B. Guo (2022) Vector quantized diffusion model for text-to-image synthesis. In CVPR, Cited by: SS1.\n' +
      '*[10]J.호, Aa 진,a nd P.Ab는 엘(20 20)데노는 뉴 rIPS, 세디비:SS1에서 이온 프로비알리스틱 모델 s.\n' +
      '*[9]Y. 그는 S. 양, H. 첸, X. 쿡, M. 샤, Y. 장, X. 왕, R. 그는 Q. 텐과 Y. 산(2023) 스칼칼라더: 확산 모델이 있는 튜닝이 없는 고해상도 시각 생성입니다. arXiv 프리프린트 arXiv:2310.07702.: SS1에 의해 계산됩니다.\n' +
      '* [10]J. Ho, A. Jain, and P. Abbeel (2020) Denoising diffusion probabilistic models. In NeurIPS, Cited by: SS1.\n' +
      '* [11]J. Ho, W. D. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J. Fleet, et al. (2022) Imagen video: high definition video generation with diffusion models. arXiv preprint arXiv:2210.02303. Cited by: SS1.\n' +
      '* [12]J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet (2022) Video diffusion models. In NeurIPS, Cited by: SS1.\n' +
      '* [13]W. Hong, M. Ding, W. Zheng, X. Liu, and J. Tang (2022) CogVideo: large-scale pretraining for text-to-video gen eration via transformers. _arXiv preprint arXiv:2205.15868_, 2022.\n' +
      '* [29] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [30] Yaosi Hu, Chong Luo, and Zhenzhong Chen. Make it move: controllable image-to-video generation with text descriptions. In _CVPR_, 2022.\n' +
      '* [31] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models, 2023.\n' +
      '* [32] Kangfu Mei and Vishal Patel. Vidm: Video implicit diffusion models. In _AAAI_, pages 9117-9125, 2023.\n' +
      '* [33] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In _ICCV_, 2019.\n' +
      '* [34] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In _ICML_, 2022.\n' +
      '* [35] Junting Pan, Keqiang Sun, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: A benchmark for generative image understanding. _arXiv preprint arXiv:2307.00716_, 2023.\n' +
      '* [36] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* [37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.\n' +
      '* [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022.\n' +
      '* [39] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _NeurIPS_, 2022.\n' +
      '* [40] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular value clipping. In _ICCV_, pages 2830-2839, 2017.\n' +
      '* [41] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoesiny. Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2. In _CVPR_, pages 3626-3636, 2022.\n' +
      '* [42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _ICML_, 2015.\n' +
      '* [43] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _ICLR_, 2021.\n' +
      '* [44] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In _CVPR_, pages 1526-1535, 2018.\n' +
      '* [45] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. _arXiv preprint arXiv:2210.02399_, 2022.\n' +
      '* [46] Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal. Mcvd-masked conditional video diffusion for prediction, generation, and interpolation. In _NeurIPS_, 2022.\n' +
      '* [47] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. _NeurIPS_, 29, 2016.\n' +
      '* [48] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. _arXiv preprint arXiv:2308.06571_, 2023.\n' +
      '* [49] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation. _arXiv preprint arXiv:2305.10874_, 2023.\n' +
      '* [50] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yin He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. _arXiv preprint arXiv:2309.15103_, 2023.\n' +
      '* [51] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, et al. Internvid: A large-scale video-text dataset for multimodal understanding and generation. _arXiv preprint arXiv:2307.06942_, 2023.\n' +
      '* [52] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. _arXiv preprint arXiv:2104.14806_, 2021.\n' +
      '* [53] Chenfei Wu, Jian Liang, Xiaowei Hu, Zhe Gan, Jianfeng Wang, Lijuan Wang, Zicheng Liu, Yuejian Fang, and Nan Duan. Nuwa-infinity: Autoregressive over autoregressive generation for infinite visual synthesis. _arXiv preprint arXiv:2207.09814_, 2022.\n' +
      '* [54] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In _CVPR_, pages 20144-20154, 2023.\n' +
      '* [55] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. _arXiv preprint arXiv:2310.12190_, 2023.\n' +
      '* [56] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In _CVPR_, 2022.\n' +
      '* [57] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videoopt: Video generation using vq-vae and transformers. _arXiv preprint arXiv:2104.10157_, 2021.\n' +
      '\n' +
      '* [58] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In _CVPR_, 2023.\n' +
      '* [59] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In _CVPR_, pages 18456-18466, 2023.\n' +
      '* [60] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng. Show-1: Marrying pixel and latent diffusion models for text-to-video generation, 2023.\n' +
      '* [61] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models, 2023.\n' +
      '* [62] Yuechen Zhang, Jinbo Xing, Eric Lo, and Jiaya Jia. Real-world image variation by aligning diffusion inversion chain. _arXiv preprint arXiv:2305.18729_, 2023.\n' +
      '* [63] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. _arXiv preprint arXiv:2211.11018_, 2022.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>