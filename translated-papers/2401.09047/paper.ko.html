<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '카탈로그에 대한 데이터 임탁\n' +
      '\n' +
      '오케이.\n' +
      '\n' +
      ' 샤오동 쿡은 마메도니아샤오신 첸({}^{\\dagger}})과 용장성(神門)이다.\n' +
      '\n' +
      '신타오 왕, 차오왓.\n' +
      '\n' +
      'AI.\n' +
      '\n' +
      'Homepage: [https://ailab-cvc.github.io/videocrafter](https://ailab-cvc.github.io/videocrafter)\n' +
      '\n' +
      'Github: [https://github.com/AILab-CVC/VideoCrafter](https://github.com/AILab-CVC/VideoCrafter)\n' +
      '\n' +
      'Discord: [https://discord.gg/RQENrunu92](https://discord.gg/RQENrunu92)\n' +
      '\n' +
      ' _사이버 정크, 네온패크 스타일, 궁후판다, 점프, 킥. 시네마틱 사진은 원뿔 아래로 피스타치오 아이스크림을 녹여줍니다. 35mm 사진, 영화, 보케. 나비로 둘러싸인 큰 운동은 소녀가 울창한 정원을 산책하고 있다._나비로 둘러쌓여 있다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '텍스트 대 비디오 생성은 주어진 프롬프트에 기초하여 비디오를 생성하는 것을 목표로 한다. 최근 여러 상용 비디오 모델은 소음, 우수한 세부 사항 및 높은 심미적 점수로 그럴듯한 비디오를 생성할 수 있게 되었다. 그러나 이러한 모델은 커뮤니티에 접근할 수 없는 대규모, 잘 여과된 고품질 영상에 의존한다. 저품질 웹Vid-10M 데이터셋을 사용하여 모델을 훈련시키는 기존 많은 연구 작업은 모델이 웹Vid-10M에 적합하도록 최적화되기 때문에 고품질 비디오를 생성하기 위해 어려움을 겪고 있다. 본 연구에서는 스테이블 디퓨전으로부터 확장된 비디오 모델의 훈련 방식을 탐색하고, 양질의 영상 및 합성 고품질 영상을 레버리징하여 고품질 비디오 모델을 얻을 수 있는 가능성을 조사한다. 먼저 비디오 모델의 공간적 모듈과 시간적 모듈 간의 연결과 낮은 품질의 비디오로의 분포 이동을 분석한다. 우리는 모든 모듈의 전체 훈련이 트레이닝 시간 모듈만 하는 것보다 공간적 모듈과 시간적 모듈 간의 더 강한 결합을 초래한다는 것을 관찰한다. 이 더 강한 결합을 기반으로 고품질 이미지로 공간 모듈을 미세화하여 움직임 저하 없이 분포를 더 높은 품질로 전환하여 일반적인 고품질 비디오 모델을 생성했다. 특히 화질, 운동, 개념 구성에서 제안된 방법의 우월성을 입증하기 위해 평가를 실시한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '확산 모델 개발(25, 43], 비디오 생성은 특히 기본 텍스트 대 비디오(T2V) 생성 모델에서 돌파구를 달성했다. 대부분의 기존 방법[14, 23, 26, 30, 48, 63]은 비디오 모델을 얻기 위한 논리를 따르며, _i.e._i. 시간 모듈을 추가하여 텍스트 대 이미지(T2I) 백본을 비디오 모델에 확장한 다음 비디오로 트레이닝한다. 여러 모델은 처음부터 비디오 모델을 훈련시키는 반면, 대부분의 모델은 사전 훈련된 T2I 모델, 전형적으로 Stable Diffusion(SD) [38])에서 시작된다. 모델은 또한 확산 모델, _i,_픽셀 공간 모델[26, 30, 60] 및 잠재 공간 모델[14, 23, 48, 63]에 의해 모델링된 공간을 기반으로 두 그룹으로 분류할 수 있다. 후자는 지배적인 접근법이다. 사진 품질, 움직임 일관성 및 개념 구성은 비디오 모델을 평가하기 위한 필수 차원이다. 그림 품질은 선명도, 소음, 왜곡, 심미적 점수와 같은 측면을 의미한다. 움직임 정합성은 프레임과 모션 스무디 사이의 외관 일관성을 의미한다. 개념 구성은 실제 비디오에서 동시에 나타나지 않을 수 있는 서로 다른 개념을 결합하는 능력을 나타낸다.\n' +
      '\n' +
      '최근 몇몇 상업 스타트업들이 최소 소음, 우수한 디테일, 높은 미적 점수로 그럴듯한 영상을 제작할 수 있는 T2V 모델[5, 6, 8, 9]을 출시했다. 그러나, 그들은 커뮤니티와 학계에 접근할 수 없는 대규모 및 잘 여과된 고품질 비디오 데이터세트에서 훈련된다. 저작권 제한과 시뮬레이션 후 가공으로 인해 수백만 개의 고품질 비디오를 수집하기가 어렵다. 비디오 이해를 위해 인터넷으로부터 수집된 오픈 소스 비디오 데이터셋이 몇 개 있지만, M.100M[33], HD-VILA-100M[56], InterVid[51]은 비디오 생성, _e,_빈곤한 화질 및 캡션, 하나의 비디오에 다중 클립, 정적 프레임 또는 슬라이드를 위한 많은 문제가 있다. 웹바이드-10M[12]은 학계에서 영상 생성 모델을 훈련시키기 위해 가장 널리 사용되는 데이터세트이다. 클립은 잘 분할되어 있으며 다양성은 좋습니다. 그러나 화질은 만족스럽지 못하며 대부분의 영상은 약 320p의 해상도를 가지고 있습니다. 고품질 데이터셋의 부족은 학계에서 고품질 비디오 모델을 훈련시키는 데 상당한 장애가 된다.\n' +
      '\n' +
      '이 작품에서는 고품질의 영상을 사용하지 않고 상당히 어려운 문제, 즉,_i,_훈련 수준 높은 비디오 모델을 대상으로 한다. 우리는 SD 기반 비디오 모델의 훈련 과정에 다이빙하여 다양한 훈련 전략하에서 공간 모듈과 시간 모듈 간의 연결을 분석하고 저품질 비디오로의 분포 이동을 조사한다. 우리는 모든 모듈의 전체 훈련이 단순한 훈련 시간 모듈보다 외관과 움직임 사이의 더 강한 결합을 초래한다는 의미 있는 관찰을 한다. 풀 트레이닝은 보다 자연스러운 동작을 달성하고 더 많은 후속적인 공간 모듈의 변형을 견딜 수 있으며, 이는 생성된 비디오의 품질을 향상시키는 열쇠이다. 연결의 관찰을 바탕으로 데이터 수준에서의 외관으로부터 모션을 무력화시켜 데이터 한계를 극복할 수 있는 방법을 제안한다. 구체적으로 양질의 영상 대신 저품질 영상을 활용하여 동작 일관성을 보장하고 고품질 이미지를 사용하여 화질 및 개념 구성 능력을 보장합니다. SDXL, 미들즈니와 같은 성공적인 T2I 모델들로부터 학습하는 것은 고해상도 및 복잡한 개념 구성을 가진 큰 이미지 세트를 얻는 것이 편리하다. 분석의 지침에 따라 SD에서 확장된 비디오 모델을 완전히 훈련시키기 위해 파이프라인을 설계한다. 그런 다음 합성된 이미지로 완전히 훈련된 모델의 공간적 및 시간적 모듈을 수정하는 다양한 방법을 탐색함으로써, 우리는 공간 가중치가 다른 방법보다 더 우수하고 직접 핀셋링이 LORA[29]보다 낫다는 것을 식별한다. 그림. 1은 우리의 방법으로 생성된 시각적 예를 보여준다.\n' +
      '\n' +
      '우리의 주요 기여금은 다음과 같이 요약된다.\n' +
      '\n' +
      '* 데이터 레벨에서의 외모에서 모션을 무색하게 하여 고품질 비디오 모델 학습을 위한 데이터를 극복하는 방법을 제안한다.\n' +
      '* 우리는 공간 모듈과 시간 모듈 간의 연결과 분포 이동을 조사한다. 우리는 고품질 비디오 모델을 얻기 위한 키를 식별한다.\n' +
      '* We는 관측치를 기반으로 효과적인 파이프라인을 설계하며, _i.e.__는 완전히 훈련된 비디오 모델을 먼저 획득하고 합성된 고품질 이미지로 공간 모듈을 튜닝한다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '영상 생성 기술의 진화는 생성 모델의 개발과 함께 진행된다. 생성적 적대 네트워크[17] 및 가변적 자동 암호화 네트워크[18]는 비디오 생성 초기 연구에서 일반적으로 사용되는 백본으로, _e,_VGAN[47], TGAN[40], MoCoGAN[44], GODIA[52], StyleGAN-V[41], MCVD[46]이다. 그런 다음 변압기가 다양한 분야에서 성공적으로 적용되었기 때문에 비디오 합성, _e,_CogV 비디오 [28], 비디오GPT[57], NUVA-인피니티[53], TATS[19], MAGVIT[58], 페나키[45]에도 도입된다.\n' +
      '\n' +
      '최근 확산 모델(DM)[25, 42, 43]은 생성 모델에서 유명 스타로서 특히 텍스트 대 이미지(T2I) 세대[13, 21, 24, 34, 36, 37, 38, 39, 62]에서 유명하다. 비디오 생성을 위해 비디오의 분포를 모델링하기 위해 비디오 디퓨전 모델(VDM)이 제안된다. VDM[27]은 무조건적인 영상 생성을 위해 픽셀 공간에서 영상을 모델링하기 위해 시공간적 요인화된 U-Net을 활용하는 것이 처음이다. 그것은 개념을 잊는 것을 피하기 위해 이미지 영상 공동 훈련 전략을 사용한다. 이미젠 비디오[26]와 메이크-a-비디오[30]는 픽셀 공간에서 텍스트 대 비디오 생성을 목표로 하는 2개의 캐스케이드 모델이다. Show-1 [60]은 IF [1]을 기본 모델로 사용하는 또 다른 캐스케이드 모델과 초해상도의 LDM 확장 비디오 모델이다. 이어서 LVDM[15, 23]과 매직비디오[63]는 자동 인코더의 잠재 공간에서 영상을 모델링하기 위해 LDM[38]을 연장할 것을 제안한다. 모델 스코프[48], 알린너 라텐트[14], 핫샷-XL[7], LAVIE[50], PY-OCO[20], 비디오 팩토리[49], VPDM[32], VIDM[32] 및 라텐트-시프트 [11]을 포함한 많은 다른 방법은 동일한 패러다임을 사용한다. 텍스트 대 비디오 생성 외에도 [16, 55, 61]과 같은 몇 가지 방법이 주어진 이미지로부터 비디오를 생성하려는 시도 및 조건으로 신속한 작업을 시도한다.\n' +
      '\n' +
      '여러 스타트업이 텍스트 대 영상 생성 서비스, _e,_ Gen-2[5], Pika Labs[9], 달발리[8], 지니모[6]를 출시한다. 그들의 모델은 최소한의 소음, 우수한 세부 사항 및 높은 심미적 점수로 그럴듯한 결과를 생성할 수 있다. 그러나 이러한 방법은 연구자에게 접근할 수 없는 대규모 잘 여과된 고품질 비디오 데이터셋으로 훈련된다. 비디오 모델도 사용할 수 없어 하류 작업 개발이 일정 정도 느려진다. 가장 널리 사용되는 비디오 데이터 세트는 스톡 비디오 사이트에서 제공되는 텍스트 설명이 있는 짧은 비디오의 대규모 데이터 세트인 웹Vid-10M이다. 동영상은 다양하고 그 콘텐츠가 풍부하며, 각 영상은 잘 분할되지만, 화질은 만족스럽지 못하며 대부분의 영상은 320p이다. 데이터 제한 하에서 고품질 비디오 모델을 훈련하는 것은 상당히 어렵다.\n' +
      '\n' +
      '미생어디프[22]는 WebVid-10M에서 학습된 비디오 모델 및 LORA SD 모델에서 시간 모듈을 조합하면 생성된 비디오의 화질을 향상시킬 수 있음을 발견했다. 그러나 이것은 일반 모델이 아니며 항상 효과가 있는 것은 아니다. 몇 가지 심각한 문제가 있습니다. 먼저, 시간 모듈은 선택된 몇 개의 LORA 모델과 결합될 수 있을 뿐 일반 모델이 아니다. 둘째, 각 LORA 모델이 개인화된 모델이기 때문에 제한된 데이터로 학습된 LORA 모델이 발생하면 구성된 비디오 모델은 분해된 개념 구성에 시달릴 수 있다. 셋째, 모듈은 LORA 모델과 잘 일치하지 않을 때 모션 품질이 저하된다. 애니즐디프와는 달리 직접 조합 대신 공간 모듈과 시간적 모듈 간의 연결을 분석하고, 파이프라인을 설계하여 데이터 수준에서 외관과 움직임을 무색하게 하여 고품질 영상 없이 일반 고품질의 영상 모델을 훈련시킨다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '고품질 영상 확산 모델을 학습하기 위한 데이터 제한을 극복하기 위한 효과적인 방법을 제안한다. 우리는 먼저 다양한 훈련 전략 하에서 SD 기반 비디오 모델의 공간적 모듈과 시간적 모듈 간의 연결을 분석한다. 그런 다음 관찰을 기반으로 데이터 수준에서 모션에서 낮은 품질의 비디오와 고품질 이미지, _i,_이젠탕링 외관만으로 고품질 비디오 모델을 훈련시키기 위해 파이프라인을 개발한다.\n' +
      '\n' +
      '공간적으로 확장된 분석.\n' +
      '\n' +
      'Base T2V 모델은 대규모 이미지 데이터셋에서 훈련된 SD에서 이전에 레버리지하기 위해 대부분의 텍스트 대 비디오 확산 모델은 Align Your228ent[14], AnimateDiff[22], LVDM[23], Magic Video[63], ModelScopeT2V[48], LAVIE[50]를 포함한 시간적 모듈을 추가하여 SD 모델을 비디오 모델에 추론한다. 그들은 VDM[27]을 따라 공간과 시간에 걸쳐 요인화되는 특정 유형의 3D-UNet을 사용한다.\n' +
      '\n' +
      '이 모델은 훈련 전략에 따라 두 그룹으로 분류할 수 있다. 하나는 비디오를 사용하여 SD 가중치를 갖는 공간 모듈과 시간 모듈을 모두 초기화, 즉 _full 훈련_으로 학습하는 것이다. 다른 하나는 _부분 훈련_이라 불리는 공간적인 모듈들로 시간적 모듈을 훈련시키는 것이다. 존경하는 라텐트와 아나펜디프는 첫 번째 그룹에 속하며 다른 T2V 모델은 다른 그룹에 속한다.\n' +
      '\n' +
      '이러한 SD 기반 T2V 모델은 유사한 아키텍처를 가지고 있지만 서로 다른 훈련 설정으로 훈련된다. 우리는 하나의 전형적인 모델을 사용하여 두 훈련 전략 하에서 공간적 모듈과 시간적 모듈 간의 연결을 조사한다. 우리는 FPS(초당 프레임) 조건을 가진 오픈 소스 비디오 Crafter1[15]의 구조를 따른다. 우리는 또한 시간적 일관성을 향상시키기 위해 시간 컨볼루션을 모델 스코프T2V [48]에 통합한다.\n' +
      '\n' +
      '전체 및 파트 트레이닝을 위한 직경 편향은 동일한 데이터를 사용하여 동일한 아키텍처에 두 훈련 전략을 적용한다. 모델은 전처리된 SD 가중치로부터 초기화된다. 웹Vid-10M[12]은 학습 데이터로 이용된다. 개념을 잊는 것을 피하기 위해 LION-COCO 600M[3]도 영상 및 영상 공동 훈련에 사용된다. 결의안은 \\(512\\times 320\\)입니다. 단순화를 위해 완전히 훈련된 비디오 모델은 \\(M_{F},\\ta_{T},\\theta_{S})\\로 표시되고 부분적으로 훈련된 비디오 모델은 \\(M_{P}(\\theta_{T},\\theta_{T},\\ta_{S},^{0})\\로 표시되며, 여기서 \\(\\_{T})는 각각 시간적 및 공간적 모듈의 학습된 파라미터이다. (\\theta_{S}^{0}\\)은 SD의 원래 공간 매개변수이다.\n' +
      '\n' +
      '공간 모듈과 시간 모듈 간의 연결 강도를 평가하기 위해 핀셋을 위해 다른 고품질 이미지 데이터세트(\\mathcal{D}_{I}\\)를 사용하여 지정된 모듈의 파라미터를 교란시켰다. 이미지 데이터는 미도제리[4]의 합성 이미지로 구성된 JDB[35]이다. JDB는 4백만 개의 이미지를 가지고 있기 때문에, 우리는 지느러미를 위해 LORA[29]를 사용한다.\n' +
      '\n' +
      '__공간적 편향__공간적 삽입._공간적 교란._공간적 변절._공간적 변절. 우리는 먼저 이미지 데이터 세트를 사용하여 두 비디오 모델의 공간 파라미터를 교란시켰다. 시간 파라미터는 동결되었습니다. 완전 훈련된 기본 모델 \\(M_{F}\\)의 섭동 과정은 그대로 나타낼 수 있다.\n' +
      '\n' +
      'MS(M_{F},\\ta_{T},\\ta_{S}),\\{{F}(\\mathcal{D})\n' +
      '\n' +
      'HLORA(\\text{PERTB}_{\\theta_{S}}^{\\text{LORA}}\\)는 LORA를 사용한 이미지 데이터세트 \\(\\mathcal{D}_{I}_{I}\\)에서 \\(\\ta_{S}\\)와 관련하여 핀셋링 \\(M_{F}\\)을 나타낸다. (\\Delta_{\\theta_{S}}\\)는 LORA 분기의 매개변수를 나타낸다. 마찬가지로 부분적으로 훈련된 비디오 모델의 교란 모델을 얻을 수 있다.\n' +
      '\n' +
      'SS_{S}}(\\ta_{S},^{S}},^{{S},^{{{{}}+\\Delta_{\\theta_{\\theta_{S}})\\lelearrow \\text{PERTB}_{\\text{PERTB}_{\\theta_{theta_{S}})\\ruarrow \\text{PERTB}}.\n' +
      '\n' +
      '쉽게 이해하기 위해 모델 \\(M_{F}^{{{{}^{\\prime}}\\) 및 \\(M_{P}^{{{}^{\\prime}}\\)에 대한 \'P-Spa-LORA\'를 나타내기 위해 \'F-Spa-LORA\'라는 이름도 사용한다. \'F\'는 완전히 훈련된 기본 모델을 나타내는 반면 \'P\'는 부분적으로 훈련된 모델을 나타낸다. \'Spa\'와 \'템프\'는 각각 미세한 공간 모듈과 시간적 모듈을 의미한다. \'LORA\'는 파이밍을 위해 LORA를 사용하는 반면, \'DIR\'은 LORA가 없는 직접 핀셋링을 의미한다. 예를 들어, \'F-공간-LORA\'는 LORA를 사용하여 완전히 훈련된 T2V 모델의 교란 공간 모듈을 나타낸다.\n' +
      '\n' +
      '두 결과 모델의 합성 영상을 비교하면 다음과 같은 관찰이 있다. 먼저, F-Spa-LORA의 움직임 품질은 P-Spa-LORA보다 안정적이다(표 4의 사용자 연구 참조). P-Spa-LORA의 움직임은 핀셋링 과정에서 빠르게 악화된다. 작성한 단계가 많을수록 동영상은 여전히 로컬 플리커(그림 2 참조)와 더 많은 경향이 있다. F-Spa-LORA의 움직임은 완전히 훈련된 염기 모델에 비해 약간 악화된다. 둘째, P-Spa-LORA는 F-Spa-LORA보다 훨씬 더 나은 시각적 품질을 달성한다(그림 2 참조). F-Spa-LORA의 화질 및 심미적 점수는 부분적으로 훈련된 베이스 모델에 비해 크게 향상된다(표 3 참조). 놀랍게도 워터마크도 제거됩니다. F-Spa-LORA는 화질 및 심미적 점수의 약간의 개선을 얻는 반면, 생성된 영상은 여전히 시끄럽다.\n' +
      '\n' +
      '두 관찰에서 완전히 훈련된 모델의 공간적 모듈과 시간적 모듈 간의 결합 강도가 부분적으로 훈련된 모델보다 강하다고 결론지을 수 있다. 부분적으로 훈련된 모델의 공간-시간적 결합이 쉽게 깨질 수 있기 때문에 빠른 움직임 변성과 화질 이동으로 이어진다. 더 강한 연결은 약한 연결보다 파라미터 섭동을 견딜 수 있다. 우리의 관찰은 AnimateDiff의 품질 개선과 움직임 변성을 설명하는 데 사용할 수 있다. 아메리칸디프는 일반 모델이 아니며 선택된 개인화된 SD 모델에 대해서만 작동합니다. 그 이유는 그 동작 모듈이 부분적으로 훈련 전략으로 얻어지고 큰 매개변수 섭동을 견딜 수 없기 때문이다. 개인화된 모델이 시간 모듈과 일치하지 않을 때, 그림과 움직임 품질은 모두 퇴화될 것이다.\n' +
      '\n' +
      '__시간적 편향__시간적 반복__시간적 변절._시간적 교란. 부분적으로 훈련된 모델은 시간 모듈만 업데이트되지만, 화질은 WebVid-10M의 품질로 이동된다. 따라서 시간 모듈은 동작뿐만 아니라 화질에도 책임을 진다. 이미지 데이터셋으로 공간 모듈을 고정하면서 시간 모듈을 교란시켰다. 변조 과정은 그대로 나타낼 수 있다.\n' +
      '\n' +
      '타타_{T}},\\ta_{S}},\\mrow{{S}},\\mrow{{S}} \\text{{F}(\\mrow{{T}:\\text{{T})\\text{{T}}(M_\\text{{T})\\text{{T},\\text{{T}},\\text{{T},\\text{{T},\\text{{T},\\text{{T},^{{T},^{{T},^{{{T},\\{{{T},^{{{T},\\{{{{T},\\{{{T},\\{{{T},\\{{{T},\\{{{T}},\\{{{T}},\\{{{T},\\{{T}},\\{{{T}},\\{{{T}},\\{{{T}},\\{{T}},\\{{T}},\\{{T}},\\{{\n' +
      '\n' +
      '우리는 P-Temp-LORA(H_{P}^{{{}^{\\prime}}\\))의 화질이 F-Temp-LORA(M_{P}^{{{{}^{\\prime}}\\)보다 낫다는 것을 관찰했다. 그러나 동영상의 전경과 배경은 더욱 흔들리고, _i,_ 시간적 일관성이 더 심해진다(그림 3 참조). F-Temp-LORA의 사진이 개선되지만, 워터마크는 여전히 있습니다. 그것의 움직임은 기본 모델에 가깝고 P-Temp-LORA(표 4 참조)보다 훨씬 우수하다. 이러한 관찰은 또한 공간 교란으로부터 얻은 결론을 뒷받침한다.\n' +
      '\n' +
      '그림 3: LORA를 이용한 흡입 시간 모듈 _<그림 3>. _Perturbing 시간적 모듈. 아크로밭 판독기로 가장 잘 보았습니다. 영상을 클릭하여 비디오 클립을 재생합니다.___ 비디오를 재생합니다.\n' +
      '\n' +
      '중요성 및 움직임의 중요도 수준 설명요.\n' +
      '\n' +
      '저작권 문제로 다양성이 높은 대규모 고품질 비디오 데이터셋 획득이 어려워 질 높은 영상을 사용하지 않고 수준 높은 비디오 모델을 학습할 수 있는 가능성을 모색한다. 웹비드-10M과 같은 저품질 비디오와 JDB와 같은 고품질 이미지에 액세스할 수 있습니다. 우리는 고품질 영상에서 화질과 미학을 학습하면서 저품질 영상으로부터 데이터 수준, _i.e._학습운동에서 외모에 의한 미관을 단절할 것을 제안한다. 우리는 먼저 비디오로 비디오 모델을 훈련한 다음 영상으로 비디오 모델을 미세 조정 할 수 있다. _ 키들은 비디오 모델을 훈련하는 방법과 이미지들로 어떻게 미세 조정해야 하는지에 놓여 있다.___의 이미지들로 나열한다.\n' +
      '\n' +
      '공간 모듈과 시간 모듈 간의 연결 연구에 따르면, 완전히 훈련된 모델은 고품질 이미지로 후속 핀셋링에 더 적합하다. 강력한 공간-시간적 커플링은 명백한 움직임 변성이 없이 공간 모듈과 시간 모듈 모두에 대한 매개변수 섭동을 견딜 수 있기 때문이다.\n' +
      '\n' +
      '다음으로 베이스 모델을 이미지로 미세 조정하는 방법을 조사해야 한다. 공간적 및 시간적 섭동(Sec. 3.1) 모두에서 화질은 향상될 수 있지만 그리 크게 향상되지는 않는다. 더 큰 품질 개선을 얻기 위해 두 가지 전략을 평가한다. 하나는 더 많은 파라미터, 즉 이미지를 가진 공간적 모듈과 시간적 모듈을 모두 변조하는_i,_핀셋링을 포함하는 것이다. 다른 하나는 LORA가 없는 직접 핀셋링을 사용하여 핀셋링 방법 _i.e.,_를 변경하는 것이다. 다음 네 가지 사례를 평가할 수 있습니다.\n' +
      '\n' +
      'Hff}(\\ta_\\{T}),\\_{{S}(\\ta_\\{S}),\\_{{F}(\\ta_\\{D}),\\_{{S})\n' +
      '\n' +
      '제1 전략을 따라 \\(M_{F}^{A}\\)(F-Spa&Temp-LORA)를 얻은 경우, 제2 전략을 사용하여 \\(M_{F}^{B}\\), \\(M_{F}^{C}\\), \\(M_{F}^{D}\\)를 얻는다. (M_{F}^{B}^{B}\\) (F-Spa-DIR) 및 \\(M_{F}^{C}\\) (F-Temp-DIR)는 각각 공간적 및 시간적 모듈을 직접 미세하게 하는 것을 나타낸다. (M_{F}^{D}\\) (F-Spa&Temp-DIR)은 모든 모듈을 직접 핀싱하는 것을 나타낸다.\n' +
      '\n' +
      '4개 모델의 생성된 비디오를 비교해보면 다음과 같은 관찰이 있다. 먼저 F-Spa&Temp-LORA는 F-Spa-LORA의 화질을 더욱 향상시킬 수 있지만, 품질은 여전히 만족스럽지 못하고 있다. 대부분의 생성된 비디오에는 워터마크가 존재하며 소음은 분명하다. 둘째, F-Temp-DIR은 F-Temp-LORA보다 더 나은 화질을 달성한다. 또한 F-Spa&Temp-LORA보다 좋습니다. 수박은 비디오의 절반으로 제거되거나 가볍습니다. 셋째, F-Spa&DIR과 F-Spa&Temp-DIR은 미세 조정 모델 중 최고의 화질을 달성한다. 그러나 F-Spa-DIR의 움직임은 더 낫다(그림 4 및 표 4 참조). F-Spa&Temp-DIR의 전경 및 배경은 특히 로컬 텍스처인 \\(M_{F}^{D}\\)에 의해 생성된 비디오에서 플러싱된다.\n' +
      '\n' +
      '핀셋링 전략과 다른 모듈을 탐색함으로써, 우리는 직접 핀셋링 전략 및 상이한 모듈로 공간 모듈을 조달하는 것을 식별한다.\n' +
      '\n' +
      '완전 훈련된 T2V 모델을 기반으로 한 모듈 선택 _그림 4: 모듈 선택은 완전히 훈련된 T2V 모델을 기반으로 한다. 아크로밭 판독기로 가장 잘 보았습니다. 영상을 클릭하여 비디오 클립을 재생합니다.___ 비디오를 재생합니다.\n' +
      '\n' +
      '그림 5: 개념 구성에 대한 이미지 데이터의 영향. ‘F-Spa-DIR-LAION’은 LAION 미학 V2를 이미지 데이터로 사용하고, ‘F-Spa-DIR’은 JDB를 사용한다. 아크로밭 판독기로 가장 잘 보았습니다. 영상을 클릭하여 비디오 클립을 재생합니다.___ 비디오를 재생합니다. 고품질 이미지는 움직임 품질의 한계 손실 없이 화질을 향상시키는 가장 좋은 방법입니다. 이 시점에서 우리의 데이터 수준 이젠텐션 파이프라인은 다음과 같이 요약될 수 있는데, 우선 낮은 품질의 비디오를 가진 비디오 모델을 완전히 훈련한 다음 고품질 이미지로만 공간 모듈을 직접 핀다.\n' +
      '\n' +
      '문서 구성 촉진\n' +
      '\n' +
      '비디오 모델의 개념 구성 능력을 향상시키기 위해 부분 핀셋링 단계에서 실제 이미지를 사용하는 대신 복잡한 개념으로 합성된 이미지를 사용할 것을 제안한다. SDXL, 미드조르네리와 같은 T2I 모델의 성공은 대규모 고품질 이미지들에 의해 구축된다. 그들은 현실 세계에서 나타나지 않는 개념들을 합성할 수 있는 능력을 가지고 있다. 그들의 훈련 이미지를 사용하는 것보다 복잡한 개념으로 이미지 세트를 합성하여 그들의 개념 구성 능력을 비디오 모델에 전달하는 것을 제안한다. 이와 같이 개념과 운동을 동시에 잘 포착하는 부담을 덜어줄 수 있다.\n' +
      '\n' +
      '합성된 이미지의 효과를 검증하기 위해 JDB와 LAION-에스테이션 V2[2]를 제2 핀셋링 단계의 이미지 데이터로 사용한다. LAION-에스티섹션 V2는 웹 수집 이미지로 구성되며 JDB에는 미도즈니가 합성한 이미지가 포함되어 있다. JDB로 훈련된 모델이 훨씬 더 나은 개념 구성 능력을 가지고 있음을 관찰한다(그림 5 및 표 3 참조). 추가 자료에 더 많은 결과가 있습니다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Settings\n' +
      '\n' +
      '데이터 한계를 극복하기 위해 고품질 영상 데이터에 대해 웹Vid-10M[12]을 저품질 영상 데이터의 원천으로, JDB[35]를 활용하였다. 웹바이드-10M은 대략 1천만 개의 텍스트 비디오 쌍을 포함하는 대규모 다양한 비디오 데이터세트이다. 대부분의 동영상의 해상도는 \\(336\\t 596\\)이며, 각 영상은 단일샷으로 구성되어 있다. 훈련 중 다양한 프레임 속도로 비디오를 샘플링합니다. JDB는 미도즈니의 약 400만 개의 고해상도 이미지를 특징으로 하는 대규모 이미지 데이터세트이며, 각각은 대응하는 텍스트 프롬프트로 주석을 달았다. 베이스 T2V 모델의 훈련 중 개념을 잊어버리는 것을 방지하기 위해 LAION-COCO[3], 이미지 및 비디오 훈련 모두에 대해 공개적으로 이용 가능한 웹 이미지에 대해 6억 개의 생성된 고품질 캡션을 포함하는 데이터 세트를 사용한다.\n' +
      '\n' +
      '지표. 정량 평가를 위해 EvalCrafter [31]을 이용합니다. EvalCrafter는 시각 품질, 콘텐츠 품질, 모션 품질 및 텍스트-선택 정렬을 위해 약 18개의 객관적인 메트릭을 포함하는 텍스트-비디오 생성 모델을 평가하는 벤치마킹이다. 약 512개의 프롬프트를 제공합니다. 객관적 메트릭은 5개의 주관적 연구, 즉 모션 품질, 텍스트 비디오 정렬, 시간적 일관성, 시각적 품질 및 사용자 선호의 사용자 의견에 정렬된다. 움직임 품질은 액션 인식, 평균 흐름, 진폭 분류 점수, 시간적 일관성 등 세 가지 메트릭을 고려하는 반면 와핑 오류, 의미 일관성, 얼굴 일관성을 고려한다. EvalCrafter의 기술적 및 심미적 점수는 DOVER[54]에서 조정된다. 또한, 모션 품질을 측정하기 위한 포괄적인 객관적인 메트릭이 아직 없기 때문에 인간 선호도에 대한 사용자 연구를 수행한다.\n' +
      '\n' +
      'Sec 3.1에서 두 기반 모델은 동일한 아키텍처를 공유하고 오픈 소스 비디오 큐래덤1[15]에서 적응했으며 모델 스코프T2V [48]의 시간적 컨볼루션을 통합한다. 공간 모듈은 SD 2.1의 가중치로 초기화되고, 시간 모듈의 출력은 제로스로 초기화된다. 훈련 해상도는 \\(512\\t 320\\)로 설정됩니다. 공동 이미지 및 비디오 트레이닝을 위해 저품질 웹Vid-10M 및 LAION-COCO 데이터 세트를 사용합니다. 모델은 배치 크기가 128인 \\(270K\\) 반복을 위한 32개의 NVIDIA A100 GPU에서 훈련되며, 학습율은 모든 훈련 작업에 대해 \\(5\\·10^{-5}\\)로 설정된다. 시간적 또는 공간 모듈의 섭동을 위해 LORA를 사용할 때, 우리는 JDB를 튜닝용으로 독점적으로 사용한다. 핀셋링은 배치 크기가 256인 \\(30K\\) 반복을 위한 8개의 A100 GPU에 대해 수행되며 JDB의 이미지가 제곱 해상도가 있음을 알 수 있으며, 지느러미 해상도를 \\(512\\·512\\)로 조정한다.\n' +
      '\n' +
      'S2V 모델은 주와 비교합니다.\n' +
      '\n' +
      '우리는 유전자-2 [5] 및 피카랩스[9]와 같은 인기 상업 모델뿐만 아니라 쇼-1[60], 비디오 스키드1[15] 및 AnimateDiff [22]와 같은 오픈 소스 모델을 포함한 여러 최첨단 T2V 모델과 접근법을 비교한다. 유전자-2, Pika Labs 및 VideoCrafter1은 모두 T2V 모델을 트레이닝하기 위해 고품질 비디오를 사용한다. 아메리칸디프와 우리 모델은 웹바이드-10M의 비디오만을 사용한다는 점은 주목할 만하다. 쇼-1은 웹Vid-10M의 워터마크를 제거하기 위해 핀셋을 위해 추가 고품질 비디오를 사용한다. 안티펜디프는 일반적인 T2V 모델이 아니며 LORA SD 모델이 시간 모듈과 호환되어야 작동한다. 우리의 비교를 위해 SD v1.5를 기반으로 한 시간 모듈(제2 버전)을 사용하고 해당 LORA 모델로 리얼리즘 비전 V2.0 [10]을 사용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline  & Visual & Text-Video & Motion & Temporal \\\\  & Quality & Alignment & Quality & Consistency \\\\ \\hline Pikal\\(\\text{ab}^{*}\\) & 63.52 & 54.11 & 57.74 & 69.35 \\\\ Gen2\\({}^{*}\\) & 67.35 & 52.30 & 62.53 & 69.71 \\\\ \\hline VideoCrafter1 & 61.64 & 66.76 & 56.06 & 60.36 \\\\ Show-1 & 52.19 & 62.07 & 53.74 & 60.83 \\\\ AnimateDiff & 58.89 & 74.79 & 51.38 & 56.61 \\\\ Ours & 63.28 & 64.67 & 53.95 & 62.02 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 에발크래더 벤치마크에 대한 비교. 더 높은 점수는 더 나은 성능을 나타낸다. * 상용 모델입니다.\n' +
      '\n' +
      '정량적 평가는 표 1에 EvalCraub를 사용하여 얻은 정량적 결과가 제시되어 있으며, 이 방법은 트레이닝을 위해 고품질 비디오를 사용하는 VideoCrafter1 및 Pika Labs와 유사한 시각적 품질을 달성한다. 이것은 고품질 이미지를 사용하여 화질과 심미성을 향상시키는 효과를 강조한다.\n' +
      '\n' +
      '그림 6: 다양한 텍스트 대 비디오 생성 모델의 비교. __ 비디오 생성 모델. 아크로봇 판독기로 가장 많이 보았어요. 영상을 클릭하여 비디오 클립을 재생합니다.___ 비디오를 재생합니다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:8]\n' +
      '\n' +
      'LAION 임상 V2 시각 품질에 대한 정량적 평가는 표 3에 나와 있으며, F-Spa-DIR은 미학적 및 기술적 점수 모두에서 F-Spa-DIR-LAION보다 훨씬 우수하다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '데이터 한계를 극복하기 위해 고품질 비디오를 사용하지 않고 고품질 비디오 확산 모델을 훈련하는 방법을 제안한다. SD 기반 비디오 모델의 훈련 계획에 대해 설명하고 공간 차원 및 시간적 차원 간의 결합 강도를 조사한다. 완전히 훈련된 T2V 모델이 부분적으로 훈련된 모델보다 더 강한 공간-시간 결합을 나타냄을 관찰한다. 이러한 관찰을 바탕으로 모션 러닝을 위한 저품질 동영상과 외관 학습을 위한 고품질 이미지를 활용함으로써 데이터 수준 _i.__에서의 동작으로부터 불협화음을 제안한다. 또한, 우리는 실제 이미지보다는 핀셋링을 위해 복잡한 컨셉이 있는 합성 이미지를 사용하는 것을 제안한다. 제안된 방법의 효과를 입증하기 위해 정량적, 질적 평가가 이루어진다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]S. 지, T. 하예, H. 양, X. Yin, G. Pang, D. Jacobs, J. Huang 및 D. Parikh(2022) 롱 비디오 생성은 시간 진단 vqgan 및 시간에 민감한 변압기를 가지고 있다. ECCV에서 pp. 102-118.: SS1에 의해 계산된다.\n' +
      '* [2]H. 텐, M. 샤, Y. >야, 야. 장, X. 스쿱, S. 양, J.Xing, Y. 리, Q. 텐, X. 왕, 알(2023) 비디오 캐러더1: 고품질 비디오 생성을 위한 오픈 확산 모델입니다. arXiv 프리프린트 arXiv:2310.19512: SS1에 의해 계산된다.\n' +
      '* [3]H. 텐, M. 샤, Y. >야, 야. 장, X. 스쿱, S. 양, J.Xing, Y. 리, Q. 텐, X. 왕, 알(2023) 비디오 캐러더1: 고품질 비디오 생성을 위한 오픈 확산 모델입니다. arXiv 프리프린트 arXiv:2310.19512: SS1에 의해 계산된다.\n' +
      '*[4]M. 텐, Y. 왕, L. 장, S. 주앙, X. 마, J유, Y. 왕, D. Lin, Y. Qiao, Z. Liu(2023) Seine: 생성 전환 및 예측을 위한 짧은 길이 비디오 확산 모델이다. arXiv 프리프린트 arXiv:2310.20700: SS1에 의해 계산된다.\n' +
      '*[5]C. 다양한 자동 암호화기에 대한 도서치(2016) 튜토리얼입니다. arXiv 프리프린트 arXiv:1606.05908: SS1에 의해 계산된다.\n' +
      '* [6]S. Ge, S. 나, G. 류, T. 훈, A. 도, B. 카탄자로, D. 제이콥스, J. 황, M. 리, Y. 발지(2021)는 동영상 확산 모델에 앞서 자신의 상관관계를 보존한다. ICCV에서: SS1에 의해 계산된다.\n' +
      '* [7]S. Ge, D. Chen, J. Bao, F. Wen, B. Zhang, D. Chen, L. L. 유안, B. 구오(2022) 벡터는 텍스트 대 이미지 합성을 위한 확산 모델을 정량화했다. CVPR에서: SS1에 의해 Cited이다.\n' +
      '* [8]Y. 구, C. 양, A. 라오, Y. 야, 야. Qiao, D. Lin 및 B. Dai(2023) Animatediff: 특정 튜닝 없이 개인화된 텍스트 대 이미지 확산 모델을 분리한다. arXiv 프리프린트 arXiv:2307.04725: SS1에 의해 계산된다.\n' +
      '*[9]Y. 그는 S. 양, H. 첸, X. 쿡, M. 샤, Y. 장, X. 왕, R. 그는 Q. 텐과 Y. 산(2023) 스칼칼라더: 확산 모델이 있는 튜닝이 없는 고해상도 시각 생성입니다. arXiv 프리프린트 arXiv:2310.07702.: SS1에 의해 계산됩니다.\n' +
      '*[10]J. 호, A. 자인 및 P. 압베엘(2020) 덴오징 확산 확률 모델. NurIPS에서: SS1로 계산되었습니다.\n' +
      '*[11]J. 호, W D. 찬, C. 사하라리아, J. 휘앙, R. 가오, A. 그리세넨코, D. 포마, B. 포레, M. 포레, M. 노루지, D. J. 플렛, et al al.(2022) Imagen 비디오: 확산 모델이 있는 고화질 비디오 생성이다. arXiv 프리프린트 arXiv:2210.02303: SS1에 의해 계산된다.\n' +
      '*[12]J. 호, T. 살리만스, A. 그리센코 W. 찬, M. 노루지, D. J. 플레인(2022) 비디오 확산 모델. NurIPS에서: SS1로 계산되었습니다.\n' +
      '*[13]W. 홍, M. 드잉, W. 정, X. Liu와 J. 탕(2022) 코그비디오: 변압기를 통한 텍스트 대 비디오 유전 통기에 대한 대규모 척. arXiv 프리프린트 arXiv:2205.15868_, 2022.\n' +
      '* [29] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [30] Yaosi Hu, Chong Luo, and Zhenzhong Chen. Make it move: controllable image-to-video generation with text descriptions. In _CVPR_, 2022.\n' +
      '* [31] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models, 2023.\n' +
      '* [32] Kangfu Mei and Vishal Patel. Vidm: Video implicit diffusion models. In _AAAI_, pages 9117-9125, 2023.\n' +
      '* [33] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In _ICCV_, 2019.\n' +
      '* [34] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In _ICML_, 2022.\n' +
      '* [35] Junting Pan, Keqiang Sun, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: A benchmark for generative image understanding. _arXiv preprint arXiv:2307.00716_, 2023.\n' +
      '* [36] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* [37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.\n' +
      '* [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022.\n' +
      '* [39] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _NeurIPS_, 2022.\n' +
      '* [40] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular value clipping. In _ICCV_, pages 2830-2839, 2017.\n' +
      '* [41] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoesiny. Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2. In _CVPR_, pages 3626-3636, 2022.\n' +
      '* [42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _ICML_, 2015.\n' +
      '* [43] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _ICLR_, 2021.\n' +
      '* [44] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In _CVPR_, pages 1526-1535, 2018.\n' +
      '* [45] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. _arXiv preprint arXiv:2210.02399_, 2022.\n' +
      '* [46] Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal. Mcvd-masked conditional video diffusion for prediction, generation, and interpolation. In _NeurIPS_, 2022.\n' +
      '* [47] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. _NeurIPS_, 29, 2016.\n' +
      '* [48] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. _arXiv preprint arXiv:2308.06571_, 2023.\n' +
      '* [49] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation. _arXiv preprint arXiv:2305.10874_, 2023.\n' +
      '* [50] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yin He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. _arXiv preprint arXiv:2309.15103_, 2023.\n' +
      '* [51] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, et al. Internvid: A large-scale video-text dataset for multimodal understanding and generation. _arXiv preprint arXiv:2307.06942_, 2023.\n' +
      '* [52] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. _arXiv preprint arXiv:2104.14806_, 2021.\n' +
      '* [53] Chenfei Wu, Jian Liang, Xiaowei Hu, Zhe Gan, Jianfeng Wang, Lijuan Wang, Zicheng Liu, Yuejian Fang, and Nan Duan. Nuwa-infinity: Autoregressive over autoregressive generation for infinite visual synthesis. _arXiv preprint arXiv:2207.09814_, 2022.\n' +
      '* [54] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In _CVPR_, pages 20144-20154, 2023.\n' +
      '* [55] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. _arXiv preprint arXiv:2310.12190_, 2023.\n' +
      '* [56] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In _CVPR_, 2022.\n' +
      '* [57] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videoopt: Video generation using vq-vae and transformers. _arXiv preprint arXiv:2104.10157_, 2021.\n' +
      '\n' +
      '* [58] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In _CVPR_, 2023.\n' +
      '* [59] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In _CVPR_, pages 18456-18466, 2023.\n' +
      '* [60] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng. Show-1: Marrying pixel and latent diffusion models for text-to-video generation, 2023.\n' +
      '* [61] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models, 2023.\n' +
      '* [62] Yuechen Zhang, Jinbo Xing, Eric Lo, and Jiaya Jia. Real-world image variation by aligning diffusion inversion chain. _arXiv preprint arXiv:2305.18729_, 2023.\n' +
      '* [63] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. _arXiv preprint arXiv:2211.11018_, 2022.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>