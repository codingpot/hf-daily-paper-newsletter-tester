<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# HexaGen3D: StableDiffes: StableDiff는 단 한 단계 떨어져 있습니다.\n' +
      '\n' +
      '3D 세대까지 가능합니다.\n' +
      '\n' +
      ' 마시 레디\n' +
      '\n' +
      '그릴라, 버거라, 홍 카이, 파티 포니키.\n' +
      '\n' +
      'AI 연구\n' +
      '\n' +
      '스쿠컴.com.{amercier, mahkercier, mahkri, ryasarla, hongcai, fporikli, 구일버그}@qti.\n' +
      '\n' +
      '인턴십시큐컴 AI 연구 과정에서 퀄컴 AI 연구 수행 작업은 퀄컴 기술인 Inc.의 이니셔티브이다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '생성 모델링의 최신 놀라운 발전에도 불구하고 텍스트 프롬프트로부터의 고품질 3D 자산의 효율적인 생성은 여전히 어려운 과제로 남아 있다. 핵심 과제는 데이터 부족, 즉 가장 광범위한 3D 데이터 세트는 수백만 개의 자산에 불과한 반면, 그들의 2D 대응물은 수십억 개의 텍스트 이미지 쌍을 포함한다. 이를 해결하기 위해, 우리는 2D 확산 모델들을 전처리하고 큰 양의 힘을 이용하는 새로운 접근법을 제안한다. 보다 구체적으로, 우리의 접근 방식인 HexaGen3D, 전처리된 텍스트 대 이미지 모델을 미세 조정하여 6개의 정형학적 투영과 상응하는 잠재 3개의 평면을 공동으로 예측한다. 그런 다음 이러한 래치들을 디코딩하여 질감 있는 메쉬를 생성합니다. HexaGen3D는 표본당 최적화가 필요하지 않으며, 7초 만에 텍스트 프롬프트에서 고품질 및 다양한 객체를 유추할 수 있어 기존 접근법과 비교할 때 품질이 훨씬 더 우수하며 절충점을 제공한다. 또한, HexaGen3D는 새로운 객체 또는 구성에 대한 강력한 일반화를 보여준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '게임, AR/VR, 그래픽 디자인, 애니메이션, 영화 등 다양한 영역에 걸쳐 중요한 3D 자산의 생성은 특히 힘들고 시간이 많이 소요된다. 과제 일부 측면에서 3D 아티스트를 돕기 위한 기계 학습 기술의 사용에도 불구하고, 효율적인 엔드 투 엔드 3D 자산 생성의 개발은 주로 고품질 3D 데이터의 제한된 가용성에 의해 병목화되는 어려운 상태로 남아 있다.\n' +
      '\n' +
      '대조적으로, 이미지 생성은 최근 수십억 개의 텍스트 이미지 쌍을 포함하는 LAION-5B [38]와 같은 대형 데이터셋에서 대형 모델[31, 34, 36, 37, 48, 14]의 훈련에 의해 구동되는 품질과 다양성의 급증을 목격했다. 이러한 2D 생성 모델링의 진행은 큐레이션된 3D 데이터의 부족과 함께 3D 자산 생성을 위한 2D 사전 학습된 모델을 적응시키기 위한 최근의 노력을 촉발했다. 한 가지 주목할만한 예는 큰 2D 확산 모델에 대한 사전 지식을 활용하여 일반적으로 NeRF[27]인 3D 표현의 최적화를 안내하는 드림퓨전[32]이다. 스카어 교차 샘플링(SDS) 및 후속 변이체로 명명된 이 접근법은 유망한 결과를 보여주었지만 주요 한계는 극히 긴 생성 시간으로 남아 있으며 20분에서 몇 시간 동안 어디서나 걸리고 종자 전반에 걸쳐 다양성이 부족하다.\n' +
      '\n' +
      '이러한 문제를 해결하기 위해 품질이나 다양성을 희생하지 않고 생성 시간을 크게 줄이는 새로운 텍스트 대 3D 모델인 HexaGen3D를 제시한다. 드림퓨전과 유사하게 사전 훈련된 2D 모델을 구축하지만 대신 직접, 피드포워드, 3D 물체의 생성을 위해 수정 및 조달할 것을 제안한다. 아래는 우리 작품의 핵심 기여입니다.\n' +
      '\n' +
      '* 우리는 3D유전자[11]와 유사한 접근법을 채택하지만 맞춤형 설계된 잠재 생성 모델을 미리 학습된 텍스트 대 이미지 모델로 대체하여 광범위한 3D 핀싱 데이터의 필요성을 감소시키고 훈련 중에 직면하지 않는 새로운 객체 또는 조성물에 일반화할 수 있도록 한다.\n' +
      '* We는 모델의 2D 사전 지식을 3D 공간 추론과 정렬시키는 새로운 기법인 "오스트럭처 헤사뷰 안내"를 소개한다. 이 중간 과제는 우리가 최종 3D 표현에 매핑한 6면 정형학적 예측을 예측하는 것을 포함한다. 이를 통해 기존 2D 확산 모델의 U-Net은 하나의 추가 U-Net 추론 단계만 필요로 하는 3D 세대로 다중 뷰 예측과 3D 자산 생성을 순차적으로 효율적으로 수행할 수 있다.\n' +
      '* HexaGen3D는 A100에서 7초만 복용하면서 기존의 품질에 대한 접근과 유리하게 경쟁하여 새로운 물체를 생성한다. 이 속도는 SDS 최적화를 기반으로 한 기존 접근 방식보다 더 빠른 순이다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '최근의 방법은 단일 이미지 및/또는 텍스트 프롬프트로부터 3D 메쉬를 생성하기 위한 크게 레버리지 확산 모델[12]이다. 본 절에서는 이러한 작품들을 간략히 검토하고 보다 포괄적인 조사를 위해 독자들을 [30]으로 지칭한다.\n' +
      '\n' +
      '스모어 교차 샘플링(**)을 사용하여 드림퓨전[32]과 라텐트-NeRF[26] 등 3D를 생성하며 스코어 교차 샘플링(SDS)을 사용하여 NeRF를 훈련한다. 그러나, 그들은 객체를 생성하기 위해 \\(\\ason\\)2 시간을 취하는 긴 샘플당 최적화가 필요하다. 이후 방법은 먼저 거친 NeRF 모델을 생성한 다음 이를 업스케일링하거나 향상시킴으로써 이 과정을 가속화하고, 예를 들어 매직3D[17], 메이크-it-3D[42] 또는 먼저 기하학을 생성한 다음 확산 [43]에서 생성된 이미지를 사용하여 질감을 베이킹한다. 이러한 기술은 예를 들어 생성 시간을 20-40분으로 상당히 감소시킨다. 보다 최근에 MVDream[40]은 지도 모델을 멀티뷰 확산 모델로 대체하여 더 느린 생성 시간의 비용으로 시각적 품질을 향상시키며, 최대 3시간을 소요하여 오브젝트를 생성한다. ATT3D[22]는 네트워크를 사용하여 NeRF 임베딩에 텍스트 프롬프트를 매핑할 것을 제안한다. 이를 통해 두 번째 내에서 메쉬를 생성할 수 있지만 메쉬 품질과 일반화 가능성이 부족하다.\n' +
      '\n' +
      '세브란스:** 세브란스 작업을 통한** 포워드 세대는 확산을 사용하여 3D 구조 또는 표현[6, 11, 15, 22, 23, 29, 41, 50, 49]을 직접 생성한다. 포인트-E[29]는 확산을 사용하여 포인트 클라우드를 생성하고 텍스트 입력으로부터 합성된 이미지에 컨디셔닝을 수행한다. Shap-E[13]은 유사한 변압기 기반 확산 구조를 사용하지만 NeRF 또는 DMTet과 같은 암묵적인 3D 기능의 매개변수를 생성한다. 보다 최근에, 3D 유전자[11]는 확산 모델에 의해 변성되고 포인트 클라우드 VAE 디코더에 공급되어 3D 메쉬를 생성할 수 있는 잠재 3평면의 표현을 채택한다. SSD-NeRF [6]도 삼면 표현을 채택하지만 NeRF 디코더를 사용한다.\n' +
      '\n' +
      '*** 인터버징 2D 뷰 합성:** 이 작품은 확산 모델을 활용하여 3D 생성을 지원하기 위해 새로운 2D 뷰를 생성한다. 3DiM[46]은 소스 뷰 및 그 포즈에 기초하여 타겟 포즈들의 여러 뷰들을 생성한다. 1-2-3-45 [18]는 제로-1 대 3 [19]를 활용하여 물체의 여러 뷰를 생성한다. 그런 다음 생성된 이미지를 기반으로 체적 표현을 구성하고 45초 이내에 메쉬를 생성할 수 있다. 다만, 2D 뷰들은 한 번에 하나씩 생성되며, 볼륨에 대한 일관된 멀티 뷰 정보를 제공하지 않을 수 있다. 다른 작품은 확산을 사용하여 SyncDreamer[20] 및 Instant3D[1]과 같은 여러 견해를 동시에 생성하고 이러한 합성된 견해를 기반으로 3D 객체를 재구성한다. 원더3D[21]는 다중 도메인 확산 모델을 생성하여 2D 이미지와 정상 맵을 모두 생성하고, 서명된 거리 함수를 훈련시켜 기본 텍스트화된 메쉬를 추출한다. 일부 논문은 GeNVS[4], NerfDiff[9], SparseFusion[51], Sparse3D[52]와 같은 이미지 합성 과정에 3D 인식을 추가로 도입하여 더 나은 다중 뷰 일관성을 제공한다.\n' +
      '\n' +
      '** 톤베이킹:** 전술한 작품 중 일부는 기존 메쉬[17, 32]의 질감을 정제하는 데에도 사용될 수 있으며, 여러 연구는 텍스처 베이킹에 초점을 맞춘다. 예를 들어, Text2Tex[5]와 TEXTure[35]는 알려져 있는 기하학에 걸쳐 반복적으로 질감을 칠하기 위해 오프-쉘 스테이블디확산 모델[36]을 사용한다. X-Mesh [25]는 동적 주의 모듈과 텍스트 프롬프트와 렌더링된 이미지 사이의 CLIP 손실을 채택하여 텍스처를 생성한다. TexFusion[3]은 렌더링된 2D 이미지에 확산 모델의 데오이저를 적용하고, 더 나은 질감 일관성을 달성하기 위해 공유 잠복 텍스처 맵에 데노징 예측을 응집시킨다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '우리의 잠재 생성 접근 방식인 HexaGen3D는 텍스트 프롬프트에서 텍스처된 메세지를 예측하기 위해 미리 학습된 텍스트 대 이미지 모델을 침출한다. 3D유전자[11]과 마찬가지로 훈련 절차는 2단계로 구성된다. 첫 번째 단계에서는 다양한 자동 암호화기(VAE) [16]를 사용하여 질감의 메쉬의 모양과 색상을 캡처하는 3중 표현을 배우게 된다. 두 번째 단계에서 우리는 이 3중 잠재 공간에서 새로운 샘플을 합성하기 위해 미리 훈련된 텍스트 대 이미지 모델을 재정렬했다. 그림 1(a)은 각 구성 요소에 대한 포괄적인 개요를 제공한다.\n' +
      '\n' +
      '시험 시간에 우리는 샘플당 최적화가 필요하지 않은 채 피드포워드 방식으로 새로운 3D 자산을 추론한다 - 더 빠르고 효율적인 생성을 허용한다. 먼저 잠재 생성기를 사용하여 무작위 노이즈에서 3중 래치들을 생성한 다음, 이 래치들을 첫 번째 단계에서 디코더를 사용하여 질감 있는 메쉬로 변환한다.\n' +
      '\n' +
      '단계 1의 경우, 우리의 접근법은 3D유전자[11](섹션 3.1에 첨부된)에서 오토 인코더의 대부분의 아키텍처 구성 요소를 유지한다. 그러나 우리의 작업의 주요 차이점은 2단계(3.2절 세부)에 대해 미리 훈련된 텍스트 대 이미지 모델을 레버리지할 것을 제안한다는 것이다. 특히, 우리의 결과는 적절한 안내 및 최소 미세 조정 데이터를 사용하여 StableD확산 전달과 같은 2D 생성 모델이 놀라울 정도로 3D 자산 생성으로 잘 전달된다는 것을 보여준다.\n' +
      '\n' +
      '### 1을 학습한다.\n' +
      '\n' +
      '이 단계에서 우리는 포인트 구름에서 파생된 질감의 3중 잠재 표현을 배우기 위해 VAE를 훈련한다. 우리의 솔루션은 방법론에서 여러 구성 요소를 재이용함에 따라 3D유전자[11]에 의해 확립된 프레임워크를 밀접하게 따른다. 초기에, 포인트넷 모델[33]은 입력 포인트 클라우드를 3중 표현으로 인코딩한다. 이러한 래치들은 후속적으로 두 개의 연속적인 네트워크를 통해 DMTet 알고리즘[39]을 탈으로 하는 디코더를 사용하여 텍스처 메쉬로 변환되고, 이어서 DMTet 알고리즘[39]을 기반으로 하는 디코더를 사용하여 텍스처 메쉬로 변환된다.\n' +
      '\n' +
      '그림 2: 방법 개요. HexaGen3D는 7초 만에 텍스트 프롬프트에서 질감 있는 메세지를 예측하여 잠재 생성을 위한 사전 훈련된 텍스트 대 이미지 모델을 활용합니다. 3D유전자[11]과 마찬가지로 훈련 절차는 2단계로 구성되며 잠재 표현은 트라이플레인 기반이다. 탑좌표(a): 3중 잠재 오토 인코더와 스테이블 디확산 기반 잠재 세대의 두 단계에 대한 하이 레벨 개요이다. 바닥좌표(c): 정육면체 확산(섹션 3.2)을 통한 스테이블디확산 기반 삼막 잠재 생성이다. 하부(d): 헥사-트리 플레인 레이아웃 변환기(섹션 3.2.3)에 대한 설명서입니다. 우측(b): 디코딩된 메쉬(섹션 3.3)의 시각적 외관을 개선하기 위한 후처리 단계로 사용되는 UV 텍스처 베이킹 절차에 대한 설명서.\n' +
      '\n' +
      '그림 1(a)에서 예측하였다.\n' +
      '\n' +
      '3D유전자와 유사하게, 저희 파이프라인은 첫 번째 정제 네트워크에 3D 인식 설득력[45]가 있는 U-Net을 사용합니다. 그러나 두 번째 네트워크의 경우 3D 유전자에 사용된 3D 인식 U-Net 아키텍처를 스테이블디퓨전 모델[36]에서 일반적으로 사용되는 VAE 아키텍처로 대체하고 이 두 번째 네트워크로부터의 저해상도 중간 특징을 잠재 세대의 표적 표현으로 활용한다.\n' +
      '\n' +
      '2D 프리퓨전 모델들을 사용한 3세대입니다.\n' +
      '\n' +
      '이 단계에서 우리의 목표는 제공된 텍스트 프롬프트와 일치하는 3중 래치들을 생성하는 것이다. 처음부터 새로운 아키텍처를 개발하고 훈련하는 데 중점을 둔 3D유전자[11] 또는 Shap-E[13]과 같은 접근법과 달리 사전 훈련된 텍스트 대 이미지 모델을 구축하고 이를 3D 영역으로 확장하는 방법을 탐구하는 것을 목표로 한다. 우리의 접근법의 핵심 측면은 2D와 3D 합성 사이의 격차를 해소하기 위해 특별히 설계된 새로운 중간 과제인 "오스트럭처 Hexaview" 예측을 도입하는 것이다.\n' +
      '\n' +
      '통계학적 검사 지침 지침##### 3.2.1.2.1 정 인구 조사 조사 지침#######\n' +
      '\n' +
      '일반적으로 문헌에 수행된 바와 같이, 우리의 트라이플타르 표현은 폭 축을 따라 연결된 3개의 특징 맵으로 구성되어 \\(1\\t 3\\) "연출" 레이아웃을 형성한다. 이 2차원 배열은 2D 아키텍처를 3중 잠재 세대에 적용할 수 있게 하지만 이 작업은 여전히 3차원 평면에 걸쳐 특징 일관성을 보장하기 위해 3차원 기하 추론 능력을 필요로 한다. 우리의 결과는 여러 개체를 부착적으로 배치하는 방법에 대한 사전 지식에도 불구하고, 잠재적으로 미세 조정 중에 사용할 수 있는 제한된 3D 데이터로 인해 그러한 롤아웃 트라이플레인 생성을 위해 직접 미세 조정될 때 투쟁하는 사전 훈련된 텍스트 대 이미지 모델이 있음을 시사한다. 이를 해결하기 위해 생성 과정을 두 단계로 분해하여 잠재 생성 과정을 안내하도록 설계된 중간 "헥사뷰" 표현을 소개한다.\n' +
      '\n' +
      '그림에서 볼 수 있듯이. 1(c), 중간 "헥사뷰" 표현은 6개의 직교 뷰(전, 후, 우, 좌, 상, 하)로부터 잠재된 특징을 포함하며, 공간적으로 연결되어 \\(2\\t 3\\) 레이아웃을 형성한다. 3중 잠재 생성 공정은 2단계로 분해된다. 우리는 먼저 수정되고 미세한 텍스트 대 이미지 U-Net(섹션 3.2.2)을 사용하여 헥사뷰 래치들을 확산시켰다. 그런 다음 이러한 래치들을 목표 3중 표현에 매핑하여 동일한 확산 U-Net을 추가 추론 단계와 작은 레이아웃 변환기(섹션 3.2.3)에 다시 사용한다. 컴퓨팅과 마찬가지로, 우리는 총 \\(N+1\\) U-Net 추론, \\(N\\) 단계를 수행하여 물체의 멀티뷰 표현을 획득하고, \\(1\\) 추가 U-Net 순 통과를 수행하여 해당 메쉬를 생성하는 것을 의미한다.\n' +
      '\n' +
      '다중 뷰 예측[1, 40]을 기반으로 하는 동시 3D 자산 생성 접근 방식과 달리, 우리의 방법의 핵심적이고 새로운 특징은 정통 투영*의 사용이다. 그림과 같이. 3, 3D 부품의 기계적 도면에서 일반적으로 사용되는 이러한 형태의 병렬 투영은 시각 왜곡을 방지하여 목표 잠재 표현에서 각 직교 뷰의 픽셀 식별 재 정렬을 해당 평면에 보장한다.\n' +
      '\n' +
      'Footnote *: [https://enwikipedia.wikipedia.org/wiki/Orthographic_project.10] (https://en.wikipedia.org/wiki/Orthographic_project.10) (https://en.wikipedia.org/wiki/Orthographic_project.\n' +
      '\n' +
      '우리의 접근법의 또 다른 새로운 측면은 동일한 2D U-Net이 다중 뷰 예측과 3D 자산 생성을 동시에 수행하기 위해 훈련된다는 것이다. 알고 있는 범위 내에서는, 이 다중 태스크 시나리오는 이전에 탐구된 적이 없다. 흥미로운 사실은 우리의 결과는 동일한 네트워크가 두 작업을 잘 수행할 수 있고 제안된 육각 가이드가 실제로 고품질 3D 자산 생성을 해제하는 데 도구적이며, 이는 이전의 큰 텍스트 대 이미지 모델의 합성 능력에 대한 자본화를 위한 새로운 기술을 제공한다는 것을 보여준다.\n' +
      '\n' +
      '그림 4: 정통 육각 예측은 생성된 메시에 비해 픽셀 공간에 현실적인 세부 사항을 포함한다. 최종 생성된 메쉬의 시각적 느낌을 크게 향상시키기 위해 UV 텍스처 맵( Sec 3.3 참조)에 대한 상세한 육각 예측을 "가짜"한다.\n' +
      '\n' +
      '그림 3: 예측 대 분류학적 견해이다. 지리적 견해는 시각 왜곡(녹색에서 높은 평면)에서 자유롭기 때문에 3중 표현으로의 재분열을 촉진한다.\n' +
      '\n' +
      '3.2.2 U-Net 아키텍처 수정####\n' +
      '\n' +
      '실험에서 우리는 StableDiffusion 모델의 U-Net 아키텍처를 적응하지만 우리의 방법은 다른 텍스트 대 이미지 모델에도 효과적으로 적용될 수 있다고 믿는다. 그림. 2c는 우리가 U-Net 아키텍처에 적용한 수정을 3D 도메인에 대한 적합성을 높이기 위한 목적으로 보여준다.\n' +
      '\n' +
      '추가 컨디셔닝 신호: 첫 번째 수정은 타임스팟 임베딩에 추가된 두 가지 유형의 학습 가능한 인코더를 도입하여 원래 U-Net에서 수행된 컨볼루션 및 주의 레이어의 타임스택 기반 설정을 수정하는 메커니즘을 제공한다. 이러한 변화는 MVDream[40]의 카메라 임베딩 잔차나 원더3D[21]에서 제안된 도메인 스위처와 같은 기술으로부터 영감을 얻는다. 첫 번째 유형의 인코더는 위치 임베딩에 기초한다. 이러한 도움은 육각 및 삼면 표현에 의해 부과되는 엄격한 공간 정렬 제약을 충족시키는 데 도움이 되며 3D 유전자[11]에서 3D 인식 컨센서스와 동일한 목적을 제공하는 것으로 생각할 수 있다. 제2 유형인 도메인 인코더는 생성 프로세스의 각 단계에 대한 별개의 특징 벡터들 - \\(N\\) 헥사뷰 확산 단계(2D 도메인)에 대한 하나 및 최종 삼평면 매핑 단계(3D 도메인)에 대한 다른 것을 배우는 것을 포함한다. 이 추가 인코딩은 우리의 모델이 잠재 생성 과정의 현재 단계에 기초하여 동적으로 처리를 조정할 수 있게 한다.\n' +
      '\n' +
      '"메이크-it-3d" 토큰: 또 다른 수정은 육각-투-트리플러 매핑 단계 동안 특별한 "메이크-잇-3d" 토큰을 도입하는 것이다. 해당 임베딩 벡터는 교차 의도 계층을 통해 참석하고 U-Net 컴포넌트를 제공하지만 3중 잠재 예측을 위해 그 행동을 구체적으로 적응시키는 또 다른 메커니즘을 제공한다.\n' +
      '\n' +
      '3.2.3 Hexa-to-트리 플레인 레이아웃 컨버터터 3.2.3 Hexa-to-트리 플레인 레이아웃 컨버터#####\n' +
      '\n' +
      '헥사뷰 확산에 따라 이전 섹션에서 논의된 3D 특이적 컨디셔닝 기술을 사용하여 추가 추론 단계를 위해 U-Net을 적용하고 음영 특징을 추출한다. 그런 다음 이러한 특징을 육각형 대 평면 레이아웃 변환기라는 전문 모듈을 사용하여 표적 삼면 표현에 매핑한다. 그림과 같이. 2d, 레이아웃 변환기는 3D 잠재 표현에서 해당 평면과 정렬하기 위해 \\(2\\t 3\\) 입력 레이아웃에서 각 뷰를 재방향화함으로써 시작된다. 이는 개별 견해를 분리하기 위해 슬라이싱, 회전하고 플러싱하여 올바르게 배향하고 평행 뷰(즉, 상하, 좌우, 앞뒤)를 연결하는 등 일련의 매개변수 없는 연산을 통해 달성된다. 여기서 정성적 투영의 사용은 픽셀 정확 레이아웃 변환을 보장하고 시각 왜곡을 제거하므로 유리하다. 그런 다음 컨볼루션 신경망(CNN)은 최종 3중 래치들을 예측한다. 실제로 CNN의 주요 목적은 독립적으로 훈련된 VAE와 정렬하기 위한 채널의 수를 조정하는 것이다. 우리의 실험은 U-Net이 멀티태스킹에 효과적이며 최종 3중 래치들에 잘 매핑되는 육각 래치들 및 3D-조건 특징들 모두를 예측하는 데 효과적임을 나타낸다. 결과적으로, 추가 CNN은 매우 복잡할 필요가 없다.\n' +
      '\n' +
      '더 나은 시각적 외관을 위해 클렌징을 제공합니다.\n' +
      '\n' +
      '최종 메쉬의 시각적 외관을 더욱 향상시키기 위해 육각 확산 동안 예측된 6면 정형학적 견해를 받아들이는 UV 질감 베이킹 접근법을 소개한다. 흥미롭게도 이러한 견해는 종종 VAE-디코딩된 메쉬에 누락된 미세한 텍스트 세부 정보를 포함한다. 이를 해결하기 위해 그림 1에 도시된 바와 같이 중간 육각물을 디코딩된 메쉬에 "배열"하는 후처리 절차를 채택한다. 2b. 이 절차는 세 가지 단계를 포함한다.\n' +
      '\n' +
      '1. **UV 매핑:** 우리는 자동 UV 풀림 알고리즘을 사용하여 UV 매핑을 생성하여 3D 메쉬에 2D 텍스처를 투사할 수 있다.\n' +
      '2. ** 패턴화:** 우리는 UV 질감을 초기화하고 각 텍셀을 해당 3D 좌표로 매핑하고 해당 3중 특징을 샘플링하고 VAE 디코더에서 컬러 MLP를 사용하여 RGB로 변환한다. 이 단계만으로는 질감 분해능이 증가하더라도 시각적 품질을 실질적으로 향상시키지 않지만 - 최종 육각 베이킹 단계에서 완전히 다루지 않는 영역에 중요하다.\n' +
      '3. **Hexaview Baking:** 우리는 UV 질감에 대한 6개(결정) 정성적 뷰로부터의 픽셀들을 프로젝트한다. 여러 헥사뷰 픽셀이 동일한 텍셀에 해당하는 경우, 우리는 그 값을 평균한다. 이 최종 단계는 그림 4와 같이 메쉬의 시각적 품질을 크게 향상시킵니다.\n' +
      '\n' +
      '이 질감 베이킹 절차는 초기 메쉬 디코딩에 의해 제기된 한계를 효과적으로 다루고 있지만 VAE 파이프라인의 추가 개선은 향후 연구를 위한 유망한 방법을 나타낸다. 우리는 잠재적으로 고해상도 래치들을 사용하는 것과 같은 이 구성 요소를 정제하면 현재의 병목 중 일부를 완화할 수 있다고 믿고 향후 작업을 위해 이 탐구를 예약한다.\n' +
      '\n' +
      '### Implementation details\n' +
      '\n' +
      '데이터 훈련. Cap3D[24]에서 생성된 자동화된 캡션으로 보완된 오바마자버스 데이터세트[8]에서 3D 자산에 대한 모델을 훈련합니다. 사전 작업 [1, 40, 11]의 결과에 따라 원본 데이터세트로부터 낮은 품질의 자산을 걸러내어 궁극적으로 \\(86,784\\) 큐레이션 오브젝트에 대한 모델을 훈련한다. 이들 자산에 대한 선택 기준은 기하 복잡성 및 텍스처 풍부도(부록에서 더 자세한 내용을 찾을 수 있다)와 같은 요인을 기반으로 하였다.\n' +
      '\n' +
      '3D 자산 외에도 잠재 확산 모델을 재정렬하면서 훈련 세트를 일반 이미지로 상향 조정합니다. 2D 데이터의 혼입은 이용 가능한 3D 데이터의 제한된 양을 완화시키고 3D 객체를 선택하지 않는 모델에 대한 모델의 일반화를 향상시키는 데 도움이 된다. 이러한 형태의 다과제 규칙화는 MVDream[40]에서 유사하게 사용되었다.\n' +
      '\n' +
      '손실 측면에서 우리는 3D 유전자를 추적하고 기하학 VAE, \\(L_{gelightenment), 깊이 손실, laplselves L_{geness 손실 및 KL 발산 손실을 결합하여 기하학 VAE, \\(L_\\alpha L_{mask})을 감독하며, \\(\\alpha=10^da=0.01\\), \\(\\gamma=0.01\\)와 L1 및 L2 손실의 합을 사용하여 VE, \\(L_{geette, laplururur, laplselves L_\\알파 L_\\알파 L_\\oxy L_\\inginginguuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuup)를 사용하여 깊이 손실, laplselves L_\\ingingings, lapl redistributionization L_\\알파 L_\\ingings, laplselves L_\\ingings, laplselves L_\\inguuuuuuuuuuuuuuuuuuuuuuu\n' +
      '\n' +
      '우리는 \\(16\\)의 배치 크기, \\(3\\10^{-5}\\)의 초기 학습률 및 코사인 어닐링 일정을 사용하여 \\(15\\) epoch에 대한 VAE 모델을 최소 학습률(10^{-6}\\)으로 훈련시킨다. 이것은 8 A100에 대략 일주일 정도 걸립니다.\n' +
      '\n' +
      '단계 2: Triplanar 라트렌트 세대와 마찬가지로 SD-v1.5 [36] 및 SD-XL [31] 두 번의 전처리된 StableDiffusion U-Net 백본으로 실험했다. 우리의 육각형 대 평면 레이아웃 변환기 내에서 사용되는 신경망은 SiLU 활성화 및 처음부터 훈련된 2층 컨볼넷이다.\n' +
      '\n' +
      '중간 육각 생성을 위해 우리는 지상 진리 메쉬의 6\\(512\\시간 512\\) 정성적 예측을 부활시키고, 이를 \\(256\\)로 하향 조정하며, 해당 Stable Diffusion 모델에서 VAE 인코더를 사용하여 이러한 이미지를 인코딩한다. 그런 다음 생성된 \\(32\\t 32\\) 래치들이 크기 \\(64\\t 96\\)의 \\(2\\t 3\\) 레이아웃으로 재조직된다. 2D 정규화를 위해, 우리는 헥사뷰 표현 내에서 개별 견해의 해상도와 일치하는 \\(256\\·256\\)로 크기의 높은 이미지를 사용한다. 스테인드디퓨전 모델은 종종 StableDiffusion 모델을 고려할 때 어려움을 겪는 경우가 많다.\n' +
      '\n' +
      '그림 5: 드림퓨전-SDv2.1(DF)[32], TextMesh-SDv2.1[43], MVDream-SDv2[40] 및 Shap-E [13]의 다른 텍스트 대 3D 접근법과 "HexaGen3D"를 비교한다. 스테이블디확산 [36] 텍스트 대 이미지 모델을 인용하는 DF, TM 및 MV 접근법을 구현하는 테레스트시오 [10] 프레임워크를 사용한다. 우리는 DF를 기반으로 하는 테레스트시오[10]의 텍스트 프롬프트를 사용한다.\n' +
      '\n' +
      '흰색 배경[1]을 설명하며, 우리는 육각에 무작위 착색 배경을 추가한다. 이 추가는 모델이 이러한 맥락에서 배경 색상을 무시하도록 효과적으로 학습하기 때문에 3중 잠재 세대에 영향을 미치지 않는다.\n' +
      '\n' +
      '훈련 중, 우리는 육각 잠재 세대, 육각 대 평면 매핑 및 이미지 잠재 생성(정규화를 위해)의 세 가지 과제를 동시에 해결하기 위해 모델을 효과적으로 최적화한다. 우리는 세 가지 작업 모두에서 구배 축적을 사용하여 이를 달성한다. 우리는 육각 확산 및 육각 간 매핑 작업에 192개의 헥사뷰 배치와 2D 정규화를 위한 32개의 일반 이미지를 배치한다. 우리는 2D 정규화 배치에서 나온 확산 손실을 요인(0.25\\)만큼 하향 조정한다. 우리는 \\(3\\t 10^{-5}\\)의 학습률을 사용하여 \\(50,000\\) 반복에 대한 잠재 생성 모델을 훈련시킨다. 이것은 8 A100에 대략 4일이 소요됩니다.\n' +
      '\n' +
      '우리는 피드포워드 접근, Shap-E[13] 및 3개의 SDS 기반 접근, 드림퓨전[32], TextMesh [43] 및 MV-꿈 [40]을 포함하여 HexaGen3D와 비교하기 위해 최근 텍스트 대 3D 접근법의 범위를 선택한다. Sap-E+ 및 MVDream++의 공식 구현을 사용하는 동안 우리는 원래 논문의 몇 가지 주목할만한 편차를 포함하는 3개의 연구 프레임워크[10] 내에서 사용할 수 있는 드림퓨전 및 TextMesh 구현들을 레버리니다. 구체적으로, 우리의 드림퓨전 설정은 오픈소스 스테이블디퓨전 v2.1을 이젠[37] 대신 안내 모델로 사용하고 해시 그리드[28]을 3D 표현으로 사용하여 Mip-Nerf360[2]에서 분기한다. TextMesh는 3D 서명 거리 필드 표현을 위해 NeuS[44]가 볼SDF[47]를 대체하는 등 지침을 위해 StableDiffusion v2.1을 유사하게 사용한다.\n' +
      '\n' +
      '부타주 †: [https://github.com/openai/sharp-e](https://github.com/openai/sharp-e)\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '그림과 같이. 1, HexaGen3D는 물체, 다양한 스타일 또는 고유한 재료 특성을 조합할 가능성이 없는 복잡한 프롬프트를 포함하여 광범위한 텍스트 프롬프트에서 고품질 텍스처 메쉬를 생성한다. 4.1절에서 우리는 HexaGen3D와 기존 방법을 비교하고 4.2절에서는 절제 연구 결과를 자세히 설명한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c|c c} \\hline \\hline\n' +
      '**Method** & \\multicolumn{2}{c|}{**Latency \\(\\downarrow\\)**} & \\multicolumn{3}{c|}{**CLIP score \\(\\uparrow\\)**} & \\multicolumn{2}{c}{**User preference score \\(\\uparrow\\)**} \\\\  & Single A100 GPU & CLIP L/14 & CLIP B/16 & CLIP B/32 & Vis. qual. & Prompt fid. \\\\ \\hline MVDream-SDv2.1 [40] & \\(\\approx 194\\) mins & **25.02** & **30.35** & **29.61** & **0.97** & **0.88** \\\\ TextMesh-SDv2.1 [43] & \\(\\approx 23\\) mins & 19.46 & 25.06 & 24.86 & 0.12 & 0.21 \\\\ DreamFusion-SDv2.1 [32] & \\(\\approx 22\\) mins & 23.76 & 28.91 & 28.96 & 0.50 & 0.52 \\\\ Shape-E [13] & \\(\\approx 7\\) secs & 19.52 & 24.33 & 24.70 & 0.17 & 0.13 \\\\ \\hline HexaGen3D-SDv1.5 & \\(\\approx 7\\) secs & 24.02 & 28.84 & 28.55 & 0.51 & 0.49 \\\\ HexaGen3D-SDXL & \\(\\approx 7\\) secs & 24.98 & 29.58 & 28.97 & 0.73 & 0.77 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: HexaGen3D와 드림퓨전 [32]의 67개 프롬프트에서 다른 접근 방식을 정량적 비교한 결과 티레스테우디오[10]에서 사용할 수 있다. 우리는 세 가지 다른 CLIP 모델에 대한 단일 A100, CLIP 점수 및 시각 품질 및 신속한 충실도 두 기준에 기반한 사용자 선호 점수에 대한 추론 시간을 보고한다. 여기에 보고된 선호도 점수는 그림 6의 쌍별 선호도 비율을 평균화하여 얻는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c} \\hline \\hline\n' +
      '**Method** & \\multicolumn{2}{c}{**CLIP score \\(\\uparrow\\)**} \\\\  & CLIP L/14 \\\\ \\hline (A1) HexaGen3D-SDv1.5 & 24.02 \\\\ (A2) No Weight Sharing & 23.43 \\\\ (A3) No Hexaview Baking & 21.85 \\\\ (A4) No Hexaview Prediction & 18.47 \\\\ \\hline (B1) HexaGen3D-SDXL & 24.98 \\\\ (B2) No Weight Sharing & 24.97 \\\\ (B3) No Hexaview Baking & 23.59 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: HexaGen3D 모델에서 서로 다른 설계 선택을 정량적으로 비교하고 섹션 4.2에서 더 자세한 설명을 제공한다.\n' +
      '\n' +
      '그림 6:(a) 시각 품질과 (b) 텍스트 프롬프트 충실도에 대한 모든 텍스트 대 3D 접근법을 비교한 사용자 연구는 그림 6:였다. 각 셀은 다른(컬럼)보다 접근(열)에 대한 사용자 선호도 점수(%)를 나타낸다. 접근 방식은 MVDream-SDv2.1(MV-SD), 드림퓨전-SDv2.1(DF-SD), Shape-E, TextMesh-SDv2.1(TM-SD), HexaGen3D-SDv1.5(우리의-SD), HexaGen3D-SDXL(s-XL)이다.\n' +
      '\n' +
      '기존 방법에 대한 비교.\n' +
      '\n' +
      '그림 5는 Shap-E[13], 드림퓨전[32], TextMesh[43] 및 MVDream [40]에 대한 HexaGen3D의 비교를 보여준다. 우리의 접근법은 특히 훨씬 더 빠른 실행 시간을 제공하는 대부분의 측면에서 우수한 성능을 보여준다. MVDream은 고품질 메스를 생산하는 데 탁월하지만 이 기준선은 NVIDIA A100 GPU --공정 \\(1600\\)에 오브젝트를 만드는 데 최대 3시간이 소요되기 때문에 광범위한 생성 시간은 실용성을 방해하며, 이는 불과 7초 만에 이를 달성한다. 또한 MVDream은 다양성 측면에서 한계를 나타내어 종종 종자 전체에 걸쳐 유사한 산출물로 이어진다. 이 점은 그림 1에 나와 있다. 동일한 프롬프트에 대해 4세대를 나타내는 7은 종자를 달리하여 더 다양한 메서를 생성하는 데 HexaGen3D의 이점을 강조한다.\n' +
      '\n' +
      '우리의 정량적 비교(Tab. 1)에서. SD v1.5 및 SDXL을 기반으로 한 변이체가 있는 HexaGen3D는 메쉬 품질에 탁월하고 이러한 기저부에 대한 신속한 충실도로 속도 측면에서 HexaGen3D와 경쟁할 수 있는 유일한 다른 솔루션인 Shape-E를 상당히 능가한다. 이전 버전의 StableD확산에도 불구하고, 사용자 연구와 CLIP 점수 평가는 HexaGen3D-SDv1.5가 더 빨리 \\(180\\)인 동안 드림퓨전과 대략적으로 수행됨을 나타낸다. SDXL 백본으로 전환하면 잠복기에 큰 영향을 미치지 않으면서 결과가 크게 증가한다. 헥사젠3D-SDXL은 드림퓨전(그림 참조)에 비해 \\(78\\%\\)의 선호율에 도달한다. 6)와 평균 프롬프트 충실도 선호도 점수 \\(0.77\\)가 이 메트릭에서 MVDream의 점수 \\(0.88\\)와의 격차를 좁혔다.\n' +
      '\n' +
      '### Ablation studies\n' +
      '\n' +
      '표 2의 절제 연구를 통해 디자인 선택의 일부를 종합적으로 검증하며, 주요 결과는 다음과 같이 요약할 수 있다.\n' +
      '\n' +
      '*** ** 멀티태스킹 멀티뷰 및 3D 세대는 유익합니다.** 동일한 U-Net이 합리적인 정확도로 헥사뷰 확산 및 헥사뷰-트리플러 매핑을 모두 수행할 수 있다는 것이 흥미롭습니다. A2열 및 B2열에서 우리는 이 두 작업에 대해 두 개의 별개의 U-Nets(게임 아키텍처, 별도의 가중치)를 활용하는 것이 성과에 어떤 영향을 미치는지 연구한다. 흥미롭게도 두 개의 개별 네트워크를 사용하면 SDv1.5(A2)로 눈에 띄는 성능 저하가 발생한다. SD-XL(B2)을 사용하면 가중치 공유를 제거하는 것이 큰 차이를 만들지 않지만 모델 크기와 메모리 발자국을 대략 두 배로 증가시키는 단점이 있다.\n' +
      '** **Hexaview 베이킹은 제빵 절차에 의해 생성된 미세한 텍스처 세부 사항을 표시하는 상당한 품질 부스트**을 SDv1.5(A3) 또는 SDXL(B3)이 있는 상당히 낮은 CLIP 점수로 변환한다.\n' +
      '****Hexaview 안내는 6차 안내(A4)가 없는 3D유전자[11]와 유사한 3차원 래치들을 직접 변성시키기 위해 동일한 모델을 트레이닝하는 데 필요한 규칙화***는 성능이 크게 악화되어 모델을 3차 잠재 생성을 위한 대리 과제로 6면 다관 예측을 사용하는 것의 중요성을 보여준다.\n' +
      '\n' +
      '그림 8: 생성된 3D 오브젝트는 박스 유물, 복잡한 또는 얇은 메쉬 구조 측면에서 작은 한계를 포함할 수 있다.\n' +
      '\n' +
      '그림 7: HexaGen3D를 입증하기 위해 각 텍스트 프롬프트에 대해 4세대(4) 세대는 기준 MVDream-SDv2.1 및 드림프렉션-SDv2.1 텍스트 대 3D 모델에 비해 더 다양한 3D 객체를 생성한다.\n' +
      '\n' +
      '## 5 Discussion\n' +
      '\n' +
      '우리의 결과는 기존의 텍스트 대 이미지 모델이 사소한 수정과 최소한의 미세 조정 데이터만으로 3D 자산 생성에 직접 적응할 수 있음을 보여준다. 이러한 지식 전달을 용이하게 하기 위해 기존 텍스트 대 이미지 모델의 2D 역량을 3D 합성을 위해 특별히 고안된 대리 과제인 정통 육각 안내를 소개합니다. 6면 정형화 예측을 예측하는 것에 해당하는 이 과제는 다과제 규칙화의 한 형태로 생각할 수 있다. 또한 3D 생성을 위한 기존 2D 확산 아키텍처에 더 잘 맞는 단순하면서도 효과적인 건축 변형을 제시한다. 우리는 광범위한 실험 결과를 통해 접근법의 효과를 보여주고 이러한 기술이 다양한 텍스트 대 이미지 모델에 널리 적용된다고 믿는다. 특히, SDXL 변이체가 메쉬 품질과 신속한 충실도 측면에서 SDv1.5 상대방을 상당히 능가하는 더 큰 사전 훈련된 텍스트 대 이미지 모델에 대한 우리의 접근 척도. 이러한 확장성은 3D 데이터가 부족한 상황에서 HexaGen3D의 잠재력을 강조한다.\n' +
      '\n' +
      '비교적 드물지만, HexaGen3D에 의해 생성된 3D 객체는 간혹 그림과 같이 3D 객체를 둘러싸는 상자 스캐폴딩과 같은 인공물을 포함한다. 8개(좌표) 이것은 보통 육각 확산 과정이 일관되지 않은 불균일한 배경을 생성할 때 발생하며, 이러한 불일치를 해결하기 위해 3중 잠재 생성 동안 주요 대상 주위에 "벽"을 추가할 수밖에 없다. 또한, 다른 많은 3D 메쉬 재구성 기술에서 관찰된 바와 같이 복잡한 구조 또는 얇은 구조로 3D 물체를 생성하는 것은 악명 높은 도전이다. 8개(오른쪽) 우선, 이러한 문제를 더 많은 3D 데이터[7]로 어느 정도 해결할 수 있는지 탐구하는 것이 흥미로울 것이다. 3D유전자[11]에서 제안된 원래 구현과 비교하여 이 작업에서 대부분 변하지 않은 VAE 파이프라인을 강화하여 향후 연구를 위한 또 다른 흥미로운 방법을 제시한다.\n' +
      '\n' +
      '결론적으로, 우리는 HexaGen3D가 3D 자산 창출 분야에서 혁신적이고 실용적인 해결책으로 두드러져 현재 발전 방식에 빠르고 효율적인 대안을 제시한다고 믿는다. HexaGen3D는 NVIDIA A100 GPU에서 7초 만에 고품질 및 다양한 질감의 메서를 생성하여 샘플당 최적화를 기반으로 한 기존 접근법보다 더 빠르게 주문한다. 대규모 이미지 데이터셋에서 사전 훈련된 레버리지 모델인 HexaGen3D는 핀셋링 동안 객체 또는 객체 구성을 포함하지 않는 광범위한 텍스트 프롬프트(MS-COCO 캡션에서 세대를 포함하여 더 많은 결과에 대한 보충 재료 참조)를 처리할 수 있다. 또한, HexaGen3D는 드림퓨전 또는 MVDream과 같은 SDS 기반 접근법에 비해 상당한 이점인 다른 종자에서 다양한 메서를 생성한다. 우리는 이러한 속성이 HexaGen3D를 3D 콘텐츠 생성의 빠르게 진화하는 영역에서 선구적인 도구로 만들었다고 믿는다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Anonymous. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. _Under Review_, 2023.\n' +
      '* [2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5470-5479, 2022.\n' +
      '* [3] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp, and Kangxue Yin. Texfusion: Synthesizing 3d textures with text-guided image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4169-4181, 2023.\n' +
      '* [4] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. Generative novel view synthesis with 3d-aware diffusion models. _arXiv preprint arXiv:2304.02602_, 2023.\n' +
      '* [5] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Niessner. Text2tex: Text-driven texture synthesis via diffusion models. _arXiv preprint arXiv:2303.11396_, 2023.\n' +
      '* [6] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf: A unified approach to 3d generation and reconstruction. _arXiv preprint arXiv:2304.06714_, 2023.\n' +
      '* [7] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Obigerves-xl: A universe of 10m+ 3d objects. _arXiv preprint arXiv:2307.05663_, 2023.\n' +
      '* [8] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Obigerves: A universe of annotated 3d objects. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13142-13153, 2023.\n' +
      '* [9] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi. Nerfdiff: Single-image view synthesis with nerfguided distillation from 3d-aware diffusion. In _International Conference on Machine Learning_, pages 11808-11826. PMLR, 2023.\n' +
      '* [10] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zixin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. Mrestudio: A unified framework for 3d content generation. [https://github.com/threesudio-project/threesudio](https://github.com/threesudio-project/threesudio), 2023.\n' +
      '* [11] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas Oguz. 3dgen: Triplane latent diffusion for textured mesh generation. _arXiv preprint arXiv:2303.05371_, 2023.\n' +
      '* [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '*[*[13] 희우준과 알렉스 니콜. 샤프-e: 제너레이션 조건부 3d 암묵적 함수. __Shap-e: 제너팅 조건부 3d 암묵적 함수. arXiv 프리프린트 arXiv:2305.02463_, 2023.\n' +
      '* [14] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-320 synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10124-10134, 2023.\n' +
      '* [15] Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten Kreis, Katja Schwarz, Daiqing Li, Robin Rombach, Antonio Torralba, and Sanja Fidler. Neuralfield-ldm: Scene generation with hierarchical latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8496-8506, 2023.\n' +
      '* [16] Diederik P Kingma, Max Welling, et al. An introduction to variational autoencoders. _Foundations and Trends(r) in Machine Learning_, 12(4):307-392, 2019.\n' +
      '* [17] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 300-309, 2023.\n' +
      '* [18] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang Xu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. _arXiv preprint arXiv:2306.16928_, 2023.\n' +
      '* [19] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object, 2023.\n' +
      '* [20] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from a single-view image. _arXiv preprint arXiv:2309.03453_, 2023.\n' +
      '* [21] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. _arXiv preprint arXiv:2310.15008_, 2023.\n' +
      '* [22] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan Lin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin, Ming-Yu Liu, Sanja Fidler, and James Lucas. Att3d: Amortized text-to-3d object synthesis. _arXiv preprint arXiv:2306.07349_, 2023.\n' +
      '* [23] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2837-2845, 2021.\n' +
      '* [24] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. _arXiv preprint arXiv:2306.07279_, 2023.\n' +
      '* [25] Yiwei Ma, Xiaoqing Zhang, Xiaoshuai Sun, Jiayi Ji, Haowei Wang, Guannan Jiang, Weilin Zhuang, and Rongrong Ji. X-mesh: Towards fast and accurate text-driven 3d stylization via dynamic textual guidance. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2749-2760, 2023.\n' +
      '* [26] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12663-12673, 2023.\n' +
      '* [27] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. _Communications of the ACM_, 65(1):99-106, 2021.\n' +
      '* [28] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Transactions on Graphics (ToG)_, 41(4):1-15, 2022.\n' +
      '* [29] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for generating 3d point clouds from complex prompts. _arXiv preprint arXiv:2212.08751_, 2022.\n' +
      '* [30] Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Jonathan T Barron, Amit H Bermano, Eric Ryan Chan, Tali Dekel, Aleksander Holynski, Angjoo Kanazawa, et al. State of the art on diffusion models for visual computing. _arXiv preprint arXiv:2310.07204_, 2023.\n' +
      '* [31] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* [32] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. _arXiv preprint arXiv:2209.14988_, 2022.\n' +
      '* [33] Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, July 2017.\n' +
      '* [34] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR, 2021.\n' +
      '* [35] Elad Richardson, Gal Metzer, Yuval Alauf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. _arXiv preprint arXiv:2302.01721_, 2023.\n' +
      '* [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.\n' +
      '* [37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.\n' +
      '* [38] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.\n' +
      '* [39] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: a hybrid representation for high-resolution 3d shape synthesis. _Advances in Neural Information Processing Systems_, 34:6087-6101, 2021.\n' +
      '* [40] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. _arXiv preprint arXiv:2308.16512_, 2023.\n' +
      '* [41] J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 3d neural field generation using triplane diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 20875-20886, 2023.\n' +
      '* [42] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior. _arXiv preprint arXiv:2303.14184_, 2023.\n' +
      '* [43] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Federico Tombari. Textmesh: Generation of realistic 3d meshes from text prompts. _arXiv preprint arXiv:2304.12439_, 2023.\n' +
      '* [44] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. _arXiv preprint arXiv:2106.10689_, 2021.\n' +
      '* [45] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, et al. Rodin: A generative model for sculpting 3d digital avatars using diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4563-4573, 2023.\n' +
      '* [46] Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel view synthesis with diffusion models. _arXiv preprint arXiv:2210.04628_, 2022.\n' +
      '* [47] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. _Advances in Neural Information Processing Systems_, 34:4805-4815, 2021.\n' +
      '* [48] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. _arXiv preprint arXiv:2206.10789_, 2(3):5, 2022.\n' +
      '* [49] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: A 3d shape representation for neural fields and generative diffusion models. _arXiv preprint arXiv:2301.11445_, 2023.\n' +
      '* [50] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. _arXiv preprint arXiv:2306.17115_, 2023.\n' +
      '* [51] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Distilling view-conditioned diffusion for 3d reconstruction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12588-12597, 2023.\n' +
      '* [52] Zi-Xin Zou, Weihao Cheng, Yan-Pei Cao, Shi-Sheng Huang, Ying Shan, and Song-Hai Zhang. Sparse3d: Distilling multiview-consistent diffusion for object reconstruction from sparse views. _arXiv preprint arXiv:2308.14078_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>