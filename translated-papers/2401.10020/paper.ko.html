<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '자동화어 모델.\n' +
      '\n' +
      ' 위즈 유안({}^{1,2}\\) 리처드 원즈허 판터({}^{1,2}\\).\n' +
      '\n' +
      '***Sainbayar Sukhbaatar\\({}^{1,2}\\)은 Jason Xu\\({}^{1,2}\\)를 징표한다.\n' +
      '\n' +
      '\\({}^{1}\\) NYU. Meta ({}^{2}\\)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '우리는 초인간 에이전트를 달성하기 위해 미래 모델은 적절한 훈련 신호를 제공하기 위해 초인간 피드백을 필요로 한다고 추정한다. 현재 접근법은 보통 인간의 선호도에서 보상 모델을 훈련시키고, 이는 인간 성능 수준에 의해 병목화될 수 있으며, 두 번째로 이러한 개별 냉동 보상 모델은 LLM 훈련 동안 개선되는 것을 배울 수 없다. 이 연구에서 우리는 언어 모델 자체가 LLM-as-a- 판결문을 통해 사용되는 _자기 재생 언어 모델_를 연구하며, 훈련 중 자신의 보상을 제공한다. 우리는 Iterative DPO 훈련 동안 능력 향상에 따른 수업뿐만 아니라 양질의 보상을 스스로 제공할 수 있는 능력을 보여준다. 우리의 접근법의 세 가지 반복에 대한 절단 라마 2 70B는 Claude 2, Gemini Pro 및 GPT-4 0613을 포함한 알파카 에발 2.0 리더보드에 있는 많은 기존 시스템을 능가하는 모델을 산출하며, 예비 연구만이 두 축에서 지속적으로 개선될 수 있는 모델의 가능성에 대한 문을 열어준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '인간 선호도 데이터를 사용하여 대형 언어 모델(LLM)을 정렬하면 전처리 모델(Ouyang et al, 2022, Bai et al., 2022)의 성능에 따른 지시를 크게 개선할 수 있다. 인간 피드백(RLHF)으로부터의 강화 학습 표준 접근법은 이러한 인간의 선호로부터 보상 모델을 학습한다. 그런 다음 보상 모델을 냉동하여 PPO(Schulman et al, 2017)를 통해 RL을 사용하여 LLM을 훈련시키는 데 사용한다. 최근 대안은 보상 모델을 아예 훈련시키는 것을 피하고, 직접 인간 선호도를 사용하여 다이렉트 프리시딩 최적화(DPO; 라파일로프 등, 2023)에서와 같이 LLM을 훈련시키는 것이다. 두 경우 모두 접근 방식은 인간의 선호 데이터의 크기와 품질에 의해 병목되고 RLHF의 경우 이들로부터 훈련된 냉동 보상 모델의 품질도 병목된다.\n' +
      '\n' +
      '이 작업에서 우리는 대신 이 병목 현상을 피하기 위해 LLM 정렬 동안 냉동하기보다는 지속적으로 업데이트되고 있다는 자기 개선 보상 모델을 훈련할 것을 제안한다. 그러한 접근법의 핵심은 보상 모델 및 언어 모델과 같은 별개의 모델로 분리하기보다는 훈련 중에 원하는 모든 능력을 보유한 에이전트를 개발하는 것이다. 업무에 따른 지시의 사전 및 멀티태스킹 교육을 한 번에 많은 과제에 대한 교육을 통해 과제 전달을 허용하는 것과 동일한 방식으로(콜로베르트와 웨스트온, 2008; 라드포드 등 2019; 오양 등은 2022) 보상 모델을 동일한 시스템에 통합하여 보상 모델링 과제와 다음 작업 사이의 태스크 전달을 허용한다.\n' +
      '\n' +
      '따라서 우리는 (i) 둘 다 주어진 프롬프트에 대한 응답을 생성하는 모델에 따라 지시 역할을 하는 에이전트인 _자기 재생 언어 모델_를 소개하고 (ii) 자신의 훈련 세트에 추가하기 위해 예시에 따라 새로운 명령어를 생성하고 평가할 수 있다. 우리는 최근 Xu et al.(2023)에 도입된 것과 유사한 Iterative DPO 프레임워크를 사용하여 이러한 모델을 훈련시킨다. 종자 모델을 시작으로 그림 1과 같이 각 반복에는 새로 생성된 프롬프트에 대한 모델에 의해 후보 반응이 생성되는 _자가 명령 생성_ 과정이 있으며, 그 다음 동일한 모델에 의해 보상이 할당된다. 후자는 LLM-as-a- 판결 프롬프트를 통해 구현되며, 이는 작업을 따르는 지시로도 볼 수 있다. 생성된 데이터로부터 선호도 데이터셋이 구축되고, 모델의 다음 반복은 DPO를 통해 학습된다.\n' +
      '\n' +
      '실험에서 우리는 Open 어시스턴트(Kopf et al., 2023)에 미세 조정된 라마 2 70B(Touvron et al., 2023) 종자 모델로 시작하여 위의 훈련 방식을 수행한다. 우리는 기준선 종자 모델에 비해 셀프 리딩 LLM 정렬에서 성능에 따른 지시가 향상될 뿐만 아니라 더 이상 고정되지 않는 보상 모델링 능력도 향상된다는 것을 발견했다. 이는 반복 훈련 중 모델이 주어진 반복에서 이전 반복보다 스스로 더 높은 품질 선호 데이터 세트를 제공할 수 있음을 의미한다. 이 효과는 실제 환경에서 포화될 가능성이 있지만 원래 인간 대체 종자 데이터에서만 훈련되었을 수 있는 것보다 우수한 보상 모델(및 따라서 LLM)을 얻을 수 있는 흥미로운 가능성을 제공한다.\n' +
      '\n' +
      '2개의 셀프 리워딩 익스프레스 모델.\n' +
      '\n' +
      '우리의 접근법은 먼저 기본 전처리 언어 모델에 대한 접근과 소량의 인간 표지 종자 데이터를 가정한다. 그런 다음 두 가지 기술을 동시에 보유하려는 모델을 구축합니다.\n' +
      '\n' +
      '1._명령 다음_: 사용자 요청을 설명하는 프롬프트를 감안할 때, 고품질, 도움이 되는(및 무해한) 응답을 생성하는 능력이 있다.\n' +
      '2. _자기 명령 생성_: 새로운 지시 제거 예를 생성하고 평가하는 능력이 자체 훈련 세트에 추가된다.\n' +
      '\n' +
      '이러한 기술은 모델이 자기 정렬을 수행할 수 있도록 사용되며, 즉 AI 피드백(AIF)을 사용하여 자신을 반복 훈련시키는 데 사용되는 구성요소이다.\n' +
      '\n' +
      '_자가 명령 생성_는 후보 응답을 생성한 다음 그 품질을 판단하는 모델 자체, 즉 자신의 보상 모델로 작용하여 외부 반응에 대한 필요성을 대체하는 것으로 구성된다. 이는 _LLM-as-a-lerge_ 메커니즘(정 등 2023)을 통해 구현되며, 즉 응답의 평가를 과제에 따른 지시로 구성하여 구현한다. 이 자체 처리된 AIF 선호도 데이터는 트레이닝 세트로 사용된다.\n' +
      '\n' +
      '우리의 전체 _자기 정렬_ 절차는 반복적인 것으로, 이러한 일련의 모델을 구축함으로써 진행되며, 이는 각각이 마지막 동안 개선된다는 것을 목표로 한다. 모형이 모두 생성 능력을 향상시킬 수 있기 때문에 중요한 것은 이러한 반복을 통해 보상 모델 자체가 개선될 수 있다는 것을 의미하며, 이는 보상 모델이 고정되는 표준 관행(오양)을 통해 자신의 보상 모델로 작용하기 때문이다.\n' +
      '\n' +
      '그림 1: ** 자기 재생 언어 모델. 우리의 자기 정렬 방법은 (i) _ 자기 명령 생성_: 새로 생성된 프롬프트가 모델 \\(M_{t}\\)에서 후보 반응을 생성하는 데 사용되며, 이는 또한 LLM-as-a- 판결 프롬프트를 통해 자신의 보상을 예측한다. (ii) 학습 후 명령어: DPO를 통한 학습에 사용되는 생성된 데이터에서 선호도 쌍이 선택되어 모델 \\(M_{t+1}\\)가 생성된다. 그런 다음 이 전체 절차를 반복하면 모델링 능력과 보상 모델링 능력에 따라 개선된 명령어가 모두 향상될 수 있다.**.\n' +
      '\n' +
      '네, 2022. 우리는 이것이 앞으로 이러한 학습 모델의 자기 개선 가능성을 높여 병목 수축을 제거할 수 있다고 믿는다.\n' +
      '\n' +
      '우리는 이러한 단계를 아래에서 더 자세히 설명한다. 접근 방법에 대한 개요는 그림 1에 나와 있다.\n' +
      '\n' +
      '### Initialization\n' +
      '\n' +
      '데이터에 따른 선택 명령어: 전처리된 기본 언어 모델에서 시작하여 감독된 미세 조정(SFT) 방식으로 학습에 사용하는 예시에 따라 인간 대체(명령 프롬프트, 응답) 일반 명령의 종자 세트를 제공한다. 그 후, 이를 명령어 Fine-Tuning(IFT) 데이터로 지칭할 것이다.\n' +
      '\n' +
      '데이터에 따른 세드 LLM-as-a- 판결 지시는 또한 우리가 학습에 사용할 수 있는 종자 세트(평가 지시 프롬프트, 평가 결과 응답) 예를 제공한다고 가정한다. 이것은 엄격하게 필요하지 않지만 IFT 데이터를 사용하는 모델은 이미 LLM-as-a- 판결문을 학습할 수 있기 때문에 이러한 학습 데이터가 개선된 결과를 제공할 수 있음을 보여준다. 이 데이터에서 입력 프롬프트는 모델이 특정 명령어에 대한 주어진 응답의 품질을 평가하도록 요청한다. 제공된 평가 결과 반응은 연쇄 추론( 정당화)으로 구성되며 최종 점수(5점 만점에)가 뒤따른다. 이러한 프롬프트에 대해 선택한 포맷은 그림 2에 나와 있으므로 LLM이 보상 모델의 역할을 수행하기 위한 훈련 데이터 역할을 한다. 이어서 이를 평가 파인튜닝(EFT) 자료로 지칭할 것이다.\n' +
      '\n' +
      '훈련 중에 이 두 종자 세트를 함께 사용합니다.\n' +
      '\n' +
      '### Self-Instruction Creation\n' +
      '\n' +
      '우리가 훈련한 모델을 사용하여 자체 훈련 세트를 자체 수정할 수 있습니다. 구체적으로 다음 훈련 반복을 위한 추가적인 학습 데이터를 생성한다.\n' +
      '\n' +
      '이는 다음과 같은 단계로 구성되어 있다.\n' +
      '\n' +
      '1._유전자의 새로운 프롬프트_: 몇 번의 샷 프롬프트를 사용하여 새로운 프롬프트(x_{i}\\)를 생성하고 왕 et al.(2022) 및 호노비치(2023)의 접근에 따라 원래 종자 IFT 데이터에서 샘플링 프롬프트를 생성한다.\n' +
      '2. _게놈 후보 반응_: 우리는 샘플링을 사용하여 우리의 모델에서 주어진 프롬프트 \\(x_{i}\\)에 대한 \\(N\\) 다양한 후보 반응(\\{y_{i}^{i}^{1},\\dots,y_{i}^{N}\\}\\)을 생성했다.\n' +
      '3. _평가 후보 응답_: 최종적으로 동일한 모델의 LLM-as-a-판결 능력을 사용하여 점수 \\(r_{i}^{n}\\in[0,5]\\)로 자신의 후보 반응을 평가한다(그림 2 참조).\n' +
      '\n' +
      '강화된 훈련.\n' +
      '\n' +
      '앞서 설명한 바와 같이, 먼저 종자 IFT 및 EFT 데이터로 학습이 수행된다(섹션 2.1). 그런 다음 AI(자체-)Feedback)를 통해 추가 데이터로 증강됩니다.\n' +
      '\n' +
      '자체 명령 생성 절차를 수행한 후 AI 피드백 훈련은 훈련을 위한 추가 예시로 종자 데이터를 증가시킬 수 있으며, 이는 AI 피드백 훈련(AIFT) 데이터로 지칭한다. 우리는 그러한 피드백의 두 가지 변형을 시도한다.\n' +
      '\n' +
      '* _ 사전 회의 쌍_: 형태의 훈련 데이터(명령 프랙션 \\(x_{i}\\), 승자 반응 \\(y_{i}^{w}\\) 및 반응 상실(y_{i}^{l}\\)을 구성한다. 우승과 패배를 형성하기 위해 우리는 Xu 등(2023년)에 이어 \\(N\\) 평가 후보 응답(2.2절 참조)에서 가장 높고 낮은 점수 반응을 취하여 점수가 같으면 쌍을 폐기한다. 이 쌍들은 선호도 튜닝 알고리즘으로 학습에 사용될 수 있다. DPO(Rafailov et al., 2023)를 사용합니다.\n' +
      '* _양성 예만_: 이 변형에서 우리는 다른 접근법(Li et al, 2023; Adolphs et al, 2023; Gulcehre et al; Gulcehre et al)에 따라 모델을 통해 큐레이션된 종자 세트에 (명령 프롬프트, 응답)의 추가 예를 추가하여 아래에 설명된 첨가제 5점 점수 시스템을 사용하여 사용자의 질문 및 해당 응답을 검토한다. 각 기준의 만족도를 바탕으로 점들이 축적된다.\n' +
      '\n' +
      '응답이 관련이 있고, 불완전하거나 일부 관련 없는 콘텐츠를 포함하더라도 사용자의 문의에 관련된 몇 가지 정보를 제공하는 경우, 10개의 1 지점이 필요하다.\n' +
      '\n' +
      '응답이 사용자의 질문의 상당 부분을 해결하지만, 질의를 완전히 해결하거나 직접 답을 제공하는 것은 아닌 경우 다른 점을 추가한다.\n' +
      '\n' +
      '응답이 AI 어시스턴트에 의해 작성된 것 같거나 블로그나 검색 결과에서 일반적으로 발견되는 요소가 있는지 여부에 관계없이 사용자의 질문의 기본 요소에 유용한 방식으로 답할 경우, 세 번째 포인트 상을 받는다.\n' +
      '\n' +
      'AI 어시스턴트의 관점에서 반응이 명확하게 작성된 경우 네 번째 점을 제시하여 사용자의 질문을 직접적이고 포괄적으로 다루며 명확성, 간결성 또는 초점 개선의 여지가 약간 있더라도 잘 정리되고 도움이 된다.\n' +
      '\n' +
      'AI 어시스턴트가 외부 정보 없이 AI 어시스턴트에 의해 사용자의 질문에 탄핵적으로 맞춘 응답의 다섯 번째 점, 전문가의 지식을 반영하고 고품질, 매력적이고 통찰력 있는 답을 입증한다.\n' +
      '\n' +
      'User: <INSTRUCTION_HER>\n' +
      '\n' +
      '<response><RESPONSE_HER></response>\n' +
      '\n' +
      '사용자의 지시와 응답을 살펴본 후\n' +
      '\n' +
      '총점은 최대 100단어로 설명합니다.\n' +
      '\n' +
      '"스코어: <총점>"이라는 형식을 사용하여 점수를 매기는 것이 그것이다.\n' +
      '\n' +
      '필요에 따라 웹 검색 지식을 활용하여 AI 어시스턴트 관점에서 평가하는 것을 기억하세요. 이 부가 점수 모델과의 정렬에서 반응을 평가하기 위해 요약된 기준에 따라 포인트를 체계적으로 속성할 것이다.\n' +
      '\n' +
      '선호 데이터를 구성하기보다는2023]입니다. 이 설정에서 우리는 후보 반응이 평가된 예만을 추가하여 \\(r_{i}^{n}=5\\)의 완벽한 점수를 제공한다.\n' +
      '\n' +
      '실험에서 두 접근법의 결과를 보고하지만 선호도 쌍으로부터의 학습이 우수한 성능을 부여하므로 그 접근법을 권장한다는 것을 발견했다.\n' +
      '\n' +
      '자활을 영위할 수 있습니다.\n' +
      '\n' +
      '우리의 전반적인 절차는 일련의 모델 \\(M_{1},\\ldots,M_{T}\\)을 훈련하며, 각 연속적인 모델 \\(t\\)이 \\(t-1^{\\text{th}}\\) 모델에 의해 생성된 증강 학습 데이터를 사용한다. 따라서 우리는 AIFT(\\(M_{t}\\))를 모델 \\(M_{t}\\)를 사용하여 생성된 AI 피드백 훈련 데이터를 의미한다.\n' +
      '\n' +
      '따라서 우리는 모델을 정의하고 그들이 사용하는 학습 데이터는 다음과 같다.\n' +
      '\n' +
      '\\(M_{0}\\): 미세 조정 없이 LLM을 전처리했다.\n' +
      '\n' +
      '\\(M_{1}\\): \\(M_{0}\\)로 분류된 다음 SFT를 사용하여 IFT+EFT 종자 데이터에 미세 조정되었다.\n' +
      '\n' +
      '\\(M_{2}\\): \\(M_{1}\\)로 분류된 다음 DPO를 사용하여 AIFT(\\(M_{1}\\)) 데이터로 훈련했다.\n' +
      '\n' +
      '\\(M_{3}\\): \\(M_{2}\\)로 분류된 다음 DPO를 사용하여 AIFT(\\(M_{2}\\)) 데이터로 훈련했다.\n' +
      '\n' +
      '이 반복 훈련은 Xu et al.(2023)에 도입된 쌍방향 체링지 최적화와 Iterative DPO에 사용된 절차와 유사하지만 해당 작업에서 외부 고정 보상 모델을 사용했다.\n' +
      '\n' +
      '그림 2: **LLM-as-a-판결은 우리의 LLM이 보상 모델***로 작용하고 자체 모델 세대에 대한 셀프 리워드를 제공하게 되었다. 이 모델은 처음에 이 작업에서 잘 수행할 수 있는 방법에 대한 종자 훈련 데이터로 훈련된 다음 자체 준수 훈련 절차를 통해 이 작업에서 더 개선된다.\n' +
      '\n' +
      'Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '실험에서 우리는 Llama 2 70B(Touvron et al., 2023)를 기본 전처리 모델로 사용한다.\n' +
      '\n' +
      '3.1.1.1 Seed 트레이닝 데이터 3.1.1 Seed 트레이닝 데이터######\n' +
      '\n' +
      'IFT 세드 데이터는 안내 미세 조정에 대해 오픈 어시스턴트 데이터세트(Kopf et al., 2023)에 제공된 인간 대체 예를 사용한다. 리 등(2023년)에 이어, 우리는 3200개의 사례를 사용하여 인간의 주석이 달린 순위(가장 높은 순위 0)를 기반으로 고품질 영어에서 첫 번째 대화 전환만 샘플링한다. 우리의 실험에서 우리는 감독된 미세 조정을 통해 이 데이터만을 사용하여 기본 모델에서 미세 조정된 모델과 비교하여 _SFT 기준선_로 지칭한다.\n' +
      '\n' +
      'EFT 세드데이터 오픈 어시스턴트 데이터는 또한 평가 미세 조정 데이터를 구성할 수 있는 프롬프트당 복수의 순위 인간 응답을 제공한다. 이를 기차 및 평가 세트로 나누고 이를 사용하여 LLM-as-a- 판단 데이터를 생성한다. 이는 점수 기준 설명으로 구성된 그림 2에 주어진 입력 포맷에 배치하여 실시하며, 평가 대상 1에 대한 주어진 지시와 응답을 훈련 대상에 직접 제공하지 않으므로 SFT 기준선을 사용하여 각 입력에 대한 이러한 출력 평가를 생성하고, 점수 순위가 데이터셋에서 인간 순위에 동의하면 훈련 세트에 수용한다. 이는 1775개의 열차 및 531개의 평가 사례(IFT 데이터와 중첩되지 않는다)를 초래한다.\n' +
      '\n' +
      '리 등은 프롬프트 1: 노트(2023)에서 파생된 프롬프트는 "웹 검색을 활용"하는 것을 언급하지만 우리의 모델은 실제로 이 조치를 할 수 없다.\n' +
      '\n' +
      '항목 3.1.2 평가항목 3.1.2.2 평가항목#### 3.1.2.2 평가항목#########\n' +
      '\n' +
      '우리는 지침을 따르는 능력과 보상 모델로서의 능력(응답을 평가할 수 있는 가능성)이라는 두 축에서 자기 선호 모델의 성능을 평가한다.\n' +
      '\n' +
      '우리는 알파카Eval 평가 프롬프트(Li et al, 2023)를 사용하여 Li et al.(2023)에 이어 다양한 출처에서 파생된 256개의 테스트 프롬프트로 GPT-4(Achiam et al., 2023)를 평가자로 사용하여 다양한 모델 간의 머리 대 머리 성능을 평가한다. 우리는 쌍별 비교를 하는 두 주문에서 프롬프트를 시도하며 GPT-4 평가가 동의하지 않는 경우 결과를 타이로 카운트한다. 우리는 또한 805개의 프롬프트에 걸쳐 평가된 알파acaEval 2.0 리더보드 형식의 결과를 보고하고 GPT-4 판단을 기반으로 기준선 GPT-4 터보 모델에 대한 승률을 계산한다.\n' +
      '\n' +
      '리워드 모델링은 섹션 3.1.1에 설명된 바와 같이 오픈 어시스턴트 데이터 세트에서 파생된 평가 세트에 대한 인간 순위와 상관 관계를 평가하며, 실행 수업은 주어진 순위와 평균 2.85개의 반응을 가지고 있다. 따라서 우리는 _쌍별 정확도_를 측정할 수 있는데, 이는 주어진 쌍 간의 순위 순서가 모델의 평가와 인간 순위 사이에 얼마나 일치하는지이다. 우리는 또한 총 주문량이 명령어에 대해 정확히 얼마나 자주 동일한지 _exact 일치하지_ 카운트를 측정한다. 우리는 또한 스피어먼 상관과 켄달의 \\(\\tau\\)를 보고한다. 마지막으로, 우리는 모델 점수가 5점 중 완벽한 5점이라는 응답이 인간이 가장 높은 순위를 차지하는 것으로 평가되는 경우가 얼마나 자주 있는지 보고한다.\n' +
      '\n' +
      '훈련용 소매는 3.1.3 훈련용 소매품 3.1.3의###########\n' +
      '\n' +
      '우리가 사용하는 훈련 초모수 학습 후 지도 내용은 다음과 같다. SFT의 경우 학습률 \\(5.5e{-6}\\)를 사용하여 선형적으로 \\(1.1e{-6}\\), 배치 크기 16 및 드롭아웃 0.1로 디코딩한다. 우리는 전체 서열 대신 표적 토큰의 손실만을 계산한다. DPO의 경우, 우리는 학습률 \\(1e{-7}\\), 배치 크기 16, 드롭아웃 0.1 및 \\(\\beta\\) 값이 0.1인\\(1e{-6}\\)를 사용한다. 우리는 Li et al(2023)에 따라 다양한 출처에서 파생된 253개의 검증 예시에서 200단계마다 체크포인트를 절약하고 세대를 평가하여 조기 정지한다. 이는 AlpacaEval 평가 프롬프트 포맷[Li et al., 2023b]을 사용하여 이전 단계의 세대에 대해 쌍으로 평가된다.\n' +
      '\n' +
      '새로운 프롬프트를 생성하기 위해 고정 모델을 사용한 반면, 램마 2-챗 70B는 8샷 프롬프트가 있는 반면, 생성 파이프라인의 다른 부분(반응 생성 및 평가)은 훈련 중인 모델을 사용한다. 후보 반응 생성을 위해 온도 \\(T=0.7\\)로 \\(N=4\\) 후보 반응을 샘플한다 \\(p=0.9\\). 후보 반응을 평가할 때 이러한 점수에 대한 분산이 있기 때문에 실험에서 샘플링된 디코딩(동일한 매개변수 포함)을 사용하고 이러한 평가를 여러 번 생성하고 평균을 취한다. 우리는 3,964개의 이러한 선호 쌍을 추가하여 DPO를 통해 \\(M_{1}\\))를 훈련시키는 데 사용된 AIFT(M_{2}\\)) 데이터셋과 6,942쌍을 추가하여 \\(M_{3}\\)을 훈련시키는 데 사용되는 AIFT(\\(M_{2}\\))를 형성한다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '고유능력 저하에 대한 명령에는 3.2.1.2.1이 있다.\n' +
      '\n' +
      '머리의 성능 결과들은 그림 3에 나와 있다.\n' +
      '\n' +
      'EFT+IFT 종자 훈련은 IFT 단독과 유사하게 수행하며, 학습에 평가 파인튜닝(EFT) 과제를 추가하는 것은 머리와 거의 동일한 머리(30.5% 대 30.9%)를 갖는 명령어 파인튜닝(IFT) 데이터만으로는 성능에 따른 지시에 영향을 미치지 않는다는 것을 발견했다. 이는 자기확보에 대한 모델의 능력 증가가 다른 기술에 영향을 미치지 않는다는 것을 의미하기 때문에 긍정적인 결과이다. 따라서 우리는 IFT+EFT 교육을 자체 재생 모델의 Iteration 1(\\(M_{1}\\))으로 사용하고 추가 반복을 실행할 수 있다.\n' +
      '\n' +
      'Iteration 2(M_{2}\\))는 Iteration 1(M_{1}\\))과 SFT BaselineIteration 2의 자기 재생 훈련(M_{2}\\))을 개선하며, Iteration 1(M_{1}\\)에 이어 우수한 지시를 제공하며, \\(M_{2}\\)은 \\(M_{1}\\)에 대한 55.7%(M_{2}\\)에 비해 Iteration 1(M_{1(M_{1(M_{2}\\)에 이어 Iteration 1(M_{1(M_{1(M_{1(M_{2}\\)에 이어 Iteration 1(M_{1(M_{1(M_{1(M_{2}\\)에 비해 \\)에 비해 \\(M_{1(M_{1(M_{1(M_{2}\\)에 비해 \\)에 비해 55.5%(M_{1(M_{1,{2}\\)에 비해 \\(M_{1(M_{ SFT 분지(49.2% 승리 대 14.5%)에서도 비슷한 이득을 제공한다. 분명히, Iteration 1의 보상 모델에 의해 제공되는 선호도 데이터 AIFT(\\(M_{1}\\))를 사용하여 \\(M_{2}\\)에서 \\(M_{2}\\)까지의 성능의 큰 점프(M_{1}\\)가 있다.\n' +
      '\n' +
      'Iteration 3(M_{3}\\))은 Iteration 2(\\(M_{2}\\)보다 향상되며, Iteration 2(M_{2}\\)보다 Iteration 3에서 더 많은 이득을 보고 있으며, 머리 평가(M_{2}\\)의 경우 12.5%에 비해 47.7%의 \\(M_{3}\\)이 나타났다. 유사하게, \\(M_{3}\\)에 대한 SFT 분지(SFT 분지)의 승률은 62.5% 대 9.8%로 증가하며, 즉 \\(M_{2}\\) 모델보다 더 자주 승리했다. 전반적으로 Iteration 2의 보상 모델에서 제공하는 선호도 데이터 AIFT(\\(M_{2}\\))를 이용한 학습을 통해 \\(M_{2}\\)에서 \\(M_{3}\\)까지의 큰 이득을 본다.\n' +
      '\n' +
      '알파시발 2 리더보드에 대해 잘 수행한다는 본 연구 결과는 알파시발 2.0 리더보드에 대한 학습 반복 수익률이 Iteration 1의 9.94%에서 Iteration 2, Gemini Pro 및 GPT4 0613의 15.38%로 GPT4-터보보다 높은 승률을 향상시켰음을 보여준다. 우리는 경쟁 모델 중 많은 부분이 독점적 정렬 데이터(예: [Touvron et al, 2023])에서 1M 주석을 초과하거나 더 강력한 모델에서 증류된 표적을 사용한다는 점에 주목한다. 대조적으로, 우리의 셀프 리딩 모델은 오픈 어시스턴트로부터의 작은 종자 데이터 세트로부터 시작하여 훈련의 추가 반복을 위해 모델 자체로부터 표적 및 보상을 생성한다.\n' +
      '\n' +
      '긍정적인 예들로의 사전 최적화 향상은 또한 미세 조정(선호 최적화 없이)을 감독하기 위해 고품질 자체 명령 생성 예들을 추가하는 대안적인 자기 학습 절차를 시도했다. 안타깝게도 우리는 이 접근법이 도움이 된 구성을 찾을 수 없었다. 예를 들어 5점 만점에 5점, 훈련에서 혼합 중량을 최적화하는 11,254점을 추가하는 것은 여전히 SFT 분지선이 29% 대 30% 승리, 즉 개선이 없는 머리로 고개를 내밀었다.\n' +
      '\n' +
      '데이터 분포 분석을 통해 A.1절에서 볼 수 있는 IFT, EFT 및 AIFT(\\(M_{1}\\)) 데이터의 t-SNE(Van der Maaten 및 Hinton, 2008) 시각화를 수행하며, EFT 예는 원하는 IFT와 AIFT(\\(M_{1}\\)) 사례 사이에 좋은 중첩을 발견하고 EFT 예는 임베딩 공간의 다른 부분에 있다. 우리는 \\(M_{1}\\)의 세대가 평균 길이가 1092이고, \\(M_{2}\\)는 1552이고, \\(M_{3}\\)는 2552이므로 모델이 더 긴 반응을 생성하도록 학습하여 상대적 성능의 요인이 될 수 있음을 관찰한다.\n' +
      '\n' +
      '효용성을 모형화한 3.2.2.2 리워드 모델 용량############# 3.2.2와 같다.\n' +
      '\n' +
      '리워드 모델링 평가 결과는 표 2에 나와 있다.\n' +
      '\n' +
      'EFT 증강은 먼저 SFT 기준선보다 개선되며, 평가 Fine-Tuning(EFT) 데이터를 학습에 추가한다는 것을 발견하며, 이는 LLM-as-a- 결정부 역할을 하는 방법의 모델에 대한 예를 제공하므로 명령어 Fine-Tuning(IFT) 데이터 단독으로 훈련하는 것과 비교하여 자연적으로 성능을 향상시킨다. IFT 데이터는 광범위한 일반적인 명령어 작업을 포괄하므로 응답 평가 능력으로 SFT 분지선을 종료하지만 EFT 데이터는 이 특정 작업의 더 많은 예를 제공한다. IFT+EFT 대 IFT 사용 시 측정된 5가지 메트릭 모두에서 개선을 발견한다. IFT 단독, 예를 들어 인간과 쌍별 정확도 일치도는 65.1%에서 78.7%로 증가한다.\n' +
      '\n' +
      '자력교육으로 리워드 모델화 능력이 향상된다는 것을 발견하는데, 자기워드 훈련의 라운드 수행 _impro가 다음 반복_에 대한 셀프 리워드를 제공할 때 모델의 능력을 향상시키며, 능력에 따라 향상된 지도도 제공한다는 것을 알 수 있다. 모형 \\(M_{2}\\) (Iteration 2)는 \\(M_{1}\\)의 보상 모델을 사용하여 학습되지만 \\(M_{1}\\)에 비해 5가지 메트릭 모두에서 향상된 성능을 제공한다. 예를 들어 쌍별 정확도는 78.7%에서 80.4%로 향상됩니다. 측정 3(\\(M_{3}\\))은 이러한 메트릭 중 일부를 추가로 비교한 결과 3(\\(H_M_{3}\\))를 개선한다.\n' +
      '\n' +
      '그림 3: ** 자기 훈련으로 능력 향상:** GPT-4를 사용하여 다양한 프롬프트에서 머리 대 머리 승률을 사용하여 모델을 평가하고 SFT 분지선은 자기 재생 Iteration 1(\\(M_{1}\\)과 일치한다. 그러나 Iteration 2(\\(M_{2}\\))는 Iteration 1(\\(M_{1}\\)) 및 SFT 분지형을 모두 능가한다. 패턴 3(M_{3}\\))은 Iteration 2(\\(M_{2}\\)보다 더 많은 이득을 부여하고, 우수 \\(M_{1}\\), \\(M_{2}\\) 및 SFT 분지선을 큰 마진으로 제공한다.\n' +
      '\n' +
      '예를 들어 쌍별 정확도는 80.4%에서 81.7%로 증가한다. 이 성능 이득은 추가 EFT 데이터가 제공되지 않았음에도 불구하고 달성되며, _ 자기 명령 생성_ 루프 동안 생성된 예는 LLM-as-a- 판단 훈련 예처럼 보이지 않는다. 우리는 모델이 이후 일반적인 지시에 따라 더 좋아지고 있기 때문에 LLM-as-a-Amge 과제에서도 개선된다고 가정한다.\n' +
      '\n' +
      '이 실험에서 LLM-as-a-판사 프롭트의 수입은 그림 2에 표시된 LLM-as-심판 프롬프트 형식을 사용했으며 예비 실험에서 우리는 또한 가장 효과적인 것을 결정하는 다양한 다른 프롬프트를 시도했다. 예를 들어, 우리는 5점 척도를 제안하지만 옵션을 다양한 품질 버킷의 여러 선택으로 설명하지만 그림 5를 볼 수 있는 리 등(2023a)에서 제안된 프롬프트를 시도했지만, 우리의 프롬프트는 요점을 품질의 다양한 측면을 덮는 첨가제로 설명한다. 우리는 큰 차이를 찾고 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline  & \\multicolumn{3}{c}{Self-Rewarding Models} \\\\ Model & SFT Baseline & Iter 1 (\\(M_{1}\\)) & Iter 2 (\\(M_{2}\\)) & Iter 3 (\\(M_{3}\\)) \\\\ \\hline Training data & IFT & IFT+EFT & IFT+EFT & IFT+EFT \\\\  & & & +AIFT(\\(M_{1}\\)) & +AIFT(\\(M_{2}\\)) \\\\ \\hline Pairwise accuracy (\\(\\uparrow\\)) & 65.1\\% & 78.7\\% & 80.4\\% & 81.7\\% \\\\\n' +
      '5-best \\% (\\(\\uparrow\\)) & 39.6\\% & 41.5\\% & 44.3\\% & 43.2\\% \\\\ Exact Match \\% (\\(\\uparrow\\)) & 10.1\\% & 13.1\\% & 14.3\\% & 14.3\\% \\\\ Spearman corr. (\\(\\uparrow\\)) & 0.253 & 0.279 & 0.331 & 0.349 \\\\ Kendall \\(\\tau\\) corr. (\\(\\uparrow\\)) & 0.233 & 0.253 & 0.315 & 0.324 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: ** 리워드 모델화 능력은 자기훈련**로 향상되며, 우리는 보유 인간 선호 데이터와의 정렬을 측정하는 다양한 메트릭을 통해 LLM-as-a- 판결문을 평가한다. 이전 반복(M_{1}\\)에서 파생된 셀프워드 모델을 사용하여 훈련되는 자체 재생 Iteration 2(모델 \\(M_{2}\\)는 Iteration 1(\\(M_{1}\\)을 능가하는 반면,\\(M_{1}\\) 자체는 명령어 Fine-Tuning(IFT) 데이터에서만 훈련된 표준 SFT 기준선 모델을 능가한다. 조정 3(모델 \\(M_{3}\\))은 Iteration 2보다 더 많은 개선을 제공한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline  & \\multicolumn{3}{c}{Alignment Targets} \\\\ Model & **Win Rate** & Distilled & Proprietary \\\\ \\hline Self-Rewarding 70B & & & \\\\ _Iteration 1_ (\\(M_{1}\\)) & 9.94\\% & & \\\\ _Iteration 2_ (\\(M_{2}\\)) & 15.38\\% & & \\\\ _Iteration 3_ (\\(M_{3}\\)) & 20.44\\% & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline \\multicolumn{4}{l}{_Selected models from the leaderboard_} \\\\ GPT-4 0314 & 22.07\\% & & ✓ \\\\ \\multicolumn{4}{l}{Mistral Medium} & 21.86\\% & ✓ \\\\ Claude 2 & 17.19\\% & & ✓ \\\\ \\multicolumn{4}{l}{Gemini Pro} & 16.85\\% & & ✓ \\\\ \\multicolumn{4}{l}{GPT-4 0613} & 15.76\\% & ✓ \\\\ \\multicolumn{4}{l}{GPT 3.5 Turbo 0613} & 14.13\\% & ✓ \\\\ \\multicolumn{4}{l}{LLaMA2 Chat 70B} & 13.87\\% & ✓ \\\\ \\multicolumn{4}{l}{Vicuna 33B v1.3} & 12.71\\% & ✓ \\\\ \\multicolumn{4}{l}{Humpback LLMa2 70B} & 10.12\\% & \\\\ \\multicolumn{4}{l}{Guanaco 65B} & 6.86\\% & \\\\ \\multicolumn{4}{l}{Davinci001} & 2.76\\% & \\\\ \\multicolumn{4}{l}{Alpaca 7B} & 2.59\\% & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **AlpacaEval 2.0 결과(GPT-4에 의해 평가된 GPT-4 터보에 대한 승률). 셀프 리딩 반복 수율은 윈드 속도를 향상시킵니다. 위치 3(\\(M_{3}\\))은 독점 학습 데이터를 사용하거나 더 강력한 모델에서 증류된 표적을 사용하는 많은 기존 모델을 능가한다.**의 두 프롬프트는 SFT 분지선을 사용할 때, 예를 들어 65.1% 쌍별 정확도, 26.6%의 쌍별 정확도만 사용한다. 자세한 내용은 A.2절을 참조하세요.\n' +
      '\n' +
      '4번 관련 작업.\n' +
      '\n' +
      '대형 언어 모델을 자동적으로 개선하거나 자체 보정하는 것이 연구의 주요 초점이 되고 있다. 최근 판 등의 설문 조사(2023년)는 주제를 요약하고자 한다. 그러나 이것은 빠르게 움직이는 지역이며, 이미 그곳에는 다루지 않는 새로운 작품이 유망하다.\n' +
      '\n' +
      '인간 피드백(RLHF)의 강화 학습(2019)은 지글러 등(2019), 스트니온(2020), 오양(2022), 바이(2022) 등은 인간 선호 데이터에서 고정된 보상 모델을 훈련시킨 다음, 식별 정책 최적화(PPO)와 같은 강화 학습(PPO)을 통해 강화 학습(RL)을 통해 교육하도록 보상 모델을 사용한다. 따라서 특정 의미의 보상 신호는 이미 이러한 작업에서도 모델에서 비롯되지만 인간 데이터에서 증류된다. 그럼에도 불구하고, 이것은 일반적으로 인간 피드백(RLHF)의 RL이라고 한다. 다이렉트 프리시즈 최적화(DPO)와 같은 방법(Rafailov et al., 2023)은 보상 모델을 완전히 훈련시키는 것을 피하고 대신 인간의 선호도를 사용하여 LLM을 직접 훈련시킨다. 다른 여러 경쟁 방법들(Zhao et al, 2023; Zuan et al, 2023; Zuan et al., 2023; Pairwise Cringe Optimization(PCO)(Xu et al., 2023)도 존재하는데, 이는 알파카 팜(Dubois et al., 2023)에서 DPO 및 PPO를 능가하는 것으로 나타났다. PCO는 고정된 보상 모델을 제외하고 작업에서 것과 유사한 반복 훈련 접근법을 사용하고 동일한 방식을 사용하여 DPO보다 Iterative DPO가 향상됨을 보여주었다.\n' +
      '\n' +
      'AI 피드백(RLAIF) 제도 AI(Bai et al., 2022)의 강화학습은 LLM을 사용하여 피드백을 주고 응답을 개선하며, 이 데이터를 사용하여 (분리, 고정) 보상 모델을 훈련시킨다. 그런 다음 \'AI 피드백의RL\'(RLAIF)을 수행하는 데 사용됩니다. 이 등(2023)은 RLAIF와 RLHF 절차를 비교하여 대략 동등하게 수행하는 방법을 찾는다. 그들은 LLM-as-a- 판결문을 수행하기 위해 "off-the-shelf" LLM을 사용하여 고정된 보상 모델을 훈련하기 위해 훈련 세트를 구축하는 것을 촉발했으며, 이는 RL 훈련에 사용된다. 그들은 또한 고정되지만 별도의 LLM-as-a- 판단 모델을 직접 사용하여 실험했으며, 저자는 PPO 훈련 내에서 사용(우리가 작업에서 사용하는 반복 접근법의 오프라인 단계보다 상대적으로 계산적으로 저렴한)으로 인해 계산적으로 비싸다고 보고한다. 마지막으로, Chen et al.(2024)는 최근 인간 라벨을 한 쌍에서 승리 반응으로 사용하여 Iterative DPO 유사 프레임워크에서 보상 모델을 완전히 피할 수 있고 마지막 반복 세대는 쌍에서 손실 반응으로 피할 수 있음을 보여주었다. 저자들은 모델 세대가 인간의 성능에 도달하면 병목화된다는 한계를 가지고 있다. 또한, 각각의 입력 프롬프트는 우리의 작업과 대조적으로 인간의 주석이 달린 응답을 갖는 것이 필요하다.\n' +
      '\n' +
      '데이터 증강(및 큐레이션)을 통해 LLM을 개선하는 여러 방법들은 미세 조정의 증가를 위해 학습 데이터를 자기 재창조함으로써 LLM을 개선했다. 자강(왕 등 알, 2022)은 베이스 LLM을 개선하는 데 사용할 수 있는 프롬프트와 응답의 자체 명령 생성 방법이다. 우리는 작업에서 유사한 기술을 사용하는 다음 자체 판독 모델을 사용하여 점수를 매긴다. 여러 접근 방식은 또한 강력한 LLM에서 증류하여 훈련 데이터를 생성했으며 더 약한 LLM이 잘 수행할 수 있음을 보여주었다. 예를 들어, 알카카(타오리 등 알, 2023)는 자기 구성 스타일로 생성된 텍스트-다빈치-003 지시를 사용하여 라마 7B 모델을 미세 조정했다. Alpagasus(Chen et al., 2023)는 강력한 LLM-as-a- 결정부ge(ChatGPT)를 사용하여 알카카 데이터셋과 필터를 더 작은 세트로 큐레이트하여 개선된 결과를 얻었다. 명령 백번역(Li et al., 2023)은 유사하게 훈련 데이터를 증가시키고 큐레이션하지만, 프롬프트를 예측하기 위해 웹 문서로부터 백번역을 통해 증가합니다. 큐레이션은 LLM(-as-a-판결) 자체에 의해 이루어지기 때문에 자기 선호 모델의 사례로 볼 수 있지만 전문화된 설정에서 볼 수 있다. 강화자력훈련(ReST)(Gulcehre et al., 2023)은 고정된 외부 보상을 사용하여 새로운 고품질 사례를 큐레이션하여 훈련 세트에 반복적으로 추가하여 성능을 향상시킵니다. 우리는 실험에서 관련 방식으로 긍정적인 예만 추가하는 것이 도움이 되지 않는 반면 선호도 쌍을 추가하는 것이 도움이 된다는 것을 발견했다.\n' +
      '\n' +
      'LLM-as-a- 판결들을 사용하여 언어 모델을 평가하기 위한 표준 접근(Dubois et al., 2023; Li et al., 2023; Fernandes et al., 2023; Bai et al., 2023; 사하구 et al., 2023)이 되었으며 전술한 바와 같이 보상 모델 또는 큐레이트 데이터 학습에도 사용되고 있다(이 등은 2023; Chen et al., 2023). 김 등(2023년)과 같은 몇몇 작품이 데이터를 통해 LLM을 학습시켜 법관과 함께 수행하도록 훈련하는 반면, 우리가 아는 한 이 교육을 우리 작품에서와 같이 기술을 따르는 일반적인 수업과 결합하는 것은 흔하지 않다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '자세대에 대한 판단 및 훈련을 통해 자체 정렬이 가능한 모델인 셀프 리딩 언어 모델을 도입했다. 이 방법은 반복 방식으로 훈련되며, 여기서 각 반복에서는 모델이 자신의 선호도 기반 명령어 학습 데이터를 생성한다. 이는 LLM-as-a- 판결 프롬프트를 통해 자체 세대에 보상을 부여하고 Iterative DPO를 사용하여 선호도에 대해 훈련함으로써 수행된다. 우리는 이 훈련이 모델의 능력에 따른 지시와 반복 전반에 걸친 보상 모델링 능력을 향상시킨다는 것을 보여주었다. 이것은 예비 연구일 뿐이지만, 이것은 모델이 일종의 선순환에 이어 수업 개선을 위한 향후 반복에서 보상을 부여할 수 있다는 것을 의미하기 때문에 이것이 흥미로운 연구 방법이라고 믿는다. 이러한 개선은 현실적인 시나리오에서 포화적일 가능성이 있지만, 오늘날 모델에 따른 보상 모델과 명령어를 구축하는 데 일반적으로 사용되는 인간의 선호를 넘어 지속적인 개선 가능성을 여전히 허용한다.\n' +
      '\n' +
      '## 6 Limitations\n' +
      '\n' +
      '유망한 실험 결과를 얻었지만, 아직 탐색하지 못한 부분이 많기 때문에 현재 예비적으로 고려하고 있으며, 그중에서도 안전 평가를 포함한 추가 평가의 주제와 반복 훈련의 한계를 이해한다.\n' +
      '\n' +
      '훈련 반복이 모델링 능력과 보상 모델링 능력을 모두 향상시키지만 단일 설정에서 세 번의 반복만 실행한다는 것을 보여주었다. 더 많은 반복을 위해 이 효과의 "스케일링 법률"을 이해하고, 다른 환경에서 능력이 더 많거나 적은 언어 모델을 갖는 다른 언어 모델을 이해하는 것이 명확한 추가 연구 라인이다.\n' +
      '\n' +
      '헤드 대 헤드 및 알파카에발 2 리더보드 스타일 평가를 사용하여 GPT-4를 사용하여 모델을 평가했지만 측정할 수 있는 다른 자동 평가 벤치마크도 많습니다. 또한, 모델 세대의 길이가 증가하는 것을 관찰했으며, 길이와 추정된 품질 사이에는 알려져 있는 상관관계가 있으며, 이는 일반적으로 더 깊이 이해해야 할 주제이며 특히 우리의 결과에서도 알려져 있다. 우리의 틀 안에서 이른바 \'복귀\'가 일어날 수 있는지, 그리고 어떤 상황에서 일어날 수 있는지 이해하는 것도 좋을 것이다. 학습 보상으로 언어 모델과 최종 평가를 위한 언어 모델을 모두 사용하고 있기 때문에 서로 다른 모델일지라도 이는 우리가 제공한 것보다 더 깊은 분석이 필요할 수 있다. 우리가 보는 자동 결과를 검증한 예비 인간(저자) 평가를 실시했지만, 보다 상세한 인간 평가는 유익할 것이다.\n' +
      '\n' +
      '또 다른 명확한 추가 연구 방법은 안전 평가를 수행하고 프레임워크 내에서 안전 교육을 탐구하는 것이다. 리워드 모델은 기존 시스템(Touvron et al., 2023)에서 안전을 위해 독점적으로 구축되었으며, 여기에서 유망한 방법은 LLM-as-a- 판단 절차를 사용하여 우리의 자폐 훈련 과정에서 안전을 구체적으로 평가하는 것이다. 훈련 반복보다 보상 모델링 능력이 향상된다는 것을 보여주었다는 점을 감안할 때, 이것은 모델의 안전도 시간이 지남에 따라 잠재적으로 개선될 수 있음을 의미할 수 있으며, 이후 반복은 더 일찍 반복할 수 없는 더 어려운 안전 상황을 포착할 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* 아치암 등 (2023) 조쉬 아치암, 스티븐 아들러, 샌드히니 아카라왈, 라마 아흐마드, 일게 아카야, 플로렌시아 레니 아맨, 디아고 알메시다, 조코 알텐슈미트, 샘 알트만, 세하말 아나드카트, GPT-4 기술 보고서. arXiv 프리프린트 arXiv:2303.08774_, 2023.\n' +
      '* 아돌프스 등은 (2023) 레너드 아돌프스, 톈유 가오, 징Xu, 커트샵, 세인바야르 숙하바토르, 제이슨 웨스트온 등이다. CRINGE 손실: 모델링하지 않는 언어를 배우는 것입니다. 안나 로저스, 요르단 보드-그레이버 및 나오아키 오카자키에서는 2023년 7월 캐나다 토론토(8854-8874페이지) 컴퓨터통계학회 제61회 연례회의 편집자 _발표자, 1: 롱 파이어스)_페이지에 대한 컴퓨팅 언어학 협회가 있다. 10.18653/v1/2023.acl-롱.493 URL[https://aclanthology.org/2023.acl-롱.493](https://aclanthology.org/2023.acl-long.493).\n' +
      '* 인류학 (2023) 인류학. Claude 2[https://www. 안트로픽.com/index/claude-2](https://www. 안트로픽.com/index/claude-2) 2023.\n' +
      '*바이 등(2022a) 유타오 바이(2022a) 유타오 바이, 앤디 존스, 카말 노두스, 아칸 아셴, 아나 첸, 노바 다스카르마, 니코 디스트레인, 스탠리슬라프 포트, 딥 강리, 톰 헨하얀 등은 인간 피드백의 강화 학습을 통해 도움이 되고 무해한 보조자를 훈련시킨다. arXiv 프리프린트 arXiv:2204.05862_, 2022a.\n' +
      '*바이 등(2022b) 유타오 바이(2022b) 유타오 바다, 사르나 카다바스, 샌디판 쿤두, 아미다 아셴, 잭슨 케니, 앤디 존스, 안나 크런, 안나 골드에, 아살리아 미로시니, 카메론 맥킨논 등은 AI 피드백의 무모함: 헌법 AI. arXiv 프리프린트 arXiv:2212.08073_, 2022b.\n' +
      '*바이 등은 (2023) 요시바이, 지아하오잉, 유신코오, 신장 Lv, 유제허, 샤오히 왕, 지판유, 카이청생, 이자청샤오, 하지아샤오, 하존샤오, 지진장, 후안지리, 레이후후이우 등이 있다. 언어 모델-모델-as-an-examiner가 있는 벤치마크 파운데이션 모델입니다. 신경 정보 처리 시스템 Datasets 및 벤치마크 트랙_, 2023년 URL [https://openreview.net/forum?＜IiRHQ7gvnq] (https://openopenreview.net/forum=IiRHQ7gvnq)에 대한 _30-seventh 콘퍼런스.\n' +
      '* 첸 등은 (2023) 창시 첸, 시양 리, 준옌, 하이 왕, 칼파 군라타나, 비카스 야다바, 정당, 비제이 시리바산, 톈이 저우, 헝황 등 데이터가 적은 더 나은 알파카를 훈련한다. arXiv 프리프린트 arXiv:2307.08701_, 2023.\n' +
      '* 첸 등은 (2024) 픽시앙 첸, 이허뜰, 후이저후위안, 가믹안지, 취안키안구 등이다. 셀프 플레이 미세 조정은 약한 언어 모델을 강한 언어 모델로 변환한다. _자기 플레이 미세 조정은 약한 언어 모델을 강한 언어 모델로 변환한다. arXiv 프리프린트 arXiv:2401.01335_ 2024.\n' +
      '*콜로베르트와 웨스트온(2008) 로난 콜로베트와 제이슨 웨스트논이 있다. 자연어 처리를 위한 통일된 아키텍처: 멀티태스킹 학습이 있는 딥 뉴럴 네트워크이다. 제25회 기계학습 국제회의 _검토에서 2008년 160-167쪽입니다.\n' +
      '* 두두아 등 (2023) 예나두이아, 주에첸 리, 로한 태리, 톈이 장, 이하안 굴라자니, 지미바, 칼로스 게스트린, 퍼시 리앙, 타쓰노리 바샤모토. Alpacafarm: 인간 피드백에서 배우는 방법에 대한 시뮬레이션 프레임워크. __Alpacafarm: A 시뮬레이션 프레임워크. arXiv 프리프린트 arXiv:2305.14387_, 2023.\n' +
      '* 페르난데스 등은 (2023) 패트릭 페르난데스, 다니엘 도이치, 마아 페르클슈타인, 파커 라일리, 안드레 FT 마르틴스, 그레이엄 네비그, 안쿠시 가리그, 조나단 H 클라크, 마르쿠스 프레타그, 오르한 피라트 등이다. 중첩은 미세 개질 기계 번역 평가를 위한 대형 언어 모델을 조작하는 것, 즉 오류이다. __ arXiv 프리프린트 arXiv:2308.07286_, 2023.\n' +
      '*굴체르 등은 (2023) 카게라 굴체르, 톰 르 페인, 시라트산 스리비바산, 케네시아 코니코스트, 롯데웨이츠, 압스헤크 샤마, 아디샤 시데만트, 알렉스 아우르, 알렉스 오르, 미오젠 왕, 첸지 구 등 언어 모델링을 위한 자체 학습(레스트)을 강화했다. arXiv 프리프린트 arXiv:2308.08998_, 2023.\n' +
      '* 호노비치 등은 (2023) 오리 호노비치, 토마스 인솜, 오머 레비, 티모 슈윅 등이 있다. 자연적이지 않은 지시: (최소한) 인간의 노동력이 없는 튀긴 언어 모델입니다. 안나 로저스, 요르단 보드-그레이버 및 나오아키 오카자키에서는 2023년 7월 캐나다 토론토에서 컴퓨터 통계 협회의 제61회 연례 회의(1: 롱 파이어스)_, 14409-14428 페이지)의 편집자, 편집자, _프로젝트가 있다. 10.18653/v1/2023.86 URL[https://aclanthology.org/2023.acl-장기806](https://aclanthology.org/2023.acl-오랜.806).\n' +
      '*하이그 등은 (2020)승원킴, 자민신, 예진조, 조엘장, 샤인롱프리, 화란이, 상도윤, 성진신, 성동김, 성동김, 제임스 토른 등 언어모델의 미세곡 평가능력 강화. arXiv 프리프린트 arXiv:2310.08491_, 2023.\n' +
      '* 코프 et al.(2023) 안드레아스 킬러, 옌스틴 킬러, 디미트리 폰 루테, 소티리스 안그나토스티디스, 지에-루이탐, 케이스 스티븐스, 압둘라 바르우움, 응우옌민 듀크, 올리버 스탠리, 리처드 나가피, 등 관련 대화-민주화 대형 언어 모델 정렬. arXiv 프리프린트 arXiv:2304.07327_, 2023.\n' +
      '* 이씨는 알(2023) 해리슨 이씨, 삼라트 포탈레, 하산 만수르, 켈리 루, 토마스 메스나드, 콜튼 비숍, 빅토르 카버네, 압히나브 라스토기 등이 있다. RLAIF: ai 피드백으로 인간 피드백에서 보강 학습을 편집하는 _RLAIF: ai 피드백으로 인간 피드백에서 강화 학습을 수행한다. arXiv 프리프린트 arXiv:2309.00267_, 2023.\n' +
      '*리 등은 알(2023a) 시안리, 핑유, 춘팅주, 티모 슈윅, 루크 제트렘거, 오머 레비, 제이슨 웨스트온, 마이크 루이스 등이 있다. 명령어 백번역과의 자기 정렬 _ _자렬은 명령어 백번역과의 자기 정렬이다. arXiv 프리프린트 arXiv:2308.06259_, 2023a.\n' +
      '*리 등은 알(2023b) 누에첸 리, 톈이 장, 야만 두비아, 로한 태리, 이하안 굴라자니, 카를로스 게스트린, 페시 리앙, 타쓰노리 바슈미모토. Alpacaeval: 지도 제거 모델의 자동 평가자[https://github.com/tatsu-lab/alpaca_eval]]. (https://github.com/tatsu-lab/alpaca_eval), 2023b.\n' +
      '*오우양(2022) 롱오우양, 제프리우, 주장, 디아고 알미다, 카롤 웨인웨이드, 파멜라 미시킨, 총 장, 샌히니 아가왈, 카타리나 슬라, 알렉스 레이 등 인간 피드백으로 지침을 따르도록 언어 모델을 훈련한다. 신경 정보 처리 시스템_, 2022년 35:27730-27744의 정보를 제공한다.\n' +
      '* 판 등은 (2023) 리앙밍 판, 마이클 사손, 윌다 주, 딥크 나타니, 시니 왕, 윌리엄 양왕 등이 있다. 대형 언어 모델을 자동 보정하는 __대형 언어 모델 조사: 다양한 자기 교정 전략의 풍경을 조사한다. arXiv 프리프린트 arXiv:2308.03188_, 2023.\n' +
      '* Radford et al. (2019) Alec Radford, 제프리 우, 레원 아동, 데이비드 루안, 다리오 암데이, 아이리카 세이츠케버, 언어 모델은 비지도 다중 과제 학습자들이다. 오픈AI 블로그_, 2019년 1(8):9.\n' +
      '* 라파일로프(2023) 라파엘 라파일로프, 아치트 샤마, 에릭 미첼, 크리스토퍼 디 마닝, 스테파노 에르몬, 첼시 핀 등이 있다. 직접 선호도 최적화: 언어 모델은 비밀리에 보상 모델입니다. Nural 정보 처리 시스템__30-seventh 콘퍼런스, 2023년 URL[https://openopenreview.net/forum?id=HPuSIXJaa9] (https://openopenreview.net/forum =HPuSIXJaa9)\n' +
      '* 사하 등은 (2023) 스와나네데프 사하, 오머 레비, Asli Celikyilmaz, 모하트 반살, 제이슨 웨스트온, Xian Li 등이 있다. 브랜치-솔베-메르지는 대형 언어 모델 평가 및 생성을 향상시킨다. __브랜드-솔베-메지가 향상시키며 큰 언어 모델 평가 및 생성을 향상시킨다. arXiv 프리프린트 arXiv:2310.15123_, 2023.\n' +
      '*삭만 등은 (2017) 존 스철만, 필리핀 웰스키, 프라풀라 다라리왈, 알레크 라드포드, 오레그 클리모프 등이다. 근위 정책 최적화 알고리즘 __근위 정책 최적화 알고리즘 _. arXiv 프리프린트 arXiv:1707.06347_ 2017.\n' +
      '* 슈테논 등은 (2020) 니산 스티니온, 롱 오우양, 제프리 우, 다니엘 지글러, 라이언 로이, 첼시 보스, 알레셀퍼드, 다리오 암데이, 폴 파키소 등이 있다. 인간의 피드백으로 요약하도록 배우는 학습 _ _  _  __는 인간 피드백으로 요약하는 학습을 한다. 신경 정보 처리 시스템_, 2020년 33:3008-3021의 발전.\n' +
      '* 타오리 등 (2023) 로한태리, 이하안 굴라자니, 톈이 장, 야나두아, 잔첸 리, 카를로스 게스트린, 페시 리앙, 타쓰노리 바슈미모토. 안시스트란포드 alpaca: 아지스트 인스티튜트.com/tatsu-lab/스탄포드_alpaca]]]는 라파카 모델. [내셔널://github.com/tatsu-lab/스탄포드_alpaca]]. (https://github.com/tatsu-lab/스탄포드_alpaca) 2023.\n' +
      '* 타우브론 등은 (2023) 허고 투브론, 루이 마틴, 케빈 스톤, 피터 알베르트, 암자드 알마헤이, 야스민 바흐에리, 니콜레이 바시코프, 소우미아 바트라, 프라자왈 바하바바, 슈투티 바하바바, 샤르티 보세일, 오픈 파운데이션 및 미세 조미 채팅 모델. arXiv 프리프린트 arXiv:2307.09288_, 2023.\n' +
      '* Van der Maaten과 Hinton(2008) Laurens Van der Maaten과 Geoffrey Hinton. t-SNE를 이용한 시각화 데이터 __ 시각화 데이터는 t-SNE를 이용하여 시각화한다. 기계 학습 연구 저널_ 2008. 9(11)\n' +
      '* 왕 등 (2022) 예간홍 왕, 예가네 코디, 세로필 미샤라, 알리사 류, 노아 아 스미스, 다니엘 카샤비, 한나네 하지시르지가 있다. 자기강사는 자구: 자기 생성 지시를 가진 수정 언어 모델. __ 자기강습: 자기 생성 지시를 가진 정렬 언어 모델. arXiv 프리프린트 arXiv:2212.10560_, 2022.\n' +
      '* 왕 et al.(2022)* Xu et al.(2023) 징Xu, 앤드루 리, 세인바야 숙하바토르, 제이슨 웨스트온 등이 있다. 어떤 것들은 다른 것들보다 더 긴장하고, __ 쌍대 범죄 손실에 대한 사전 최적화: 쌍대 범죄 손실에 대한 사전 최적화이다. arXiv 프리프린트 arXiv:2312.16682_, 2023.\n' +
      '*유안 등은 (2023) 홍이위안, 정위안, 추아니탄, 웨왕, 송강황, 필황 등이 있다. RRHF: 언어 모델과 인간의 피드백을 정렬하기 위한 순위 응답. 신경정보처리시스템_, 2023. URL[https://openreview.net/forum?id=EdIGMCHk4l] (https://openopenreview.net/forum=EdIGMCHk4l)에 대한 _30-seventh 콘퍼런스.\n' +
      '* 자오 등은 (2023) 요오 자오, 리샤바 조시, 톈키 류, 미샤 칼만, 모하마드 세일링, 피터 J 류 등이 있다. 인간 피드백을 사용한 __SLiC-HF: 정격 우도 보정. _SLiC-HF: 정격 우도 보정. arXiv 프리프린트 arXiv:2305.10425_, 2023.\n' +
      '*정 등은 (2023) 추지정, 피게, 정장, 미니황 등이 있다. 클릭: 시퀀스 가능성 대비 학습을 갖는 제어 가능한 텍스트 생성. 안나 로저스, 요르단 보드-그라버 및 나오아키 오카자키 협회의 편집자인 ACL 2023_, 페이지 1022-1040, 페이지 캐나다 토론토, 2023a 7월. 컴퓨팅 로직에 대한 연관입니다. 10.18653/v1/2023. URL[https://aclanthology.org/2023.services-acl.65](https://aclanthology.org/2023 찾기.org/2023 찾기-acl.65).\n' +
      '*정 등은 (2023) 리안민정, 위린치앙, 위린치앙, 예잉 선가, 시유안 주황, 장하오 우, 용하오 주앙, 지린, 주한 리, 다청 리, 에릭 시잉, 하오 장, 조셉 곤졸레스, 이온 스투카가 있다. MT-벤치 및 챗봇 선두로 LLM-as-a-판정을 결정한다. Nural 정보 처리 시스템 Datasets 및 벤치마크 트랙_, 2023b에 대한 _35-seventh 콘퍼런스에서. URL[https://openreview.net/forum?id=uccHPGDlao](https://openopenreview.net/forum?id=uccHPGDlao)\n' +
      '* 지글러 등은 (2019) 다니엘 미 지글러, 니산 스티논, 제프리 우, 톰 B 브라운, 알레크 라드포드, 다리오 암데이, 폴 크리스토, 거프리 어빙 등이다. 인간의 선호로 인한 __ 고정 조정 언어 모델. _ 인간 선호로 인한 고정 조정 언어 모델. arXiv 프리프린트 arXiv:1909.08593_ 2019.\n' +
      '\n' +
      'Appendix\n' +
      '\n' +
      'IFT, EFT 및 AIFT 데이터에는 IFT, EFT 및 AIFT 데이터 이용이 포함됩니다.\n' +
      '\n' +
      'IFT, EFT 및 AIFT 데이터에 대한 명령어 분포와 그림 4의 IFT, EFT 및 AIFT 데이터에 대한 응답 분포를 도표팅했으며 IFT 데이터와 EFT 데이터가 매우 다른 분포에서 나온 반면 IFT 및 AIFT 데이터는 유사한 분포에서 나온다는 것이 분명하다.\n' +
      '\n' +
      '우리가 시도한 EFT 프롬프트.\n' +
      '\n' +
      '처음에 우리는 그림 5와 같이 Li 등(2023a)에서 EFT 프롬프트를 받았으나, 이 프롬프트는 과제를 객관식 문제로 취급해야 하는 모델이 필요하기 때문에 부가적인 점수 집계 프롬프트만큼 효과적이지 않으며, 해당 반응의 다양한 측면을 평가하는 것과 관련된 하위 문제로 모델을 분해하기가 어렵다는 것을 발견했다. 3,200 IFT 데이터에서만 훈련된 모델을 사용할 때, 우리의 추가 점수 집계 프롬프트 및 Li et al.(2023a)로부터의 프롬프트를 사용하여 EFT 테스트 세트에 대한 성능이 표 3에 나와 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline EFT Prompt & Multiple Choice prompt & Ours \\\\ \\hline Pairwise accuracy (\\(\\uparrow\\)) & 26.6\\% & 65.1\\% \\\\\n' +
      '5-best \\% (\\(\\uparrow\\)) & 23.5\\% & 39.6\\% \\\\ Exact Match \\% (\\(\\uparrow\\)) & 1.1\\% & 10.1\\% \\\\ Spearman corr. (\\(\\uparrow\\)) & -0.18 & 0.25 \\\\ Kendall \\(\\tau\\) corr. (\\(\\uparrow\\)) & -0.16 & 0.23 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3:3,200 IFT 데이터로 트레이닝된 모델을 사용하여 다양한 LLM-as- 판결 프롬프트를 시도했으며 우리의 부가 점수-카운팅 프롬프트가 가장 잘 작동한다는 것을 발견했으며 이는 Li et al.(2023a)가 사용하는 프롬프트와 비교하여 EFT 성능의 상당한 개선을 보여준다.\n' +
      '\n' +
      '그림 4: IFT, EFT 및 AIFT 데이터에 대한 지침과 반응 모두의 취약점.\n' +
      '\n' +
      '아래는 사용자 및 후보 응답의 질문이다. 다음의 기준을 이용하여 5점 척도로 응답해 주시기 바랍니다.\n' +
      '\n' +
      '1: 정답은 불완전하고 모호하며, 비토성적이며, 논란의 여지가 있거나, 사용자가 요구하는 내용이 정확히 아닌 것을 의미한다. 예를 들어, 일부 콘텐츠는 누락되고 번호가 매겨진 리스트가 처음부터 시작되지 않는 것으로 보이며, 개구부 문장은 사용자의 질문을 반복한다. 또는 응답은 개인 경험(예: 블로그 게시물에서 촬영한)을 가진 타인의 관점에서 이루어지거나 포럼의 답처럼 보인다. 또는 홍보 텍스트, 내비게이션 텍스트 또는 기타 관련 없는 정보가 포함되어 있습니다.\n' +
      '\n' +
      '2: 사용자로부터의 대부분의 질문에 대한 답변 주소를 의미합니다. 사용자의 질문을 직접 다루지 않습니다. 예를 들어, 그것은 사용자의 질문에 대한 정확한 해결 대신 고차원적인 방법론에 대해서만 제공한다.\n' +
      '\n' +
      '3: 정답은 도움이 되지만 AI 어시스턴트가 작성하지 않는다는 것을 의미합니다. 그것은 사용자의 모든 기본 질문을 다룬다. AI 보조자의 관점에서 응답이 작성되지 않고 다른 사람의 관점에서 응답한다는 단점이 완비되어 있다. 콘텐츠는 블로그 포스트, 웹 페이지 또는 웹 검색 결과로부터 발췌한 것처럼 보입니다. 예를 들어, 개인 경험이나 의견, 댓글 섹션 언급 또는 소셜 미디어 공유 등이 포함되어 있습니다.\n' +
      '\n' +
      '4: 지시를 해결하는 데 명확한 초점을 두고 AI 보조자의 관점에서 답을 작성한다는 것을 의미한다. 누락되거나 무관하지 않은 정보 없이 사용자의 질문이나 지시에 대한 완전하고 명확하며 포괄적인 응답을 제공합니다. 그것은 잘 정리되어 있고, 자기 조절되고, 도움이 되는 어조로 쓰여 있다. 개선할 수 있는 작은 공간이 있으며, 예를 들어 더 간결하고 집중적입니다.\n' +
      '\n' +
      '5: AI 어시스턴트의 완벽한 답변입니다. 유익한 문장 없이 사용자의 질문이나 지시를 해결하기 위해 반응이 의도적으로 작성된 것처럼 보이는 유용한 AI 어시스턴트라는 것에 대한 분명한 초점을 가지고 있다. 답변은 이 지역에서 전문가 지식을 보여주는 고품질 콘텐츠를 제공하며, 매우 잘 쓰여지고, 논리적이고, 단단하며, 매력적이며 통찰력이 있습니다.\n' +
      '\n' +
      'User: <INSTRUCTION_HERE>\n' +
      '\n' +
      '<response><RESPONSE_HERE></response>\n' +
      '\n' +
      '먼저 추론(100단어 미만)을 간략히 서술한 후 마지막 줄에 "스코어: <레이팅>을 써주세요. AI 어시스턴트의 스타일로 필요한 경우 웹 검색의 지식을 제공합니다. 기준으로 최종 점수를 도출하기 위해 단계적으로 생각해보자.\n' +
      '\n' +
      '그림 5: LLM-as-a-판사 프롬프트는 Li et al. [2023a]에서 가져왔다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>