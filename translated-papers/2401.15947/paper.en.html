<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# MoE-LLaVA: Mixture of Experts for Large Vision-Language Models\n' +
      '\n' +
      'Bin Lin\n' +
      '\n' +
      'Zhenyu Tang\n' +
      '\n' +
      'Yang Ye\n' +
      '\n' +
      'Jiaxi Cui\n' +
      '\n' +
      'Bin Zhu\n' +
      '\n' +
      'Peng Jin\n' +
      '\n' +
      'Junwu Zhang\n' +
      '\n' +
      'Munan Ning\n' +
      '\n' +
      'Li Yuan\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'For Large Vision-Language Models (LVLMs), scaling the model can effectively improve performance. However, expanding model parameters significantly increases the training and inferring costs, as all model parameters are activated for each token in the calculation. In this work, we propose a novel training strategy **MoE-tuning** for LVLMs, which can constructing a sparse model with an outrageous number of parameter but a constant computational cost, and effectively addresses the performance degradation typically associated with multi-modal learning and model sparsity. Furthermore, we present the **MoE-LLaVA** framework, a MoE-based sparse LVLM architecture. This framework uniquely activates only the top-k experts through routers during deployment, keeping the remaining experts inactive. Our extensive experiments highlight the excellent capabilities of MoE-LLaVA in visual understanding and its potential to reduce hallucinations in model outputs. Remarkably, with just 3 billion sparsely activated parameters, MoE-LLaVA demonstrates performance comparable to the LLaVA-1.5-7B on various visual understanding datasets and even surpasses the LLaVA-1.5-13B in object hallucination benchmark. Through MoE-LLaVA, we aim to establish a baseline for sparse LVLMs and provide valuable insights for future research in developing more efficient and effective multi-modal learning systems. Code is released at [https://github.com/PKU-YuanGroup/MoE-LLaVA](https://github.com/PKU-YuanGroup/MoE-LLaVA).\n' +
      '\n' +
      'Machine Learning, ICML, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'In Large Vision-Language Models (LVLMs), LLaVA (Liu et al., 2023c) and MiniGPT-4 (Zhu et al., 2023) have shown promising results by leveraging an image encoder and several visual projection layers to enhance the visual perception capabilities of the Large Language Model (LLM). Typically, increasing the model size (Zhang et al., 2023a; Bai et al., 2023b) and dataset scale (Zhang et al., 2023c; Zhao et al., 2023a; Chen et al., 2023d) can improve model performance. Further advancements, such as InternVL (Chen et al., 2023e), have extended the image encoder to 6B parameters. A series of works (Li et al., 2022; Dai et al., 2023; Liu et al., 2023b) have expanded the backend of LVLM to 13B parameters and achieved state-of-the-art performance on downstream tasks. IDEFICS (Laurencon et al., 2023) even trained an LVLM with 80B parameters. This approach has demonstrated superior performance even in LLMs, where LLMs are typically pretrained on 34B (SUSTech-IDEA, 2023; 01-ai, 2023; FlagAI-Open, 2023) or 70B parameters (Touvron et al., 2023a;b; Bai et al., 2023a; DeepSee-AI, 2024; Zhang and Yang, 2023), and models surpassing 100B parameters (Brown et al., 2020; Zeng et al., 2022; Zhang et al., 2022; Scao et al., 2022; Li et al., 2023c; falcony, 2023) are not uncommon.\n' +
      '\n' +
      'In practical applications with high-quality training data, model scaling is crucial for improving model performance (Lepikhin et al., 2020). However, training and deploying such large models demand significant computa\n' +
      '\n' +
      'Figure 1: **Comparison between MoE-LLaVA-1.8Bx4 and open-source Models on object hallucination benchmark.** We report the average performance on three subsets of POPE (Li et al., 2023d): Adversarial, Random, and Popular. The red dashed line represents the linear fit to the data points of all models except MoE-LLaVA.\n' +
      '\n' +
      'tional costs and efficient implementation on parallel devices, which can be extremely expensive. This is because each token requires computations with all model parameters, called the dense model. In contrast, sparse Mixtures of Experts (MoE) (Jacobs et al., 1991; Eigen et al., 2013) effectively scale model capacity by using fixed activated parameters to process data, which has thrived in the field of NLP (Fedus et al., 2022; Zoph et al., 2022; Komatsuzaki et al., 2022). Recently, Mistral LLM (Jiang et al., 2023) equipped with the MoE layers has gained popularity in LLMs. Mistral-MoE-8x7B (Jiang et al., 2024) achieves performance comparable to LLaMA 2-70B with fewer computational resources.\n' +
      '\n' +
      'However, directly applying MoE to train sparse LVLMs is challenging. We observe that simultaneously converting LLM to LVLM and sparsifying the model leads to significant performance degradation. After multiple attempts, we find that proper initialization is crucial for sparsifying the LVLM, so we introduce a novel three-stage training strategy **MoE-tuning**. Specifically, as shown in Figure 2, we first train an MLP that adapts visual tokens to the LLM in stage I. Then, we pre-empower the LVLM with a general multi-modal understanding capability by training the whole LLM\'s parameters in stage II. Furthermore, in stage III we replicate the FFN as the initialization weights for the experts and only train the MoE layers. Finally, the sparse model gradually transitions from a general LVLM initialization to sparse mixture of experts.\n' +
      '\n' +
      'In this work, we explore a baseline for the LVLM with mixture of experts called **MoE-LLaVA**, which incorporates mixture of experts and learnable routers. MoE-LLaVA consists of multiple sparse paths where each token is dispatched to different experts through the router. The activated experts collectively process the tokens, while the inactive paths remain silent. By iteratively stacking MoE encoder layers, MoE-LLaVA provides a sparse path towards a larger and more powerful LVLM.\n' +
      '\n' +
      'As a result, in Figure 1, our MoE-LLaVA with only 2.2B sparse activated parameters outperforms models with similar activated parameters and LLaVA-1.5-13B, surpassing it by a large margin on the POPE object hallucination benchmark. Additionally, MoE-LLaVA achieves performance on par with InternVL-Chat-19B, which has approximately 8 times the activated parameters. We further scale MoE-LLaVA to 3.6B sparse activated parameters, which outperform LLaVA-1.5-7B by 1.9%, 1.2%, 29.8%, and 0.6% in ScienceQA, MMBench, LLaVA\\({}^{\\text{W}}\\), and MM-Vet, respectively. Extensive experiments validate the rationality of our MoE-LLaVA architecture and MoE-tuning strategy.\n' +
      '\n' +
      'We summarize our primary contributions as follows:\n' +
      '\n' +
      '* We explore the _MoE-tuning_, a novel three-stage training strategy for adapting MoE to LVLMs and prevent the model degradation caused by sparsity.\n' +
      '* We propose _MoE-LLaVA_, a MoE-based sparse LVLM framework, which significantly expands the number of parameters while maintaining computational costs.\n' +
      '* Our extensive experiments demonstrates that _MoE-LLaVA_ shows great potential for multi-modal understanding and hallucination inhibition. Our method achieves comparable performance to state-of-the-art 7B models with only 3B sparse activated parameters on multiple visual understanding datasets, and outperforms LLaVA-1.5-13B by 1.1% on the POPE hallucination benchmark with 2.2B activated parameters.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '### Large Vision-Language Models\n' +
      '\n' +
      'Powerful LLMs (OpenAI, 2023; Touvron et al., 2023; Wei et al., 2022; Touvron et al., 2023; Zheng et al., 2023; Team, 2023; Sun et al., 2023; Du et al., 2021; Bai et al., 2023; Yang et al., 2023; Penedo et al., 2023; Taori et al., 2023; Wei et al., 2023; Cui et al., 2023) with strong instruction-following and generalization capabilities have been applied to large-scale visual language models (LVLMs). Early works such as BLIP-2 (Li et al., 2023) and FROMAGe (Koh et al., 2023) encoded visual signals into a sequence of visual tokens, successfully adapting vision to LLMs through several projection layers. Subsequently, recent works have focused on improving performance through methods such as expanding the instruction-tuning dataset (Liu et al., 2023;c; Zhang et al., 2023; Zhao et al., 2023; Chen et al., 2023; Zhu et al., 2023), optimizing training strategies (Bai et al., 2023; Chen et al., 2023), increasing resolution of image (Liu et al., 2023;\n' +
      '\n' +
      'Figure 2: **Illustration of MoE-tuning.** The MoE-tuning consists of three stages. In stage I, only the MLP is trained. In stage II, all parameters are trained except for the Vision Encoder (VE). In stage III, FFNs are used to initialize the experts in MoE, and only the MoE layers are trained. For each MoE layer, only two experts are activated for each token, while the other experts remain silent.\n' +
      '\n' +
      'Bai et al., 2023; Wang et al., 2023) enhancing image encoders (Chen et al., 2023; Zhang et al., 2023; Bai et al., 2023), aligning the input (Lin et al., 2023) and projection layers (Cha et al., 2023; Alayrac et al., 2022; Bai et al., 2023; Dai et al., 2023; Ye et al., 2023; Zhao et al., 2023). These works empowered LVLMs with powerful visual understanding capabilities by expanding the visual instruction fine-tuning datasets and model scales.\n' +
      '\n' +
      'Currently, some works have endowed LVLMs with fine-grained image understanding capabilities, such as region understanding (Chen et al., 2023; Zhao et al., 2023; Liu et al., 2023), multi-region understanding (Wang et al., 2023; Pi et al., 2023; Peng et al., 2023), and pixel-wise grounding (Rasheed et al., 2023; Lai et al., 2023). However, the cost of scaling up dense visual data and models is challenging to bear (Liu et al., 2022; Dosovitskiy et al., 2020). In this work, we aim to make state-of-the-art LMM research more accessible by leveraging mixture of experts.\n' +
      '\n' +
      '### MoE in Multi-modal Learning\n' +
      '\n' +
      'Mixture of Experts (MoE) (Jacobs et al., 1991; Eigen et al., 2013) is a hybrid model consisting of multiple sub-models, known as experts, which are integrated together. The key concept of MoE is the use of a router to determine the token set that each expert handles, thereby reducing interference between different types of samples.\n' +
      '\n' +
      '**Hard routers** In the hard routers mode, each expert is typically pre-defined as a specific pattern. This is because multi-modal data naturally exhibit gaps (Liang et al., 2022), making it difficult for soft routers to learn the optimal patterns for assigning tokens to different experts. A series of works (Bao et al., 2022; Long et al., 2023; Satar et al., 2022; Wang et al., 2022; Shen et al., 2023) naturally decouple experts based on modal categories and pre-define each expert to handle a specific modality. An important feature of these hard-based routers is that they do not require learning the router. This mode is also widely applied in the task-specific MoE (Li et al., 2023; Zhu et al., 2022; Ma et al., 2023; Kudugunta et al., 2021).\n' +
      '\n' +
      '**Soft routers** In natural language processing, a series of works (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022; Zoph et al., 2022; Komatuszaki et al., 2022) have explored the MoE based on soft routers. Soft routers enable dynamic allocation of data among different experts, allowing each expert to focus on its expertise and achieve model sparsity. Therefore, our main focus is on leveraging soft routers in the MoE. Small-scale (million-level) models based on soft routers have also been explored in the context of multimodal learning, such as EVE (Chen et al., 2023) and LIMoE (Mustafa et al., 2022), which attempt fusion of data by using soft routers. The work most relevant to ours is MoCLE (Gou et al., 2023). However, MoCLE clusters different instructions sets and distributes them to different experts, which compromises the flexibility and autonomy of the experts. On the contrary, MoE-LLaVA relies on knowledge-rich routers to distribute tokens to different paths.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Overview\n' +
      '\n' +
      'As shown in Figure 3, MoE-LLaVA consists of a vision encoder, a visual projection layer (MLP), a word embedding layer, multiple stacked LLM blocks, and MoE blocks. We first introduce the model architecture of MoE-LLaVA in three stages in Section 3.2. Furthermore, in Section 3.3, we explain how to train MoE-LLaVA. Finally, in Section 3.4, we elaborate on the training objectives of MoE-LLaVA.\n' +
      '\n' +
      '### Architecture of MoE-LLaVA\n' +
      '\n' +
      'As shown in Table 1, we present the detailed configuration of MoE-LLaVA and more details can be found in Appendix A.1. Given a RGB image \\(\\mathbf{v}\\in\\mathbb{R}^{H\\times W\\times 3}\\), where \\(H\\) and \\(W\\) are the origin resolution. The vision encoder processes input images to obtain a visual token sequence \\(\\mathcal{Z}=[z_{1},z_{2},\\cdots,z_{P}]\\in\\mathbb{R}^{P\\times C}\\), where \\(P=\\frac{H\\times W}{14^{2}}\\) represents the sequence length of visual tokens. A visual projection layer \\(f\\) is used to map \\(\\mathcal{Z}\\in\\mathbb{R}^{P\\times C}\\) to \\(\\mathcal{V}\\in\\mathbb{R}^{P\\times D}\\), where \\(D\\) represents the hidden size of LLM. Similarly, the text undergoes a word embedding layer \\(g\\) and is projected to obtain the sequence tokens \\(\\mathcal{T}=[t_{1},t_{2},\\cdots,t_{N}]\\in\\mathbb{R}^{N\\times D}\\), where \\(N\\) represents the sequence length of text tokens.\n' +
      '\n' +
      'Subsequently, we concatenate the visual tokens and text tokens together and feed them into a large language model. Instead, we solely train the visual projection layer. The large language model consists of stacked multi-head self-attention (MSA) and feed-forward neural networks (FFN). Layer normalization (LN) and residual connections are applied within each block (Wang et al., 2019; Baevski and Auli, 2018).\n' +
      '\n' +
      '\\[\\mathbf{x}_{0}=[v_{1},v_{2},\\cdots,v_{P},\\cdots,t_{1},t_{2},\\cdots,t_{N}] \\tag{1}\\]\n' +
      '\n' +
      '\\[\\mathbf{x}^{\\prime}_{\\ell}=\\mathrm{MSA}(\\mathrm{LN}(\\mathbf{x}_{\\ell-1}))+ \\mathbf{x}_{\\ell-1},\\ell=1\\ldots L \\tag{2}\\]\n' +
      '\n' +
      '\\[\\mathbf{x}_{\\ell}=\\mathrm{MoE}(\\mathrm{LN}(\\mathbf{x}^{\\prime}_{\\ell}))+ \\mathbf{x}^{\\prime}{}_{\\ell},\\ell=1\\ldots L \\tag{3}\\]\n' +
      '\n' +
      '\\[\\mathcal{Y}=\\mathrm{LN}(\\mathbf{x}_{L}) \\tag{4}\\]\n' +
      '\n' +
      '**MoE Forward** Typically, a MoE layer consists of multiple FFNs. As an initialization step, we replicate the FFNs from stage 1 to form an ensemble of experts \\(\\mathcal{E}=[e_{1},e_{2},\\cdots,e_{E}]\\). The router is a linear layer that predicts the probability of each token being assigned to each expert.\n' +
      '\n' +
      '\\[\\mathrm{GATE}(\\mathbf{x})=\\mathrm{softmax}\\left(\\mathbf{W}\\cdot\\mathbf{x}\\right) \\tag{5}\\]where \\(\\mathbf{W}\\in\\mathbb{R}^{D\\times E}\\) represents the lightweight training parameters and \\(E\\) represents the number of experts. Each token is processed by the top-k experts with the highest probabilities, and the weighted sum is calculated based on the softmax results of the probabilities.\n' +
      '\n' +
      '\\[\\mathrm{MoE}(\\mathbf{x})=\\sum_{i=1}^{k}\\mathrm{GATE}(\\mathbf{x})_{i}\\cdot \\mathcal{E}_{i}(\\mathbf{x}) \\tag{6}\\]\n' +
      '\n' +
      '### MoE-tuning\n' +
      '\n' +
      '**Stage I** In the understanding training stage, our objective is to adapt the image tokens to LLM, allowing the LLM to comprehend the instances in the images. To achieve this, we employ an MLP to project the image tokens into the input domain of the LLM, treating the image patches as pseudo-text tokens. During this stage, the LLM is trained to describe the images. MoE layers are not applied to the LLM during this stage.\n' +
      '\n' +
      '**Stage II** Tuning with multi-modal instruction data is a key technique to enhance the capabilities and controllability of large models (Zhang et al., 2023b). In this stage, the LLM is adjusted to become an LVLM with multi-modal understanding. We use more complex instructions, including tasks such as image logical reasoning and text recognition, which require the model to have stronger multi-modal understanding. Typically, for dense models, the LVLM training is considered complete at this stage. However, we encounter challenges in simultaneously transforming the LLM into an LVLM and sparsifying the LVLM. Therefore, MoE-LLaVA utilizes the weights from the second stage as initialization for the third stage to alleviate the learning difficulty of the sparse model.\n' +
      '\n' +
      '**Stage III** As an initialization, we replicate the FFN multiple times to initialize the experts. When image tokens and text tokens are fed into the MoE layers, the router calculates the matching weights between each token and the experts. Each token is then processed by the top-k experts, and the outputs are aggregated by weighted summation based on the router\'s weights. When the top-k experts are activated, the remaining experts remain silent. This modeling approach forms the MoE-LLaVA with infinitely possible sparse pathways, offering a wide range of capabilities.\n' +
      '\n' +
      '### Objective\n' +
      '\n' +
      'The \\(\\mathcal{L}_{\\text{total}}\\) consists of auto-regressive loss \\(\\mathcal{L}_{\\text{regressive}}\\) and auxiliary loss \\(\\mathcal{L}_{\\text{aux}}\\), and auxiliary loss will be scaled by the balancing coefficient \\(\\alpha\\).\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{total}}=\\mathcal{L}_{\\text{regressive}}+\\alpha\\cdot \\mathcal{L}_{\\text{aux}} \\tag{7}\\]\n' +
      '\n' +
      '**Auto-regressive Loss** We optimize the output of LLM through a generative loss in an autoregressive manner. Given an image and text, MoE-LLaVA generates the output se\n' +
      '\n' +
      'Figure 3: **Training framework and strategy.** MoE-LLaVA adopts a two-stage training strategy. (a) We solely train the MLP to adapt the LLM to visual inputs. (b) Training the LLM backend empowers multi-modal understanding capability and MoE layers are not involved. (c) In this stage, we replicate the weights of the FFN to initialize each expert. We also train routers that sparsely assign tokens to different experts based on the input.\n' +
      '\n' +
      'quence \\(\\mathcal{Y}=[y_{1},y_{2},\\cdots,y_{K}]\\in\\mathbb{R}^{K\\times D}\\) by progressively generating each element, where \\(K=P+D\\) represents the length of the output sequence.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{regressive}}=-\\sum_{i=1}^{N}\\text{log }p_{\\theta}\\left(\\mathcal{Y}^{[P+i]}\\mid\\mathcal{V},\\mathcal{T}^{[i-1]}\\right) \\tag{8}\\]\n' +
      '\n' +
      'where \\(\\theta\\) is a trainable parameter and we only calculate the loss for the generated text.\n' +
      '\n' +
      '**Auxiliary Loss** Due to the presence of multiple experts, it is necessary to impose load balancing constraints on the MoE layer. We incorporate Differentiable Load Balancing Loss (Fedus et al., 2022) into each MoE layer to encourage experts to handle tokens in a balanced manner.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{aux}}=E\\cdot\\sum_{i=1}^{E}\\mathcal{F}_{i}\\cdot\\mathcal{P}_ {i} \\tag{9}\\]\n' +
      '\n' +
      'where \\(\\mathcal{F}\\) represents the fraction of tokens processed by each expert \\(\\mathcal{E}_{i}\\), and \\(\\mathcal{P}\\) represents the average routing probability of \\(\\mathcal{E}_{i}\\), which can be expressed by the following formula:\n' +
      '\n' +
      '\\[\\mathcal{F}=\\frac{1}{K}\\sum_{i=1}^{E}1\\{\\operatorname{argmax} \\operatorname{GATE}(\\mathbf{x})=i\\} \\tag{10}\\] \\[\\mathcal{P}=\\frac{1}{K}\\sum_{i=1}^{K}\\operatorname{GATE}(\\mathbf{ x_{i}}) \\tag{11}\\]\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '**Model Settings** Following LLaVA 1.5 (Liu et al., 2023), we utilize CLIP-Large (Radford et al., 2021) as the vision encoder, and the MLP consists of two linear layers with GELU activation function (Hendrycks and Gimpel, 2016) between them. Unless otherwise specified, MoE-LLaVA employs an alternating replacement of FFN with MoE layers, meaning that the number of MoE layers is half of the total number of layers. The value of balancing coefficient \\(\\alpha\\) is 0.01. We provide additional training details in Appendix A.2.\n' +
      '\n' +
      '**Data Details** As shown in Table 2, we reorganize the currently available data for the three-stage training. For the first stage of pretraining, we use the pretrained data of LLaVA 1.5-558k (Liu et al., 2023). For the second stage, we collect datasets from MIMIC-IT (Li et al., 2023), LRV (Liu et al., 2023), SViT (Zhao et al., 2023) and LVIS (Wang et al., 2023) to provide a robust initialization for MoE-LLaVA. For the third stage, we utilize the same data pipeline as LLaVA-mix-665k (Liu et al., 2023).\n' +
      '\n' +
      '### Image Understanding Evaluation\n' +
      '\n' +
      '**Zero-shot Image Question-answering** As shown in Table 3, since MoE-LLaVA is a sparse model equipped with soft router based on LVLM, we categorize the previous models as dense models. We evaluate the performance of MoE-LLaVA on five image question-answering benchmarks and report the number of activated parameters. Compared to the state-of-the-art method LLaVA 1.5, MoE-LLaVA demonstrates powerful image understanding capabilities and performs very close to LLaVA-1.5 on five benchmarks. Specifically, MoE-LLaVA-Phi-2.7Bx4 surpasses LLaVA-1.5-7B by 1.9% on SQA1 using 3.6B sparse activated parameters. Notably, MoE-LLaVA-StableLM-1.6Bx4 achieves comprehensive superiority over IDEFICS-80B with only 2.0B activated parameters. Furthermore, we observe the recent small-scale vision-language model, LLaVA-Phi. MoE-LLaVA-Phi-2.7Bx4 outperforms LLaVA-Phi by more than 5.7% on VQAv2, highlighting the strong comprehension abilities of MoE-LLaVA in natural vision.\n' +
      '\n' +
      '**Evaluation under Benchmark Toolkits** To comprehen\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c|c c c c c|c c} \\hline \\hline\n' +
      '**Name** & **Experts** & **Top-k** & **MoE** & **Embedding** & **Width** & **Layers** & **FFN** & **FFN** & **Heads** & **Activated** & **Pram** & **Total** \\\\ \\hline StableLM-1.6B (Team) & - & - & - & 100352 & 2560 & 32 & 10240 & 2 & 32 & 1.6B & 1.6B \\\\ MoE-LLaVA-1.6Bx4-Top2 & 4 & 2 & 16 & 100352 & 2560 & 32 & 10240 & 2 & 32 & 2.0B & 2.9B \\\\ \\hline Qwen-1.8B (Bai et al., 2023) & - & - & - & 151936 & 2048 & 24 & 5504 & 3 & 16 & 1.8B & 1.8B \\\\ MoE-LLaVA-1.8Bx4-Top2 & 4 & 2 & 12 & 151936 & 2048 & 24 & 5504 & 3 & 16 & 2.2B & 3.1B \\\\ \\hline Phi2-2.7B (Microsoft, 2023) & - & - & - & 51200 & 2560 & 32 & 10240 & 2 & 32 & 2.7B & 2.7B \\\\ MoE-LLaVA-2.7Bx4-Top2 & 4 & 2 & 16 & 51200 & 2560 & 32 & 10240 & 2 & 32 & 3.6B & 5.3B \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Architecture details of the MoE-LLaVA model. “FFN Factor” represents the number of linear layers in the FFN. “=” denotes the dimension of the hidden states for the keys (k) and values (v) is 1024. “1.6Bx4-Top2” represents a dense foundation model with 1.6B parameters, which will be equipped with a total of four experts, with two of them being activated.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline\n' +
      '**Data group** & **Usage** & **Source** & **\\#Sample** \\\\ \\hline LLaVA-PT & Stage I & LLaVA 1.5-558k & 558k \\\\ \\hline \\multirow{2}{*}{Hybird-FT} & Stage II & \n' +
      '\\begin{tabular}{c} SViT-157k, LVIS-220k \\\\ LRV-331k, MIMIC-IT-256k \\\\ \\end{tabular} & 964k \\\\ \\hline LLaVA-FT & Stage III & LLaVA 1.5-mix-665k & 665k \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **Composition of the data groups. For MIMIC-IT, and SViT datasets, we only use the LA split, and core split, respectively.**sively evaluate the multi-modal understanding capabilities of MoE-LLaVA, we evaluate its performance on four benchmark toolkits. These benchmark toolkits typically involve open-ended answers, serving as tools to verify a model\'s ability to engage in natural language questioning. In Table 3, MoE-LLaVA-Qwen-1.8Bx4 surpasses Qwen-VL-7B by 21.5%, on MMBench, despite the latter utilizing higher image resolutions. These results collectively demonstrate that the sparse model MoE-LLaVA achieves comparable or even superior performance to dense models with fewer activated parameters.\n' +
      '\n' +
      '### Object Hallucination Evaluation\n' +
      '\n' +
      'We adopt the evaluation pipeline of POPE (Li et al., 2023), a polling-based query method, to evaluate object hallucination in MoE-LLaVA. The results are presented in Table 4, where MoE-LLaVA exhibits the best performance, indicating that MoE-LLaVA tends to generate objects consistent with the given image. Specifically, MoE-LLaVA-1.8Bx4 surpasses LLaVA-1.5-13B by 1.0%, 1.5%, and 0.8% in adversarial sampling, popular sampling, and random sampling, respectively, with 2.2B activated parameters. Additionally, we observe that the yes ratio of MoE-LLaVA remains relatively balanced, indicating that our sparse model is capable of providing accurate feedback based on the given questions.\n' +
      '\n' +
      '### Visualization\n' +
      '\n' +
      '**Routing Distributions** In Figure 4, we present the expert loads (leftmost plot) and the modalities preferences of different experts (four subplots on the right) through MoE-LLaVA-2.7Bx4-Top2 on ScienceQA. More visualization can be found in Appendix B.2. To begin with, the expert loads in all MoE layers are totally balanced. However, as the model gradually becomes sparser, the expert 3 loads for\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c|c c c|c c c|c c c} \\hline \\hline \\multirow{2}{*}{**Methods**} & \\multirow{2}{*}{**LLM**} & \\multirow{2}{*}{**Activated**} & \\multicolumn{3}{c|}{**Adersarial**} & \\multicolumn{3}{c|}{**Popular**} & \\multicolumn{3}{c}{**Random**} \\\\  & & Acc & F1-Score & Yes & Acc & F1-Score & Yes & Acc & F1-Score & Yes \\\\ \\hline \\multicolumn{12}{l|}{_Dense Model_} \\\\ mPLUG-Owl (Ye et al., 2023) & L-7B & 6.7B & 82.4 & 81.6 & 45.2 & 85.5 & 84.3 & 42.1 & 86.3 & 85.3 & 42.3 \\\\ MM-GPT (Gong et al., 2023) & L-7B & 6.7B & 50.0 & 66.7 & 100.0 & 50.0 & 66.7 & 100.0 & 50.0 & 66.7 & 100.0 \\\\ MiniGPT-4 (Zhu et al., 2023) & V-13B & 13B & 66.6 & 71.4 & 66.7 & 68.3 & 72.2 & 64.1 & 77.8 & 78.9 & 54.8 \\\\ InstructBLIP (Dai et al., 2023) & V-13B & 13B & 74.4 & 78.5 & 69.0 & 81.4 & 83.5 & 62.6 & 88.7 & 89.3 & 55.2 \\\\ LLaVA-1.5 (Liu et al., 2023b) & V-13B & 13B & 85.5 & 84.4 & 43.3 & 87.4 & 86.2 & 41.3 & 88.0 & 87.1 & 41.7 \\\\ \\hline \\multicolumn{12}{l|}{_Sparse Model_} \\\\ MoE-LLaVA-1.6Bx4-Top2 & S-1.6B & 2.0B & 85.9 & 84.3 & 39.9 & 84.7 & 83.2 & 41.5 & 86.7 & 85.4 & 39.8 \\\\ MoE-LLaVA-1.8Bx4-Top2 & Q-1.8B & 2.2B & **86.1** & **85.4** & 44.9 & **88.6** & **87.7** & 42.3 & **88.6** & **87.9** & 42.8 \\\\ MoE-LLaVA-2.7Bx4-Top2 & P-2.7B & 3.5B & 85.2 & 83.8 & 41.5 & 86.9 & 84.5 & 40.5 & 87.6 & 86.6 & 40.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: **Zero-shot object hallucination evaluation results. “Yes” indicates the proportion of positive responses to the given question.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c|c c c c c c|c c c} \\hline \\hline \\multirow{2}{*}{**Methods**} & \\multirow{2}{*}{**LLM**} & \\multirow{2}{*}{**Act.**} & \\multirow{2}{*}{**Res.**} & \\multicolumn{4}{c|}{**Image Question Answering**} & \\multicolumn{4}{c}{**Benchmark Toolkit**} \\\\  & & & & & VQA\\({}^{\\text{\\text{\\tiny{2}}}}\\) & GQA & VisWiz & SQA\\({}^{\\text{\\tiny{1}}}\\) & VQA\\({}^{\\text{\\tiny{T}}}\\) & POPE & MMB & LLaVA\\({}^{\\text{\\tiny{W}}}\\) & MM-Vet \\\\ \\hline \\multicolumn{12}{l|}{_Dense Model_} \\\\ I-80B (Laurencon et al., 2023) & L-65B & 65B & 224 & 60.0 & 45.2 & 36.0 & - & 30.9 & - & 54.5 & - & - \\\\ BLIP-2 (Li et al., 2023) & V-13B & 13B & 224 & 41.0 & 41.0 & 19.6 & 61.0 & 42.5 & 85.3 & - & 38.1 & 22.4 \\\\ InstruckBLIP (Dai et al., 2023) & V-13B & 13B & 224 & - & 49.5 & 33.4 & 63.1 & 50.7 & 78.9 & - & 58.2 & 25.6 \\\\ Owen-VL (Bai et al., 2023) & Q-7B & 6.7B & 448 & 78.8\\({}^{\\text{\\tiny{5}}}\\) & 59.3\\({}^{\\text{\\tiny{3}}}\\) & 35.2 & 67.1 & 63.8 & - & 38.2 & - & - \\\\ LLaVA-1.5 (Liu et al., 2023b) & V-7B & 6.7B & 336 & 78.5\\({}^{\\text{\\tiny{5}}}\\) & 62.0\\({}^{\\text{\\tiny{5}}}\\) & 50.0 & 66.8 & 58.2 & 85.9 & 64.3 & 63.4 & 30.5 \\\\ TinyGPT-V (Yuan et al., 2023) & P-2.7B & 2.7B & 448 & - & 33.6\\({}^{\\text{\\tiny{5}}}\\) & 33.4 & - & - & - & - & - & - \\\\ MobileVLM (Chu et al., 2023) & M-2.7B & 2.7B & 336 & - & 59.0\\({}^{\\text{\\tiny{*}}}\\) & - & 61.0 & 47.5 & 84.9 & 59.6 & - & - \\\\ LLaVA-Phi (Zhu et al., 2024) & P-2.7B & 2.7B & 336 & 71.4\\({}^{\\text{\\tiny{*}}}\\) & - & 35.9 & 68.4 & 48.6 & 85.0 & 59.8 & - & 28.9 \\\\ \\hline \\multicolumn{12}{l|}{_Sparse Model_} \\\\ MoE-LLaVA-1.6Bx4-Top2 & S-1.6B & 2.0B & 336 & 76.0\\({}^{\\text{\\tiny{*}}}\\) & 60.4\\({}^{\\text{\\tiny{*}}}\\) & 37.2 & 62.6 & 47.8 & 84.3 & 59.4 & 85.9 & 26.1 \\\\ MoE-LLaVA-1.8Bx4-Top2 & Q-1.8B & 2.2B & 336 & 76.2\\({}^{\\text{\\tiny{*}}}\\) & **61.5\\({}^{\\text{\\tiny{*}}}\\)** & 32.3 & 63.1 & 48.0 & **87.0** & 59.7 & 88.7 & 25.3 \\\\ MoE-LLaVA-2.7Bx4-Top2 & P-2.7B & 3.6B & 336 & **77.1\\({}^{\\text{\\tiny{*}}}\\)** & 61.1\\({}^{\\text{\\tiny{*}}}\\)** & **43.4** & **68.7** & **50.2** & 85.0 & **65.5** & **93.2** & **31.1** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: **Comparison between different LVLMs on image understanding benchmarks. “Res.”**, “Act.”, “L.”, “V.”, “S.”, “Q.”, “P.”, “M.” and “I” respectively represent the input image resolution, activated parameters, LLaMA (Touvron et al., 2023), Vicuna (Chiang et al., 2023), StableLM (Team), Qwen (Bai et al., 2023), Phi-2 (Microsoft, 2023) MobileLaMA (Chu et al., 2023) and IDEFICS (Laurencon et al., 2023). Benchmark names are abbreviated due to page limitations. VQA-v2 (Goyal et al., 2017), GQA (Hudson and Manning, 2019); VisWiz (Gurari et al., 2018); SQA\\({}^{\\text{\\tiny{1}}}\\): ScienceQA-IMG (Lu et al., 2022); VQA\\({}^{\\text{\\tiny{T}}}\\): TextVQA (Singh et al., 2019); POPE (Li et al., 2023); MMBBench (Liu et al., 2023); LLaVA\\({}^{\\text{\\tiny{W}}}\\): LLaVA-Bench (In-the-layers 17 to 27 suddenly increase, and they even dominate the workload of almost all tokens. For the shallow layers (5-11), experts 2, 3, and 4 mainly collaborate. It is worth noting that expert 1 only works predominantly in the first few layers, and as the model becomes deeper, expert 1 gradually withdraws from the workload. Therefore, the experts in MoE-LLaVA have learned a certain pattern that allows them to divide their tasks in a specific manner.\n' +
      '\n' +
      'Furthermore, we show the distribution of modalities across different experts in Figure 5. Similarly, experts develop their own preferences. Additionally, we find that the routing distributions for text and image are highly similar. For example, when expert 3 is actively working in layers 17-27, the proportions of text and image that MoE-LLaVA processes are similar. Each expert in MoE-LLaVA is capable of handling both text tokens and image tokens simultaneously, which demonstrates that MoE-LLaVA does not exhibit a clear preference for any modality. This serves as evidence of its strong interaction in multimodal learning.\n' +
      '\n' +
      '**Token Pathways** Furthermore, we examine the behavior of experts at the token level. More visualization can be found in Appendix B.3 and Appendix B.4. We track the trajectories of all tokens on downstream tasks. For all activated pathways, we employ PCA (Pearson, 1901) to obtain the top-10 pathways, as shown in Figure 6. We found that for a given unseen text token or image tokens, MoE-LLaVA consistently tends to assign experts 2 and 3 to handle them in the deeper layers of the model. Regarding experts 1 and 4, they tend to handle the tokens during the initialization phase. These findings contribute to a better understanding of the behavior of sparse models in multi-modal learning.\n' +
      '\n' +
      '### Ablation Results\n' +
      '\n' +
      'In this section, we first validate the necessity of the three-stage training strategy. We then explore the impact of different base models and conduct ablation studies on the number of experts, the number of active experts, and the MoE structure.\n' +
      '\n' +
      '**Effect of Training Strategy** In Table 5, we conduct three variant experiments to demonstrate the rationale behind using the second-stage instruction tuning as the initialization for the third-stage MoE tuning. When adapting MoE to LVLMs, a straightforward approach is to replace the classic\n' +
      '\n' +
      'Figure 4: **Distribution of expert loadings.** The discontinuous lines represent a perfectly balanced distribution of tokens among different experts or modalities. The first figure on the left illustrates the workload among experts, while the remaining four figures depict the preferences of experts towards different modalities.\n' +
      '\n' +
      'Figure 5: **Distribution of modalities across different experts.** The discontinuous lines represent a perfectly balanced distribution of tokens.\n' +
      '\n' +
      'Figure 6: **Visualization of activated pathways.** We highlight top-10 activated pathway on text and image. Among them, the non-gray paths represent the top-2 paths, while the gray paths represent the remaining 8 paths.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c|c c c} \\hline \\hline  & **MoE** & **Stage II** & **Stage III** & **GQA** & **SQA\\({}^{1}\\)** & **POPE** & **LLaVA\\({}^{\\textbf{W}}\\)** \\\\ \\hline (a) & \\(\\bigvee\\) & - & LV+Hb & 58.4 & 58.1 & 81.9 & **88.0** \\\\ (b) & \\(\\bigvee\\) & Hb & LV & **61.5** & **63.1** & **87.0** & **88.7** \\\\ (c) & \\(\\bigtimes\\) & LV+Hb & - & 60.9 & 60.2 & 86.4 & 86.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Ablation study about **different training strategies.** “LA” and “Hb” respectively represent LLaVA-FT and Hybrid-FT in Table 2.\n' +
      '\n' +
      'LLaVA\'s FFN with a MoE layer and train it according to the original second-stage script, denoted as variant (a). However, variant (a) perform the worst, suggesting that the current multi-modal instruction dataset is insufficient to support both the conversion from LLM to LVLM and the conversion from LVLM to a sparse model simultaneously. Therefore, we collect more data, referred to as Hybrid-FT, and initially convert LLM to LVLM in the second stage. Subsequently, in the third stage, LVLM is sparsified through using the LLaVA-FT dataset, resulting in variant (b). Additionally, we expand the data of the original LLaVA\'s second stage for fair comparison, denoted as variant (c). The results indicate that variant (b) outperformed variants (a) and (c). These findings demonstrate that providing a reasonable LVLM initialization allows the model to transition rapidly from a dense model to a sparse model, validating the rationale behind our three-stage training strategy.\n' +
      '\n' +
      '**Effect of Tuning the Parameters of Different Subsets**\n' +
      '\n' +
      'In Table 5(a), we examine the performance of fine-tuning different parts of the parameters. "FFN" represents fine-tuning all FFN layers and MoE layers in the model. "All" indicates fine-tuning all parameters. The results indicate tuning the FFN is sufficient to achieve results comparable to full-parameter tuning, but it requires only approximately 75% of the time. Therefore, to enhance generalization and reduce training costs, we only fine-tune FFN layers.\n' +
      '\n' +
      '**Effect of the Number of Experts**\n' +
      '\n' +
      'Typically, increasing the number of experts directly leads to higher performance (Lepnikhin et al., 2020; Fedus et al., 2022). In Table 5(b), we change the number of experts while keeping the number of activated experts the same, so the number of activated parameters for both models remains the same. More sparse experts outperform the single expert dense model by 1.1% on POPE and 0.6% on SQA1, respectively. The results demonstrate that sparse experts can deliver superior performance.\n' +
      '\n' +
      '**Effect of the Value of Top-k**\n' +
      '\n' +
      'To investigate the impact of the number of activated experts, we provide ablation results on the top-k values in Table 5(c). When the number of activated experts increased from 1 to 2, the model\'s performance exhibits a significant improvement, while the training time only increased by 1h. These results indicate that improving the performance of MoE-LLaVA can be achieved by activating more experts. To leverage the advantages of the MoE architecture, we set the number of activated experts to 2.\n' +
      '\n' +
      '**Effect of the Architectures**\n' +
      '\n' +
      'In Table 5(d), we explore four variations of MoE architecture. Specifically, "First-Half" indicates that MoE layers are applied only to the first half of the model while the second half retains the original dense architecture. "Second-Half" means that MoE layers are placed in the second half of the model while the first half remains dense. "Interval" represents alternating occurrences of MoE layers and dense layers. "All" indicates that all layers are sparse MoE layers. Intuitively, it is expected that incorporating all MoE will enhance performance. However, using "All" do not yield better results and results in longer training times compared to other architectures. Therefore, MoE-LLaVA alternates the insertion of MoE layers.\n' +
      '\n' +
      '**Effect of the Model Size**\n' +
      '\n' +
      'As shown in Table 7, we compare the performance of models with different parameter sizes as the foundation models for MoE-LLaVA. For smaller\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 6: **Ablation study about training setting and architecture design decisions.** Settings for results in Table 3 and Table 4 are highlighted in \\(\\overline{\\text{blue}}\\). We report the training time on 8 V100-32G.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 7: Ablation study about **the model size of MoE-LLaVA.** “Act.” represent the activated parameters.\n' +
      '\n' +
      'models such as Phi2-MoE and Qwen-MoE, the performance with MoE surpasses that of dense models. We provide additional results and related discussions on model scaling in Appendix B.1.\n' +
      '\n' +
      '## 5 Conclusion and Future Directions\n' +
      '\n' +
      'In this work, we propose the MoE-tuning to adapting the MoE architecture to LVLMs, and construct the MoE-based spare model MoE-LLaVA, which can find a sparse pathway by simultaneously handling image and text features. Our framework demonstrates strong ability of multi-modal understanding and rich potential for hallucination inhibition, achieving comparable performance of LLaVA-1.5-7B with only 3B activated parameters.\n' +
      '\n' +
      'While MoE-LLaVA demonstrates competitive capabilities, we observe some difficulties in training stability, particularly with 16-bit float precision. Furthermore, due to the presence of multiple experts specializing in different abilities, MoE-LLaVA can easily be expanded to handle additional tasks such as detection, segmentation, generation, or handling more modalities such as video, depth, and thermal. Additionally, it is currently unknown how the MoE architecture would perform on larger LVLMs, and we propose more data is need to drive larger MoE-LVLMs.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]O. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. (2022) Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems35, pp. 23716-23736. Cited by: SS1.\n' +
      '* [2]A. Baevski and M. Auli (2018) Adaptive input representations for neural language modeling. arXiv preprint arXiv:1809.10853. Cited by: SS1.\n' +
      '* [3]J. Bai, S. Bai, Y. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou (2023) Qwen-vl: a frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966. Cited by: SS1.\n' +
      '* [4]H. Bao, W. Wang, L. Dong, Q. Liu, O. K. Mohammed, K. Aggarwal, S. Som, S. Piao, and F. Wei (2022) Vlmo: unified vision-language pre-training with mixture-of-modality-experts. Advances in Neural Information Processing Systems35, pp. 32897-32912. Cited by: SS1.\n' +
      '* [5]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.\n' +
      '* [6]J. Cha, W. Kang, J. Mun, and B. Roh (2023) Honeybee: locality-enhanced projector for multimodal llm. arXiv preprint arXiv:2312.06742. Cited by: SS1.\n' +
      '* [7]J. Chen, L. Guo, J. Sun, S. Shao, Z. Yuan, L. Lin, and D. Zhang (2023) Eve: efficient vision-language pre-training with masked prediction and modality-aware moe. arXiv preprint arXiv:2308.11971. Cited by: SS1.\n' +
      '* [8]J. Chen, D. Zhu, X. Shen, X. Li, Z. Liu, P. Zhang, R. Krishnamoorthi, V. Chandra, Y. Xiong, and M. Elhoseiny (2023) Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478. Cited by: SS1.\n' +
      '* [9]K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao (2023) WileA: unleashing multimodal llm\'s referential dialogue magic. arXiv preprint arXiv:2306.15195. Cited by: SS1.\n' +
      '* [10]L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin (2023) Sharegpt4v: improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793. Cited by: SS1.\n' +
      '* [11]Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, Z. Muyan, Q. Zhang, X. Zhu, L. Lu, et al. (2023) Interrvl: scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238. Cited by: SS1.\n' +
      '* [12]W. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zhuang, S. Zhuang, Y. Gonzalez, et al. (2023) Vicuna: an open-source chatbot impressing gpt-4 with 90%* chatgrt quality. See https://vicuna. lmsys. org (accessed 14 April 2023). Cited by: SS1.\n' +
      '* [13]X. Chu, L. Qiao, X. Lin, S. Xu, Y. Yang, Y. Hu, F. Wei, X. Zhang, B. Zhang, X. Wei, et al. (2023) Mobilevlm: a fast, reproducible and strong vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886. Cited by: SS1.\n' +
      '* [14]Y. Cui, Z. Yang, and X. Yao (2023) Efficient and effective text encoding for chinese llama and alpaca. arXiv preprint arXiv:2304.08177. Cited by: SS1.\n' +
      '* [15]W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. Cited by: SS1.\n' +
      '* [16]W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. Cited by: SS1.\n' +
      '* [17]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. (2020) An image is worth 16x16 words: transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Cited by: SS1.\n' +
      '* [18]Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang (2021) GIM: general language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360. Cited by: SS1.\n' +
      '* [19]D. Eigen, M. Ranzato, and I. Sutskever (2013) Learning factored representations in a deep mixture of experts. arXiv preprint arXiv:1312.4314. Cited by: SS1.\n' +
      '* [20]F. Falcon-180b (2023) [https://falconllm.tii.ae/](https://falconllm.tii.ae/). Cited by: SS1.\n' +
      '* [21]W. Fedus, B. Zoph, and N. Shazeer (2022) Switch transformers: scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research23 (1), pp. 5232-5270. Cited by: SS1.\n' +
      '* [22]J. Gao, X. Liu, S. Wang, and J. Wang (2020) Multi-modal deep learning for visual-language pre-training. arXiv preprint arXiv:2009.10853. Cited by: SS1.\n' +
      '* [23]W. Gong, C. Lyu, S. Zhang, Y. Wang, M. Zheng, Q. Zhao, K. Liu, W. Zhang, P. Luo, and K. Chen (2020) Multimodal-gpt:A vision and language model for dialogue with humans. _arXiv preprint arXiv:2305.04790_, 2023.\n' +
      '* Gou et al. (2023) Gou, Y., Liu, Z., Chen, K., Hong, L., Xu, H., Li, A., Yeung, D.-Y., Kwok, J. T., and Zhang, Y. Mixture of cluster-conditional lora experts for vision-language instruction tuning. _arXiv preprint arXiv:2312.12379_, 2023.\n' +
      '* Goyal et al. (2017) Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 6904-6913, 2017.\n' +
      '* Gurari et al. (2018) Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and Bigham, J. P. Vizwiz grand challenge: Answering visual questions from blind people. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 3608-3617, 2018.\n' +
      '* Hendrycks & Gimpel (2016) Hendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). _arXiv preprint arXiv:1606.08415_, 2016.\n' +
      '* Hudson & Manning (2019) Hudson, D. A. and Manning, C. D. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 6700-6709, 2019.\n' +
      '* Jacobs et al. (1991) Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. Adaptive mixtures of local experts. _Neural computation_, 3(1):79-87, 1991.\n' +
      '* Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b, 2023.\n' +
      '* Jiang et al. (2024) Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral of experts, 2024.\n' +
      '* Koh et al. (2023) Koh, J. Y., Salakhutdinov, R., and Fried, D. Grounding language models to images for multimodal generation. _arXiv preprint arXiv:2301.13823_, 2023.\n' +
      '* Komatsuzaki et al. (2022) Komatsuzaki, A., Puigcerver, J., Lee-Thorp, J., Ruiz, C. R., Mustafa, B., Ainslie, J., Tay, Y., Dehghani, M., and Houlsby, N. Sparse upcycling: Training mixture-of-experts from dense checkpoints. _arXiv preprint arXiv:2212.05055_, 2022.\n' +
      '* Kudugunta et al. (2021) Kudugunta, S., Huang, Y., Bapna, A., Krikun, M., Lepikhin, D., Luong, M.-T., and Firat, O. Beyond distillation: Task-level mixture-of-experts for efficient inference. _arXiv preprint arXiv:2110.03742_, 2021.\n' +
      '* Lai et al. (2023) Lai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., and Jia, J. Lisa: Reasoning segmentation via large language model. _arXiv preprint arXiv:2308.00692_, 2023.\n' +
      '* Laurencon et al. (2023) Laurencon, H., Saulnier, L., Tronchon, L., Bekman, S., Singh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush, A. M., Kiela, D., Cord, M., and Sanh, V. Obelics: An open web-scale filtered dataset of interleaved image-text documents, 2023.\n' +
      '* Lepikhin et al. (2020) Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling giant models with conditional computation and automatic sharding. _arXiv preprint arXiv:2006.16668_, 2020.\n' +
      '* Li et al. (2022a) Li, B., Zhang, Y., Chen, L., Wang, J., Pu, F., Yang, J., Li, C., and Liu, Z. Mimic-it: Multi-modal in-context instruction tuning. _arXiv preprint arXiv:2306.05425_, 2022a.\n' +
      '* Li et al. (2022b) Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International Conference on Machine Learning_, pp. 12888-12900. PMLR, 2022.\n' +
      '* Li et al. (2023b) Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023b.\n' +
      '* Li et al. (2023c) Li, X., Yao, Y., Jiang, X., Fang, X., Meng, X., Fan, S., Han, P., Li, J., Du, L., Qin, B., et al. Flm-101b: An open llm and how to train it with 100 k budget. _arXiv preprint arXiv:2309.03852_, 2023c.\n' +
      '* Li et al. (2023d) Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and Wen, J.-R. Evaluating object hallucination in large vision-language models. _arXiv preprint arXiv:2305.10355_, 2023d.\n' +
      '* Li et al. (2023e) Li, Y., Hui, B., Yin, Z., Yang, M., Huang, F., and Li, Y. Pace: Unified multi-modal dialogue pre-training with progressive and compositional experts. _arXiv preprint arXiv:2305.14839_, 2023e.\n' +
      '* Liang et al. (2022) Liang, V. W., Zhang, Y., Kwon, Y., Yeung, S., and Zou, J. Y. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. _Advances in Neural Information Processing Systems_, 35:17612-17625, 2022.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      'conference on computer vision and pattern recognition_, pp. 8317-8326, 2019.\n' +
      '* Sun et al. (2023) Sun, T., Zhang, X., He, Z., Li, P., Cheng, Q., Yan, H., Liu, X., Shao, Y., Tang, Q., Zhao, X., et al. Moss: Training conversational language models from synthetic data. _arXiv preprint arXiv:2307.15020_, 7, 2023.\n' +
      '* SUSTech-IDEA (2023) SUSTech-IDEA. Sus-chat: Instruction tuning done right. [https://github.com/SUSTech-IDEA/SUS-Chat](https://github.com/SUSTech-IDEA/SUS-Chat), 2023.\n' +
      '* Taori et al. (2023) Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpaca: A strong, replicable instruction-following model. _Stanford Center for Research on Foundation Models. [https://crfm.stanford](https://crfm.stanford). edu/2023/03/13/alpaca. html_, 3(6):7, 2023.\n' +
      '* Team (2023) Team, I. Interlm: A multilingual language model with progressively enhanced capabilities, 2023.\n' +
      '* Team (2023) Team, S. A. L. Stable Im 2 1.6b. URL [[https://huggingface.co/stabilityai/stablelm-2-1.6b](https://huggingface.co/stabilityai/stablelm-2-1.6b)] ([https://huggingface.co/stabilityai/stablelm-2-1.6b](https://huggingface.co/stabilityai/stablelm-2-1.6b)).\n' +
      '* Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023a.\n' +
      '* Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023b.\n' +
      '* Wang et al. (2023a) Wang, G., Cheng, S., Zhan, X., Li, X., Song, S., and Liu, Y. Openchat: Advancing open-source language models with mixed-quality data. _arXiv preprint arXiv:2309.11235_, 2023a.\n' +
      '* Wang et al. (2023b) Wang, J., Meng, L., Weng, Z., He, B., Wu, Z., and Jiang, Y.-G. To see is to believe: Prompting gpt-4v for better visual instruction tuning. _arXiv preprint arXiv:2311.07574_, 2023b.\n' +
      '* Wang et al. (2019) Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., and Chao, L. S. Learning deep transformer models for machine translation. _arXiv preprint arXiv:1906.01787_, 2019.\n' +
      '* Wang et al. (2022) Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O. K., Singhal, S., Som, S., et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. _arXiv preprint arXiv:2208.10442_, 2022.\n' +
      '* Wang et al. (2023c) Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou, J., Qiao, Y., et al. Visionlm: Large language model is also an open-ended decoder for vision-centric tasks. _arXiv preprint arXiv:2305.11175_, 2023c.\n' +
      '* Wang et al. (2023d) Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al. Cogylm: Visual expert for pretrained language models. _arXiv preprint arXiv:2311.03079_, 2023d.\n' +
      '* Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.\n' +
      '* Wei et al. (2023) Wei, T., Zhao, L., Zhang, L., Zhu, B., Wang, L., Yang, H., Li, B., Cheng, C., Lu, W., Hu, R., et al. Skywork: A more open bilingual foundation model. _arXiv preprint arXiv:2310.19341_, 2023.\n' +
      '* Yang et al. (2023) Yang, A., Xiao, B., Wang, B., Zhang, B., Bian, C., Yin, C., Lv, C., Pan, D., Wang, D., Yan, D., et al. Baichuan 2: Open large-scale language models. _arXiv preprint arXiv:2309.10305_, 2023.\n' +
      '* Ye et al. (2023) Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al. mplug-owl: Modularization empowers large language models with multimodality. _arXiv preprint arXiv:2304.14178_, 2023.\n' +
      '* Yu et al. (2023) Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. Mm-vet: Evaluating large multimodal models for integrated capabilities. _arXiv preprint arXiv:2308.02490_, 2023.\n' +
      '* Yuan et al. (2023) Yuan, Z., Li, Z., and Sun, L. Tinygpt-v: Efficient multimodal large language model via small backbones. _arXiv preprint arXiv:2312.16862_, 2023.\n' +
      '* Zeng et al. (2022) Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zheng, W., Xia, X., et al. Glm-130b: An open bilingual pre-trained model. _arXiv preprint arXiv:2210.02414_, 2022.\n' +
      '* Zhang et al. (2023a) Zhang, P., Wang, X. D. B., Cao, Y., Xu, C., Ouyang, L., Zhao, Z., Ding, S., Zhang, S., Duan, H., Yan, H., et al. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. _arXiv preprint arXiv:2309.15112_, 2023a.\n' +
      '* Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.\n' +
      '* Zhang et al. (2022) Zhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang, T., Wu, F., et al. Instruction tuning for large language models: A survey. _arXiv preprint arXiv:2308.10792_, 2023b.\n' +
      '* Zhang & Yang (2023) Zhang, X. and Yang, Q. Xuanyuan 2.0: A large chinese financial chat model with hundreds of billions parameters. In _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management_, pp. 4435-4439, 2023.\n' +
      '* Zhang et al. (2023) Zhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang, D., and Sun, T. Llavar: Enhanced visual instruction tuning for text-rich image understanding. _arXiv preprint arXiv:2306.17107_, 2023c.\n' +
      '* Zhao et al. (2023a) Zhao, B., Wu, B., and Huang, T. Svit: Scaling up visual instruction tuning. _arXiv preprint arXiv:2307.04087_, 2023a.\n' +
      '* Zhao et al. (2023b) Zhao, Y., Lin, Z., Zhou, D., Huang, Z., Feng, J., and Kang, B. Bubogpt: Enabling visual grounding in multi-modal llms. _arXiv preprint arXiv:2307.08581_, 2023b.\n' +
      '* Zheng et al. (2023) Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. _arXiv preprint arXiv:2306.05685_, 2023.\n' +
      '* Zhu et al. (2023) Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023.\n' +
      '* Zhu et al. (2022) Zhu, J., Zhu, X., Wang, W., Wang, X., Li, H., Wang, X., and Dai, J. Uni-perceiver-moe: Learning sparse generalist models with conditional moes. _Advances in Neural Information Processing Systems_, 35:2664-2678, 2022.\n' +
      '* Zhu et al. (2024) Zhu, Y., Zhu, M., Liu, N., Ou, Z., Mou, X., and Tang, J. Llava-phi: Efficient multi-modal assistant with small language model, 2024.\n' +
      '* Zoph et al. (2022) Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., and Fedus, W. St-moe: Designing stable and transferable sparse expert models. _arXiv preprint arXiv:2202.08906_, 2022.\n' +
      '\n' +
      '## Appendix A Implementation Details\n' +
      '\n' +
      '### More Model Architecture\n' +
      '\n' +
      'In Table 8, we present additional variants of the MoE-LLaVA. We introduce how the total parameters is calculated. When the number of activated experts is 2, setting \\(Experts=2\\) yields the number of activated parameters.\n' +
      '\n' +
      '\\[\\begin{split} Total\\_Parameters=& Embedding\\cdot Width\\\\ &+Layers\\cdot(4\\cdot Width\\cdot Width+Width\\cdot FFN\\cdot FFN\\_ Factor+2\\cdot Width)\\\\ &+Width+Width\\cdot Embedding\\\\ &+MoE\\_Layers\\cdot(Experts-1)\\cdot(Width\\cdot FFN\\cdot FFN\\_ Factor+2\\cdot Width)\\\\ &+MoE\\_Layers\\cdot(Width\\cdot Experts)\\\\ \\end{split} \\tag{12}\\]\n' +
      '\n' +
      '### Training Details\n' +
      '\n' +
      'As shown in Table 9, we present the training hyperparameters for all models, which are applicable to Qwen, StableLM, Phi and OpenChat. For the training process in all stages, we consistently train for 1 epoch, as we find that the models overfit when training for 2 epochs. The batch size for the first stage is 256 and 128 for the second and third stages. We use an image resolution of 336x336 for all three stages. **Additionally, for smaller models like Qwen-1.8B, it is feasible to train them on 8 V100-32G GPUs. However, during the training process, using fp16 may sometimes lead to loss becoming NaN.** Since our models are smaller than 7B, we can train them in _zero2_ mode. However, for stage 3, deepsped temporarily does not support training MoE architecture in _zero3_ mode. Therefore, we choose _zero2_offload_ to further reduce the memory requirements and enable running on 8 A100 GPUs. We enable the gradient checkpoint mode for all training stage.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c c c c|c c} \\hline \\hline \\multirow{2}{*}{**Name**} & \\multirow{2}{*}{**Experts**} & \\multirow{2}{*}{**Top-k**} & \\multicolumn{2}{c|}{**MoE**} & \\multirow{2}{*}{**Embedding**} & \\multirow{2}{*}{**Width**} & \\multirow{2}{*}{**Layers**} & \\multicolumn{2}{c|}{**FFN**} & \\multirow{2}{*}{**Heads**} & **Activated** & **Total** \\\\  & & & & & & & & & & & & & & \\\\ \\hline StableLM-1.6B (Team) & - & - & - & 100352 & 2560 & 32 & 10240 & 2 & 32 & 1.6B & 1.6B \\\\ MoE-LLaVA-1.6Bx4-Top2 & 4 & 2 & 16 & 100352 & 2560 & 32 & 10240 & 2 & 32 & 2.0B & 2.9B \\\\ MoE-LLaVA-1.6Bx4-Top2 & 4 & 2 & 32 & 100352 & 2560 & 32 & 10240 & 2 & 32 & 2.5B & 4.1B \\\\ \\hline Qwen-1.8B (Bai et al., 2023a) & - & - & - & 151936 & 2048 & 24 & 5504 & 3 & 16 & 1.8B & 1.8B \\\\ MoE-LLaVA-1.8Bx4-Top2 & 4 & 2 & 12 & 151936 & 2048 & 24 & 5504 & 3 & 16 & 2.2B & 3.1B \\\\ MoE-LLaVA-1.8Bx4-Top2 & 4 & 2 & 24 & 151936 & 2048 & 24 & 5504 & 3 & 16 & 2.6B & 4.3B \\\\ \\hline Phi2-2.7B (Microsoft, 2023) & - & - & - & 51200 & 2560 & 32 & 10240 & 2 & 32 & 2.7B & 2.7B \\\\ MoE-LLaVA-2.7Bx4-Top2 & 4 & 2 & 16 & 51200 & 2560 & 32 & 10240 & 2 & 32 & 3.6B & 5.3B \\\\ MoE-LLaVA-2.7Bx4-Top2 & 4 & 2 & 32 & 51200 & 2560 & 32 & 10240 & 2 & 32 & 4.5B & 7.8B \\\\ \\hline OpenChat-7B (Wang et al., 2023a) & - & - & - & 32000 & 4096\\({}^{*}\\) & 32 & 14336 & 3 & 32 & 6.7B & 6.7B \\\\ MoE-LLaVA-7Bx4-Top2 & 4 & 2 & 16 & 32000 & 4096\\({}^{*}\\) & 32 & 14336 & 3 & 32 & 9.6B & 15.2B \\\\ MoE-LLaVA-7Bx4-Top2 & 4 & 2 & 32 & 32000 & 4096\\({}^{*}\\) & 32 & 14336 & 3 & 32 & 12.4B & 23.7B \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: **More architecture details of the MoE-LLaVA model.** “FFN Factor” represents the number of linear layers in the FFN. “*” denotes the dimension of the hidden states for the keys (k) and values (v) is 1024. “1.6Bx4-Top2” represents a dense foundation model with 1.6B parameters, which will be equipped with a total of four experts, with two of them being activated. “\\(\\dagger\\)” donates all layers will equipped with MoE layer.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline Config & Stage I & Stage II & Stage III \\\\ \\hline Experts & - & - & 4 \\\\ Top-k & - & - & 2 \\\\ \\hline Deepspeed & Zero2 & Zero2 & Zero2\\_offload \\\\ Data & LLaVA-PT & Hybrid-PT & LLaVA-FT \\\\ Image resolution & & 336x336 \\\\ Image encoder & & CLIP-Large/336 \\\\ Feature select layer & & -2 \\\\ Image projector & 2 Linear layers with GeLU \\\\ Epoch & & 1 \\\\ Learning rate & 1e-3 & 2e-5 & 2e-5 \\\\ Learning rate schulte & & Cosine & \\\\ Weight decay & & 0.0 & \\\\ Text max length & & 2048 & \\\\ Batch size per GPU & 32 & 16 & 16 \\\\ GPU & & \\(8\\times\\) A800-80G \\\\ Precision & & Bf16 & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: **Training hyperparameters.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:16]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      '### Exhibition Board\n' +
      '\n' +
      'In Table 11, we present some classic examples using images from LLaVA (Liu et al., 2023c) and LLaVA-1.5 (Liu et al., 2023b). We observe that MoE-LLaVA performs comparably to them on these classic images, despite using fewer parameters.\n' +
      '\n' +
      'Figure 11: Distribution of modalities across different experts on **MoE-LLaVA-OpenChat-7Bx4-Top2**.\n' +
      '\n' +
      'Figure 10: Distribution of expert loadings and expert preferences on **MoE-LLaVA-StableLM-1.6Bx4-Top2**.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c} \\hline \\hline\n' +
      '**Visual input example, Tricky Question and Image:** & \\hline \\hline\n' +
      '**Visual input example, Common Question:** \\\\ \\hline User & If there are factual errors in the questions, point it out; if not, proceed answering the question. \\\\  & What’s happening in the desert? \\\\ \\hline LLaVA-1.5 & There are no deserts in the image. The image features a beach with palm trees, a city skyline, and a large body of water. \\\\ \\hline MoE-LLaVA & There are no deserts in the image. The scene features a city with a beach and a large body of water, which is more likely to be a coastal city. \\\\ \\hline User & Do you know who drew this painting? \\\\ \\hline MoE-LLaVA & The painting is a portrait of the famous artist, Leonardo da Vinci. \\\\ \\hline User & Do you know who drew this painting? \\\\ \\hline MoE-LLaVA & The painting is a drawing of a dog, and it is not possible to definitively determine the artist who created it based on the image alone. \\\\ \\hline User & What is the name of the man in the photo? \\\\ \\hline MoE-LLaVA & The man in the photo is named Elon Musk. \\\\ \\hline User & What is the name of the man in the photo? \\\\ \\hline MoE-LLaVA & The man in the photo is named Elon Musk. \\\\ \\hline \\hline \\end{tabular} \n' +
      '\\begin{tabular}{c c} \\hline \\hline\n' +
      '**Visual input example, Common Question:** \\\\ \\hline User & Can you explain this meme? \\\\ \\hline GPT-4 (OpenAI, 2023) & This meme is a joke that combines two unrelated things: pictures of the earth from space and chicken muggets. The text of the meme suggests that the image below is a beautiful picture of the earth from space. However, the image is actually of chicken muggets arranged to vaguely resemble a map of the world. The humor in this meme comes from the unexpected jurisdiction of the text and the image. The text sets up an expectation of a majestic image of the earth, but the image is actually something mundane and silly. \\\\ \\hline MoE-LLaVA & The image features a pan filled with golden-brown fried chicken pieces arranged in the shape of a world map. There are numerous chicken pieces spread across the pan, with some pieces located near the center, while others are positioned towards the edges. The chicken pieces are placed in a way that resembles the continents and countries, creating a visually appealing and creative presentation. The pan itself is placed on a table, ready to be served and enjoyed. \\\\ \\hline User & What is unusual about this image? \\\\ \\hline MoE-LLaVA & The unusual aspect of this image is that a man is standing on a clothesline that is attached to the back of a moving taxi cab. This is not a typical scene, as clotheslines are usually found in residential areas, patios, or laundry rooms, and not on the back of a vehicle. The man is holding a shirt while standing on the clothesline, which adds to the unusualness of the situation. It is likely that the man is performing some task or trying to dry the shirt in an unconventional manner. \\\\ \\hline \\hline \\end{tabular} \n' +
      '\\begin{tabular}{c c} \\hline \\hline MoE-LLaVA & The image is a graph or chart with a red arrow pointing to the bottom. The graph is color-coded, with pink and green lines, and it appears to be a line graph. The red arrow is located at the bottom of the graph, indicating a specific point or value. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 11: **Exhibition Board of MoE-LLaVA. MoE-LLaVA demonstrates the ability to detect and answer challenging questions when prompted to verify them.**Figure 14: Distribution of modalities across different experts on **MoE-LLaVA-StableLM-1.6Bx4-Top2**.\n' +
      '\n' +
      'Figure 12: Distribution of modalities across different experts on **MoE-LLaVA-Phi-2.7Bx4-Top2**.\n' +
      '\n' +
      'Figure 13: Distribution of modalities across different experts on **MoE-LLaVA-Qwen-1.8Bx4-Top2**.\n' +
      '\n' +
      'Figure 16: Visualization of activated pathways on **MoE-LLaVA-Phi-2.7B\\(\\times\\)4-Top2**.\n' +
      '\n' +
      'Figure 17: Visualization of activated pathways on **MoE-LLaVA-Qwen-1.8B\\(\\times\\)4-Top2**.\n' +
      '\n' +
      'Figure 15: Visualization of activated pathways on **MoE-LLaVA-OpenChat-7B\\(\\times\\)4-Top2**.\n' +
      '\n' +
      'Figure 18: Visualization of activated pathways on **MoE-LLaVA-StableLM-1.6B\\(\\times\\)4-Top2**.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>