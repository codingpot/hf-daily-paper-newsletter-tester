<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '가벼운 무게와 패스트Frequency-Fiffusion Vocodererrad입니다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문의 목표는 프로그라드로 명명된 가볍고 빠른 확산 기반 보컬로 현실적인 오디오를 생성하는 것이다. 우리의 프레임워크는 (1) 복잡한 파형을 하위 밴드 파형으로 분해하는 이산 웨이브렛 변환을 사용하여 프로그라드가 간단하고 간결한 특징 공간에서 작동하도록 돕고, (2) 주파수 인식을 높이는 주파수 인식 확장 컨벌루션을 설계하여 정확한 주파수 정보로 스피치를 생성하며, (3) 제안된 모델의 생성 품질을 높이는 트릭 백을 소개합니다. 우리 실험에서 FreGrad는 출력 품질을 희생시키지 않고 모델 크기를 \\(0.6\\) 시간(1.78\\) 시간만큼 감소시키면서 기준선에 비해 더 빠른 훈련 시간과 \\(2.2\\) 더 빠른 추론 속도를 달성한다. Audio 샘플은 [https://mm.kaist.ac.kr/프로젝트/FreGrad] (https://mm.kaist.kr/프로젝트/FreGrad)에서 사용할 수 있다.\n' +
      '\n' +
      '김재훈 한국과학기술원({}^{*}\\)은 영준 장 감독, 보컬, 경량 모델, 확산, 빠른 확산, 빠른 확산+우옌\\({}^{*}\\), 지훈 한국과학기술원({}^{*}})\n' +
      '발주”: 이 저자들은 이 작업에 동등하게 기여했다. 이 작품은 한국 정부(과학기술정보통신부, RS-2023-00212845)와 ITRC(정보기술연구센터) 지원 프로그램(IITP-2024-RS-2023-00259991)이 IITP(정보통신기술기획평가 기관)가 주관한 국가연구재단에서 지원받았다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '신경 보컬러는 중간 음향 특징(예: 멜-스펙트로그램)으로부터 가청 파형을 생성하는 것을 목표로 한다. 노래 음성 합성[1, 2], 음성 변환[3, 4], 텍스트 대 말하기[5, 6, 7] 등 수많은 음성 관련 과제의 필수 건축물 블록이 된다. 앞서 신경 보컬들[8, 9]은 자기회귀(AR) 아키텍처를 기반으로 하며, 이는 매우 자연스러운 발화를 생성하는 능력을 보여준다. 그러나 그들의 내재적 아키텍처는 상당한 수의 순차적 연산을 요구하여 매우 느린 추론 속도를 초래한다. 흐름[10, 11], 생성 적대 네트워크[12, 13, 14], 신호 처리[15, 16]를 기반으로 비AR 아키텍처에 대한 추론 프로세스 속도를 높이는 많은 노력이 이루어졌다. 이러한 접근법은 추론 속도를 가속화했지만 AR 방법에 비해 품질이 낮은 파형을 자주 생성한다. 비AR 어휘 중 확산 기반 어휘는 최근 유망한 생성 품질[17, 18, 19, 20, 21, 22, 23]로 인해 주목을 받고 있다. 고품질 합성 스피치에도 불구하고 확산 기반 보컬은 느린 훈련 수렴 속도, 비효율적인 추론 과정 및 높은 계산 비용을 겪고 있다. 이러한 요인은 저자원 장치에서 확산 기반 보컬의 활용과 실제 시나리오에서 응용을 방해한다. 많은 작품[19, 21, 24]이 훈련과 추론 시간을 최소화하려고 노력했지만, 여전히 계산 비용을 줄이기 위한 제한된 탐구가 남아 있다.\n' +
      '\n' +
      '앞서 언급한 문제점을 한꺼번에 해결하기 위해 본 논문에서는 합성 오디오의 품질을 유지하면서 낮은 메모리 소비와 빠른 처리 속도를 모두 달성하는 프로그라드라는 새로운 확산 기반 보코더를 제안한다. 우리의 아이디어의 핵심은 복잡한 파형을 두 개의 단순 주파수 서브 밴드 서열(즉, 웨이브렛 특징)으로 분해하여 모델이 무거운 계산을 피할 수 있도록 하는 것이다. 이를 위해, 우리는 정보 손실(25, 26]) 없이 복잡한 파형을 두 개의 주파수-파종 및 차원-감소된 파편 특징들로 변환하는 이산 웨이브렛 변환(DWT)을 사용한다. FreGrad는 모델 파라미터와 데모즈 처리 시간을 모두 상당한 마진으로 성공적으로 감소시킨다. 또한, 출력 품질을 높이는 주파수 인식 확장 컨볼루션(Freq-DConv)이라는 새로운 건물 블록을 소개합니다. 확장된 컨볼루션 레이어에 DWT를 통합함으로써, 주파수 정보의 유도 편향을 모듈에 제공하여 모델이 실제 오디오 합성의 열쇠 역할을 하는 정확한 스펙트럼 분포를 학습할 수 있다. 품질 향상을 위해 각 웨이플릿 특징에 대한 사전 분포를 설계하고, 최적이 아닌 노이즈 일정을 대체하는 노이즈 변환을 통합하고, 주파수 인식 피드백을 제공하는 다중 해상도 크기 손실 함수를 레버리지한다.\n' +
      '\n' +
      '실험 결과에서 광범위한 메트릭을 가진 FreGrad의 효과를 보여준다. FreGrad는 생성 품질을 유지하면서 모델 효율을 높이는 데 주목할만한 향상을 보여준다. 프로그라드는 <표 1>과 같이 추론 시간을 \\(2.2\\) 시간만큼 높이고 기존 작품과 비슷한 평균 의견 점수(MOS)로 모델 크기를 \\(0.6\\) 배 감소시킨다.\n' +
      '\n' +
      '## 2 Backgrounds\n' +
      '\n' +
      '디노징 확산 확률 모형은 시끄러운 신호[27]를 표시하여 데이터 분포를 학습하는 잠재 변수 모형이다. _forward_ 프로세스 \\(q(\\cdot)\\)는 마르코프 프로세스로 파라미터화된 가우시안 전환을 통해 데이터 샘플을 확산시킨다.\n' +
      '\n' +
      '(\\mathbf{x}_{t},\\mathbf{x}_{t},\\mathbf{x}_{t})\\mathbf{x}.\n' +
      '\n' +
      '그림 1: FreGrad는 합성 품질을 유지하면서 실시간 인자와 파라미터 수를 모두 성공적으로 감소시킨다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:2]\n' +
      '\n' +
      'HH(\\mathtt{cat}\\)는 연결된 동작을 나타낸다. 추출된 특징은 채널 차원을 따라 단일 숨겨진 표현으로 변환되고, 최종적으로 iDWT는 입력 피처(\\mathbf{y}_{h}\\mathbf{f}}{l})로 길이 일치하도록 변형된다.\n' +
      '\n' +
      '\\[\\mathbf{y}^{\\prime}=\\Phi^{-1}(\\mathbf{y}^{\\prime}_{l};\\mathbf{y}^{\\prime}_{h}), \\tag{7}\\]\n' +
      '\n' +
      '\\(\\mathbf{y}^{\\prime}\\in\\mathbb{R}^{\\frac{L}{k}\\times D}\\\\)는 Freq-DConv의 출력을 나타낸다. 그림에서 볼 수 있듯이. 2, 우리는 Freq-DConv를 모든 ResBlock에 장식했다.\n' +
      '\n' +
      '확장된 컨볼루션 이전에 히든 신호를 분해하는 목적은 커널 크기를 변경하지 않고 시간 축을 따라 수용 필드를 증가시키는 것이다. DWT 결과, 각 웨이플렛 특징은 모든 시간 상관 관계를 보존하면서 시간적 차원이 감소한다. 이는 각 컨볼루션 레이어가 동일한 커널 크기에도 시간 차원을 따라 더 큰 수용 필드를 보유하도록 돕는다. 나아가, 각 숨겨진 특징의 저주파 및 고주파 서브밴드를 별도로 탐색할 수 있다. 결과적으로 주파수-지속 파형의 생성을 용이하게 하는 모델에 주파수 정보의 귀납적 편향을 제공할 수 있다. 우리는 Sec 4.3에서 Freq-DConv의 효과를 확인한다.\n' +
      '\n' +
      '질질용 트릭스 바.\n' +
      '\n' +
      '** 사전 분포*** 이전 작업에서 입증된 바와 같이 [20, 22], 스펙트로그램 기반 사전 분포는 샘플링 단계가 적더라도 파형 변성 성능을 크게 향상시킬 수 있다. 이를 기반으로 멜-스펙트로그램을 기반으로 각 웨이플릿 서열에 대한 사전 분포를 설계한다. 각 서브 밴드 시퀀스는 특정 저주파 또는 고주파 정보를 포함하기 때문에 각 웨이플릿 특징에 대해 별도의 사전 분포를 사용한다. 구체적으로, 멜-스펙트로그램을 주파수 차원을 따라 두 부분으로 나누고 [20]에서 제안된 기술을 채택하여 각 세그먼트에서 별도의 사전 분포 \\(\\{\\mathbf{\\ 시그마}^{l}^{f{\\sigma}^{h}\\}\\)를 얻었다.\n' +
      '\n' +
      '** 노이즈 일정 변환**. [31, 32]에서 논의된 바와 같이 신호 대 잡음 비율(SNR)은 이상적으로 _forward_ 공정의 최종 타임스탬프 \\(T\\)에서 0이어야 한다. 그러나 이전 작품에서 채택된 소음 일정[17, 18, 20]은 그림 4와 같이 최종 단계에서 0에 가까운 SNR에 도달하지 못하며, 최종 단계에서 0 SNR에 도달하지 못하고, 다음과 같이 공식화될 수 있는 [32]에서 제안된 알고리즘을 채택한다.\n' +
      '\n' +
      '}}{0\\tau},\\q{\\tau}_{tau.\n' +
      '\n' +
      '아이티(\\tau\\)가 샘플링 과정에서 0으로 나눗셈을 피하는 데 도움이 된다.\n' +
      '\n' +
      '**Loss 기능** A 확산 보컬의 공통 훈련 목표는 주파수 측면에서 명시적인 피드백이 부족한 예측 진리 노이즈와 지상 진리 노이즈 사이의 L2 규범을 최소화하는 것이다. 모델에 주파수 인식 피드백을 제공하기 위해 다중 해상도 단시간 푸리에 변환(STFT) 크기 손실(\\(\\mathcal{L}_{mag}\\)을 추가한다. 이전 작품과 다른 것[14, 24], FreGrad는 _ 전망 융합 손실_ 하향 조정의 출력 품질을 통합하는 것을 경험적으로 찾기 때문에 크기 부분만 사용한다. \\(M\\)는 STFT 손실의 수이고, 이어서 \\(\\mathcal{L}_{mag}\\)는 그대로 나타낼 수 있다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{mag}=\\frac{1}{M}\\sum_{i=1}^{M}\\mathcal{L}_{mag}^{(i)}, \\tag{9}\\]\n' +
      '\n' +
      '\\(\\mathcal{L}_{mag}^{{(i)}\\)은\\(i^{th}\\) 분석 설정[14]에서 STFT 크기 손실이다. 우리는 확산 손실을 저주파 및 고주파 하위 밴드에 별도로 적용하고, 최종 훈련 목표를 그대로 정의한다.\n' +
      '\n' +
      '}(\\mathbf{e}^{i},\\bathcal{i})+\\mathcal{L}(\\mathbf{\\ep{{i}.^{i})\n' +
      '\n' +
      '여기서 \\(\\mathbf{\\tilde{e}}\\)는 추정된 노이즈를 의미한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Training Setup\n' +
      '\n' +
      '우리는 13,100개의 샘플을 포함하는 단일 영어 스피커 LJSpeech1에 대한 실험을 수행한다. 훈련용 무작위 샘플 13,000개와 테스트용 나머지 샘플 100개를 사용합니다. Mel-스펙트로그램은 80Hz에서 8,000Hz 범위의 연속 확산 보컬, 256개의 홉 길이를 갖는 그라운드 진리 오디오에서 계산되며, 웨이브그라드, 2 DiffWave3 및 프레그래드4의 홉 길이는 1M 단계로 훈련되며, 공정 비교는 모든 모델은 DiffWave [17] 및 프레그래드[20]에서 기본 설정인 50개의 확산 단계를 통해 생성된 50개의 확산 단계를 통해 모든 오디오가 생성된다.\n' +
      '\n' +
      '폐경 1: [전도성://keithito.com/LJ-Speech-Dataset] (https://keithito.com/LJ-Speech-Dataset)\n' +
      '\n' +
      '폐경 2: [https://github.com/lnmt-com/파그라드](https://github.com/lnmt-com/파스트)\n' +
      '\n' +
      '폐경 3: [https://github.com/lnmt-com/확산] (https://github.com/lnmt-com/diffwave)\n' +
      '\n' +
      '폐경 4: [https://github.com/마이크로소프트/신경스피치] (https://github.com/마이크로소프트/신경스피치) (https://github.com/마이크로소프트/신경스피치)\n' +
      '\n' +
      'FreGrad는 \\(7\\)의 확장 주기 길이를 갖는 \\(30\\) 주파수 인식 잔차 블록과 \\(32\\)의 숨겨진 차원으로 구성된다. 시간 임베딩 및 멜 업샘플러를 위한 디프웨이브[17]의 구현을 따르지만 DWT에 의해 시간적 길이가 반감되기 때문에 업샘플링 속도를 절반으로 줄였다. \\(\\mathcal{L}_{mag}\\)의 경우 FFT 크기 \\([512,1024,2048]\\)와 윈도우 크기 \\([240,600,1200]\\)로 \\(M=3\\)를 설정했다. 우리는 Eqn에 대해 \\(\\tau=0.0001\\) 및 \\(\\lambda=0.1\\)를 선택한다. (8)와 Eqn. 각각 (10)입니다. 우리는 \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.999\\), 고정 학습률 \\(0.0002\\) 및 배치 크기 16으로 아담 최적기를 사용한다.\n' +
      '\n' +
      '그림 4:잡음 수준과 타임스팟을 통한 로그 SNR. "기준"은 50개의 확산 단계에 대해 \\(0.0001\\)에서 \\(0.05\\) 범위의 동일한 선형 베타 일정(\\mathbf{\\beta}\\)을 사용하는 [17, 18, 20]의 작업을 의미한다.\n' +
      '\n' +
      '그림 3: 주파수 인식 확장 컨볼루션입니다.\n' +
      '\n' +
      '오시오 품질 및 샘플링 지시.\n' +
      '\n' +
      '다양한 메트릭에 대한 FreGrad의 효과를 검증한다. 오디오 품질을 평가하기 위해 우리는 25명의 피험자가 50개의 오디오 샘플의 자연성을 증가시키는 멜-세프스토랄 왜곡(\\(\\text{MCD}_{13}\\)) 및 5 규모의 MOS를 얻는다. 또한, 생성된 진리와 접지 진리 오디오 사이에 평균 절대 오차(MAE), \\(f0\\) 뿌리 평균 제곱 오차(\\(\\text{RMSE}_{f0}\\) 및 다중 해상도 STFT 오류(MR-STFT)를 계산한다. 모델 효율을 비교하기 위해 AMD EPYC 7452 CPU와 단일 GeForce RTX 3080 GPU에서 측정되는 모델 파라미터(#params) 및 실시간 인자(RTF)의 수를 계산한다. MOS를 제외하고 모든 메트릭은 100개의 오디오 샘플에서 얻는다.\n' +
      '\n' +
      '표 1에서 알 수 있듯이 프로그라드는 모델 매개변수의 수뿐만 아니라 CPU와 GPU 모두에서 추론 속도를 크게 감소시킨다. 또한 프로그라드는 MOS를 제외한 모든 품질 평가 메트릭에서 최상의 결과를 얻을 수 있다. 인간의 저주파 소리에 대한 민감도가 높아진다는 점을 감안할 때, 우리는 FreGrad의 MOS 분해가 저주파 분포에서 비롯된다고 가정한다. 그러나 전체 주파수 스펙트럼의 관점에서, FreGrad는 MAE, MR-STFT, \\(\\text{MCD}_{13}\\) 및 \\(\\text{RMSE}_{f0}\\)에 의해 확인된 바와 같이 기존 방법과 비교하여 지속적으로 우수한 성능을 보여준다. 멜-스펙트로그램 시각화 분석(그림 5) 알소는 정확한 주파수 분포를 재구성하는 데 있어 FreGrad의 효과를 보여준다. 또한 프로그라드는 빠른 훈련 시간을 크게 활용할 수 있다. 170 GPU 시간으로 프리그라드보다 3.7배 빠른 46 GPU 시간이 수렴해야 한다.\n' +
      '\n' +
      '제안하는 지침.\n' +
      '\n' +
      '각 FreGrad 성분의 효과를 검증하기 위해 비교 MOS(CMOS), \\(\\text{RMSE}_{f0}\\) 및 RTF를 사용하여 절제 연구를 수행한다. CMOS 테스트에서 쥐들은 \\(-3\\)에서 \\(+3\\)까지의 두 시스템의 오디오 샘플의 품질을 비교하도록 요청된다. 표 2와 같이 각 구성 요소는 독립적으로 프로그라드의 합성 품질을 향상시키는 데 기여한다. 특히, FreqDConv의 활용은 추론 속도의 약간의 절충으로 품질을 상당히 높이며, 이는 증가된 RTF가 여전히 기존 접근법의 품질을 능가한다. 생성 특성은 제안된 개별 이전 및 0 SNR 기술이 적용되지 않을 때 상대적으로 작지만 눈에 띄는 분해를 보여준다. \\(\\mathcal{L}_{mag}\\)의 부재는 \\(\\text{RMSE}_{f0}\\) 측면에서 최악의 성능을 초래하며, 이는 \\(\\mathcal{L}_{mag}\\)가 효과적인 주파수 인식 피드백을 제공한다는 것을 나타낸다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '확산 기반 가볍고 빠른 보컬인 프라이그라드를 제안했습니다. 프레스그라드는 무손실 분해 방법을 채택하여 간단하고 간결한 웨이브렛 특징 공간에 동작한다. 작은 계산 오버헤드에도 불구하고 FreGrad는 확산 기반 보컬을 위해 특별히 설계된 Freq-DConv 및 트릭 백의 도움으로 합성 품질을 보존할 수 있다. 광범위한 실험은 프로그라드가 출력 품질을 저하시키지 않으면서 모델 효율을 크게 향상시킨다는 것을 보여준다. 더욱이, 절제 연구에 의해 각 FreGrad 성분의 효과를 검증한다. 프로그라드의 효능은 제한된 계산 자원을 가진 에지 디바이스에서도 인간과 유사한 오디오의 생산을 가능하게 한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{CMOS \\(\\uparrow\\)} & \\(\\text{RMSE}_{f_{0}}\\downarrow\\) & \\multicolumn{2}{c}{RTF on GPU \\(\\downarrow\\)} \\\\ \\hline\n' +
      '**FreqGrad** & \\(0.00\\) & \\(38.73\\) & \\(0.29\\) \\\\ \\hline w/o Freq-DConv & \\(-1.34\\) & \\(39.05\\) & \\(0.18\\) \\\\ w/o separate prior & \\(-0.26\\) & \\(38.91\\) & \\(0.29\\) \\\\ w/o zero SNR & \\(-0.69\\) & \\(39.17\\) & \\(0.29\\) \\\\ w/o \\(\\mathcal{L}_{mag}\\) & \\(-0.68\\) & \\(39.82\\) & \\(0.29\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: FreGrad 구성요소에 대한 Ablation 연구는 표 2와 같다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline\n' +
      '**Model** & \\(\\text{MOS}\\uparrow\\) & MAE \\(\\downarrow\\) & MR-STFT \\(\\downarrow\\) & \\(\\text{MCD}_{13}\\downarrow\\) & \\(\\text{RMSE}_{f_{0}}\\downarrow\\) & \\#params \\(\\downarrow\\) & RTF on CPU \\(\\downarrow\\) & RTF on GPU \\(\\downarrow\\) \\\\ \\hline Ground truth & \\(4.74\\pm 0.06\\) & \\(-\\) & \\(-\\) & \\(-\\) & \\(-\\) & \\(-\\) & \\(-\\) & \\(-\\) \\\\ \\hline WaveGrad & \\(3.14\\pm 0.09\\) & \\(0.59\\) & \\(1.39\\) & \\(3.06\\) & \\(39.97\\) & \\(15.81\\)M & \\(\\mathbf{11.58}\\) & \\(\\mathbf{0.29}\\) \\\\ DiffWave & \\(4.00\\pm 0.10\\) & \\(0.56\\) & \\(1.18\\) & \\(3.20\\) & \\(40.10\\) & \\(2.62\\)M & \\(29.99\\) & \\(0.64\\) \\\\ PriorGrad & \\(\\mathbf{4.19\\pm 0.10}\\) & \\(0.47\\) & \\(1.14\\) & \\(2.22\\) & \\(40.42\\) & \\(2.62\\)M & \\(29.20\\) & \\(0.65\\) \\\\ \\hline\n' +
      '**FreqGrad** & \\(4.12\\pm 0.11\\) & \\(\\mathbf{0.45}\\) & \\(\\mathbf{1.12}\\) & \\(\\mathbf{2.19}\\) & \\(\\mathbf{38.73}\\) & \\(\\mathbf{1.78M}\\) & \\(11.95\\) & \\(\\mathbf{0.29}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 평가 결과는 다음과 같다. MOS 결과는 95% 신뢰 구간으로 표시된다. 다만 MOS 결과는 95% 신뢰 구간으로 표시된다. (성하형)은 더 높은 것을 의미하며, \\(\\downarrow\\)는 더 낮은 것을 나타낸다.\n' +
      '\n' +
      '그림 5: 프로그라드와 프리그라드에 대한 분광그램 분석에서는 그림 5:였다. 프리그라드는 과잉 괴사 결과를 겪지만 FreGrad는 특히 적색 상자에서 상세한 스펙트럼 상관 관계를 재현한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]J. 리, C. Li, Y. Ren, F. Chen 및 Z. 자오(2022) 디프가수: 얕은 확산 메커니즘을 통해 음성 합성을 노래한다. 프로스에서. AAA1, SS2로 받았습니다.\n' +
      '*[2]Y. Ren, X. 탄, T. Qin, J Luan, Z. 자오, T. 루(2020) 딥 가수 : 웹에서 채굴된 데이터로 음성 합성을 노래한다. 프로스에서. KDD는 SS1, SS2에 의해 계산된다.\n' +
      '*[3]K. 미안, Y. 장, S. 창, X. 양, M. Hasegawa-Johnson(2019) AutoVC: 오토인코더 손실만으로 제로샷 음성 스타일 전송입니다. 프로스에서. ICML: SS1, SS2로 계산됩니다.\n' +
      '*[4]H. 최, 제이, W. 김씨, J. 이씨, 허씨, K. 이(2021)신경학적 분석과 합성: 자기 지도적 표현에서 연설을 재구성한다. 네르IPS에서: SS1, SS2로 분류되었습니다.\n' +
      '*[5]J]. 선, R. 포, R. J 웨이스, M. 군집, N. 잠깐, Z. 양, Z. 텐, Y. 장, Y. 왕, R. 라이언, R. A. 사이로우스, Y. 아지오미리아나키스, Y. Wu(2018) 천연 TTS 합성은 멜 스펙트로그램 예측에 웨븐을 컨디셔닝하여 합성한다. 프로스에서. ICASSP: SS1, SS2입니다.\n' +
      '*[6]V. 팝코프, I. Vovk, V. 고고리안, T. 사데코커와 M. A. 쿠디노프(2021) Grad-TTS: 텍스트 대 말하기에 대한 확산 확률 모델이다. 프로스에서. ICML: SS1, SS2로 계산됩니다.\n' +
      '*[7]J. 김, S. 김, J콩, S. 윤(2020)꽃-TTS: 단조 정렬 검색을 통한 텍스트 대 말하기 생성 흐름이다. 네르IPS에서: SS1, SS2로 분류되었습니다.\n' +
      '* [8]J. 김, S. 김, J콩, S. 윤(2020)꽃-TTS: 단조 정렬 검색을 통한 텍스트 대 말하기 생성 흐름이다. 네르IPS에서: SS1, SS2로 분류되었습니다.\n' +
      '*[9]J. 김, S. 김, S. 김씨와 Y씨. 음성 신경 보컬을 노래하기 위한 미쓰푸지(2022) 계층 확산 모델. 프로스에서. ICASSP: SS1, SS2입니다.\n' +
      '*[10]J. 김, S. 김, J콩, S. 윤(2020)꽃-TTS: 단조 정렬 검색을 통한 텍스트 대 말하기 생성 흐름이다. 네르IPS에서: SS1, SS2로 분류되었습니다.\n' +
      '*[11]J. 김, S. 김, S. 김씨와 Y씨. 음성 신경 보컬을 노래하기 위한 미쓰푸지(2022) 계층 확산 모델. 프로스에서. ICASSP: SS1, SS2입니다.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>