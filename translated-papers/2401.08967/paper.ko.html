<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# ReFT.\n' +
      '\n' +
      ' 진, 항리리 루엉\\({}^{**}\\), 신보 장\\({}^^{**}\\), 한밍 재즈({}^^{{{**}\\), 장셍\\({}^^{{**}\\), 장셴\\({}^^{{**}\\), 자이\'({}^{{ty{{**}\\) 등 펭한밍 재즈({.\n' +
      '\n' +
      'ByteDance Research\n' +
      '\n' +
      '{trung.luong, zhangxinbo.freya, allan}@bytedance.com\n' +
      '\n' +
      '간진, 간항.lh}@bytedance.comwanhesong, xiaoran.\n' +
      '\n' +
      '원자격은 동일한 기여도를 나타내는 상응하는 저자이고, \\(다거\\)는 동일한 기여도를 나타낸다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델(LLM)의 추론 능력을 향상시키는 한 가지 방법은 사상(CoT) 주석을 사용하여 슈퍼엔드 파인 트닝(SFT)을 수행하는 것이다. 그러나 이 접근법은 주어진 CoT 데이터에만 의존하기 때문에 충분히 강력한 일반화 능력을 나타내지 않는다. 수학 문제 해결에서, 예를 들어, 학습 데이터에는 보통 각 질문에 대해 하나의 주석을 달았던 추론 경로가 있다. 직관적으로, 알고리즘이 질문을 받은 여러 개의 주석이 달린 추론 경로로부터 배우는 것이 더 좋을 것이다. 이 문제를 해결하기 위해 수학 문제 해결을 예로 하여 추론용 LLM의 일반화 가능성을 높이기 위해 _Rein 강화한다 Fine-Tuning_(ReFT)라는 단순하면서도 효과적인 접근법을 제안한다. ReFT는 먼저 SFT로 모델을 따뜻하게 한 다음, 본 논문의 온라인 강화 학습, 특히 PPO 알고리즘을 사용하여 질문을 통해 다양한 추론 경로가 자동으로 샘플링되고 보상은 지상 신뢰 답변에서 자연적으로 파생된 모델에 추가 미세 조정한다. GSM8K, MathQA 및 SVAMP 데이터셋에 대한 광범위한 실험은 ReFT가 SFT를 상당히 능가하고 다수의 투표와 재순위 등 추론 시간 전략을 결합하여 성능을 잠재적으로 더욱 높일 수 있음을 보여준다. rFT는 추가적인 또는 증강된 훈련 질문에 의존하지 않고 SFT와 동일한 훈련 질문에서 학습하여 개선을 획득한다는 점에 유의한다. 이것은 ReFT에 대한 우수한 일반화 능력을 나타낸다. 이 작업의 코드는 공개적으로 이용 가능한 1입니다.\n' +
      '\n' +
      '폐경 1: [https://github.com/lqtrung1998/mmp_ReFT](https://github.com/lqtrung1998/mmp_ReFT)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '수학 문제 해결을 위한 최첨단 접근 방식(Uesato et al., 2022; 루오 et al., 2023; 왕 et al., 2023)은 슈퍼브레이팅 Fine-Tuning(SFT)을 사용하여 아이디어(CoT) 주석(Wei et al., 2022)을 사용하여 모델을 훈련시킨다. 그림 1과 같이 CoT 주석에서는 수학 문제 해결을 위한 중간 추론 단계를 정리한다.\n' +
      '\n' +
      '일반적으로 훈련 데이터에 각 질문에 대한 하나의 CoT 주석, 즉 SFT에서 사용되는 하나의 올바른 추론 경로가 있다. 우리는 이것이 SFT 모델의 상대적으로 약한 일반화 능력을 초래할 수 있음을 관찰한다. 동일한 질문(Cobbe et al, 2021, Zhang et al, 2023)에 대해 다수의 유효한 CoT 주석들이 존재하는 경우가 종종 있으며, 이는 보다 강력한 미세 조정 접근의 필요성을 강조한다. 이 문제를 해결하기 위해 그림 1의 하단에 묘사된 _Rein 강화된 Fine-Tuning_(ReFT)라는 단순하면서도 효과적인 접근법을 제안한다.\n' +
      '\n' +
      'ReFT는 1~2개의 에포치(그림 1, 음영 박스)에서 슈퍼엔드 파인튜닝(SFT)과 관련된 평가 단계로 시작된다. 이 초기 단계.\n' +
      '\n' +
      '<그림 1>은 GSM8K(Cobbe et al 2021)에서 질문(x\\), CoT(e\\) 및 답변(\\(y\\))의 예시이다. SFT 프로세스는 학습 데이터에 대해 여러 에포크를 반복한다. 제안된 SFT로부터의 ReFT 평가 및 동일한 데이터에 대한 RL 학습을 수행한다.\n' +
      '\n' +
      '사전 작업 Cobbe 등(2021)에서 입증된 바와 같이 수학적 문제에 대한 올바른 반응을 어느 정도 생성할 수 있는 능력을 가진 모델을 동일하게 한다. 다음으로, ReFT는 본 논문에서 온라인 강화 학습(RL) 알고리즘 Sutton and Barto(2018), 구체적으로 동일시 정책 최적화(PPO) Schulman 등(2017)의 활용을 통해 모델을 더욱 개선한다. 이와 같이 ReFT는 다수의 올바른 추론 경로 또는 CoT 주석을 샘플링하여 그로부터 학습할 수 있다(그림 2, 우측).\n' +
      '\n' +
      '학습데이터에는 지상진실답이 포함되므로 PPO 훈련 시 자연스럽게 황금보상이 도출될 수 있다. 결과적으로 별도로 훈련된 보상 모델에 대한 요구 사항은 없다. 대조적으로, RLHF Ouyang et al.(2022)는 인간 표지된 데이터로부터 학습된 보상 모델을 활용해야 한다.\n' +
      '\n' +
      '평가 단계 동안 ReFT는 지도 학습으로 일정 수준의 정확도를 획득한다. RL 단계에서 ReFT는 다양한 CoT 추론 경로 샘플링을 통한 강화 학습으로 능력을 더욱 향상시킨다. 이러한 방식으로 ReFT는 SFT보다 훨씬 더 풍부한 감독 신호를 얻는다. 이 접근법은 ReFT가 수학 문제 해결 가오 등의 일반화를 크게 향상시킬 수 있게 한다(2018년), 브라운 등(2020년). WFT는 추가 또는 증강 훈련 질문에 의존하지 않고 SFT와 동일한 훈련 질문을 사용하여 SFT를 능가한다. 실제로 ReFT는 이러한 데이터 엔지니어링과 충돌하지 않으며, 이를 원활하게 결합할 수 있다.\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약될 수 있다.\n' +
      '\n' +
      '* We introduce a novel fine-tuning approach, reinforced fine-tuning (ReFT), which utilizes reinforcement learning to solve math problems. ReFT exhibits enhanced generalization capabilities compared to conventional supervised fine-tuning (SFT) when trained on the same dataset.\n' +
      '* 우리는 GSM8K Cobbe et al.(2021), MathQA Amini et al.(2019) 및 SVAMP Patel et al.(2021)의 세 가지 표준 수학적 데이터 세트에 대해 두 가지 발견된 모델인 CodeLLAMA Touvron et al.(2023); 로지레 et al.(2023) 및 갈락시스 테일러 et al.(2022)를 사용하여 광범위한 실험을 수행한다. 우리의 실험은 자연 언어와 프로그램 기반 CoT를 모두 포괄하여 ReFT의 상당히 향상된 성능과 일반화 능력을 보여준다.\n' +
      '* Additionally, we demonstrate that ReFT benefits from both majority voting Wang et al. (2023) and reward model reranking Uesato et al. (2022) at inference-time, further improving its performance.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '최근 연구 노력을 해결하는 수학 문제 해결은 CoT 프롬프트 설계 및 데이터 엔지니어링에 중점을 둔다. 대부분 CoT를 종합적이고 미세하게 만들어 단계적 추론 솔루션 노예(2021), 푸 등(2023), 저우 등(2023), 카이트 등(2023), 이마니 등(2023), 미오 등(2023)을 제시하려고 시도했다. Gao et al.(2023)는 파이썬 프로그램을 CoT 프롬프트로 사용하도록 추가로 제안하여 자연 언어 CoT Wei et al.(2022)보다 더 정확한 추론 단계와 상당한 개선을 보여준다. 주 등은 GPT-4 오픈AI(2023)로 중간 추론 단계를 확인하기 위해 코드를 생성하는 프롬프트 방식을 도입하여 GSM8K 코브 등(2021)과 MATH 헨드롭 등(2021)에서 최첨단 성능을 달성했다. 또 다른 작업 라인은 CoT 왕 등의 품질 향상(2023), 류 등은 알(2023), 유 등은 CoT 데이터 루노(2023)의 양을 늘리고 OpenAI의 ChatGPT(gpt-3.5-turbo) 또는 GPT-42로부터 CoT 데이터 루노(2023)의 양을 늘리는 데 중점을 둔다.\n' +
      '\n' +
      '부타주 2: [부신차트[부신차트 오픈카이.com/] (부시아나이. 오프바이.com/)].\n' +
      '\n' +
      '우리의 연구는 인간 선호 오양 등의 정렬을 위한 자연어 과정에 PPO Schulman et al.(2017)을 적용하는 최근 작업과 대체로 관련이 있다. 이후 직접 선호도 최적화(DPO) 라파일로프(2023), 동일성 선호 최적화(IPO) 아자일로프(2023) 등 정렬을 효율적으로 개선하기 위해 여러 훈련 알고리즘이 제시되었다.\n' +
      '\n' +
      '그림 2: CoT 대안이 있는 경우 SFT와 ReFT 사이의 비교이다.\n' +
      '\n' +
      '야라자 등 2023년. 정렬의 목적 이외의 기존 감독 미세 조정보다 성능을 향상시키기 위한 미세 조정 패러다임으로서 강화 학습을 채택하는 것을 목표로 한다.\n' +
      '\n' +
      '구체적으로 수학 문제 해결을 위해 우사토 등(2022년)과 라이트맨(2023년)은 SFT와 다수결 왕(2023년)에 비해 훨씬 더 나은 성과를 내기 위해 코베 등(2021년)을 재위화하는 결과 기반 또는 프로세스 기반 보상 모델을 훈련했다. 우리의 접근법은 정책 자체의 성과를 향상시키는 것을 목표로 하지만 이러한 보상 모델 재위 접근 방식은 결과적인 정책 모델에 쉽게 통합될 수 있다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '이 연구에서 우리는 파이썬을 사용하여 _자연 언어 CoT_(**N-CoT**) Wei et al.(2022)(그림 1) 및 _프로그램 기반 CoT_Gao et al.(2023)(**P-CoT**)에 중점을 둔다. 가오 등은 수학 문제 해결을 위한 프로그램 기반 CoT를 제안하였다(2023). 우리는 간단히 프로그램을 실행하여 답을 얻을 수 있습니다. 명확성을 보장하고 모호성을 피하기 위해 N-CoT 및 P-CoT를 사용하여 본 논문의 나머지 부분에서 각각 자연어 및 프로그램 기반 CoT를 나타낸다.\n' +
      '\n' +
      '### Reinforced Fine-Tuning\n' +
      '\n' +
      '제안된 재강화 파인튜닝(ReFT) 과정은 평가단계와 강화학습단계의 두 단계로 구성된다. 전체 알고리즘은 알고리즘 1에 나와 있다.\n' +
      '\n' +
      '이 단계에서 정책은 "(_question_, _CoT_)" tuples: \\(\\mathbf{x},\\mathbf{e})\\로 구성된 데이터세트 상에서 몇 개의 epoch에 대해 미세 조정된다. 모델이 질문3에 대한 적절한 응답을 생성하기 위해 기본적인 문제 해결 능력을 가질 수 있도록 하고, 형식적으로는 CoT 생성 과정을 다음 토큰 예측 행동의 시퀀스로 분해할 수 있다. 마지막 액션 토큰 <eos>는 생성 과정을 신호를 하여 종료한다. CoT \\(\\mathbf{e}\\)는 그대로 작성된다.\n' +
      '\n' +
      '발주 3: 기본 개념은 검증자 훈련 코베 등(2021)과 유사하여 다중 솔루션을 생성한다.\n' +
      '\n' +
      '\\[\\mathbf{e}=[a_{1},a_{2},...,a_{L-1},a_{L}\\textlessless]\\]\n' +
      '\n' +
      '\\[s_{ t+1}= \\begin{cas es} \\mathbf {x},&t= 0\\,&1\\leq t\\leqL \\leqL \\en d{case s}.\n' +
      '\n' +
      '\\[s_{t+1}=\\begin{cases}\\mathbf{x},&t=0\\\\,&1\\leq t\\leq L\\end{cases}.\\]\n' +
      '\n' +
      'As the produced action corresponds to the <eos> token, the resulting state \\(s_{L+1}\\) is the terminal state and the generation process is finished. With this notation, the loss function for a sample can be written as in Equation 1:\n' +
      '\n' +
      '\\[\\mathcal{L}_{SFT}(\\mathbf{\\theta})=-\\mathbb{E}_{\\mathbf{e}\\sim\\mathcal{D}}\\left[\\sum_{ i=1}^{L}\\log\\left(\\mathbf{\\pi_{\\theta}}(a_{t}|s_{t})\\right)\\right] \\tag{1}\\]\n' +
      '\n' +
      '이 단계에서 정책은 (_question_, _ansuro_) tuples: \\(\\mathbf{x},\\mathbf{y})\\로 구성된 데이터 세트를 사용하여 온라인 자기 학습의 형태를 통해 성능을 향상시킨다. 구체적으로 정책모형은 반복적으로 샘플링 응답(그림 2)을 통해 학습하며, 응답의 정답을 평가하고, 온라인 패션(알고리즘 1의 라인 7-14)에서 그 파라미터를 업데이트한다. 우리는 훈련을 위한 클램핑된 객관적인 알고리즘을 사용하여 PPO Schulman et al.(2017)를 사용한다. 지글러(2019)에 이어 평가 단계 이후 모델인 정책 모델 \\(\\pi_{\\theta}\\)의 마지막 숨겨진 상태 위에 선형값 헤드를 적용함으로써 가치 모델 \\(V_{\\파이}\\)을 구축한다. 0의 보상은 비말단 상태를 초래하는 모든 작용에 대해 주어진다. 단말 상태에서 우리는 국가의 CoT에서 추출한 답변과 지상 진실한 답변 \\(\\mathbf{y}\\)를 직접 비교하는 보상 함수를 사용한다. 여기서, 보상함수는 정답이 옳다고 판단되면 1을 반환하고, 그렇지 않으면 0을 반환한다. 답변이 모두 숫자인 데이터세트에서 _부분 보상_Z홍 등(2017); 0.1의 Le et al.(2022)은 답을 추출할 수 있는 경우와 숫자 유형일 때 적용될 수 있다. \\(1\\leq t\\leq L\\)에 대해 기록합니다.\n' +
      '\n' +
      '(s_{t+1})\\neq\\texttt{EXTRACT}=\\neq\\mathbf{y},\\neq\\mathbf{y}}\\.\n' +
      '\n' +
      '이러한 부분 보상은 희박한 보상 라이더러 등(2018년)에서 학습의 효과를 줄이는 데 도움이 될 수 있다(2019년). 또한 정(2023)에 이어 우리의 총보상은 보상 기능 점수와 풀백-리블러(KL) 발산 쿨백과 라이프블러(1951)의 합으로 학습된 RL 정책과 계수 계수 \\(\\beta\\)로 스케일링된 초기 정책이다.\n' +
      '\n' +
      '\\[\\mathbf{pi},\\_{t}}(\\cathbf{t}),\\mathbf{pi}(\\cath_{t}),\\mathbf{{{(\\cot|_{t})\\(\\cot|_{t})\n' +
      '\n' +
      '우위 계산을 위해 Schulman et al.(2018)의 일반화된 장점 추정치를 사용한다.\n' +
      '\n' +
      '\\[\\hat{A}_{t}=\\sum_{l=0}^{L-t}(\\gamma\\lambda)^{l}\\delta_{t+l},\\]\n' +
      '\n' +
      'TD(multporal Difference, TD)가 프로파일링 Difference(TD)인 경우, 암호화 Difference(TD)로 정의된다.\n' +
      '\n' +
      '}=V_{t\\prime}(_{t^{\\prime}})+{t^{\\prime}}+{t^{{\\prime}(_{t^{\\prime}+1})\\.\n' +
      '\n' +
      '말단 상태 값 \\(V_{\\파이}(s_{L+1}):=0\\), \\(\\lambda\\in(0,1]\\)는 보상을 위한 할인 요소이며, \\(\\gamma\\in[0,1]\\)는 TD의 할인 요소이다. 수익률 추정을 위해 일반화된 장점 추정치와 가치 추정치의 합으로 표기할 수 있는 \\(\\lambda\\)-레턴 \\(\\hat{R}_{t}\\)를 인용한다.\n' +
      '\n' +
      '\\[\\hat{R}_{t}=\\hat{A}_{t}+V_{\\mathbf{\\phi}}(s_{t})\\]\n' +
      '\n' +
      '마지막으로 정책가치목표는 아래 두 식과 같이 표기할 수 있다.\n' +
      '\n' +
      '}}\\frac{\\mathbf{\\pi}}}\\Biggf{\\mathbf{\\pi}} (a_{\\mathbf{\\f{\\ta}})=_{\\mathbf{\\o}}(a_{t}{\\math{f}}:{\\math{f{\\d{\\math{f{\\math{f{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\math{\\f{\\mathbf{\\math{\\math{\\math{\\bf{\\math{\\f{\\math{\\f{\\math{\\f{\\f{\\f}}}}}}}_{\\math{\\bf{\\f}}}}}}}_{\\math{\\f{\\f}}}}}}} <{\\math{f{\\f}}}}} [좌표].\\left.\\[\\left. <\\mathbf{e}} <\\math{f}}} <\\math{f}} <\\math{f}}}> <\\math{{t}}> <\\math{{t}} <\\math{{t}> <\\math{{t}} <\\math{{t}>>}. 좌측\\|\\text{clip}\\lele\\\\text{clip}\\left(V_{\\파이}(s_{t})-\\hat{R}_{t},\\hat{A} _{t}-\\epsilon,\\hat{A}_{t}_{t}_{t}_{t}+\\epsilon\\ar)\\ 오른쪽 <\\ar]\\ar.\\ar.\\ar.\\ar:\\ar)\\\\|^{clip}(s_{t},\\hat{A}_{t},\\hat{A}_{t}_{t}_{t}_{t}_{t}_{t}_{t}_{t}_{t}_{t}_{t}+\\epsilon 및+\\epsilon\\ar:+\\epsilon\\ar)\\ar.\\ar.\\ar.\\ar)\\ar.\\ar.\\ar.\\ar.\\ar)\\ar.\\ar.\\ar.\\ar.\\ar.\n' +
      '\n' +
      '\\(\\mathbf{\\pi}_{\\theta_{\\text{old}}}}), \\(V_{\\phi_{\\text{old}}}}})는 CoT 및 컴퓨팅(\\hat{A}_{t}_{t}\\), \\(\\hat{R}_{t}_{t}\\)에 사용된다. 통일 손실 함수는 위의 목적들의 가중 합이다.\n' +
      '\n' +
      'cal{L}(\\mathbf{\\mathcal{})\n' +
      '\n' +
      '여기서 \\(\\alpha\\)는 가치함수 손실에 대한 계수이다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      'GSM8K 코베 등 3개 수학 문제 데이터셋(2021), SVAMP 파텔 등(2021), MathQA 아미니 등(2019)에 대한 실험을 진행한다. GSM8K와 SVAMP 모두 답변 형식은 숫자 값이다. MathQA에서 형식은 대신 다중 선택 목록(즉, ABCD)이다. 표 1은 모든 데이터 세트의 통계를 제시한다. 우리는 몇 번의 샷 프롬프트 웨이(2022)를 수행하며, GPT-3.5-트루보(2023)를 사용하여 N-CoT 및 P-CoT 주석4를 모두 얻기 위해 Gao et al. Jie et al.(2023)에 따라 N-CoT 및 P-CoT 주석을 얻는다. 우리는 또한 형식이 숫자 값인 MathQA(Jie and Lu, 2023) 숫자 버전에 대한 추가 실험을 수행했다. 이러한 실험은 MathQA(SS4.4)에 대한 잠재적인 보상 해킹 현상(Skalse et al., 2022)의 가정을 입증하는 데 사용된다.\n' +
      '\n' +
      '### Baseline\n' +
      '\n' +
      '우리는 ReFT와 SFT 및 자가 훈련(Xie et al., 2020; Amini et al., 2022) 기저부를 비교한다. SFT는 단순히 학습 데이터에 대한 언어 모델을 미세 조정한다. 자가 훈련 방법을 사용한 실험은 이러한 모든 방법이 모델에서 생성된 샘플을 사용하는 메커니즘을 공유하기 때문에 비교적 공정한 비교를 보장한다.\n' +
      '\n' +
      'We implemented Offline Self-Training (**Offline-ST**) (He et al., 2020), and Online (Hoi et al., 2021) Self-Training (**Online-ST**). The Offline-ST method is similar to expert iteration (Anthony et al., 2017; Uesato et al., 2022). We first use the SFT checkpoint from the early checkpoint to sample the CoTs and verify them against the ground truth. We only retain those expert samples that have a correct answer. We perform supervised fine-tuning on the combination of original training data and the expert samples.\n' +
      '\n' +
      'The Online-ST method is made to be closely comparable to ReFT. Following ReFT, Online-ST has the same warm-up process. After that, we perform continual training with the samples generated on the fly. At each training step, the model first samples CoTs for a batch and only retains those with correct answers. The resulting batch consists of both sampled and ground-truth CoTs. We then update the model parameters on this batch with the supervised fine-tuning objective \\(\\mathcal{L}_{SFT}\\). Compared with ReFT, Online-ST neither makes use of negative responses (with an incorrect answer) nor has a dedicated mechanism to prevent the model from significantly diverging from the initial model, which can manifest as task-specific overfitting and training instability.\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '우리는 갈락티카-6.7B5(타일러 et al., 2022)와 코델라마-7B6(Roziere et al., 2023)의 두 가지 기초 모델을 사용한 실험을 수행한다. 두 모델 모두 수학 문제 해결에 강한 성과를 보이는 것으로 보고되고 있으며, 추론 과제에 대한 최근 문헌(Yue et al., 2023; Luo et al., 2023)에서 일반적으로 채택된다. 바젤과의 비교 외에도 GSM8K에 공통 기법, 다수결(왕 등 알, 2023) 및 보상 모델 재위(라이트맨 등 2023)를 적용한다.\n' +
      '\n' +
      '부타주 5: [국무신경부당 표면.코/면책/갈락티카-6.7b] (국무신경부당 표면.co/비대북/갈락시카/갈락토카-6.7b)\n' +
      '\n' +
      '부타주 6: [국경://hugging.co/코델라마/코델라마/코델라마/코델라마-7b-hf] (https://hugging/Codellama/Codellama-7b-hf)\n' +
      '\n' +
      'Hyper-parameterIn all experiments, the training is done with 8 A100-80GB GPUs using DeepSpeed (Rajbhandari et al., 2020; Rasley et al., 2020) Zero stage 2 and HuggingFace Accelerate (Gugger et al., 2022). During the warm-up stage of ReFT, we use AdamW (Loshchilov and Hutter, 2017) optimizer with 0.1 warm-up ration. The batch size is set to 48 and learning rate is \\(1e\\)-\\(5\\). The maximum length is set to \\(1024\\). The number of epochs in the warm-up stage is either \\(1\\) or \\(2\\) in all settings except on MathQA\\({}_{\\text{MCQ}}\\) and MathQA\\({}_{\\text{numeric}}\\) where we use upto 5 and 10 respectively. The model is trained for \\(300\\) epochs with a learning rate of \\(3e\\)-\\(7\\). Following Ziegler et al. (2019), the \\(\\lambda\\), \\(\\gamma\\), \\(\\alpha\\), \\(\\epsilon\\) and \\(U\\) in PPO are set to \\(1\\), \\(0.95\\), \\(5\\), \\(0.2\\), and \\(2\\), respectively. The KL coefficient \\(\\beta\\) is set to \\(0.01\\) for P-CoT and is set to \\(0.05\\) for N-CoT experiments. Further hyperparameter settings about ReFT can be found in Appendix B.\n' +
      '\n' +
      'For SFT baseline, we train the model for 40 epochs and choose the checkpoint with best performance. This number of epochs has been chosen to be sufficiently large to ensure SFT converges. For Offline-ST baseline, we sample the CoTs by using the checkpoint from the ReFT warm-up stage. Using the generation temperature of 1.0 and max length of 1024, we sample 100 CoTs for each question and only keep those with a correct answer. Following Singh et al. (2023), we then subsample the CoTs to 10 random unique CoTs per question to balance difficulties of questions. As mentioned in SS4.2, the Online-ST baseline tries to mimic the same setting as in ReFT. We have the same warm-up process and the hyperparameter setting is roughly the same as ReFT.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l} \\hline \\hline  & **GSM8k** & **SVAMP** & **MathQA\\({}_{\\text{MCQ}}\\)** & **MathQA\\({}_{\\text{numeric}}\\)** \\\\ \\hline\n' +
      '******N-CoT** & 7,465 & 14,862 & 8,955입니다.\n' +
      '3,356 & 15,250&3,672 \\\\*****P-CoT*********** 7,356 & 7,043&3,672 \\\\*********\n' +
      '**Test** & 1,319 & 1,000 & 1,605 & 1,605 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 1>은 훈련 집합과 시험 집합에서 두 종류의 CoT의 Dataset 통계량이다.\n' +
      '\n' +
      '모델 컴플로우.\n' +
      '\n' +
      '코브 등(2021년)에 이어 우사토 등(2022년)에 따라 CoT의 정확도를 결정하기 위해 보상 모델(RM)을 훈련한다. RM 학습 데이터를 구성하기 위해 평가 단계에서 모델을 사용하고 샘플링을 수행하여 훈련 세트에서 각 질문에 대한 100개의 CoT를 얻었다. CoT는 중복되고 이항 라벨은 추출된 답변과 근거 진리를 비교하여 얻을 수 있다.\n' +
      '\n' +
      '공통 관행으로서 보상 모델은 최고의 SFT 체크포인트 코브(Cobbe et al.(2021), 오양 등(2022)에서 초기화된 언어 모델이다. 결과 기반 보상 모델(ORM) Uesato et al.(2022)과 유사하게, 보상 모델은 "_c 보정_" 또는 "_incision_" 솔루션을 나타내는 이진 라벨을 예측하도록 훈련된다. 입력이 보상 모델을 통과하면 마지막 토큰의 히든 상태에 대한 선형 분류기로 분류를 수행한다. 마지막으로 후보 중 \'올바른\' 점수가 가장 높은 솔루션이 최종 답안으로 선정된다. 우리는 배치 크기 48과 최대 길이 700을 사용하여 3epochs에 대한 RM 모델을 훈련한다.\n' +
      '\n' +
      '#### Evaluation\n' +
      '\n' +
      '모든 데이터 세트에서 N-CoT와 P-CoT 모두에 대한 가치 정확도를 보고한다. 특히 다수결 및 재순위(표 4)에 대해서는 평가를 위해 100개의 CoT를 표본으로 한다. 투표에서 다수의 카운트를 갖는 유효한 답변은 컴퓨팅 정확도를 위한 최종 답으로 선택된다. 재위권에서는 가장 높은 점수를 가진 CoT를 선택하여 답을 추출한다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '세븐은 SFT입니다.\n' +
      '\n' +
      '표 2는 기저부 간의 성능을 비교하고 GSM8K, SVAMP 및 MathQA 데이터 세트에 대한 ReFT를 제안했다. 우리는 ReFT가 MathQAMCQ N-CoT를 제외하고 SFT와 자기 훈련 가족 접근법에 비해 훨씬 더 나은 성능을 일관되게 달성한다는 것을 관찰할 수 있다. 구체적으로 GSM8K N-CoT 및 P-CoT에서 코드LLAMA가 있는 SFT에 비해 각각 \\(9\\)-포인트 및 \\(8\\)포인트 개선이 더 많다. 평균적으로 N-CoT 및 P-CoT의 모든 데이터 세트에 대해 코드LLAMA로 각각 3.7점 및 5.9점 개선을 달성했다. 더 중요한 것은 ReFT에서 추가 주석이나 보상 모델이 사용되지 않는다는 것이다. 이러한 강력한 결과는 ReFT(분석 SS5.1 참조)의 강력한 일반화 및 강화 학습 루 등(2023)으로 훈련 데이터를 더 탐색할 수 있는 엄청난 잠재력을 보여준다.\n' +
      '\n' +
      '커뮤니케이션 셀프 트레이닝은 미세 조정의 초기 정책으로부터의 샘플링 데이터를 포함한다. 이 간단한 기준선은 때때로 SFT He et al.(2020)에 비해 성능을 향상시킬 수 있지만 Gulcehre et al.(2023)는 ReFT가 만든 것보다 훨씬 뒤처져 있다. 이러한 비교는 ReFT에서 좋은 성능을 가지려면 "_exploring_"가 필수적임을 나타낸다. 온라인 셀프트레이닝은 갈락티카로 약간의 개선을 달성하지만, 여전히 평균적으로는 ReFT에 크게 뒤처져 있다. 이 결과는 잘못된 사례도 더 나은 탐구를 위해 모델을 안내하는 데 매우 필수적임을 나타낸다. 자가 훈련과의 비교에서도 표준 데이터 증강 접근법보다 온 정책 샘플링과 강화 학습이 제안된 접근법이 더 낫다고 제안한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & \\multirow{2}{*}{**Size**} & \\multicolumn{2}{c}{**GSM8K**} & \\multicolumn{2}{c}{**SVAMP**} & \\multicolumn{2}{c}{**MathQA**MCQ} & \\multicolumn{2}{c}{**Average**} \\\\  & & **N-CoT** & **P-CoT** & **N-CoT** & **P-CoT** & **N-CoT** & **P-CoT** & **N-CoT** & **P-CoT** \\\\ \\hline Galactica + SFT & 6.7B & \\(41.0\\) & \\(57.1\\) & \\(53.8\\) & \\(69.3\\) & \\(58.7\\) & \\(64.8\\) & \\(51.2\\) & \\(63.7\\) \\\\ Galactica + Offline Self-Training & 6.7B & \\(45.0\\) & \\(61.0\\) & \\(56.5\\) & \\(70.8\\) & \\(\\mathbf{60.7}\\) & \\(67.5\\) & \\(54.1\\) & \\(66.5\\) \\\\ Galactica + Online Self-Training & 6.7B & \\(45.7\\) & \\(61.9\\) & \\(58.5\\) & \\(73.7\\) & \\(59.7\\) & \\(62.4\\) & \\(54.6\\) & \\(66.0\\) \\\\ Galactica + ReFT & 6.7B & \\(\\mathbf{46.8}\\) & \\(\\mathbf{68.4}\\) & \\(\\mathbf{62.3}\\) & \\(\\mathbf{73.9}\\) & \\(58.3\\) & \\(\\mathbf{70.4}\\) & \\(\\mathbf{55.8}\\) & \\(\\mathbf{70.9}\\) \\\\ \\hline \\hline CodeLLAMA + SFT & 7B & \\(44.0\\) & \\(64.4\\) & \\(59.6\\) & \\(76.2\\) & \\(56.5\\) & \\(64.2\\) & \\(53.4\\) & \\(68.3\\) \\\\ CodeLLAMA + Offline Self-Training & 7B & \\(38.8\\) & \\(65.0\\) & \\(54.2\\) & \\(72.5\\) & \\(57.6\\) & \\(62.8\\) & \\(50.2\\) & \\(66.8\\) \\\\ CodeLLAMA + Online Self-Training & 7B & \\(40.0\\) & \\(64.3\\) & \\(59.7\\) & \\(75.4\\) & \\(55.5\\) & \\(68.2\\) & \\(53.1\\) & \\(69.3\\) \\\\ CodeLLAMA + ReFT & 7B & \\(\\mathbf{53.5}\\) & \\(\\mathbf{72.8}\\) & \\(\\mathbf{60.0}\\) & \\(\\mathbf{78.4}\\) & \\(\\mathbf{57.9}\\) & \\(\\mathbf{71.5}\\) & \\(\\mathbf{57.1}\\) & \\(\\mathbf{74.2}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 기저부의 가치 정확도 비교 및 모든 데이터셋에 두 개의 기반 모델로 미세 조정된 ReFT 방법을 제안했다.\n' +
      '\n' +
      '그림 3: MathQAMCQ의 실시예 예측은 보상 해킹을 보여준다.\n' +
      '\n' +
      'MathQA\\({}_{\\text{MCQ}}\\)에 대한 음성 결과에 대한 우리의 조사 결과는 ReFT가 훈련 중 선택형 질문에 대한 보상 해킹(Skalse et al., 2022)에 시달리고 있음을 나타낸다. 그림 3은 샘플링된 솔루션이 RL 훈련이 겪는 "_누적 보상_"을 생성하는 방법을 보여준다. 우리가 볼 수 있듯이, 샘플링된 CoT는 "_18_" 및 "22"의 산물이 아닌 잘못된 답변 "_344_"를 얻는다. 그러나 최종 추론 단계는 모델이 중간 CoT7의 정확성에 관계없이 항상 {A, B, C, D, E}로부터 옵션 중 하나를 예측할 것이기 때문에 최종 답으로 "_C_" 옵션을 예측하는데, 이러한 오해의 소지가 있는 CoT는 이를 올바른 CoT로 처리하기 위한 양의 보상 "1"을 받고 모델을 오도할 것이다. 기본 보상 해킹 현상은 모델 훈련(Everitt et al, 2021)을 심각하게 변조한다. 이는 보상 해킹 효과를 줄이기 위해 MathQA에 대한 평가 단계가 더 긴 체크포인트를 선택한 이유이기도 하다.\n' +
      '\n' +
      '부츠 7: 프로그램 기반 CoT가 자연어보다 엄격해 고통을 받을 가능성이 낮다는 것을 발견했다.\n' +
      '\n' +
      'MCQ 문항의 부정적인 영향을 추가로 입증하기 위해 Jie와 Lu(2023), MathQA\\({}_{\\text{numeric}}\\)에 의한 MathQA 변이체에 대한 실험을 수행하여 질문의 옵션을 제거하고 숫자 답을 직접 예측한다. 표 3은 SFT에 대한 비교를 나타낸다. 우리는 ReFT가 Galactica와 CodeLLAMA를 모두 사용하여 SFT를 일관되게 능가한다는 것을 관찰할 수 있다.\n' +
      '\n' +
      'Majority Voting and Reranking Benefit ReFT Following Wang et al. (2023); Uesato et al. (2022); Lightman et al. (2023), we also perform majority voting and reward model reranking to show that ReFT can benefits from these common techniques. Specifically, we perform sampling from both SFT and ReFT policies. We sample \\(100\\) CoT solutions for each question and apply the reward model described in SS4.3. Table 4 shows that ReFT consistently achieves the best performance on GSM8K by reward model reranking. ReFT + Voting significantly outperforms SFT + Voting by \\(9.2\\) points on average across all settings. ReFT with reranking outperforms SFT with reranking by \\(3.3\\) points on average.\n' +
      '\n' +
      'Compared with existing open-source approaches (Luo et al., 2023; Wang et al., 2023; Yue et al., 2023) (Table 4 bottom8), our best P-CoT variant achieves the best performance with accuracy \\(79.3\\) on GSM8K. In addition, these approaches mainly include extra data generated from ChatGPT and perform distillation during fine-tuning. In contrast, we improve the policy itself by exploiting the potential of existing training data and pushing the limit of the policy performance. Our best result reported in Table 4, i.e., the CodeLLAMA + ReFT + Reranking with P-CoT setting, even slightly surpasses GPT-3.5-turbo. However, we obtain the result with a model that is only in the size of 7B.\n' +
      '\n' +
      '발주 8: 은어는 원래 논문에서 가져옵니다. MAmmoTH-Coder에 대한 N-CoT 및 P-CoT 결과는 부록에서 보고된다.\n' +
      '\n' +
      '직관적으로 작은 모델에 대한 실험은 소규모 언어 모델로 불완전한 시연으로 이어질 수 있다. 우리는 수술을 합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & \\multirow{2}{*}{**Size**} & \\multicolumn{2}{c}{**GSM8K**} \\\\  & & \\multicolumn{1}{c}{**N-CoT**} & **P-CoT** \\\\ \\hline Galactica + SFT + Voting & 6.7B & 50.8 & 61.1 \\\\ Galactica + ReFT + Voting & 6.7B & 55.7 & 70.7 \\\\ Galactica + SFT + Reranking & 6.7B & 56.5 & 72.4 \\\\ Galactica + ReFT + Reranking & 6.7B & **62.8** & **76.6** \\\\ \\hline \\hline CodeLLAMA + SFT + Voting & 7B & 53.8 & 67.9 \\\\ CodeLLAMA + ReFT + Voting & 7B & 65.1 & 75.0 \\\\ CodeLLAMA + SFT + Reranking & 7B & 61.5 & 77.6 \\\\ CodeLLAMA + ReFT + Reranking & 7B & **65.7** & **79.3** \\\\ \\hline \\hline Extra Training Unit & -1 & -1 & -1 \\\\ WizardMath (Luo et al., 2023) & 7B & 54.9 & - \\\\ WizardMath (Luo et al., 2023) & 13B & 63.9 & - \\\\ MathCoT (Wang et al., 2023) & 7B & 67.8 & - \\\\ MAmmoTH-Coder (Yue et al., 2023) & 7B & 22.2 & 58.8 \\\\ MAmmoTH-Coder (Yue et al., 2023) & 70B & 72.4 & 76.7 \\\\ \\hline \\hline GPT-3.5-turbo (Jie et al., 2023) & N.A. & \\(75.3\\) & \\(78.0\\) \\\\ GPT-4 (OpenAI, 2023; Zhou et al., 2023a) & N.A. & \\(93.0\\) & \\(97.0\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: GSM8K에 대한 SFT 및 ReFT에 대한 다수결 및 보상 모델의 정확성 판별 모델 재순위이다. 우리는 또한 비교를 위한 기존 접근법을 포함한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline \\multicolumn{2}{c}{**Method**} & \\multicolumn{2}{c}{**N-CoT**} \\\\ \\hline \\multirow{2}{*}{**Galactica**} & SFT & \\(41.1\\) \\\\  & ReFT & \\(44.9\\) \\\\\n' +
      '**Codellama** & SFT & \\(36.3\\) \\\\  & ReFT & \\(41.0\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 갈락티카-125M9를 사용한 P-CoT 데이터에 대한 MathQA\\({}_{\\text{numeric}}\\) 벤치마킹에 대한 2개의 기반 모델을 사용한 SFT 및 ReFT의 측정 결과 SFT와 ReFT의 성능 비교는 표 5에 나와 있다. 놀랍게도, ReFT는 작은 모델에서도 3개의 데이터 세트에서 SFT를 여전히 능가한다. 이러한 개선은 합리적인 프로그램의 탐색 동안 ReFT의 견고성을 보여준다.\n' +
      '\n' +
      '부츠 9: 갈락티카에서 사용할 수 있는 가장 작은 모델 크기[https://huggingface.co/facebook/갈락토카-125m] (https://huggingface.co/facebook/갈락토카-125m)\n' +
      '\n' +
      '우리는 GSM8K P-CoT에 대한 코드LLAMA를 사용하여 절제 연구를 수행한다(표 6). 부분 보상 없이 ReFT는 더 낮은 정확도 \\(70.9\\)를 얻지만 SFT보다 여전히 훨씬 낫다. SS3.1에서 언급한 바와 같이 이러한 부분 보상은 훈련 중 희소 보상(Trott et al., 2019)의 효과를 줄이는 데 도움이 될 수 있다. 또한, KL 계수 \\(\\beta\\)를 \\(0\\)로 설정하면 정책 분포가 쉽게 붕괴되어 예상치 못한 결과(즉, \\(0\\) 정확도를 발생시킬 것이다. 정책이 탐구하는 공간(오양 등 2022년)에 제약을 가하는 것이 분명히 중요하다. 초기 평가 단계는 본질적으로 그러한 제약을 만들고 정책은 \\(\\beta\\)에 의해 지배되는 범위 내에서 더 탐색할 수 있게 한다. 마지막으로 정책 모델(그리고 2021년, 코브 등, 2021년)과 공유하는 파라미터가 없는 가치 모델을 실험한다. 개별 가치 모형은 매개 변수를 정책 모형과 동일하게 초기화한다. 이러한 설정이 모델이 더 빠르게 수렴할 수 있고 결국 동등한 성능에 도달하지만 각 배치에 대해 두 번 전진 패스를 수행해야 하므로 원래 계산 오버헤드의 두 번 희생한다는 것을 발견했다.\n' +
      '\n' +
      '## 5 Analysis\n' +
      '\n' +
      '### Generalization\n' +
      '\n' +
      '그림 4는 GSM8K P-CoT에서 ReFT10을 훈련하는 동안 평균 보상, 평가 정확도 및 KL 분기를 보여준다. SFT는 40\\({}^{th}\\) 에포치에 접근할 때 수렴하여 과대 적합해진다. 그러나 40\\({}^{th}\\) epoch에서 ReFT 정책에 대해 평균 보상은 약 80~90%이며, 가치 정확성도 증가하고 있다. 또한 KL 발산(그림 4 (c))이 초기에 매우 큰 후 \\(0\\)와 \\(10\\) 사이의 합리적인 값을 유지하는 것을 알 수 있다. 안정적인 KL 분기는 우리의 정책이 적절한 프로그램을 포함하는 공간 내에서 탐색을 수행하는 것을 나타낸다. 기본 강화 학습 메커니즘은 ReFT(브라운 et al., 2020)의 일반화 능력을 크게 향상시킵니다.\n' +
      '\n' +
      '부츠 10: 일러스트 목적의 경우 \\(60\\) epochs에 대한 평균 보상 및 KL만 보여준다.\n' +
      '\n' +
      '### ReFT가 SFT를 뛰어넘을 때?\n' +
      '\n' +
      '우리는 ReFT와 SFT 사이의 관계를 추가로 조사하기 위해 SFT의 다른 수의 평가 단계로 ReFT 교육을 수행한다. 그림 5는 SFT11에 대한 다양한 ReFT 변이체의 가치 정확도를 보여주고 있으며, 특히 평가 단계가 \\(3\\)인 경우 정책 초기화(3^{rd}\\)-포흐 SFT 체크포인트로부터 정책 초기화를 의미한다. 우리는 모든 ReFT 정책이 에포치가 \\(8\\) 미만인 초기에 더 나쁜 성과를 보이고 있음을 알 수 있다. 공유값 모델의 선형층은 무작위로 초기화되어 분포 조절을 위해 몇 개의 에포크를 취할 수 있기 때문이다. I\\(30^{th}\\) 에포치를 시작으로 SFT 수렴과 모든 ReFT 변이체가 여전히 개선되고 있다. 또한 모든 변이체가 상당한 마진으로 SFT를 능가하고 특정 ReFT 변이체의 명백한 이점이 없음을 볼 수 있다.\n' +
      '\n' +
      '부츠 11: 일러스트 목적으로 60 에포치만 보여줍니다. 후기 에포치에 대한 공연은 부록에서 보여질 것이다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'I nc 온톨레트 oS FT, R eF는 구별 가능한 n개의 에토디프 에토드프 orf를 삭제한다.\n' +
      '\n' +
      '두 개의 기초 모델을 사용하여 세 개의 데이터 세트에 대한 광범위한 실험을 통해 ReFT가 성능 및 일반화 능력 측면에서 SFT를 능가한다는 것을 입증했다. 더욱이, 우리는 다수의 투표(왕 등 2023), 보상 모델 재위(Cobbe et al, 2021; Uesato et al., 2022)와 같은 기술로 ReFT로 훈련된 모델의 호환성을 보여주었다.\n' +
      '\n' +
      'Furthermore, ReFT has exhibited superior performance compared to several publicly available open-source models of comparable sizes in math\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline\n' +
      '**Model Setting** & **Accuracy** \\\\ \\hline CodeLLAMA + ReFT & \\(72.7\\) \\\\ – remove partial reward & \\(70.9\\) \\\\ – KL coefficient \\(\\beta=0\\) & _collapse_ \\\\ – non-shared value model & \\(72.6\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: GSM8K P-CoT에 대한 Ablation 연구는 다음과 같다.\n' +
      '\n' +
      'problem-solving. This demonstrates the effectiveness and practical value of the ReFT approach.\n' +
      '\n' +
      '## 7 Future Work\n' +
      '\n' +
      '우리는 수학 문제 해결을 위해 LLM을 미세 조정하기 위해 강화 학습, 특히 PPO 알고리즘 Schulman et al.(2017)을 적용하려는 첫 번째 시도를 했다. 우리의 미래 작업은 오프라인 강화 학습 기법 Levine et al.(2020)의 활용, Gulcehre et al.(2023), 훈련 효율성과 성능을 높이기 위한 _Å-up 자유_ 방법의 개발, 이에 따라 재위법과의 격차를 줄이는 것을 포함한다. 또한 라이트맨 등(2023)은 잘 훈련된 프로세스 기반 보상 모델(PRM)이 성능을 크게 향상시킬 수 있음을 시사한다. 따라서 강화 학습 훈련에서 과정 기반 보상의 구현을 탐색할 가치가 있을 것이다. 마지막으로 ReFT가 다재다능한 접근인 만큼 CoT로 추론을 공식화할 수 있는 보다 일반적인 추론 작업에 적용하고자 한다.\n' +
      '\n' +
      'Limitations\n' +
      '\n' +
      'Training EfficiencyAs depicted in Figure 4 (b), it is evident that ReFT necessitates a greater number of epochs to reach convergence compared to SFT. This is primarily due to the fact that ReFT optimizes a non-differentiable objective and requires exploration of the generation space to attain correct answers. While a larger learning rate may expedite convergence, it also makes the policy more susceptible to instability and potential collapse. Alternatively, using a larger batch size is a viable option; however, it comes at the expense of increased computational costs.\n' +
      '\n' +
      'Reward HackingOur reward function relies solely on the final answer to determine the reward. However, as demonstrated in the experiments conducted on the MathQAMCQ N-CoT dataset, the policy can be easily manipulated if the possible space of final answers is limited, such as A,B,C,D. To mitigate the issue of reward hacking, it may be necessary to employ a more detailed or process-based reward function that takes into account a broader range of factors.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. Amini, S. Gabriel, S. Lin, R. Koncel-Kedziorski, Y. Choi, and H. Hajishirzi (2019)MathQA: towards interpretable math word problem solving with operation-based formalisms. In Proceedings of NAACL, Cited by: SS1, SS2.\n' +
      '* M. Amini, V. Feofanov, L. Pauletto, E. Devijver, and Y. Maximov (2022)Self-training: a survey. arXiv preprint arXiv:2202.12040. Cited by: SS1, SS2.\n' +
      '* M. Andrychowicz, A. Raichuk, P. Stanczyk, M. Orsini, S. Girgin, R. Marinier, L. Hussenot, M. Geist, O. Pietquin, M. Michalski, et al. (2021)What matters in on-policy reinforcement learning? a large-scale empirical study. In Proceedings of ICLR, Cited by: SS1, SS2.\n' +
      '* T. Anthony, Z. Tian, and D. Barber (2017)Thinking fast and slow with deep learning and tree search. In Proceedings of NeurIPS, Cited by: SS1, SS2.\n' +
      '*\n' +
      '\n' +
      '그림 4는 GSM8K P-CoT에 대한 훈련 epoch에 대한 ReFT, 평가 정확도, KL의 훈련 보상이다.\n' +
      '\n' +
      '그림 5: 평가-업 에포치의 수가 다른 SFT와 ReFT 간의 친화도 비교이다.\n' +
      '\n' +
      '* Azzar et al. (2023) Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Remi Munos. 2023. A general theoretical paradigm to understand learning from human preferences. _arXiv preprint arXiv:2310.12036_.\n' +
      '* Brown et al. (2020) Daniel S Brown, Wonjoon Goo, and Scott Niekum. 2020. Better-than-demonstrator imitation learning via automatically-ranked demonstrations. In _Proceedings of Conference on Robot Learning_, pages 330-359.\n' +
      '* Cobbe et al. (2021a) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021a. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_.\n' +
      '* Cobbe et al. (2021b) Karl W Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman. 2021b. Phasic policy gradient. In _Proceedings of ICML_.\n' +
      '* Ethayarajh et al. (2023) Kawin Ethayarajh, Winnie Xu, Dan Jurafsky, and Douwe Kiela. 2023. Human-centered loss functions (halos). Technical report, Contextual AI.\n' +
      '* 에버트 등은 (2021) 톰 에버트, 마커스 허터, 라마나 쿠타르, 빅토리아 카락코프나 등이 있다. 강화학습의 문제 및 해결방안, 즉 인과관계 영향 다이어그램 관점 __omplete tampering 문제와 해결방안 강화학습의 해결방안. _rward tampering 문제와 해결책 : 인과관계 영향 다이어그램 관점. 합성_, 198(공급 27):6435-6467이다.\n' +
      '* Fu et al. (2023) Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2023. Complexity-based prompting for multi-step reasoning. In _Proceedings of ICLR_.\n' +
      '* Gao et al. (2023) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. PAL: Program-aided language models. In _Proceedings of ICML_.\n' +
      '* Gao et al. (2018) Yang Gao, Huazhe Xu, Ji Lin, Fisher Yu, Sergey Levine, and Trevor Darrell. 2018. Reinforcement learning from imperfect demonstrations. _arXiv preprint arXiv:1802.05313_.\n' +
      '* Gugger et al. (2022) Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. 2022. Accelerate: Training and inference at scale made simple, efficient and adaptable. [https://github.com/huggingface/accelerate](https://github.com/huggingface/accelerate).\n' +
      '2022.Accel 소거,Philip pS chm id,Zac haryMu 타원체, 464 004 33 275 94127 395]) 그리울카 r, 마르카순, 앤벤자미 nBossa n. (46400 4332759412739 5)\n' +
      '* He et al. (2020) Junxian He, Jiatao Gu, Jiajun Shen, and Marc\'Aurelio Ranzato. 2020. Revisiting self-training for neural sequence generation. In _Proceedings of ICLR_.\n' +
      '* 헨드렉스 등은 (2021) 단켄드랙스, 콜린 번스, 소라바 카다브리스, 아쿨 아로라, 스티븐 바마트, 에릭 탕, 전당송, 제이콥 슈타인하르트 등이다. 2021. 수학 데이터셋으로 수학적 문제 해결을 해결합니다. 신경 정보 처리 시스템 Datasets 및 벤치마크 트랙(Round 2)_에 관한 제5차 회의의 _검토에서.\n' +
      '* Hoi et al. (2021) Steven CH Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao. 2021. Online learning: A comprehensive survey. _Neurocomputing_, 459:249-289.\n' +
      '* Imani et al. (2023) Shima Imani, Liang Du, and Harsh Shrivastava. 2023. Mathprompter: Mathematical reasoning using large language models. _arXiv preprint arXiv:2303.05398_.\n' +
      '* Jie와 루(2023) 자밍 지와 웨루. 2023. 수적 추론을 위해 몇 샷으로 훈련 데이터를 추적합니다.\n' +
      '* 지타 l. ( 2023)Z 한밍J ie, T rungQ uocL uong,boZ 매달리는X,a ndH angL, i.2 023.D esigno fc에 있는X iaoranJ, aa ndH angL i.2 023.D esigno fc는 항노화 nm 운동 문제 해결 후각._에 해당한다. arXivp reprinta rXiv:2309.11054_.\n' +
      '* Khot et al. (2023) Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2023. Decomposed prompting: A modular approach for solving complex tasks. In _Proceedings of ICLR_.\n' +
      '* Kullback and Leibler (1951) Solomon Kullback and Richard A Leibler. 1951. On information and sufficiency. _The annals of mathematical statistics_, 22(1):79-86.\n' +
      '* Le et al. (2022) Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. 2022. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. In _Proceedings of NeurIPS_.\n' +
      '* Levine et al. (2020) Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. 2020. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_.\n' +
      '* Lightman et al. (2023) Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let\'s verify step by step. _arXiv preprint arXiv:2305.20050_.\n' +
      '브링빈 류(2023) 빙빈 류, 세바스티엔 비브크, 온엔E 란단, J aardhanK ulkarni,Y uanzhiL i,A nhN 메넨,RchelW ard,a ndY iZ는 접합체에서 023T에 매달려 있다. arXivp reprinta rXiv:2312.09241_.\n' +
      '* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_.\n' +
      '* Lu et al. (2023) Xiuyuan Lu, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian Osband, Zheng Wen, et al. 2023. Reinforcement learning, bit by bit. _Foundations and Trends(r) in Machine Learning_, 16(6):733-865.\n' +
      '\n' +
      'Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-ardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. _arXiv preprint arXiv:2308.09583_.\n' +
      '* Miao et al. (2023) Ning Miao, Yee Whye Teh, and Tom Rainforth. 2023. Selfcheck: Using lms to zero-shot check their own step-by-step reasoning. _arXiv preprint arXiv:2308.00436_.\n' +
      '* Nye et al. (2021) Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratch-pads for intermediate computation with language models. _arXiv preprint arXiv:2112.00114_.\n' +
      '* 오픈AI(2023) 오픈AI입니다. 2023. GPT-4 기술 보고서.\n' +
      '* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. In _Proceedings of NeurIPS_.\n' +
      '* Patel et al. (2021) Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems? In _Proceedings of NAACL_.\n' +
      '* Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. In _Proceedings of NeurIPS_.\n' +
      '* Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In _SC20: International Conference for High Performance Computing, Networking, Storage and Analysis_.\n' +
      '* Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In _Proceedings of SIGKDD_.\n' +
      '* 라에밀러 등은 (2018) 마르틴 러블러, 롤랑 하프너, 토마스 람페, 마이클 네베르트, 조나스 데그레브, 톰 위글, 블라디 미니, 니콜라스 헤센, 조스트 토비타스 스프링켄버그 등이다. 2018학년도에는 처음부터 희박한 보상 과제를 해결하여 학습합니다. ICML_검토에서.\n' +
      '* Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. 2023. Code llama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_.\n' +
      '* Schulman et al. (2018) John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2018. High-dimensional continuous control using generalized advantage estimation.\n' +
      '* Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_.\n' +
      '알렉산 아틀레키 리, 압스네르키, 알렉산드 라예스키, 아즈네르키, 아자네트 나르네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자우, 이즈네트, 베냐에, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자르네, 아자르네, 아자르네, 아자르네, 아자르네, 아자르네, 아자르네, 아 바일치, 노아 콘스탄트, 루마니아 노바크, 로산네 류, 트리스 와르센틴, 예디칸탈, 야미니 반살, 이단 다이어, 베남 네세르, 자스차 소힐-디케슈타인, 노아 피젤 등이 있다. 2023. 인간 데이터를 넘어 언어 모델과의 문제 해결을 위한 자기 학습을 수행한다.\n' +
      '* Skalse et al. (2022) Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. 2022. Defining and characterizing reward gaming. In _Proceedings of NeurIPS_.\n' +
      '* Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. 2018. _Reinforcement learning: An introduction_. MIT press.\n' +
      '* Taylor et al. (2022) Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. _arXiv preprint arXiv:2211.09085_.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_.\n' +
      '* Trott et al. (2019) Alexander Trott, Stephan Zheng, Caiming Xiong, and Richard Socher. 2019. Keeping your distance: Solving sparse reward tasks using self-balancing shaped rewards. In _Proceedings of NeurIPS_.\n' +
      '* Uesato et al. (2022) Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with process-and outcome-based feedback. _arXiv preprint arXiv:2211.14275_.\n' +
      '* 왕(2023a) Ke왕, 허싱 르, 아오준 주, 지무 루, 시쿤 루노, 위쿠앙 시, 르루이 장, 린치 송, 명지 자한, 홍정 리가 있다. 2023a. Math-coder: 향상된 수학적 추론을 위한 llms의 무질서한 코드 통합. __ath-coder: 향상된 수학적 추론을 위한 llms의 무질서한 코드 통합. arXiv 프리프린트 arXiv:2310.03731_.\n' +
      '* 왕 등은 왕(2023b) 자에히 왕, 제이슨 웨이, 데일 슈무르만스, 퀀오 V 르, 에드 하치, 샤란 나랑, 아악카 초위헤이, 데니 저우 등이 있다. 2023b. 자기 대응성은 언어 모델에서 사고 추론의 사슬을 향상시킨다. ICLR_의 _수익률.\n' +
      '\n' +
      'Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. In _Proceedings of NeurIPS_.\n' +
      '* Zhang eta l2023) 멍크스 ueZh ang,Zichao 왕,Z hichao 양, Wei qiFe n g,a nd그리고 rewann23.Iter는 g._P 로세 디밍소프ACL_에서 후추추기 폐 p-부틸란 문제를 전했다.\n' +
      '* Yu et al. (2023) Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models. _arXiv preprint arXiv:2309.12284_.\n' +
      '*유 등은 (2023) 샤랑유, 시웨이 Qu, 거장, 야오후, 원하오황, 후안순, 유수, 원후첸 등이다. 하이브리드 명령어 튜닝을 통한 빌딩 수학 일반주의 모델 __Mammoth: __Mammoth: 하이브리드 명령어 튜닝을 통한 빌딩 수학 일반주의 모델. arXiv 프리프린트 arXiv:2309.05653_입니다.\n' +
      '* Zhang et al. (2023) Mengxue Zhang, Zichao Wang, Zhichao Yang, Weiqi Feng, and Andrew Lan. 2023. Interpretable math word problem solution generation via step-by-step planning. In _Proceedings of ACL_.\n' +
      '* Zheng et al. (2023) Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang Weng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, and Xuanjing Huang. 2023. Secrets of rhlf in large language models part i: Ppo.\n' +
      '* Zhong et al. (2017) Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language using reinforcement learning. _arXiv preprint arXiv:1709.00103_.\n' +
      '* Zhou et al. (2023a) Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. 2023a. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. _arXiv preprint arXiv:2308.07921_.\n' +
      '* Zhou et al. (2023b) Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2023b. Least-to-most prompting enables complex reasoning in large language models. In _Proceedings of ICLR_.\n' +
      '* Ziegler et al. (2019) Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. _arXiv preprint arXiv:1909.08593_.\n' +
      '* Guestin (2020)**Question:** Weng earns $12 an hour for babkystring. _Vesterday, she just did 50 minutes of babkystring: How much did she earn?\n' +
      '\n' +
      '**[N-CoT]:** 우리는 그녀의 시간당 비율을 계산한 다음 자신이 일했던 시간만큼 곱해야 한다.\n' +
      '\n' +
      '먼저 50분부터 몇 시간까지 전환해야 합니다.\n' +
      '\n' +
      '한 시간 안에 60분이 있어 50분은 50/60 = 55시간과 같다.\n' +
      '\n' +
      '다음으로, 우리는 월의 수익을 시간당 비율에 근무 시간만큼 곱하여 계산할 수 있다.\n' +
      '\n' +
      '$12hour x 5/6 hour = $10.\n' +
      '\n' +
      'Therefore, Weng earned $10 for 50 minutes of babkystring.\n' +
      '\n' +
      '답은 10입니다.\n' +
      '\n' +
      '**[P-CoT]:**\n' +
      '\n' +
      'def solution():\n' +
      '\n' +
      '“어제는 바베키스터링에 1시간12달러를 버텨줬어요. _예, 그녀는 50분 동안 바베키스터링을 했어요.\n' +
      '\n' +
      '시간별 = 12분\n' +
      '\n' +
      '##는 N-CoT 및 P-CoT 설명서 A 예.\n' +
      '\n' +
      'We present examples of natural language CoT and program-based CoT from GSM8K dataset in Figure 6. We follow Jie et al. (2023) to perform few-shot prompting and obtain the CoT representations. The natural language CoT is generally the same as the one presented in Wei et al. (2022). The format program-based CoT is similar to the one in PAL (Gao et al., 2023), where we use a function to solve the problems. The function starts with a Python docstring that repeats the question and then a list of statements as reasoning steps.\n' +
      '\n' +
      '부록 B.\n' +
      '\n' +
      'Supervised Fine-TuningWe train the model for 40 epochs with the batch size of 48 and the maximum length of \\(1024\\).. For small models, we increase the learning rate to \\(2e\\)-\\(5\\), and the number of epoch for training MathQA\\({}_{\\text{MCQ}}\\) to 100 epochs.\n' +
      '\n' +
      '갈락티카의 경우 N-CoT와 P-CoT 모두에 대해 GSM8K, SVAMP에서 2epochs에 대한 평가 작업을 수행한다. MathQA\\({}_{\\text{MCQ}}\\) 측면에서 MathQA\\({}_{\\text{MCQ}}\\) N-CoT 및 MathQA\\({}_{\\text{MCQ}}}}}) P-CoT에서 5epochs에 대한 평가 작업을 수행한다. 코드LLAMA의 경우 SVAMP, GSM8K에서 2epoch, MathQA\\에서 5epochs, MathQA\\({}_{\\text{MCQ}}\\) N-CoT 및 MathQA\\({}_{\\text{MCQ}}\\) P-CoT에서 2epoch에 대한 평가 작업을 수행한다. 특히 MathQA\\({}_{\\text{numeric}}\\)에 대해 구체적으로 수행합니다.\n' +
      '\n' +
      '그림 6: N-CoT 및 P-CoT 예를 GSM8Km8K그림 6: N-CoT 및 P-CoT 예시를 그림 6: N-CoT 및 P-CoT 예이다.\n' +
      '\n' +
      '이 데이터셋이 훨씬 더 어렵고 추론 사슬의 수가 다른 데이터세트보다 더 길기 때문에 10epochs에 대한 데일리업. 작은 모델의 경우, 우리는 GSM8K 및 SVAMP의 경우 10epoch이며 MathQA\\({}_{\\text{MCQ}}\\)의 경우 40 epoch이다.\n' +
      '\n' +
      '질문에 대한 최대 길이는 300으로 설정되며 샘플링 중 최대 길이는 \\(700\\)로 설정된다. 배치 크기는 32로 값 모델의 추가 메모리 소비로 인해 SFT보다 작다. RL 단계(즉, ppo epoch)당 업데이트 횟수는 2(Ziegler et al., 2019)로 설정된다. 지글러 등(2019년)에 이어 체중 붕괴와 탈락은 사용하지 않습니다. 작은 모델의 경우 \\(3e\\)-\\(6\\)의 학습률과 256의 글로벌 배치 크기로 700epoch를 훈련합니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>