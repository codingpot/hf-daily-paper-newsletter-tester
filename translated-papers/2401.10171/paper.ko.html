<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '_B_RDF 최적화 _I_n-\n' +
      '\n' +
      ' 그리고\n' +
      '\n' +
      '투베르빙겐넨의 투버링겐멘 대학의 투버링대 대학.\n' +
      '\n' +
      '현재 소속은 안정성 AI입니다.\n' +
      '\n' +
      'Amit Raj\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      'Mark Boss\n' +
      '\n' +
      'Unity\n' +
      '\n' +
      'Yunzhi Zhang\n' +
      '\n' +
      'Stanford University\n' +
      '\n' +
      'Abhishek Kar\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      'Yuanzhen Li\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      'Deqing Sun\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      '세르도 마흐라.\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      '조나단 T. 위 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로 정면으로\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      '펜드리크 P. A. LenschchHendrik P.\n' +
      '\n' +
      '투베르빙겐넨의 투버링겐멘 대학의 투버링대 대학.\n' +
      '\n' +
      'Varun Jampani\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '다양한 조명, 포즈, 배경으로 캡처된 객체 이미지로부터 형상, 재료 및 조명의 재구성을 위한 엔드 투 엔드 프레임워크인 SHINOBI를 제시한다. 미제약 이미지 컬렉션에 기반한 객체의 역 렌더링은 컴퓨터 비전 및 그래픽의 오랜 도전이며 모양, 라디턴스 및 포즈에 대한 관절 최적화가 필요하다. 우리는 다중 해상도 해시 인코딩에 기반한 암묵적인 형상 표현이 사전 작업을 능가하는 관절 카메라 정렬 최적화로 빠르고 견고한 형상 재구성을 가능하게 한다는 것을 보여준다. 또한 조명 및 객체 반사율(즉 물질)의 편집을 가능하게 하기 위해, 우리는 대상체의 모양과 함께 BRDF 및 조명을 공동으로 최적화한다. 우리의 방법은 AR/VR, 영화, 게임 등과 같은 여러 용도 사례에 대해 재발 가능한 3D 자산을 생산하기 위해 대상물의 일반적인 이미지 컬렉션에서 수업 진단 및 작업이다. 프로젝트 페이지: [국보신비.아겅엘하르트닷컴][국보신비.아메가하르트닷컴]\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '제약되지 않은 인 윙 이미지 모음에서 물체의 3D 모양과 재료 특성을 공동으로 재구성하기 위한 카테고리 진단 기술을 제시한다. 이 데이터 레짐은 다양한 배경, 조명, 카메라 포즈 및 침입을 초래하는 다양한 장치를 사용하여 다른 환경에서 이미지가 캡처됨에 따라 여러 문제를 제기한다. 또한 카메라 바젤이 큰 경향이 있습니다. 그림. 1(좌표)은 입력 영상 집합의 예시를 나타낸다. AR/VR, 게임 및 영화에서 많은 그래픽 애플리케이션은 실제 물체의 고품질 3D 자산에 의존한다. 물체를 새로운 환경에 통합하려면 물리적 기반 재료가 필수적이다. 종래의 획득에는 3D 모델링, 텍스처 페인팅, 광 보정 또는 스케일링하기 어려운 제어 집합[6, 53]과 같은 힘든 작업이 포함된다. 많은 개체에 대해 인터넷으로부터 스마트폰이나 이미지 수집물에서 카지노로 캡처된 이미지를 얻는 것이 더 쉽다.\n' +
      '\n' +
      '이러한 도전적 상황 [14, 33]에서 COLMAP[64, 65]과 같은 기존의 구조-움직임 기법들은 이미지 수집들을 재구성하지 못한다. 물체 내 거짓말을 제약했음에도 불구하고, 특히 NAVI[33] 인-팬드 장면의 맥락에서 조회수의 절반 미만이 완전히 실패한 장면의 절반으로 평균적으로 등록된다. 결과적으로, 우리는 카메라 포즈 최적화가 이 환경에서 재구성 품질에 가장 큰 영향을 미친다는 것을 관찰한다. 모양 및 재료 추정에 대한 기존 많은 작품[6, 13, 68, 79, 81, 86, 89]은 일정한 카메라 침입을 가정하고 카메라의 초기화는 진정한 포즈에 가깝다. 우리는 SAMURAI [14] 및 NeRS [83]에서와 같이 지반 진리와 잠재적으로 거리가 먼 포즈를 가진 거친 사분면 기반 포즈 초기화를 갖는 360\\({}^{\\ 회로}\\) 멀티뷰 데이터를 지원한다. 도전적인 데이터를 위해 이미지 수집당 몇 분 안에 주석을 달 수 있습니다. SAMURAI[14]에서 카메라 포즈는 매우 거친 방향에서 초기화될 수 있지만, 약간의 오프셋은 종종 최종 재구성에서 지나치게 부드러운 질감과 모양을 유발할 수 있다. 또한, 카메라 포즈 최적화를 통한 기존 물질 분해 방법은 느리며, 종종 단일 객체 [14, 37]에서 12시간 이상 실행된다. 대조적으로, 최적화 중에 더 짧은 시간에 더 많은 광선을 처리할 수 있는 다중 해결 해시 격자[51]를 기반으로 파이프라인을 제안한다. 이러한 이점을 사용하여 우리는 여전히 경쟁적인 런타임(Tab. 1)을 유지하면서 SAMURAI에 비해 재구성 품질을 향상시킬 수 있다.\n' +
      '\n' +
      '다중 해상도 해시 그리드의 이동성 통합은 입력 위치에 대한 구배에서 중단으로 인한 카메라 포즈 추정에 적합하지 않다. 카메라 포즈 최적화를 안정화하고 날카로운 기능을 장려하기 위해 함께 작동하는 여러 구성 요소를 제안합니다. SHINOBI의 주요 구별 특징에는 포함되어 있다.\n' +
      '\n' +
      '레벨 어닐링으로__* _하이브리드 다중 해결 Hash 코딩._ 수준 어닐링으로._* _하이브리드 다중 해결 Hash 코딩. 우리는 다중해결 해시 기반 인코딩[51]을 입력 좌표의 일반 푸리에 특징 변환과 결합하여 저주파 구배 전파를 규칙화한다. 이것은 최적화를 작은 오버헤드만 추가하는 동안 훨씬 더 견고하게 만듭니다. 최근 Zhu[91]에서 다른 작업에 대해 유사한 접근법이 제안되었다. 카메라 포즈 최적화에도 유익하다는 것을 보여줍니다.\n' +
      '___Camera 다중 제약___Camera 다중 제약._** _Camera 다중 제약. 우리는 카메라 회전의 과모수화를 피하기 위해 SAMURAI의 카메라 파라미터화를 수정한다. 또한, 우리는 초기 단계에서 최적화를 매끄럽게 하는 데 더 도움이 되는 다중 내부의 카메라 제안들에 대한 일관성을 실행하기 위해 투영 기반 손실로 카메라 최적화를 제약한다.\n' +
      '뷰 중요도 가중치___Per-view 중요도 가중치.__** _Per-view 중요도 가중치를 -* _Per-view 중요도 가중치. 우리는 일부 견해가 다른 견해보다 최적화에 더 유용하다는 중요한 관찰을 레버리지하기 위해 뷰당 중요도 가중치를 제안한다. 구체적으로, 최적화 과정에서 재구성을 고정하기 위해 잘 작동하는 카메라를 사용합니다.\n' +
      '___Patch 기반 정렬 손실.__Patch 기반 정렬 손실._** _Patch 기반 정렬 손실._* _Patch 기반 정렬 손실. SHINOBI는 카메라 정렬을 돕기 위해 새로운 패치 레벨 손실을 제안하고 3D 정렬에 더 나은 이미지를 위해 Lensch[38]에서 영감을 받은 실루엣 손실을 추가로 소개한다. NAVI [33] 인-패션 데이터 세트에 대한 실험은 런타임이 감소된 기존 작품에 비해 SHINOBI로 더 나은 뷰 합성 및 재조명 결과를 보여준다. SAMURAI에 비해 결과는 더 선명해 보이고 평균 런타임은 반으로 절단됩니다. 그림. 1(오른쪽)은 SHINOBI에 의해 생성된 3D 자산을 사용한 일부 샘플 적용 결과를 보여준다. 우리의 표현은 외관 파라미터의 편집, 조명 및 메쉬 추출에 기초하여 다운스트림 그래픽 파이프라인의 다양한 작업을 용이하게 할 수 있다.\n' +
      '\n' +
      '2개의 관련 작업이요.\n' +
      '\n' +
      '**신경 필드***는 _e.g_의 네트워크 가중치에서 공간 정보를 인코딩하기 위해 늦게의 인기 있는 기술로 등장했다. 좌표[16, 49, 56, 69]를 간단히 질의함으로써 검색할 수 있는 MLP이다. NRF[50]와 같은 작품은 뷰 의존적 외관 변이로 광현실적 뷰 합성 결과를 달성하기 위해 이러한 신경 볼륨 렌더링을 레버리니다. 신경 분야의 신속한 연구는 [55, 71, 74, 75, 77, 80]의 표면 표현 [8, 32, 46, 54, 60, 70, 78]에서 재구성을 허용했으며 3D 기하학 및 재료[12, 37, 52, 83]를 추출하거나 [12, 13, 14, 43, 81, 5, 12] 장면을 재배치할 수 있었다. 그러나 대부분의 이전 작업은 COLMAP[64, 65]에서 추출한 포즈 정보에 의존하며, 이는 복잡한 설정이나 희소 데이터 체제에서 부정확하거나 완전히 실패할 수 있다. SHINOBI는 특징 매칭_에 의존하고 매우 _코크기 초기화_에 강력한 모든 _목적 재구성과 무관하다.\n' +
      '\n' +
      '** 인핸트 신경그래픽 프리티브***(I-NGP) [51]는 다중 해상도 해시 테이블을 기반으로 하는 인코딩 방식을 사용하여 메모리 활용이 향상된 빠른 최적화를 가능하게 하는 대중적인 기하학적 표현이다. 속도 향상에도 불구하고 I-NGP는 카메라 포즈 최적화[31, 91, 91]를 복잡하게 하는 해시 기반 인코딩을 통해 불연속적이고 진동하는 구배 흐름을 겪는다. 해시 그리드를 사용하여 카메라가 미세 조정되도록 하기 위해 허[31]는 보간 가중치에 수정을 제안하며, BAA-NGP[45]는 저해상도 특징을 동적으로 복제하고 CAMP[58]은 카메라 사전 조건을 사용하여 강력한 샘플링 방식[4] 쌍을 맺는다. 그러나 세메토드는 카메라 초기화 및 조명 조건에 민감합니다. 이러한 작품과 달리 SHINOBI는 _coarser re_를 지원하는 것 외에도 _varying 조명 및 배경_에서 캡처된 이미지로부터 일관된 객체를 재구성할 수 있다.\n' +
      '\n' +
      '**Joint 카메라 및 형상 추정***는 매우 모호한 작업이며 전통적으로 정확한 형상 재구성을 위한 정확한 포즈에 의존하며 그 반대의 경우도 마찬가지이다. 종종 기술은 카메라 포즈[64, 65]를 추정하기 위해 이미지 전반에 걸쳐 대응치에 의존한다. 최근의 접근법은 카메라 보정을 신경 부피 훈련과 통합하고, SCNeRF[34] 및 NopeNeRF[8]는 각각 대응 및 단안 깊이 이미지를 사용한다. 다른 최근의 방법은 카메라의 거친 초기화, 글로벌 정렬 또는 관절 최적화를 위한 템플릿 모양[15, 44, 76, 83]에 의존한다. 다른 방법은 변압기 기반 모델[23]을 사용하여 이미지 수집[67, 84]에서 초기 포즈를 예측한다. 이에 비해 SHINOBI는 다양한 카메라 파라미터와 객체 환경을 포함한 _unconstrained 이미지 수집_에서 작동하는데, 기존 방법은 깊이처럼 일반화하기 위해 고군분투하거나 추가 입력 데이터를 필요로 한다.\n' +
      '\n' +
      '**BRDF와 조명 추정***는 도전적이고 모호한 문제이다. 캐주얼 BRDF 추정은 간단한 카메라와 공동 위치된 카메라 플래시와의 현장 재료 획득을 가능하게 한다. 이러한 기술은 종종 단일 샷[2, 9, 20, 30, 40, 61], 소수의 샷[2] 또는 다중샷[3, 10, 21, 22, 26] 포획으로 문제를 평면 표면으로 제약한다. 캐주얼 포획은 또한 전체 장면 [41, 66]에서도 관절 BRDF 및 형상 재구성[53, 52, 52, 11, 11, 55, 61, 82]으로 확장될 수 있다. 그러나 이러한 방법의 대부분은 알려져 있는 활성 조명을 필요로 한다. 알려지지 않은 수동 조명 하에서 BRDF를 복구하는 것은 조명으로부터 BRDF를 무시해야 하기 때문에 훨씬 더 어려운 일이다. 최근 신경장 기반 분해는 다양한 조명[12, 13] 또는 고정 조명[43, 85, 86, 88, 89] 하에서 장면 분해를 달성했다. 내재적NeRF[81]는 단순화된 반사율 모델의 비용으로 더 큰 장면으로 분해를 확장한다. 그러나 이러한 모든 접근법은 알려져 있는 거의 완벽한 카메라 포즈가 필요한 반면, SHINOBI는 이미지당 조명을 복구하기 위해 _중단된 이미지 수집_와 함께 작동할 수 있다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      'SHINOBI의 목적은 2D 이미지 컬렉션을 최소 수동 작업으로 3D 표현으로 변환하는 것이다. 표현에는 형상, 재료 파라미터 및 뷰 내 조명 등이 포함되어 있어 반복이 있는 뷰 합성이 가능하다.\n' +
      '\n' +
      '***문제 설정, \\(C_{j}\\in\\mathbb{R}^{s_{j}\\t 3};j\\in\\{1,\\ldot,q\\}\\)의 수집으로 구두 데이터를 정의하며, 이는 잠재적으로 다양한 해상도 \\(s_{j}\\)를 갖는 상이한 배경, 조명 및 카메라로 캡처된 동일한 객체를 나타내는 \\(C_{j}\\(C_{j}\\) 이미지 \\(C_{j}\\(s_{j}\\) 이미지 \\(s_{j}\\(C_{j}\\-wild) 이미지 \\(s_{j}\\(s_{j})의 서로 다른 배경, 조명 및 카메라로 캡처된 것과 동일한 객체를 나타내는 \\(s_{j}Ifffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff 또한 거친 카메라 초기화를 가정합니다. 우리의 실험을 위해 우리는 카메라를 주석을 달면 SAMURAI[14]에서와 같이 사분면이 된다. 사용 가능하거나 자동으로 생성되어 이 시점에서 불완전할 수 있는 경우 산림 마스크를 추가할 수 있습니다. B_{mathb}\\b}\\) 금속성 \\(b_{mathb}\\b}\\in{b}\\math{R}\\)의 금속성 \\(b_{r}\\math{R}\\b}\\in{b}\\in{b}\\)의 BRDF 매개변수를 추정한다(b_{math{R}\\math{b}\\math{R}\\b}\\math{b}\\math{b}\\b}\\math{b}\\b}\\math{b}\\b}\\b}\\b}\\math{b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\ 분해를 가능하게 하기 위해 잠재 영상당 조명 벡터 \\(\\mathbf{z}_{j}^{l}\\in\\mathbb{R}^{128};j\\in\\{1,\\ldots,q\\}\\)[13]도 추정한다. 또한 이미지당 카메라 포즈 및 본질적으로 추정합니다. 다음으로, 전제 조건에 대한 간략한 개요: NeRF[50], 인판트NGP[51] 및 SAMURAI [14]를 제공한다.\n' +
      '\n' +
      '\\bf{x}\\mathb{R}^{3}\\) 및 뷰 방향 \\(\\mathbf{d}\\in\\mathb{d}\\in\\mathb{R}\\, NeRF*50])을 생성하기 위해 조밀한 신경망을 사용하여 뷰 의존적 출력 색상 \\(\\mathbf{d}\\in\\mathb{d}\\in\\mathb{d}\\in\\mathb{d}\\in\\mathbf{d}\\in\\in\\mathbf{d}\\in\\mathbf{d}\\in\\mathb{d}\\in\\mathb{d}\\in\\mathb{d}\\in\\mathb{d}\\in\\mathb{d}\\in\\mathb{d}\\in\\mathb{d}\\in\\mathb{d}\\in\\mathb{d}\\in\\b{d}\\in\\b{R}\\b{R}\\b{R}\\b{R}\\b{R}\\ 만데네힐 _et al_[50]]. 분광 편향은 스펙트럼 편향이다.\n' +
      '\n' +
      '그림 2: ** SHINOBI 파이프라인. 두 개의 해상도 가열냉각된 인코딩 가지, 다중해결 해시 격자 \\(H(\\mathbf{x})\\ 및 푸리에 임베딩 \\(\\gamma(\\mathbf{x})\\)를 사용하여 입력 좌표에서 조건화된 신경 부피를 학습한다. 이는 형태, 재료 및 조명과 공동으로 카메라 파라미터의 강력한 최적화를 가능하게 하고*****는 형상, 재료 및 조명과 공동으로 카메라 파라미터의 강력한 최적화를 가능하게 한다.\n' +
      '\n' +
      '두 번째 함수로 입력 좌표를 변환함으로써 MLP, \\(\\mathbb{R}\\)에서 \\(\\mathbb{R}^{2L}\\)로 매핑하는 \\(\\gamma\\)를 인코딩하는 주파수[50, 69]\n' +
      '\n' +
      '(2^{0}\\pi\\mathbf{x})\\cos(2^{L-1}\\pi\\mathbf{x})\\cos(2^{L-1}\\pi\\mathbf{x})\n' +
      '\n' +
      '**EEantNGP**[51]은 MLP 기반 볼륨 표현을 현재 GPU 하드웨어에 맞춘 다중 해결 복셀 해시 그리드로 대체함으로써 NeRF 최적화를 획기적으로 가속화한다. 해시 크기 \\(T\\)의 경우 그리드 정점들은 큰 고유 소수 \\(\\pi_{i}\\)를 사용하여 공간 해시 함수 \\(h(x)=\\left(\\빅포러스_{i=1}^{d}\\mathbf{x}_{i}\\pi_{i}\\right)\\ T\\)에 의해 인덱싱된다[51]. 각 복셀 정점에서 \\(d\\)차원 임베딩이 최적화되어 있다. 푸리에 임베딩 대신 3D 좌표 \\(\\mathbf{x}\\)는 각 수준에서 이웃 정점들 사이의 삼선형 보간에 직접 사용된다. 결과는 표현을 디코딩하기 위해 MLP에 연결 및 공급된다. 우리는 \\(H(\\mathbf{x})로 보간 및 접합을 포함한 전체 인코딩 기능을 나타낸다.\n' +
      '\n' +
      'SAMURAI** SAMURAI의** 짧은 개요는 3D 형상, BRDF, 이미지당 카메라 파라미터 및 주어진 손목 이미지 컬렉션에 대한 조명의 공동 최적화를 위한 방법이다. SAMURAI[14]는 위에서 설명한 NeRF 아이디어를 따르지만 물리적 기반 상이한 렌더링을 위해 신경-PIL[13] 방법을 사용한다. 3D 위치를 입력으로 하고 부피 밀도 및 BRDF 파라미터를 출력한다. 추가 GLO(생성 잠재 최적화) 임베딩 모델은 이미지를 가로질러 출현(다른 조명으로 인한)의 변화를 모델링한다. 신경-PIL[13]은 이미지당 잠재 조명 임베딩 \\(\\mathbf{z}_{j}^{l}\\) 및 고속 렌더링을 위한 전문 조명 사전 통합(PIL) 네트워크의 사용을 도입했으며, 이는 우리가 \'PIL 렌더링\'이라고 한다. 신경-PIL은 이미지별 조명을 모델링하기 위해 이미지당 임베딩을 최적화한다. 렌더링된 출력 색상 \\(\\mathbf{\\hat{c}}\\)는 NeRF의 출력 \\(\\mathbf{c}\\)에 해당하지만 명시적인 BRDF 분해 및 조명 모델링으로 인해 재조명 및 재료 편집이 가능하다. 가벼운 이미지에 대한 정확한 카메라 파라미터의 불가능성을 해결하기 위해 SAMURAI는 매우 거친 초기화로부터 카메라 압출 및 뷰 내관을 공동으로 최적화한다. 이는 일반적인 가열냉각 [44] 외에도 시간 경과에 따른 손실에서 성능에 따라 뷰당 다수의 카메라 제안을 유지하고 가중하는 다중 최적화 방식으로 달성된다.\n' +
      '\n' +
      'HHINOBI 코딩으로 최적화합니다.\n' +
      '\n' +
      '우리는 잘못 정렬되고 일관되지 않은 카메라 포즈를 복잡한 재구성의 주요 제한 요소로 식별한다. 공동 형태와 카메라 최적화는 심각한 과소 결정된 문제이다. 재건축은 일반적으로 느리고 질감과 모양에 고주파 디테일이 부족한 경우가 많다. 인판-NGP[51]의 다중해결 해시 그리드는 재구성 속도를 높일 수 있는 동시에 더 큰 광선 카운트를 처리하고 이에 따라 시각적 품질과 정렬을 향상시킬 수 있다(Tab. 1 참조). 그러나 Hash 그리드로 부호화하는 점들의 순진한 대체는 관절 카메라의 재구성 품질과 견고성을 감소시키고 형상 최적화를 감소시킨다.\n' +
      '\n' +
      '해시 그리드는 개별 뷰에 더 빠르게 적응하여 오작동된 카메라가 있는 상태에서 시끄러운 모양을 만든다. 앞서 보고된 바와 같이, 입력 위치에 대해 기본 선형 보간 역프로파이트 시끄럽고 불연속 구배를 갖는 42, 45, 91] 다중 해상도 해시 그리드가 있다. 또한, 카메라 미세 조정에 자주 사용되는 BARF[44]의 거친 간 방식은 해시 그리드로 직접 전달될 수 없다. 따라서 카메라 멀티플렉스를 사용하여 추가 기하학적인 제약을 추가하고 재구성 속도와 품질을 모두 향상시킬 수 있는 새로운 인코딩 방식을 제공하는 접근법을 제안한다. 다음으로 각 구성 요소를 자세히 설명합니다.\n' +
      '\n' +
      '** 건축 개요** SHINOBI 아키텍처의 고수준의 개요** A 고수준 개요는 그림 1에 나와 있다. SAMURAI[14]의 골격을 \'PIL 렌더러\' [13]로 따르는 2. 그러나 새로운 하이브리드 인코딩을 사용하여 입력 좌표 \\(\\mathbf{x}\\)를 매핑한다. 결합된 임베딩은 I-NGP[51]에서와 같은 작은 MLP에 의해 처리되어 밀도 \\(\\sigma\\)를 예측하고, 주어진 이미지 패치에 대한 뷰 및 외관이 조건화된 방사선을 예측한다. 또한 [12, 14]에서와 같이 초기 훈련 단계를 안정화시키기 위해 규칙적인 방향 의존적 방사도 \\(\\mathbf{\\tilde{c}\\)를 예측한다. BRDF 디코더는 SAMURAI[14]에서와 같이 동작하며, 특징 표현을 BRDF(베이스 색상, 금속성, 거칠기)로 확장한다. 샘플로, 우리는 밀도 w.r.t. 입력 위치 \\(\\frac{\\partial\\sigma}{\\부분\\mathbf{x}\\\\)의 첫 번째 순서 유도체에서 정상적인 방향을 추정한다. NRF[50]로부터의 체적 렌더링이 수행되고 주어진 픽셀 좌표에 대한 음영이 BRDF, 정규 및 신경PIL 네트워크에 의해 추정된 사전 통합 조명을 사용하여 결정된다. 건축물에 대한 자세한 내용은 보충 자료를 참조하세요.\n' +
      '\n' +
      '**Camera 포즈 초기화 및 매개변수화** 카메라 포즈 최적화는 매우 비코브렉스 문제이며 지역 최소에 빠르게 갇히는 경향이 있다. 우리의 초기 카메라 포즈는 많은 관련 작품[37, 76]에 비해 초기 포즈와 진정한 포즈 사이의 더 큰 거리를 가지고 있다. 이를 해결하기 위해 SAMURAI[14] 및 NeRS[83]에 따라 카메라 형태의 거친 초기화가 사분면을 갖는다고 가정한다. 카메라 매개변수에 대한 \'회색 + 방향\' 표현을 사용하여 아이 위치(\\mathb{p}{eathb}{eathb}{R}\\\\)에 대한 초기 값과 오프셋(d_{eathb{R}\\\\b}\\\\)을 저장하고 회전 길이(\\mathb{R}\\\\)를 회전 방향(\\mathb{R}^{R}\\in\\b}\\)에 대한 초기 값과 오프셋(\\mathb}\\mathb}\\b}\\b}\\b}\\i.{eathb}\\b}\\)에 대해 초기 값을 저장하고 회전 거리(d_{eathb}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\)로 인코딩한 회전 방향,\\b}\\b}\\b}\\b}\\b}\\)으로 인코딩된 회전 방향,\\b}\\b}\\b}\\b}\\b}\\b}\\)에 대한 오버파라미터,\\b}\\b}\\b}\\b}\\b}\\)의 초점 거리(\\in\\b}\\b} 이 제형은 최근에 제안된 다른 표현[58, 90]과 비교하여 설정에서 가장 잘 수행된다.\n' +
      '\n' +
      '**하이브리드 위치 인코딩*** 우리는 구배 흐름 w.r.t.에 투입된 좌표 \\(\\mathbf{x}\\)를 개선하기 위해 해시 그리드 하이브리드를 좌표 인코딩으로 사용한다. 작은 MLP는\\(\\gathbf{x})에 이어 다중해결 해시 격자 \\(H(\\mathbf{x})의 출력과 연결된 염기 임베딩을 생성하여 신경 부피 \\(F_{\\oplus}\\left(H(\\mathbf{x}),\\gamma(\\mathbf{x}),\\gathbf{x})\\의 다음 제형을 생성한다. \\(\\gamma\\)에서는 BARF의 [44] 푸리에 어닐링을 적용한다. 유사하게, 우리는 해시 그리드 인코딩에 해상도 수준을 점진적으로 추가한다. 낮은 해상도 조밀한 격자로부터의 특징만을 시작으로 시간이 지남에 따라 더 높은 해상도 수준의 가중치를 점차 증가시킨다(cf[42, 45).\n' +
      '\n' +
      '**Camera 다중**** 카메라 포즈 최적화가 로컬 최소에 끼울 가능성을 줄이는 효과적인 방법은 카메라 멀티플렉서[27, 14]이다. 각 이미지에 대해 \\(m\\) 카메라는 초기 카메라를 중심으로 재팅되고 동시에 최적화된다. 시간이 지나면서 최악의 공연 카메라가 \\(m=1\\)까지 반복 퇴색된다. 이 과정은 그림 3에서 시각화되어 있으며, 어차피 주어진 이미지에 대한 여러 제안을 제공하기 때문에 프로젝티브 기하학을 사용하여 최적화를 더욱 제약할 수 있는 기회를 본다. 구체적으로, 우리는 체적 렌더링에서 추정된 깊이 \\(D_{i}\\)를 사용하여 다중체의 현재 최고 순위 카메라 \\(\\Theta_{0}\\)에 \\(m-1\\) 구성체가 렌더링하는 점 세트(X_{i}\\)를 투영한다. 그런 다음 \\(\\Theta_{0}\\)를 사용하여 예상 좌표를 렌더링하고 렌더링된 색상과 알파 값을 원래 \\(\\Theta_{1\\dots m-1}\\)에서 렌더링된 것과 비교한다.\n' +
      '\n' +
      '}} (P_\\math{i})\\mathcal{i} (P_\\mathcal{i}, P_\\math{i})\\mathcal{i} (P_\\math{i},\\mathcal{i}.\n' +
      '\n' +
      'HPA(P_{i,0}\\)가 카메라 \\(i\\)의 이미지 좌표에서 기준 카메라까지의 시각 와프이다. (F_{\\mathcal{V}}\\)는 컬러 \\(\\hat{c}\\) 및 마스크 값 \\(\\alpha\\)을 출력하는 신경장과 연결된 렌더링 함수이다. 이 규칙화는 멀티플렉스에 카메라를 추가하는 데 드는 비용으로 대략적으로 제공됩니다. 필요하면 \\(X_{i}\\)의 하위 샘플링이 기억 발자국을 감소시킬 수 있다. \\(X_{i}\\) (\\mathcal{L}_{\\mathrm{ 이미지}}\\) 및 \\(\\mathcal{L}_{\\mathrm{mask}}}\\)는 Sec 3.2에 설명된 대로 당시 활성인 최적화 손실이 있는 동안만 활성이다. 추가 손실로 사용된 것은 카메라 최적화를 제약하는 데 놀라울 정도로 효과적이어서 전체 최적화의 견고성을 높이는 것으로 나타났다. 본질적으로, 우리는 초기 카메라 포즈를 중심으로 생성되고 최적화 풍경을 매끄럽게 하기 위해 일관된 표면을 구현하고 있다.\n' +
      '\n' +
      '**View 입력 이미지의 중요도 스케일링.** 모든 입력은 동일한 방식으로 재구성하는 데 기여할 수 있으며 현재 3D 형상과 정렬되지 않은 개별 뷰는 전체 최적화 진행에 부정적인 영향을 미칠 수 있다. 재건축에서 고주파 디테일을 개선하기 위해 손실로 잘 작동하는 카메라를 사용하여 최적화를 고정하면서 잠재적으로 잘못 정렬된 카메라의 영향을 줄입니다. 최근 이미지당 손실로 약 1000개 요소의 원형 버퍼를 유지합니다. SAMURAI와 마찬가지로 이는 주어진 컬렉션의 이미지(\\mathcal{L}_{\\mathrm{p}}_{\\mathcal{L}_{\\mathrm{j})에 따라 이미지를 재조명하는 데 사용된다.\n' +
      '\n' +
      '종종(\\frac{\\_{l})}(\\mathcal{L}_{\\mathcal{L}}}{\\{{(j){\\mathrm{L}}}}}{\\{{{(j){\\{L}}}}}.\n' +
      '\n' +
      '손실 완충액의 평균 \\(\\mu_{l}\\) 및 표준 편차 \\(\\sigma_{l}\\)와 함께. 이는 나쁜 정렬된 카메라 포즈가 형상 재구성에 미치는 영향을 제한한다. 또한 손실 이력을 감안할 때 잘 수행하고 있는 견해의 구배 크기를 줄이는 \\(\\mathcal{L}_{\\mathrm{camera}}\\)에 대한 중요 가중치를 적용한다. 특히, 단계(\\)에서 우리는\\(\\mathcal{L}_{\\mathrm{L}_{\\mathrm{L}_{\\mathrm{camera})를 산출한다.\n' +
      '\n' +
      '\\[s_{\\texttt{q}_{j,t}= s_{\\texttt{q}_{j,t-1}}\\lambda_{p}\\max\\= s_{\\texttt{q}_{j,t-1}}\\lambda_{p}\\max\\) 금! H.\\.\\.\\.\\. atanh\\! 종종(\\frac{ \\mu_{l}-(\\mathcal{L}_{\\mathrm{L}}}^{{(j)}+\\mathcal{L}_{\\mathrm{L}}_{\\mathrm{L}}}^{{{(j)}}{\\sigma_{l}}\\ 오른쪽).+\\. 1,1,1\\! \\[+(1-\\lambda_{p}) <{\\texttt{q}_{j,t-1}} \\tag{4}\\]]]\n' +
      '\n' +
      '실제로 우리는 하이퍼파라미터 \\(\\lambda_{p}\\)를 0.05로 설정했다.\n' +
      '\n' +
      '## 로스와 Optimizationization### Losses 및 Optimizationes.\n' +
      '\n' +
      '** 다단계 패치 손실***입니다. 무작위 광선 샘플링의 짧은 초기 단계 후에 업데이트와 무작위 광선 샘플링의 짧은 초기 단계 후에 무작위로 샘플링된 16x16에서 32x32의 패치를 제약하는 것이 목표다.\n' +
      '\n' +
      '그림 3: ** 구축 카메라 멀티플렉서** 우리는 이미지당 여러 개의 카메라 제안을 최적화하고 손실량에 대한 카메라의 성능에 따라 재구성 기여도를 가중한다. 멀티플렉스의 카메라 사이에는 프로젝션 기반 규칙화를 추가하면 모든 멤버의 포인트들이 현재 최고의 카메라에 투사된 다음 일관된 기하학을 실행하기 위해 새로운 렌더링과 비교된다.\n' +
      '\n' +
      '특히 지역 동네에서 일치해야 할 정렬입니다. 따라서 단순 빌리네르 리샘플링에 의해 4가지 다른 분해능 수준에서 탄소의 손실을 구성하는 렌더링된 색상 \\(\\hat{c}\\)에 다중 스케일 패치 손실을 추가한다. 우리는 다른 픽셀 카운트를 보완하기 위해 각 레벨을 측정하고 먼저 정렬하기 위해 저해상도 버전을 실행한다.\n' +
      '\n' +
      '*** 마스크 손실** 우리는 패치 기반 샘플링이 활성화될 때마다 실루엣 손실 \\(\\mathcal{L}_{\\mathrm{ 은우엣}}\\)를 추가한다. 여기서 렌더링 및 입력 마스크[38]에 대한 \\(xor\\) 연산의 결과로 해석할 수 있는 두 실루엣 사이의 면적을 처벌한다. 두 마스크 모두 패치 크기를 기준으로 반경이 휴리스틱하게 선택되는 가우시안 블러링을 사용하여 필터링된다. 그림. 4는 손실이 정렬 작업에 어떻게 도움이 되는지를 시각화한다. 우리는 이 손실을 마스크 값에 대한 규칙적인 이진-교차 손실뿐만 아니라 투명한 배경을 구현하는 손실을 결합합니다.\n' +
      '\n' +
      '** 규칙화 손실***. 우리는 [4]에서 제안한 바와 같이 정규화된 체중 붕괴를 적용하여 순진한 체중 붕괴에 비해 콥저 그리드 수준에 더 높은 벌칙을 가한다. 또한, 우리는 카메라 포즈 및 정상 출력에 규칙화를 적용합니다. 자세한 내용 및 사용된 하이퍼모수용 보충제를 참조하세요.\n' +
      '\n' +
      '**Optimization***, 총 3개의 최적기를 사용하여 네트워크, 해시 그리드 임베딩 및 카메라에 대해 각각 1개의 ADAM[36] 최적화기를 사용한다. 학습률은 모든 최적화기에 기하급수적으로 부패합니다. 카메라 최적화기의 구성 위에서 언급한 카메라 표현 및 제약 외에도 해시 그리드와 관련하여 성공에 중요한 것으로 나타났다. 우리는 카메라 업데이트에서 소음을 매끄럽게 하기 위해 0.2로 감소된 \\(\\beta 1\\) 값을 갖는 ADAM을 사용한다. 학습율은 장면 크기에 따라 1e-3~2e-3 사이로 조정된다. 또한, 최적화 일정을 원활하게 전환하기 위해 3개의 페이딩 \\(\\lambda\\) 변수를 사용한다. 활성 멀티플렉서 카메라의 수가 감소하는 동안 최적화 전반부에 걸쳐 우라더 해상도가 지속적으로 증가한다. 직접 색상 최적화는 BRDF 최적화로 퇴색되고 부호화 어닐링은 최적화의 제1 제3에 걸쳐 수행된다. 초점 길이 업데이트와 시야 중요도 가중치는 초기 형상이 형성될 때까지 지연된다. 최적화 스케줄링의 상세한 설명과 시각화를 위한 보충 자료를 참조한다.\n' +
      '\n' +
      '** 구현*** 우리는 Tensorflow[1]에 대한 맞춤형 CUDA 확장으로서 다중 해상도 해시 그리드를 구현한다. 시행은 대략 공식 CUDA 구현[51]을 따른다. 우리는 분석 표면 정규화를 컴퓨팅할 수 있도록 인코딩을 위한 1차 및 2차 구배를 가능하게 한다. 나머지 구성 요소는 텐서 흐름으로 구현됩니다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '***Dataset** 평가를 위해 여러 모바일 장치를 사용하여 다양한 환경에서 포획된 객체들을 특징으로 하는 NAVI 데이터세트[33]의 길쭉한 컬렉션을 사용한다. 고품질 주석이 달린 카메라 포즈로 포즈 추정에 대한 정량적 평가를 납치하고 수행할 수 있습니다.\n' +
      '\n' +
      '** 기본*** Sec에서 우리의 과제 윤곽을 다룰 수 있는 가장 가까운 이전 작업입니다. 3은 우리의 방법을 기반으로 하는 SAMURAI [14]이다. 우리는 SAMURAI와 기준선으로 비교하고 NeROIC[37], GNeRF[48], 수정된 버전의 NeRS[83](보충제의 세부)를 사용하여 실험을 수행한다. 관절 모양과 포즈 추정에 대한 실험을 위해 NeRS, SAMURAI 및 SHINOBI(계절)에 대해 동일한 사분면 기반 포즈 초기화를 사용하고 NeROIC(COLMAP) 및 GNeRF(Random)에 대한 방법의 기본 포즈를 초기화하는 방법을 사용한다.\n' +
      '\n' +
      '*** 평가** 우리는 평가를 위해 두 가지 전략을 사용한다. 먼저, 보유 시험 영상에서 PSNR, SSIM 및 LPIPS [87] 점수를 측정하는 학습된 부피를 사용하여 표준 신규 뷰 합성 메트릭을 측정합니다. 두 번째로 카메라 포즈 w.r.t GT 포즈 w.r.t GT 포즈를 평가하기 위해 Procrustes 분석 [28]을 사용하여 프로스테스테스 분석을 사용하여 2회째를 정렬하여 카메라 포즈 w.r.t GT 포즈를 평가한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} Method & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & \\multicolumn{2}{c}{LPIPS\\(\\downarrow\\)} & Runtime \\\\ \\hline \\(S_{C}\\) & \\(\\sim S_{C}\\) & \\(\\sim S_{C}\\) & \\(\\sim S_{C}\\) & \\(\\sim S_{C}\\) & \\(\\sim S_{C}\\) \\\\ \\hline NeROIC [37] & 22.75 & 21.31 & 0.91 & 0.90 & 0.0984 & 0.0845 & 18 hours (4 GPUs) \\\\ NeRS [83] & 17.92 & 18.02 & 0.93 & 0.93 & 0.114 & 0.198 & 3 hours (1 GPU) \\\\ SAMURAI [14] & 25.34 & 24.61 & 0.92 & 0.91 & 0.0958 & 0.1054 & 12 hours (1 GPU) \\\\ SHINOBI & **27.69** & **27.79** & **0.94** & **0.94** & **0.067** & **0.957** & 4 hours (1 GPU) \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: NAVI** 뷰 합성 메트릭에 대한 뷰 합성을 위한 ** 메트릭은 COLMAP(\\(S_{C}\\) / \\(\\sim S_{C}\\)의 성공에 따라 모든 야생형에서 두 개의 하위 집합에 대해 계산된다. 접점 품질은 테스트 뷰의 홀드아웃 세트에 대해 평가됩니다. NAVI[33]에서 제공하는 GT 포즈로 초기화한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} Method & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & Transl.\\(\\downarrow\\) & Rot. \\({}^{\\circ}\\)\\(\\downarrow\\) \\\\ \\hline w/o Multiplex Consistency Loss & 25.80 & **0.93** & **0.29** & 23.12 \\\\ w/o Per View Importance & 22.43 & 0.90 & 0.36 & 35.10 \\\\ w/o Coarse-to-fine (annealing) & 21.47 & 0.90 & 0.37 & 30.44 \\\\ w/o Hybrid Encoding & 25.31 & **0.93** & 0.30 & 23.33 \\\\ w/o Patch-based Training & 20.60 & 0.89 & 0.45 & 41.30 \\\\\n' +
      '**Full** & **25.87** & **0.93** & 0.30 & **22.90** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: ** 삭제 연구** 프레임워크의 구성 요소를 조작하면 더 나쁜 시각 합성 및 재배치 결과(NAVI의 "키서북" 및 "학교 버스" 장면으로 대체됨)가 그 중요성을 보여준다.\n' +
      '\n' +
      '그림 4: ** 우리의 실루엣 기반 정렬 손실**는 참조 및 렌더링된 회색 스케일 마스크를 제공하는 정렬되지 않은 픽셀을 처벌한다.\n' +
      '\n' +
      '카메라를 추출한 다음 카메라에서의 평균 절대 회전 및 번역 차이를 계산하면 사용 가능한 모든 뷰에 대한 추정치가 된다. 평가 목적을 위해 테스트 이미지에 대한 카메라와 조명을 최적화할 수 있지만 테스트 이미지가 다른 네트워크 부품 또는 해시 그리드 임베딩에 영향을 미치지 않는다. 공정한 비교를 위해 우리는 모든 방법에 대한 입력으로서 그라운드 진리 마스크를 사용하지만, 우리의 방법에는 자동 분할 마스크를 생성하는 기능성도 포함된다. 우리는 장면당 단일 Nvidia A100 또는 V100 GPU에서 실험을 실행한다.\n' +
      '\n' +
      '*** 결과** Tab. 1은 NAVI의 GT 포즈 사용 시 일정 내 재구성을 위한 다양한 방법의 성능을 보여준다. NAVI [33] 후, 우리는 COLMAP가 작동하는지 여부(\\(S_{C}\\))를 기반으로 장면들을 두 개의 하위 집합으로 나누고(\\(\\(\\,\\ S_{C}\\))를 NeROIC와 같은 일부 기술은 사용되지 않은 이미지 수집품에 대한 COLMAP가 필요하기 때문에). 제공된 주석이 달린 포즈(SHINOBI)를 사용하여 뷰 합성 작업(Tab. 1)에 대해 분명히 최선을 다한다. 이것은 우리의 하이브리드 인코딩 방식과 길쭉한 장면에 대한 이전 방법에 대한 패치 기반 손실의 이점을 보여준다. 다른 기술의 최적화 실행은 우리가 차세대 SAMURAI 접근보다 3배 더 빠르다는 것을 보여준다.\n' +
      '\n' +
      '타브. 2는 GT 카메라 포즈가 입력으로 제공되지 않을 때 관절 모양 및 길쭉한 이미지 컬렉션으로부터의 최적화 결과를 보여준다. SHINOBI는 SAMURAI로 일치하면서 NeROIC와 NeRS를 모두 건강한 마진으로 능가한다. SHINOBI의 PSNR은 SAMURAI와 유사하지만, 우리의 방법은 번역 및 회전 포즈 오류(포즈 메트릭의 표준 편차 또한 낮음)로 장면을 일관되게 재구성할 수 있다. 이것은 SHINOBI가 SAMURAI에 비해 더 나은 LPIPS 지각 메트릭을 얻는 결과를 초래한다. SAMURAI와 비교하여 온-파 평균 PSNR은 대부분 개별 테스트 카메라에서 제대로 정렬되지 않았다. 이것은 또한 다른 방법에 대해 발생하지만 SHINOBI에서 더 빠른 최적화 스케줄링에 의해 강조되는 것으로 판단된다. NeROIC는 또한 카메라 포즈가 그라운드 진리에 가깝지만 COLMAP 기반 초기화가 불가능한 많은 장면에서 실패할 경우 좋은 결과를 얻을 수 있다. NeRS는 또한 모든 장면을 재구성하는 데 성공합니다. 그러나, 낮은 품질의 카메라 정렬을 달성합니다. 그림. 7은 다양한 방법의 뷰 합성 결과를 시각적으로 비교하며, 이는 SHINOBI가 입력 이미지에 더 충실한 더 선명한 결과를 생성할 수 있음을 시각적으로 확인시켜준다. NAVI 데이터 세트에 대한 추가 결과는 그림 1에 나와 있다. GT 포즈 또는 거친 사분면으로 초기화된 SHINOBI에 의해 예측된 새로운 견해를 보여주는 6. 시각적 결과는 SHINOBI가 포즈를 회수하고 두 설정 모두에서 일관된 조명 w.r.t 지상-진실 목표 뷰를 제공할 수 있음을 분명히 보여준다.\n' +
      '\n' +
      '** 결정 결과** 그림 5는 동일한 출력 방식을 사용할 수 있는 SAMURAI로 SHINOBI의 BRDF 및 조명 분해를 비교한다. 비공식 결과는 시각적 결과를 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c c c c c c c c} \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{Pose Imit} & \\multicolumn{2}{c}{PSNR\\(\\uparrow\\)} & \\multicolumn{2}{c}{SSIM\\(\\uparrow\\)} & \\multicolumn{2}{c}{LPIPS\\(\\downarrow\\)} & \\multicolumn{2}{c}{Translation\\(\\downarrow\\)} & \\multicolumn{2}{c}{Rotation\\({}^{\\circ}\\downarrow\\)} \\\\ \\cline{3-13}  & & \\(S_{C}\\) & \\(\\sim S_{C}\\) & \\(S_{C}\\) & \\(\\sim S_{C}\\) & \\(S_{C}\\) & \\(\\sim S_{C}\\) & \\(S_{C}\\) & \\(\\sim S_{C}\\) & \\(S_{C}\\) & \\(\\sim S_{C}\\) & \\(S_{C}\\) & \\(\\sim S_{C}\\) \\\\ \\hline GNeRF [48] & Random & 8.30 & 6.25 & 0.64 & 0.63 & 0.52 & 0.57 & 1.02\\(\\pm\\) 0.16 & 1.04\\(\\pm\\) 0.09 & 93.15\\(\\pm\\) 26.54 & 80.22\\(\\pm\\) 27.64 \\\\ \\hline NeROIC [37] & COLMAP & 19.77 & - & 0.88 & - & 0.150 & - & 0.09\\(\\pm\\) 0.12 & - & 42.11\\(\\pm\\) 17.19 & - \\\\ NeRS [33] & Directions & 18.67 & 18.66 & **0.92** & **0.93** & 0.108 & 0.107 & 0.49\\(\\pm\\) 0.21 & 0.52\\(\\pm\\) 0.19 & 122.41\\(\\pm\\) 10.61 & 123.63\\(\\pm\\) 8.80 \\\\ SAMURAI [14] & Directions & **25.34** & 24.61 & **0.92** & 0.91 & **0.996** & 0.105 & **0.22\\(\\pm\\) 0.17** & 0.35\\(\\pm\\) 0.24 & 26.16\\(\\pm\\) 22.72 & 36.59\\(\\pm\\) 29.98 \\\\ \\hline SHINOBI & Directions & 25.15 & **24.77** & **0.92** & 0.92 & **0.900** & **0.095** & \\(0.250\\pm\\) 0.08 & **0.28\\(\\pm\\) 0.09** & **22.84\\(\\pm\\) 16.19** & **33.04\\(\\pm\\) 19.97** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 3D 형상에 대한 **메타 메트릭과 NAVI** 뷰 합성에 대한 포즈를 취하고 COLMAP(\\(S_{C}\\) / \\(\\sim S_{C}\\)의 성공에 따라 모든 야생 집합의 두 하위 집합에 대해 메트릭을 내린다. 점화 품질은 형태 회복에 기여하지 않고 최적화의 일부로 정렬되는 테스트 뷰의 홀드아웃 세트에 대해 평가된다. 우리는 이 방법이 다중 결합 데이터를 위해 설계되지 않았지만 별도의 기준선으로서 GNeRF를 포함한다. 우리는 방법의 기본 카메라 초기화 및 NAVI[33]에 제공된 주석에 대한 평가를 사용하여 메트릭을 보고한다.\n' +
      '\n' +
      '그림 5: ** SAMURAI 분해와 개선된 정렬 및 표현 더 높은 주파수 세부 사항에 대한 SAMURAI 분해와 비교하면 SAMURAI에 비해 모양과 BRDF 성분이 재구성된다. 개선된 질감 디테일과 실루엣을 알려줍니다.\n' +
      '\n' +
      'SAMURAI에 비해 SHINOBI를 사용한 훨씬 더 고주파 상세하고 그럴듯한 물질 매개변수가 있다.\n' +
      '\n' +
      '*** 삭제 연구*** 중간 복잡성의 NAVI [33]에서 두 가지 인-팬드 세트인 "키 서부와 "스쿨 버스"를 사용하여 재구성 메트릭 측면에서 SHINOBI의 다양한 측면을 증폭한다. 타브의 메트릭. 3은 해상도 어닐링 조대-미세 방식 및 패치 기반 손실이 최종 품질에 가장 크게 기여한다는 것을 보여준다. 후자는 간단한 픽셀별 손실에 비해 지역 내역과 등록 정확도를 향상시킨다. 뷰 중요도 가중치는 향상된 선명도를 위한 또 다른 중요한 요소이다. 초기 해상도 어닐링 일정이 끝난 후 최적화를 안정화하는 데 도움이 됩니다. 하이브리드 인코딩 및 카메라 다중 정합성은 양적으로 큰 영향을 미치지 않는 것으로 보이지만, 서로 다른 장면 유형 및 스케일에 대한 최적화를 안정화시키는 데 중요한 역할을 한다. 그것들이 없으면, 최적화는 초기화에 따라 더 오래 걸리거나 전혀 수렴되지 않을 수 있다. 상기 특정 연마물의 시각적 예들을 상기 보충 재료들에서 비교한다.\n' +
      '\n' +
      '**Application** NeRF [50] 표현을 사용한 새로운 뷰 합성 외에도 모수 재료 모델은 물체의 외관을 제어한 편집을 허용한다. 또한, 사실적인 합성물에 대해 예를 들어 조명을 조절할 수 있습니다. 메쉬 추출은 실시간 렌더링을 포함한 표준 그래픽 파이프라인의 추가 편집 및 통합을 허용한다. SHINOBI는 엔터테인먼트와 교육을 위한 3D AR과 VR뿐만 아니라 전자 상거래 응용을 위한 재발 가능한 3D 자산을 얻는 데 도움을 줄 수 있다. 재발, 재료 편집 등에 대한 샘플 시각 결과에 대한 보충 자료를 참조하세요.\n' +
      '\n' +
      '** 한계** 공동 포즈와 형상 재구성은 본질적으로 불치적인 문제이다. SHINOBI는 이전 작업에 걸쳐 개선되지만 특히 대칭적인 물체 및 고도로 사구체적인 물질은 그림 8과 같이 실패 케이스로 이어질 수 있지만, 거친 투-미세 방식은 불면성을 해결할 수 없으며 카메라 포즈가 국부적으로 최소에 끼어 있다. 기존의 모든 방법들은 이러한 한계를 어느 정도 보여주고 있다. 일부 지역에서는 여전히 잘못 정렬된 뷰와 조명 표현[13]의 제한된 고주파 능력으로 인해 고주파 디테일이 제대로 재구성되지 않는다. 또한, 우리의 BRDF 및 조명 분해는 그림자 및 반사 간 모델링을 할 수 없다. 주로 단일 객체 분해에 관한 것으로 그림자와 성찰 간 성찰은 중요하지 않다. 이 방법을 보다 복잡한 광 수송 모델링으로 확장하면 중요한 미래 작업이 형성된다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '우리는 구성되지 않은 인-패션 이미지 수집에서 물체들의 형상, 포즈 및 조명 추정을 위한 프레임워크인 SHINOBI를 제시한다. 당사의 새로운 하이브리드 해시 그리드 인코딩은 다진화 해시 그리드를 사용하여 더 쉬운 카메라 포즈 최적화를 가능하게 합니다. 또한, 뷰당 중요도 가중치 및 패치 기반 정렬 손실과 함께 카메라 매개변수의 선택은 더 나은 이미지 대 3D 정렬을 가능하게 하여 고주파 세부 정보를 사용하여 더 나은 재구성을 가능하게 한다. SHINOBI는 어떤 범주에서도 물체의 기하학을 회복할 수 있지만, 그 성능은 얇은/투명 구조로 제한되며 극단적인 조명 변화 하에서 고주파 세부 사항을 복구하지 못하며, 이는 향후 작업에 대한 탐사로 남겨두고 있다.\n' +
      '\n' +
      '그림 8: **F 실패 사례.** 고도로 대칭적인 객체 또는 균질한 표면을 특징으로 하는 제한되지 않은 이미지 수집은 여전히 도전을 제기하고 잠재적으로 추가적인 지원을 필요로 한다.\n' +
      '\n' +
      '카메라로 초기화된 모델을 사용하여 SHINOBI의 NAVI*** Renderings에 대한 그림 6: **View 합성은 입력 이미지에 비해 NAVI[33]가 제공하는 4중 또는 GT를 제공한다.\n' +
      '\n' +
      '그림 7: ** 노블 뷰 합성은 NAVI [33] 인-팬드 테스트 세트의 예시적 관점에서 다른 방법에 비해**와 비교하여, SHINOBI는 미세한 디테일을 보존하고 조명을 현실적으로 재현한다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '이 작업은 독일의 우수 전략 - EXC 번호 2064/1 - 프로젝트 번호 390727645 및 SFB 1233, TP 02 - 프로젝트 번호 276693517에 따라 도이체 포스충스게린차프트(DFG, 독일 연구 재단)가 부분적으로 자금을 지원했으며 독일 연방 교육 연구 센터, FKZ: 01IS18039A의 지원을 받았다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, and Andy Davis et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.\n' +
      '* [2] Miika Aittala, Timo Aila, and Jaakko Lehtinen. Reflectance modeling by neural texture synthesis. _ACM TOG_, 2018.\n' +
      '* [3] Rachel Albert, Dorian Yao Chan, Dan B. Goldman, and James F. O\'Brian. Approximate svBRDF estimation from mobile phone video. _Eurographics Symposium on Rendering_, 2018.\n' +
      '* [4] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields. _ICCV_, 2023.\n' +
      '* [5] Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan Sunkavalli, Milos Hasan, Yannick Hold-Geoffroy, David Kriegman, and Ravi Ramamoorthi. Neural reflectance fields for appearance acquisition. _arXiv_, 2020.\n' +
      '* [6] Sai Bi, Zexiang Xu, Kalyan Sunkavalli, Milos Hasan, Yannick Hold-Geoffroy, David Kriegman, and Ravi Ramamoorthi. Deep reflectance volumes: Relightable reconstructions from multi-view photometric images. _ECCV_, 2020.\n' +
      '* [7] Sai Bi, Zexiang Xu, Kalyan Sunkavalli, David Kriegman, and Ravi Ramamoorthi. Deep 3d capture: Geometry and reflectance from sparse multi-view images. _CVPR_, 2020.\n' +
      '* [8] Wenjing Bian, Zirui Wang, Kejie Li, Jiawang Bian, and Victor Adrian Prisacariu. Note-nerf: Optimising neural radiance field with no pose prior. _CVPR_, 2023.\n' +
      '* [9] Mark Boss and Hendrik P.A. Lensch. Single image brdf parameter estimation with a conditional adversarial network. _arXiv_, 2019.\n' +
      '* [10] Mark Boss, Fabian Groh, Sebastian Herholz, and Hendrik P. A. Lensch. Deep Dual Loss BRDF Parameter Estimation. _Workshop on Material Appearance Modeling_, 2018.\n' +
      '* [11] Mark Boss, Varun Jampani, Kihwan Kim, Hendrik P.A. Lensch, and Jan Kautz. Two-shot spatially-varying BRDF and shape estimation. _CVPR_, 2020.\n' +
      '* [12] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, and Hendrik P.A. Lensch. NeRD: Neural reflectance decomposition from image collections. _ICCV_, 2021.\n' +
      '* [13] Mark Boss, Varun Jampani, Raphael Braun, Ce Liu, Jonathan T. Barron, and Hendrik P.A. Lensch. Neural-pil: Neural pre-integrated lighting for reflectance decomposition. _NeurIPS_, 2021.\n' +
      '* [14] Mark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen Li, Deqing Sun, Jonathan T. Barron, Hendrik P.A. Lensch, and Varun Jampani. SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections. _NeurIPS_, 2022.\n' +
      '* [15] Yue Chen, Xingyu Chen, Xuan Wang, Qi Zhang, Yu Guo, Ying Shan, and Fei Wang. Local-to-global registration for bundle-adjusting neural radiance fields. _CVPR_, pages 8264-8273, 2023.\n' +
      '* [16] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. _CVPR_, 2019.\n' +
      '* [17] Weihao Cheng, Yan-Pei Cao, and Ying Shan. Id-pose: Sparse-view camera pose estimation by inverting diffusion models. _arXiv preprint arXiv:2306.17140_, 2023.\n' +
      '* 3D 모델링 및 렌더링 패키지_입니다. 블렌더 재단, 스티클링 블렌더 재단, 암스테르담 2018.\n' +
      '* [19] Robert L. Cook and Kenneth E. Torrance. A reflectance model for computer graphics. _ACM TOG_, 1982.\n' +
      '* [20] Valentin Deschaintre, Miika Aitalla, Fredo Durand, George Drettakis, and Adrien Bousseau. Single-image SVBRDF capture with a rendering-aware deep network. _ACM TOG_, 2018.\n' +
      '* [21] Valentin Deschaintre, Miika Aitalla, Fredo Durand, George Drettakis, and Adrien Bousseau. Flexible SVBRDF capture with a multi-image deep network. _Eurographics Symposium on Rendering_, 2019.\n' +
      '* [22] Valentin Deschaintre, George Drettakis, and Adrien Bousseau. Guided fine-tuning for large-scale material transfer. _Eurographics Symposium on Rendering_, 2020.\n' +
      '* [23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. _ICLR_, 2021.\n' +
      '* [24] Charles Dugas, Yoshua Bengio, Francois Belisle, Claude Nadeau, and Rene Garcia. Incorporating second-order functional knowledge for better option pricing. In _NeurIPS_. MIT Press, 2000.\n' +
      '* [25] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. _Neural networks : the official journal of the International Neural Network Society_, 107:3-11, 2017.\n' +
      '* [26] Duan Gao, Xiao Li, Yue Dong, Pieter Peers, and Xin Tong. Deep inverse rendering for high-resolution SVBRDF estimation from an arbitrary number of images. _ACM Transactions on Graphics (SIGGRAPH)_, 2019.\n' +
      '* [27] Shubham Goel, Angjoo Kanazawa, and Jitendra Malik. Shape and viewpoint without keypoints. _ECCV_, 2020.\n' +
      '* [28] John C Gower and Garmt B Dijksterhuis. _Procrustes problems_. OUP Oxford, 2004.\n' +
      '* [29] Richard Hahnloser, Rahul Sarpeshkar, Misha Mahowald, Rodney Douglas, and H. Seung. Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit. _Nature_, 405:947-51, 2000.\n' +
      '\n' +
      '* [30] Philipp Henzler, Valentin Deschaintre, Niloy J Mitra, and Tobias Ritschel. Generative modelling of BRDF textures from flash images. _ACM Transactions on Graphics (SIGGRAPH ASIA)_, 2021.\n' +
      '* [31] Hwan Heo, Taekyung Kim, Jiyoung Lee, Jaewon Lee, Soohyun Kim, Hyunwoo J. Kim, and Jin-Hwa Kim. Robust camera pose refinement for multi-resolution hash encoding. _ICML_, 2023.\n' +
      '* [32] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis. _ICCV_, 2021.\n' +
      '* [33] Varun Jampani, Kevis-Kokitsi Maninis, Andreas Engelhardt, Arjun Karpur, Karen Truong, Kyle Sargent, Stefan Popov, Andre Araujo, Ricardo Martin-Brualla, Kaushal Patel, Daniel Vlasic, Vittorio Ferrari, Ameesh Makadia, Ce Liu, Yuanzhen Li, and Howard Zhou. Navi: Category-agnostic image collections with high-quality 3d shape and pose annotations. _NeurIPS_, 2023.\n' +
      '* [34] Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Animashere Anandkumar, Minsu Cho, and Jaesik Park. Self-calibrating neural radiance fields. _ICCV_, 2021.\n' +
      '* [35] Berk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio Ferrari, and Luc Van Gool. Uncalibrated neural inverse rendering for photometric stereo of general surfaces. _ICCV_, 2021.\n' +
      '* [36] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv_, 2014.\n' +
      '* [37] Zhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng Huang, Panos Achlioptas, and Sergey Tulyakov. NeROIC: Neural object capture and rendering from online image collections. _arXiv_, 2022.\n' +
      '* [38] Hendrik P. A. Lensch, Wolfgang Heidrich, and Hans-Peter Seidel. Automated texture registration and stitching for real world models. _Pacific Graphics_, 2000.\n' +
      '* [39] Axel Levy, Mark Matthews, Matan Sela, Gordon Wetzstein, and Dmitry Lagun. MELON: NeRF with Unposed Images Using Equivalence Class Estimation.\n' +
      '* [40] Zhengqin Li, Kalyan Sunkavalli, and Manmohan Chandraker. Materials for masses: SVBRDF acquisition with a single mobile phone image. _ECCV_, 2018.\n' +
      '* [41] Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker. Inverse rendering for complex indoor scenes: Shape, spatially-varying lighting and SVBRDF from a single image. _CVPR_, 2020.\n' +
      '* [42] Zhaoshuo Li, Thomas Muller, Alex Evans, Russell H Taylor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. _CVPR_, 2023.\n' +
      '* [43] Ruofan Liang, Huiting Chen, Chunlin Li, Fan Chen, Selvakumar Panneer, and Nandita Vijaykumar. ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting. _arXiv_, 2023.\n' +
      '* [44] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. BARF: Bundle-Adjusting Neural Radiance Fields. _ICCV_, 2021.\n' +
      '* [45] Sainan Liu, Shan Lin, Jingpei Lu, Shreya Saha, Alexey Supikov, and Michael Yip. BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives. _arXiv_, 2023.\n' +
      '* [46] Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and Wenping Wang. Sparseneus: Fast generalizable neural surface reconstruction from sparse views. _ECCV_, 2022.\n' +
      '* [47] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. _CVPR_, 2021.\n' +
      '* [48] Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, and Jingyi Yu. GNeRF: GAN-based Neural Radiance Field without Posed Camera. _ICCV_, 2021.\n' +
      '* [49] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. _CVPR_, 2019.\n' +
      '* [50] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. _ECCV_, 2020.\n' +
      '* [51] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM TOG_, 2022.\n' +
      '* [52] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Mueller, and Sanja Fidler. Extracting Triangular 3D Models, Materials, and Lighting From Images. _CVPR_, 2022.\n' +
      '* [53] Giljoo Nam, Diego Gutierrez, and Min H. Kim. Practical SVBRDF acquisition of 3d objects with unstructured flash photography. _ACM Transactions on Graphics (SIGGRAPH ASIA)_, 2018.\n' +
      '* [54] Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. _CVPR_, 2022.\n' +
      '* [55] Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. _ICCV_, 2021.\n' +
      '* [56] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. _CVPR_, 2019.\n' +
      '* [57] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Deformable neural radiance fields. _ICCV_, 2021.\n' +
      '* [58] Keunhong Park, Philipp Henzler, Ben Mildenhall, Jonathan T. Barron, and Ricardo Martin-Brualla. Camp: Camera preconditioning for neural radiance fields. _ACM Trans. Graph._, 2023.\n' +
      '* [59] G. Ponimatkin, Y. Labbe, B. Russell, M. Aubry, and J. Sivic. Focal length and object pose estimation via render and compare. In _CVPR_, 2022.\n' +
      '* [60] Daniel Rebain, Mark Matthews, Kwang Moo Yi, Dmitry Lagun, and Andrea Tagliasacchi. LOLNeRF: Learn from One Look. _CVPR_, 2022.\n' +
      '* [61] Shen Sang and Manmohan Chandraker. Single-shot neural relighting and SVBRDF estimation. _ECCV_, 2020.\n' +
      '\n' +
      '* [62] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine: Robust hierarchical localization at large scale. In _CVPR_, 2019.\n' +
      '* [63] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperGlue: Learning feature matching with graph neural networks. In _CVPR_, 2020.\n' +
      '* [64] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. _CVPR_, 2016.\n' +
      '* [65] Johannes Lutz Schonberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. _ECCV_, 2016.\n' +
      '* [66] Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu, David W. Jacobs, and Jan Kautz. Neural inverse rendering of an indoor scene from a single image. _ICCV_, 2019.\n' +
      '* [67] S. Sinha, J. Y. Zhang, A. Tagliasacchi, I. Gilitschenski, and D. B. Lindell. Sparsepose: Sparse-view camera pose regression and refinement. _CVPR_, 2023.\n' +
      '* [68] Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and Jonathan T. Barron. NeRV: Neural reflectance and visibility fields for relighting and view synthesis. _CVPR_, 2021.\n' +
      '* [69] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. _NeurIPS_, 2020.\n' +
      '* [70] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, and Federico Tombari. Sparf: Neural radiance fields from sparse and noisy poses. _CVPR_, 2023.\n' +
      '* [71] Itsuki Ueda, Yoshihiro Fukuhara, Hirokatsu Kataoka, Hiroaki Aizawa, Hidehiko Shishido, and Itaru Kitahara. Neural density-distance fields. _ECCV_, 2022.\n' +
      '* [72] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, and Pratul P. Srinivasan. Ref-neRF: Structured view-dependent appearance for neural radiance fields. _CVPR_, 2022.\n' +
      '* [73] Jianyuan Wang, Christian Rupprecht, and David Novotny. PoseDiffusion: Solving pose estimation via diffusion-aided bundle adjustment. _ICCV_, 2023.\n' +
      '* [74] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. _NeurIPS_, 2021.\n' +
      '* [75] Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, and Lingjie Liu. Neus2: Fast learning of neural implicit surfaces for multi-view reconstruction. _ICCV_, 2023.\n' +
      '* [76] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. NeRF\\(--\\): Neural radiance fields without known camera parameters. _arXiv_, 2021.\n' +
      '* [77] Jiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu. A Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction. _cvmj_, 2022.\n' +
      '* [78] Jiawei Yang, Marco Pavone, and Yue Wang. FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization. _CVPR_, 2023.\n' +
      '* [79] Yao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. NeILF: Neural Incident Light Field for Physically-based Material Estimation. _ECCV_, 2022.\n' +
      '* [80] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. _NeurIPS_, 2021.\n' +
      '* [81] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis. _ICCV_, 2023.\n' +
      '* [82] Jianzhao Zhang, Guojun Chen, Yue Dong, Jian Shi, Bob Zhang, and Enhua Wu. Deep inverse rendering for practical object appearance scan with uncalibrated illumination. _ACG_, 2020.\n' +
      '* [83] Jason Zhang, Gengshan Yang, Shubham Tulsiani, and Deva Ramanan. NeRS: Neural reflectance surfaces for sparse-view 3d reconstruction in the wild. _NeurIPS_, 2021.\n' +
      '* [84] Jiahui Zhang, Fangneng Zhan, Rongliang Wu, Yingchen Yu, Wenqing Zhang, Bai Song, Xiaoqin Zhang, and Shijian Lu. VMRF: View Matching Neural Radiance Fields. _ACM MM_, 2022.\n' +
      '* [85] Jingyang Zhang, Yao Yao, Shiwei Li, Jingbo Liu, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. Neill++: Inter-reflectable light fields for geometry and material estimation. _ICCV_, 2023.\n' +
      '* [86] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. PhySG: Inverse rendering with spherical Gaussians for physics-based material editing and relighting. _CVPR_, 2021.\n' +
      '* [87] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. _CVPR_, 2018.\n' +
      '* [88] Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul Debevec, William T. Freeman, and Jonathan T. Barron. Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. _ACM Trans. Graph._, 40(6), 2021.\n' +
      '* [89] Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, and Xiaowei Zhou. Modeling indirect illumination for inverse rendering. _CVPR_, 2022.\n' +
      '* [90] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. _CVPR_, 2019.\n' +
      '* [91] Hao Zhu, Fengyi Liu, Qi Zhang, Xun Cao, and Zhan Ma. Rhino: Regularizing the hash-based implicit neural representation. _arXiv_, 2023.\n' +
      '\n' +
      '### Overview\n' +
      '\n' +
      '양형, 조명 및 재료의 3D 관절 재구성을 위한 방법인 SHINOBI 보충제에서 먼저 방법의 아키텍처(Sec.A.1) 및 최적화(Sec.A.4)에 대한 추가 세부 사항을 제시한다. SecB에서 NAVI 데이터세트[33]의 객체 재구성에서 추가적인 정성적 결과를 소개하고 절제 연구에 시각적 예를 추가한다. 마지막으로 재구성된 데이터의 애플리케이션은 Sec.B.5에 나와 있습니다. 이 작업의 개요 및 추가 시각적 결과 영상을 위해 **프로젝트 페이지**를 방문하십시오.\n' +
      '\n' +
      '수요법.\n' +
      '\n' +
      '### Architecture\n' +
      '\n' +
      '** 하이브리드 해시 인코딩 구성** 하이브리드 인코딩** 하이브리드 인코딩은 두 가지 가지를 특징으로 한다. 베이스 인코딩을 위해 위치 인코딩에 대해 10개의 무작위 오프셋 가열냉각된 푸리에 주파수를 사용하고 \\(64\\) 치수와 실루 활성화 [25]가 있는 단일 은닉층을 특징으로 하는 작은 MLP를 사용한다. 출력은 다시 Zhu _et al_[91]에 의해 수행됨에 따라 입력 차원(\\(3\\)과 같다. 우리는 축 정렬 주파수로부터 유물을 방지하기 위해 BARF의 [44] 푸리에 어닐링을 적용하고 로그 이격 주파수[69, 14]에 오프셋으로 랜덤 주파수를 추가한다. 다중해결 해시 그리드는 \\(8\\)의 베이스 해상도와 \\(2048\\)의 최대 목표 해상도로 \\(16\\) 레벨로 구성된다. 상기 임베딩 치수는 \\(2\\) 또는 \\(4\\)이다. 우리의 논문의 Sec.4에 보고된 실험은 \\(2\\) 차원을 사용하여 생성된다. 메모리 소비 증가와 런타임의 비용으로 치수를 높임으로써 약간 더 나은 분해 품질을 달성할 수 있다. 따라서 인코딩 및 연결 후 최종 특징 차원은 \\(35\\) 또는 \\(67\\)이다. 해시 그리드에 적용된 어닐링 전략에 대한 설명을 위해 Sec.A.1을 참조하세요.\n' +
      '\n' +
      '**Net웍스*** 부호화된 특징을 취하는 주요 네트워크는 64개의 채널로 3개의 ReLU[29] 활성층으로 구성된다. 추가 선형 계층은 64 채널 활성화로부터 \\(\\sigma\\) 밀도 파라미터에 대한 출력을 생성한다. 소프트플러스 \\(\\mathrm{softplus}(x)=\\ln(1+\\mathrm{e}^{x})\\)[24]가 원시 \\(\\sigma\\)에 적용된다. 방향은 마덴힐 _et al_에서와 같이 4개의 비응축 일반 푸리에 성분을 사용하여 인코딩된다.[50] 그런 다음 최적화의 시작에 사용된 뷰 방향 의존적 방사도 \\(\\mathbf{\\tilde{c}\\)를 예측하기 위해 2차 MLP에 공급되는 메인 네트워크 출력과 연결한다. 2차 조건부 네트워크는 우리의 경우 32의 숨겨진 차원을 가지고 있다. BRDF 예측을 위해 단일 선형 계층은 메인 네트워크 출력을 16개 채널로 압축한다. 거기에서 BRDF 디코더는 각각 64개의 채널과 ReLU 활성화로 구성된 또 다른 2개의 층으로 구성된다. 각 BRDF 출력; 염기색, 금속성 및 거칠기는 자체 출력 레이어에 이어 S자형 활성화 [13]가 있다. 출력 전에 베이스 컬러 가지에 컨디셔닝으로 추가 확산 임베딩이 추가된다. 뷰당 잠재 벡터를 디코딩하는 조명 네트워크는 신경-PIL[13]에 요약된 바와 동일한 매핑 레이어의 구성에 의해 조건화된다.\n' +
      '\n' +
      '** 다중해결 해시 그리드 레벨 어닐링** BARF[44] 및 Nerfies[57]에 의해 영감을 받은** 인피스는 서로 다른 그리드 레벨을 가중하여 해시 그리드에 거친 가열냉각을 적용한다. 낮은 해상도 조밀한 격자 및 0으로 설정된 다른 모든 특징에서만 시작하여 시간이 지남에 따라 더 높은 해상도 수준의 가중치를 점차 증가시킨다(cf[42, 45]). 린 _et al_에 의한 구현과 유사하다. 우리는 그것을 잘린 한인 창으로 공식화합니다.\n' +
      '\n' +
      '(2^{k}\\mathbf{x})\\cos(2^{k}\\mathbf{x})\n' +
      '\n' +
      'Hf(L\\)를 갖는 \\(\\알파\\in[0,L]\\)가 해시 그리드 인코딩의 해상도 레벨 수인 경우.\n' +
      '\n' +
      '우리는 또한 저해상도 수준에서 BAA-NGP[45] 복제 임베딩의 개념을 테스트했지만 최적화 환경에서 감소된 성능을 관찰했다. 유사하게, [31]에서 제안된 바와 같이 해시 그리드에 보간에 직선 스루 오퍼레이터를 추가하는 데 성공하지 못했다.\n' +
      '\n' +
      '### Camera Parameterization.\n' +
      '\n' +
      '우리는 Left _vs_의 3가지 간단한 바이너리 질문을 기반으로 초기 포즈들을 레이블링한다. 맞아, 압브 _vs_. 아래와 전선 _vs_. 백. 전형적인 80개의 이미지 컬렉션은 약 4-5분밖에 걸리지 않습니다. 또는, 우리의 프레임워크는 초기화를 하나 이상의 사분면에 걸쳐 있는 카메라 다중화로 확장할 수 있게 한다. 이것은 전방 대면 장면과 회전 캠을 특징으로 하는 이미지 세트가 완전히 무작위 초기화를 가능하게 할 수 있다.\n' +
      '\n' +
      '그림 9: ** 노블 뷰 합성은 기존 방법과 비교하여 NAVI[33] 인-팬드 이미지 컬렉션의** 무조건 예시 객체들과 비교된다. SHINOBI는 예를 들어 NeROIC[37]가 일부 장면에서 성공하지 않는 동안 외향적으로 거친 포즈로 초기화될 때에도 강력하게 재구성한다.\n' +
      '\n' +
      '레비 _et al_[39]에서 보는 바와 같이 고정된 물체 거리를 가진 라이터이다. 이러한 제약된 설정은 여기서 폐기하는 길쭉한 컬렉션에 흔하지 않기 때문입니다. 우리는 시각 핀홀 카메라 모델과 53.13도의 초기 시야를 사용한다. 본 논문에서 설명한 대로 \'회색 + 방향\' 파라미터화의 원래 카메라 매개변수로 오프셋을 최적화한다. 여기에서 우리는 훈련 가능한 룩트 매개변수 \\(\\Delta\\mathbf{d}\\)를 두 가지 방향 구성 요소, \\(\\phi\\), \\(\\theta\\)로 직접 인코딩하여 업데이트된 \\(\\dot{\\mathbf{d}}\\)를 얻었다.\n' +
      '\n' +
      '}(I\\math{d},\\mathf{d})\n' +
      '\n' +
      '우리는 \\(\\Delta\\mathbf{d}\\)를 \\([-0.5\\pi,0.5\\pi]\\ 범위로 제한한다.\n' +
      '\n' +
      '우리는 또한 저우 _et al_에 의한 인기 있는 6D 회전 표현과 같은 다른 카메라 파라미터를 시도했습니다. [90] 최근 카메라 미세 조정[58]을 사용하여 NeRF에 적용된or FocalPose[59]이다. 흥미롭게도, 우리의 룩샷 + 방향 파라미터화는 카메라 포즈 상의 정규화와 잘 작용하는 것 같기 때문에 설정에서 최고의 성능을 발휘합니다.\n' +
      '\n' +
      '### 정기화 및 상실입니다.\n' +
      '\n' +
      '***는 바론 _et al_[4] 다중해결 해시 격자 규칙화에 의해 제안된 다음 정규화된 체중 붕괴를 사용하는 해시 그리드를 규칙화하기 위해 바르론 _et al_[4] 다중해결 해시 격자 규칙화에 의해 제안된 바와 같이 사용하는 해시 그리드를 규칙화한다. \\(\\mathcal{L}_{\\mathrm{Grid}}=\\sum_{l}\\text{mean}(V_{l}\\)\\)는 해상도 레벨 \\(l\\)에서 그리드 임베딩을 참조한다. 레벨당 평균의 합을 계산하는 것은 모든 매개변수에 걸쳐 순진한 체중 붕괴에 비해 콕서 그리드 수준에 더 높은 과징금을 한 번에 부여한다. 우리는 설정에서 0.02~0.05의 가중치가 잘 작동하고 최종 값으로 0.02에 대해 정착한다. 우리는 \\(0.1\\)의 규범에 의해 네트워크의 구배에 구배 스케일링을 적용한다. 또한, 파라미터 업데이트 전에 카메라 구배에는 \\(2.5\\)의 클립 값으로 클램핑하는 구배 규범이 적용된다.\n' +
      '\n' +
      '*** 표면 정규화** 우리는 정상 방향 손실 \\(\\mathcal{L}_{\\mathrm{adir}}\\)를 [72]에서 사용하여 광선이 표면에 도달할 때까지 카메라를 향하도록 정규화를 제한한다. 이것은 플로터 유물 없이 더 선명한 표면을 제공하는 데 도움이 됩니다. 또한, 명시적인 렌더링 단계가 예측된 방사선만을 사용하여 최적화에 비해 노이즈가 감소함에 따라 표면 정규화를 제약하는 데 도움이 된다는 것을 관찰했다.\n' +
      '\n' +
      '**Camera 규칙화** SAMURAI의 카메라 정규화 손실은 특히 표백 방향(\\(\\mathcal{L}_{\\mathrm{{ lipat}}\\))을 가리키도록 강요하고 카메라는 바운딩 볼륨(\\(\\(\\mathcal{L}_{\\mathrm{Bounds}_{\\mathrm{Bounds}}}\\)) [14]에서 너무 멀리 떨어진 곳에서 움직이는 것을 방지하기 위해 (\\,\\(\\mathcal{L}_{\\mathrm{L}_{\\mathrm{L}_{\\mathrm{L}_{\\)에서 너무 멀리 이동하는 것을 방지하기 위해)과 카메라(\\(\\)에서 너무 멀리 이동하지 않도록 하기 위해 하나)에서 출발지(\\,\\(\\(\\mathrm{L}_{\\mathrm{L}_{\\mathrm{L}_{\\mathrm{L}_{\\mathrm{L}_{\\-dathrm{Bounds}}}}}}}}}}:\\)에서 지나치게 멀리 이동하는 것을 방지하기 위해 카메라 오프셋 파라미터의 크기에 대한 추가 용어는 최적화 초기에 강력한 업데이트로 인해 초기 위치에 대해 카메라가 너무 빨리 이동하지 않도록 하는 데 도움이 된다.\n' +
      '\n' +
      'BRDF 및 조명의**BRDF 손실*** 공동 추정은 섬세한 노력이다. 예를 들어, 조명은 쉽게 국소 최소값에 빠질 수 있습니다. 그런 다음 객체는 블러쉬 컬러로 틴팅되고, 조명은 예를 들어 보다 중립적인 컬러 톤을 표현하기 위한 오렌지 색상이다. 이미지 컬렉션은 다수의 조명을 가지므로 베이스 컬러 \\(\\mathbf{b}_{c}\\)를 강요하여 입력 이미지로부터 픽셀 색상을 복제할 수 있다. 이렇게 하면 데이터셋에 대한 평균 색상이 학습되어 로컬 최소에 갇힐 가능성이 낮아진다. 이를 위한 Mean Squared Error(MSE)를 평가한다. 또한, 우리는 BRDF 추정 [14]를 추가로 규칙화하기 위해 UNISURF[55]에서 사용된 것과 유사한 정상, 거칠기 및 금속 매개변수에 대해 평활성 손실 \\(\\mathcal{L}_{\\mathrm{Smooth}}\\)를 추가한다.\n' +
      '\n' +
      '**Image 재구성 손실**는 \\(\\mathcal{L}_{\\mathrm{Image}}(g,p)=\\sqrt{(g-p)^{2}+0.001^{2}}\\) 픽셀 \\(s\\)의 입력 색상과 네트워크 \\(C\\)의 해당 예측 색상(\\mathbf{\\tilde{c}}\\)의 상응하는 예측 색상 사이의 카본니어 손실이다. 또한 시간이 지남에 따라 주요 손실이 되는 렌더링된 색상 \\(\\hat{c}\\)로 손실을 계산합니다. 이 손실은 Sec에 설명된 대로 여러 해상도 수준에 걸쳐 계산된다. 패치를 렌더링할 때마다 메인 페이퍼의 3개입니다.\n' +
      '\n' +
      '** 마스크 손실*** 총 3개의 마스크 손실 조건을 사용합니다. Sec 3.2에 설명된 바와 같이 \\(\\mathcal{L}_{\\mathrm{실루엣}}\\)와 부피 환원 마스크와 추정된 전경 물체 마스크와 배경 손실(\\mathcal{L}_{\\mathrm{L}}}}\\) 사이의 이진 교차 비효율 손실(\\mathcal{L}_{\\mathrm{L}_{\\mathrm{L}_{\\mathrm{mathrm{L}_{\\mathrm{mathrm{mathrm{L}_{\\{mathrm{mathrm{Mathrm{Mathrm{Mathrm{BCE}}}}}}}}}}}}})과 NeRD [12]의 부피 환원 마스크와 배경 손실(\\mathrm{L}_{\\mathrm{mathrm{mathrm{Mathrm{BCE}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}) 및 NeRD[12]의 부피 후자는 모든 광선을 배경에 캐스팅하여 \\(0\\)를 반환한다. MS_{\\mathcal{L}}} +\\mathcal{L}}} +\\mathcal{L}}}{\\mathcal{L}}}{\\mathcal{L}}}}}}로 정의되며, 이는 \\(\\lambda_{\\mathcal{L}) 및 \\(\\lathda_{\\mathcal{Mathcal{Mathcal{Mathcal{Mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{mathcal{Mathcal{Mathcal{Mathcal{Mathcal{Mathcal{Mathcal{Mathcal{Mathcal{Mathcal{Mathcal{Mathcal{Mask}}}}}}\n' +
      '\n' +
      '***F 최종 손실 앙상블***는 두 개의 손실 항 \\(\\mathcal{L}_{\\mathrm{Network}}\\) 및 \\(\\mathcal{L}_{\\mathrm{Camera}}\\)를 계산하는데, 이는 광측정 렌더링 손실 및 정렬 손실과 각각의 규칙화의 상이한 가중 버전으로 구성된다. }\\mathda_{math{d}}}\\math{math{s}}}{math{math{s}}}\\math{math{s}}{math{math{s}}} <\\math{math{s}}} <\\math{s}}}}{math{math{s}}}{math{math{s}}}}{math{math{s}}}}{math{math{math{math{math{s}}}}}}{math{math{math{math{s}}}}}}}}{math{math{math{math{s}}}}}}}}}{math{math{math{s}}}}}}}}}}}{math{math{s}}}}}}}}}}}}{math{math{math{s}}}}}}}}}}}}}}}}}}}}} 여기서 \\(\\lambda_{b}\\)와 \\(\\lambda_{a}\\)는 아래에 설명된 최적화 스케줄링 가중치이다. 카메라 멀티플렉스가 크기 \\(m>1\\)를 갖는 한, 카메라 다중 정합성 손실이 다음과 같이 추가되는 한, \\(\\mathcal{L}_{\\mathrm{Network}=\\mathcal{L}_{\\mathrm{Network}_{\\mathrm{L}}}+0.1(\\mathcal{L}_{ \\mathrm{multiplex}})\\(\\mathcal{L}_{\\mathrm{L}_{\\mathrm{L}_{\\mathrm{L}_{\\mathrm{L}_{\\mathrm{Mathrm{Network}_{\\mathrm{L}_{\\mathrm{L}_{\\mathrm{L}_{mathrm{L}_{mathrm{L}_{mathrm{Mathrm{Mathrm{Network}:{mathrm{L}_{mathrm{Mathrm{Network}/{mathrm{Mathrm{Network} 이러한 손실에 대해 카메라 후방 스케일링은 SAMURAI [14]에서와 같이 적용된다. 대신 우리의 뷰 중요도 스케일링에 따라 카메라 손실이 가중됩니다. 매우 초기화된 카메라 포즈는 손실 측면에서 잘 수행되는 카메라가 최적화에서 점진적으로 퇴색되는 동안 잠재적으로 큰 업데이트를 얻음으로써 훈련 기간 동안 여전히 회복될 수 있다. 또한, 위의 규칙화, \\(\\mathcal{L}_{\\mathrm{Bounds}}\\) 및 \\(\\mathcal{L}_{\\mathrm{ lipat}}\\)가 추가된다.\n' +
      '\n' +
      '### Optimization\n' +
      '\n' +
      '*** 최적화 스케줄*** 우리는 3개의 페이딩 \\(\\lambda\\) 변수를 사용하여 그림 그림과 같이 최적화 일정을 매끄럽게 제거합니다. 액티브 멀티플렉서 카메라의 수가 감소하는 동안 최적화 전반부에 걸쳐 10 Render 분해능이 지속적으로 증가하고 있다. 이것은 \\(\\lambda_{c}\\)에 의해 제어된다. 입력 이미지 해상도는 100 픽셀에서 훈련의 전반부에 걸쳐 더 긴 이미지 측의 400 픽셀의 해상도로 증가한다. 더 높은 최종 출력 해상도를 위해서는 훨씬 더 큰 다운샘플 팩터(\\(>4\\))가 필요할 수 있다. 이 전략은 이미지 패치가 객체들의 훨씬 더 큰 구조를 포함하고 카메라 정렬을 향상시킬 수 있게 한다. 직접 색상 최적화는 BRDF 최적화로 퇴색되고 부호화 어닐링은 최적화의 제1 제3에 걸쳐 수행된다. (\\lambda_{b}\\)은 BRDF 전환에 사용되며 가열냉각에는 독립적인 \\(\\alpha\\) 값이 유지된다. 마지막으로 \\(\\lambda_{a}\\)는 비선형 방식으로 약간의 손실을 스케일링하는 데 사용된다. 초점 길이 업데이트는 최적화 시간의 4분의 1까지 지연됩니다. 어닐링 일정의 반방향 지점에서 보기 중요도 가중치로 시작합니다. SHINOBI는 각 업데이트 단계에 더 많은 컨텍스트를 추가하는 대부분의 훈련 시간 동안 이미지 패치를 렌더링하여 카메라 정렬에 맞춘 새로운 손실을 추가할 수 있습니다. 첫 번째 1000단계는 일반 랜덤 광선 샘플링을 사용하여 훈련되지만, 렌더링 해상도와 해시 그리드 해상도가 모두 낮은 동안 전 세계 형태를 빠르게 초기화하는 데 도움이 된다.\n' +
      '\n' +
      '***Optimizer 설정** ADAM[36] 최적화기는 훈련 시간에 따라 기하급수적으로 약화된 1e-3의 학습률로 \\(\\mathcal{L}_{\\mathrm{Network}}\\)를 기반으로 네트워크 가중치를 업데이트한다. 해시 그리드 임베딩과 관련된 최적화기에 동일한 붕괴율이 적용된다. 구위는 해시 그리드 특이적 규칙화 \\(\\mathcal{L}_{\\mathrm{Network}}\\)를 사용하여 \\(\\mathcal{L}_{\\mathrm{Grid}}\\)를 기반으로 계산된다. 카메라 최적화의 학습 속도는 40k 단계마다 크기만큼 기하급수적으로 부패한다. I\\(\\beta 1\\) 매개변수 이전에 언급한 바와 같이, 시끄러운 구배가 있는 상태에서 훈련을 안정화시키기 위해 카메라 최적기에 대해 \\(0.2\\)로 설정된다. 그것은 \\(\\mathcal{L}_{\\mathrm{Camera}}\\)를 기반으로 계산된 구배를 사용한다. 프레임워크는 플로트16 혼합 정밀도를 사용하여 트레이닝됩니다. 인코딩에 입력되는 좌표는 렌더링 및 조명 평가에서와 같이 \\(32\\) 비트이다. 다른 MLP 및 특히 해시 그리드의 보간은 \\(16\\) 비트 정밀도로 실행된다.\n' +
      '\n' +
      '부록 B.\n' +
      '\n' +
      '프로젝트 방법에 대한### 소매입니다.\n' +
      '\n' +
      'Sec에 소개된 SAMURAI 외에. 본 논문의 3개는 길쭉한 물체 재구성에 대한 두 가지 최근의 방법과 비교된다.\n' +
      '\n' +
      '***NeRS*****은 메쉬 기반 표현을 사용하여 재구성을 제한하는 신경 반사 서클[83]을 나타낸다. 수작업으로 주석이 달린 거친 초기 포즈 및 템플릿 메쉬에서 시작하여 객체가 표면 메쉬로 분해되고, 알베도와 정음성으로 매개변수가 되는 조명 및 표면 반사율이 표시된다. 우리는 [33, 83]에 따라 각 장면에 대한 물체의 바운딩 박스에 근사하는 초기 큐비노이드의 차원을 정의한다.\n' +
      '\n' +
      '**NeROIC***는 온라인 이미지 수집에서 객체의 기하학 및 재료 특성을 재구성하기 위한 다단계 접근법을 제시한다. 카메라는 COLMAP 기반 파이프라인으로 초기화되고 첫 번째 재구성 단계에서 미세 조정된다. 2단계 동안 고품질 표면 규범에 따라 추정됩니다. 마지막으로, 재료 특성 및 조명은 새로운 뷰 합성 외에도 재전이 가능하도록 최적화되어 있다.\n' +
      '\n' +
      '확실한 결과.\n' +
      '\n' +
      '그림. 9는 기준 방법과 비교하여 NAVI 데이터세트로부터의 객체에 대한 추가 정성적 결과를 보여준다. 참고로, 방법은 다양한 이미지 해상도에서 작동하며 원래 출력을 보여준다. NeROIC는 초기 포즈가 좋지만 유물을 보여주거나 다른 사람들에게 실패하는 장면들에 대해 고주파 디테일을 재구성할 수 있다. NeRS는 낮은 해상도 메쉬 표현과 종종 부정확한 카메라 정렬에 시달린다. SAMURAI와 SHINOBI는 모두 우수한 분해 능력으로 인해 원래 조명 설정에 더 가까운 외관을 재현하고 SHINOBI는 더 많은 고주파 디테일을 회복한다.\n' +
      '\n' +
      '표현의 정성적 결과.\n' +
      '\n' +
      '그림. 11은 본고에서 보고된 절제 연구의 수치 결과에 해당하는 질적 결과를 보여준다. 우리의 방법의 전체 구성을 사용하여만 강력한 재구성이 가능하다는 것을 관찰할 수 있다. 그럼에도 불구하고.\n' +
      '\n' +
      '그림 10: ** 최적화 일정** 우리는 최적화 파라미터의 원활한 흐름을 가능하게 하기 위해 손실을 스케일링하기 위해 3개의 \\(\\lambda\\) 파라미터를 사용한다. 또한, 뷰 중요도 가중치가 도입된 시점, 초점 거리 파라미터가 업데이트되기 시작하고 인코딩 어닐링이 종료되는 시점을 나타낸다.\n' +
      '\n' +
      '다중 일관성 손실은 이 예에 최소한의 영향을 미칠 뿐, 결과는 여전히 일부 가시적인 유물들과 전체적인 소음 수준을 보여준다.\n' +
      '\n' +
      '다른 카메라인에 대한 비교.\n' +
      '\n' +
      '타브. 4는 NAVI의 길쭉한 장면 [33]에 대한 카메라 포즈 추정 방법을 비교한다. 신경 특징 검출 및 매칭과 짝을 이루는 COLMAP[64, 65]와 같은 전통적인 SfM 방법은 큰 정확도로 포즈를 회복할 수 있지만 장면과 이미지의 하위 집합에서만 성공할 수 있다. PoseDiffusion[73] 및 ID-Pose[17], 두 가지 완전 신경 모델 모두 큰 데이터셋에 대해 훈련되어 이러한 분포 외 예에서 투쟁한다. 우리는 여기서 예시로서 PoseDiffusion에 대한 전체 평가만 보고한다. 우리는 이러한 모델이 객체 중심 이미지 세트의 배경에서도 중요한 포즈 신호를 취하는 것을 관찰한다. 이는 일반적인 이미지 수집에 대한 좋지 않은 결과로 이어진다. 마스크 영상에 대한 간단한 미세 조정은 성능을 향상시키지 않았다. 우리의 실험에서 카메라 포즈 추정은 보통 서로 다른 조명 및 객체 척도를 특징으로 하는 스케줄 내 예들에 대한 전면 대면 카메라 레이아웃으로 회귀한다. 결과적으로, 우리의 접근법은 카메라 포즈 품질의 좋은 상충 관계에 있는 것으로 판단된다.\n' +
      '\n' +
      '### Downstream Applications\n' +
      '\n' +
      '오브젝트 분해가 BRDF로 분해되고 조명 및 모양은 예를 들어 객체를 재조명하기 위해 형상 표현과 독립적으로 조명 및 물질을 편집할 수 있게 한다. 또한, 우리는 신경 표현을 메쉬와 같은 모수 모델 및 쉽게 통합할 수 있는 물리적 기반 소재로 표준 그래픽 파이프라인으로 변환할 수 있다. ****메쉬 추출 및 자산 생성 및 자산 생성****메쉬 추출 및 자산 생성****메쉬 추출 및 자산 생성****메쉬 추출 및 자산 생성.****메쉬 추출 및 자산 생성.****메쉬 추출 및 자산 생성.****메쉬 추출 및 자산 생성.****\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{Translation\\(\\downarrow\\)} & \\multicolumn{2}{c}{Rotation \\({}^{\\circ}\\downarrow\\)} \\\\ \\cline{2-5}  & \\(S_{C}\\) & \\(\\sim S_{C}\\) & \\(S_{C}\\) & \\(\\sim S_{C}\\) \\\\ \\hline PoseDiffusion [73] & \\(0.51\\pm\\) 0.09 & \\(0.43\\pm\\) 0.11 & \\(41.33\\pm\\) 15.15 & \\(43.50\\pm\\) 13.67 \\\\ HLoc [62, 63] & \\(0.07\\pm\\) 0.13 & \\(0.06\\pm\\) 0.10 & \\(9.10\\pm\\) 18.75 & \\(9.72\\pm\\) 20.08 \\\\ SHINOBI & \\(0.250\\pm\\) 0.080.28\\(\\pm\\) 0.09 & \\(22.84\\pm\\) 16.19 & \\(33.00\\pm\\) 19.97 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 4> < **Pose>는 NAVI[33] 실내 장면에서 정렬한 후 절대 회전 및 번역 오류 평가***. 우리는 특수 카메라 포즈 추정 솔루션과 SHINOBI를 비교한다. HLoc[62]는 5개의 장면에서 완전히 실패하여 평균적으로 \\(55\\%\\)의 조회수만 회복할 수 있다는 점에 주목한다.\n' +
      '\n' +
      '그림 11: ** 정성적 절제 연구** 우리는 방법의 구성 요소를 남용하는 NAVI의 \'학교버스\' 장면에 대한 새로운 시각 합성 결과를 보여준다. 시각적 결과는 각 부분의 중요성을 강조합니다.\n' +
      '\n' +
      '그림 12: ** 통합 및 편집**는 처음에 다양한 조명 설정 하에서 포획되지만, 객체가 결국 여러 개체를 현장에서 일관되게 통합할 수 있다. BRDF 파라미터는 조명과 독립적으로 수정될 수 있다.\n' +
      '\n' +
      'SAMURAI[14]에서 메쉬 추출 컴포넌트의 수정된 버전을 사용하여 학습 부피 및 해당 재료 파라미터로부터 삼각 메세지를 추출한다. 전치 큐브는 초기 메쉬를 만드는 데 사용됩니다. 메쉬를 후 처리하고 블렌더[18]를 사용하여 자동 UV 풀림을 수행한다. 마지막으로 구운 표면 위치 주위에 BRDF에 대한 파이프라인을 쿼리하여 텍스처를 추출한다. 메쉬의 추출은 약 3분 정도 걸립니다.\n' +
      '\n' +
      '** 관계 및 재료 편집*** 우리 재구성된 자산은 기존 그래픽 파이프라인에 쉽게 통합될 수 있습니다. 그림에서. 12는 AR 및 VR 애플리케이션에 필요하기 때문에 새로운 일관된 조명 환경에서 NAVI 데이터세트로부터의 객체들을 특징으로 하는 SHINOBI 시간화된 장면을 보여준다. 또한 조명과는 독립적으로 BRDF 파라미터를 수정할 수 있습니다. 그림. 13은 동일한 카메라 뷰의 렌더링을 비교하지만 다른 환경 조명과 라이트한다. 주어진 애플리케이션에 대한 더 많은 예를 포함하는 보충 비디오를 시청하는 것도 고려하십시오.\n' +
      '\n' +
      '그림 13: ** 관계 적용** 뷰 합성은 "돌기" 장면에서 샘플 뷰에 대해 추정된 분해를 사용하여 세 가지 다른 조명 설정 하에서이다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>