<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization\n' +
      '\n' +
      ' 우리야오왕.\n' +
      '\n' +
      '메타FAIR에서 MetaFAIR.\n' +
      '\n' +
      'Pierre Gleize\n' +
      '\n' +
      'Hao Tang\n' +
      '\n' +
      'Xingyu Chen\n' +
      '\n' +
      '메타FAIR에서 MetaFAIR.\n' +
      '\n' +
      '상상 케빈.\n' +
      '\n' +
      '메타FAIR에서 MetaFAIR.\n' +
      '\n' +
      'Matt Feiszli\n' +
      '\n' +
      '메타FAIR에서 MetaFAIR.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '신경방사성 필드(NeRF)는 2D 이미지 세트를 감안할 때 노벨뷰 합성(NVS)에 대해 놀라운 성능을 나타낸다. 그러나 NeRF 훈련은 일반적으로 구조로부터 모션(SfM) 파이프라인에 의해 얻어지는 각각의 입력 뷰에 대한 정확한 카메라 포즈를 필요로 한다. 최근 작품들은 이러한 제약을 완화하려고 시도했지만, 여전히 그들이 다듬을 수 있는 괜찮은 초기 포즈에 의존하는 경우가 많다. 여기에서 포즈 초기화에 대한 요구 사항을 제거하는 것을 목표로 한다. 2D 비디오 프레임에서 NeRF를 트레이닝하기 위한 최적화 절차인 인스테이션 CONfidence(ICON)을 제시한다. ICON은 포즈에 대한 초기 추측을 추정하기 위해 부드러운 카메라 동작만을 가정한다. 또한 ICON은 구배를 동적으로 재체중화하는 데 사용되는 모델 품질의 적응적 측정인 "신뢰"를 소개한다. ICON은 NeRF를 배우기 위한 고신력 포즈, 포즈를 배우기 위해 고신력 3D 구조(NRF로 인코딩된)에 의존한다. 우리는 사전 포즈 초기화가 없는 ICON이 SfM 포즈를 사용하는 방법에 비해 CO3D와 HO3D 모두에서 우수한 성능을 달성한다는 것을 보여준다.\n' +
      '\n' +
      '미야오위앙@meta.com; mdf@meta.com. mdf@metang@meta.com에서 마트페리졸리.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '2D 비디오에서 3D로 물체를 쉽게 들어올리는 것은 광범위한 애플리케이션으로 인해 어려운 문제이다. 예를 들어, 가상, 혼합 및 증강 현실(2016)의 발전은 가상 3D 객체와의 새로운 상호 작용을 해제하고 있으며, 3D 객체 이해는 로봇 공학(예: 조작 카펠러 등(2018년), Wen et al.(2022년), Qi et al.(2023년) 및 학습별 Wen et al.(2023년)에도 중요하다.\n' +
      '\n' +
      '물체를 3D로 옮기는 것은 3D 구조를 추출하고 6DoF 포즈를 추적하는 것을 모두 필요로 하지만 기존 접근 방식은 많은 와나 ndB ekr은 (202 1); 아진 난형이다. (202 2) 와네 탈. 어귀는 아찔한 해녀를 낳았다. (202 0); 리즈 에스티네살.\n' +
      '\n' +
      'Figure 1: **Novel view and pose visualizations of ICON and BARF when no initial pose is available. We train on a flyaround video of book from CO3D Reizenstein et al. (2021). BARF trajectories exhibit fragmentation: camera poses split into two forward-facing clusters and create two books. ICON provides high-quality view synthesis and recovers poses very precisely. The colored triangle meshes represent ICON predicted poses and grey ones represent groundtruth.**Here we aim to tackle both problems jointly, learning both an implicit 3D representation and per-frame camera poses from a single monocular RGB video. We supervise both 6DoF poses and reconstruction with a dense photometric loss, projecting the 3D representation onto the 2D input frames. Specifically, we represent objects/scenes as a Neural Radiance Field (NeRF) Mildenhall et al. (2020) to obtain 2D rendering.\n' +
      '\n' +
      'While recent works Yen-Chen et al. (2021); Lin et al. (2021); Wang et al. (2021); Jeong et al. (2021); Lin et al. (2023); Truong et al. (2023) have shown that poses can to some extent be (jointly) learned in this setting, they are most effective when used to refine initial poses with moderate noise. For example, Wang et al. (2021) shows they begin to fail when pose noise exceeds approximately 20 degrees of rotation error; more complex trajectories are unrecoverable. Indeed, these methods also fail on even moderately-complex trajectories, for example a full 360-degree flyaround of an object (Sec. 4). This means SfM preprocessing remains a prerequisite for constructing a radiance field.\n' +
      '\n' +
      'One way forward would be to focus on the large-noise case, working to resolve larger pose changes. This is promising Meng et al. (2021), but here we go the other way, and focus on the incremental case. This arises naturally in real-world settings where video is input, e.g., embodied AI. We take inspiration from incremental SfM Schonberger and Frahm (2016) and SLAM Davison (2003), training pose and NeRF jointly in an incremental setting. In this setup, the model takes a stream of video frames, one at a time. Leveraging a motion-smoothness prior, we initialize an incoming frame with the previous frame\'s pose. Information between frames is exchanged through view synthesis from NeRF.\n' +
      '\n' +
      'A major challenge comes from the interdependence between 3D structure and pose: high photometric error may be attributable to a poor 3D model despite good pose, or a large error in pose despite a good model. We observe and analyze several interesting failure modes, including fragmentation, a generalization of the classical Bas-Relief ambiguity Belhumeur et al. (1999), and overlapping registration (see Fig. 3).\n' +
      '\n' +
      'To address the difficulties, we propose ICON (Incremental CONfidence). The intuition is simple (Fig. 2): "When pose is good, learn the NeRF; when the NeRF is good, learn pose." ICON interpolates between these two regimes, using a measure of confidence obtained from photometric error, and maintaining a NeRF-style "Neural Confidence Field" to store confidence in 3-space. Confidence is also used as a signal to guide optimization; in particular it can help identify (and escape from) local minima.\n' +
      '\n' +
      'We perform quantitative evaluation of ICON on CO3D Reizenstein et al. (2021), HO3D Hampali et al. (2020), and LLFF Mildenhall et al. (2019). While joint pose-and-3D baselines often fail catastrophically, ICON achieves strong performance on CO3D, comparable to NeRFs trained on COLMAP Schonberger and Frahm (2016) pose and surpassing a wide selection of baselines, such as DROID-SLAM Teed and Deng (2021) and PoseDiffusion Wang et al. (2023). In addition, we evaluate on CO3D videos with background removed; this\n' +
      '\n' +
      '그림 2: **ICON 개요***. ICON은 각 3D 위치에 대한 신뢰 \\(\\zeta\\)를 인코딩하기 위해 NeRF 위에 신경 진증 분야를 구성한다. 그런 다음 신뢰는 최적화 프로세스를 안내하는 데 사용됩니다.\n' +
      '\n' +
      '배경 텍스처가 카메라를 쉽게 추출하기 때문에 어려움을 크게 증가시킨다. 우리는 이 경우(분리되는 단일 마스킹 오브젝트)가 상당히 가치가 있다는 점에 주목하며, 여기에서의 성공은 카메라가 움직이고 있는지, 객체가 이동하는지, 또는 둘 다 작동하는지 여부를 의미한다. ICON은 NeRF+COLMAP 포즈와 바젤린의 광범위한 선택에 대해 우수한 성능을 달성하며, 핀란드 ICON은 RGB 바셀린을 능가하며 HO3D의 동적 휴대용 객체에 대해 SOTA RGB-D 방법 연방(SDF Wen et al.(2023)과 비슷하다.\n' +
      '\n' +
      '요약하자면, 우리는 다음과 같은 기여를 합니다.\n' +
      '\n' +
      '1. 우리는 공동 포즈 및 NeRF 최적화에 대한 증분 등록을 제안한다. 이 설정은 일반적인 비디오 설정에서 포즈 초기화를 위한 요구 사항을 제거한다.\n' +
      '2. 우리는 이 증분 설정을 체계적으로 연구하고 몇 가지 과제를 발견합니다. 관찰에 기초하여 공간 위치와 포즈에 대한 신뢰를 기반으로 하는 최적화 프로토콜인 ICON을 제안한다.\n' +
      '3. 물체 중심 데이터셋에 중점을 두고 ICON을 평가한다. ICON은 RGB 전용 방법 중 SOTA이며 SOTA RGB-D 방법에서도 경쟁력이 있다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '*** 신경 방사선 필드***(NeRF) 마덴힐 등(2020)은 새로운 뷰 합성을 위해 제기된 2D 이미지로부터 3D를 나타내는 강력한 기술이다. NRF의 한 가지 주요 한계는 정확한 카메라 포즈 요구 사항에 있다. Nerf- Wang et al(2021), BARF Lin et al.(2021), SCNeRF Lin et al.(2021), SiNeRF Xia et al al.(2022), NeuROIC Kuang et al al.(2022), IDR Yariv et al.(2020), GARF Chng et al.(2022), SPARF Truong et al.(2023) 등 최근 작품은 포즈와 NeRF를 공동 최적화하여 이 요구 사항을 완화하려고 시도했다. 유망한 방향에도 불구하고 시끄러운 초기 포즈를 정제할 때 가장 잘 작동하며 초기 포즈 추정 방법의 견고성에 의해 제한된다. 포즈에 대한 의존성을 더욱 줄이기 위해 커뮤니티의 한 가지 방향은 GAN 멍 등(2021), SLAM 로신올 등(2022), 모양 제사 장(2021), 깊이 Bian et al.(2023) 및 거친 주석 보스(2022)와 같은 초기 포즈 추정에 대한 추가 구성 요소 또는 신호를 추가하는 것이다. 우리는 이 문제를 다른 각도에서 해결하고 관절 NeRF의 증분 설정을 제안하고 최적화를 제기한다. 우리의 제안된 방법 ICON은 추가 신호를 사용하지 않으며 카메라 포즈가 얻기 어려울 때 도전 시나리오에 대한 강력한 성능을 달성한다.\n' +
      '\n' +
      '**Pose estimation (Object)** aims to infer the 6 Degrees-of-Freedom (DoF) pose of an object from image frames. The line of work can be classified into two main categories: image pose estimation Xiang et al. (2018); Labbe et al. (2020) and video pose tracking Muller et al. (2021); Stoiber et al. (2022); Teed and Deng (2020), where the former mostly focuses on inferring pose from sparse frames and the latter takes the temporal information into consideration. However, many methods in video or image pose estimation assume known instance- or category-level object representations, including object CAD models Xiang et al. (2018); Labbe et al. (2020, 2022); Sundermeyer et al. (2018); Wang et al. (2019); Stoiber et al. (2022); Muller et al. (2021) or pre-captured reference views with known poses Liu et al. (2022); Park et al. (2020). Recently, BundleTracks Wen and Bekris (2021) removes the need for such object priors, thus generalizing to pose tracking for unseen novel objects, and BundleSDF Wen et al. (2023) improves pose tracking by constructing a neural representation for the object. However, both require depth information, limiting their applications.\n' +
      '\n' +
      '**SLAM(Simultaneous Localization and Mapping)** builds a map of its environment while simultaneously determining its own location within that map Mur-Artal et al. (2015); Mur-Artal and Tardos (2017); Davison et al. (2007); Engel et al. (2014, 2017); Klein and Murray (2007); Zubizarreta et al. (2020). While most SLAM methods focus on understanding camera pose movement in a static environment, object-centric SLAM McCormac et al. (2018); Merrill et al. (2022); Runz et al. (2018); Salas-Moreno et al. (2013); Sharma et al. (2021) focus on learning object pose in a dynamic environment. However, most of those methods require depth signal Runz et al. (2018); McCormac et al. (2018); Merrill et al. (2022) and struggle with large occlusion or abrupt motion Wen et al. (2023).\n' +
      '\n' +
      'Method\n' +
      '\n' +
      'ICON은 RGB 비디오 프레임을 입력으로 스트리밍하고 3D 재구성 및 카메라 포즈 추정치를 생성한다. ICON은 신뢰에 의해 유도되는 3D 재구성을 최적화하기 위해 각 입력 프레임을 증분 등록한다: 3D 재구성은 높은 신뢰 포즈가 있는 프레임에서 더 많이 학습되고 포즈는 3D 재구성의 더 높은 신뢰 영역에서 3D-2D 재주입에 의존한다.\n' +
      '\n' +
      '라메니컬 라일스 펜스__프라이날리.\n' +
      '\n' +
      '(\\mathb{R}}\\mathb{f}}\\mathf{d\\)\\ a\\b{R}\\math{d}를 나타내는\\math{f}}\\math{d\\(\\math{d\\)는 입력 \\(\\math{d\\)\\b}}\\s{d\\(\\math{d\\:\\s{d\\)\\b}}\\b\\b}\\s{d\\)\\b\\b}\\s{d\\s{d\\s{d\\.\\b}\\s{d\\)\\b}\\b}\\s{d\\b}\\b}\\s{d\\)\\b}\\b}\\b}\\b}\\b}\\d\\d\\d\\)\\b}\\b}\\b}\\d\\d\\d\\I}를 구성하고\\d\\d\\)\\b}\\b}\\d\\d\\(\\)\\b}\\b}\\d\\d\\(\\)\\b}\\d\\d\\d\\d\\d\\d\\)\\ P_{i}\\(P_{i}\\) 카메라 포즈(P_{i}\\)에서 이미지\\(\\hat{I}_{i}\\)에서 장면의 2D 렌더링을 생성하기 위해, NeRF는 카메라 센터(p=(u,v)에서 광선을 따라 방사선을 집계하는 렌더링 함수 \\(\\)를 사용한다.\n' +
      '\n' +
      '}}}(z)\\mathf{\\mathf{r}}(z))\\mathf{\\mathsf{c}}(z),ddz(\\math{s})\\math{R}(p,P_{i\\)\n' +
      '\n' +
      '>{\\int_{\\mathsf{r}}}}(z)){\\mathsf{r}}}(z))\\)는 광선을 따라 축적된 투과율이며, \\(\\mathbf{\\mathsf{r}}(z)=o_{i}+zd\\)은 카메라 포즈 \\(P_{i}\\)에 의해 결정된 바와 같이 \\(f{nath_{f{naths}}}}. NeRF는 광선을 따라 샘플링된 지점을 통해 적분을 근사화하여 \\(\\mathcal{R}\\)를 구현하며, 모든 이미지(i=1, N\\)에 대해 접지 진위 뷰 \\(I_{i}\\)와 렌더링된 뷰 \\(\\hat{I}_{i}\\) 사이의 광학적 손실을 통해 훈련된다.\n' +
      '\n' +
      '타\\{*=\\arg\\et{L}_{p}(\\hat{I}},\\hat{I}_{p},\\hat{I})\n' +
      '\n' +
      '구축 프레임 등록.\n' +
      '\n' +
      '이러한 관절 포즈와 NeRF 최적화 방법에 대한 주요 한계는 좋은 초기 포즈에 대한 요구 사항이다. I\\(\\{P_{i}\\}\\)가 다양한 관점을 포함하고 있으며 모든 관점을 동일성에서 초기화한다면 이러한 방법은 종종 붕괴된다. 예를 들어, 단순하지만 일반적인 붕괴 용액은 단편화이며, 각 프레임은 자체 단편화된 3D 표현을 생성하며, 모두 다른 뷰들(** 프래그먼트** 무화과 3)에 상호 보이지 않는다. 실제로, BARF Lin et al.(2021)는 포즈 \\(\\{P_{i}\\}\\)가 닫힌 루프 플라이밍(Tab. 1 참조)으로 구성될 때 CO3D 데이터셋의 모든 서열에서 붕괴된다. 왕 등(2021)에서 살펴본 바와 같이, 포즈 이전이 제공되지 않는 경우 전체 궤적에 대해 20도 회전차이의 파괴점이 관찰된다.\n' +
      '\n' +
      '이 문제를 해결하기 위해 단순하면서도 효과적인 직관에 의존하며, 비디오의 카메라 동작은 매끄럽습니다. 따라서 비디오에서 프레임 \\(I_{i}\\)를 감안할 때, 카메라 포즈 \\(P_{i}\\)는 \\(P_{i-1}\\)에 가까울 가능성이 높다. 우리는 이 관찰을 레버리지하고 시간적 순서에 따라 프레임들을 확장적으로 등록할 것을 제안한다.\n' +
      '\n' +
      '** 구현*** 훈련 시작 시, 우리는 처음 두 프레임 \\(\\{I_{1},I_{2}\\}\\)에서 NeRF 매개변수 \\(\\{P_{1}, P_{2}\\}\\)를 공동으로 최적화하고 \\(\\{P_{1},I_{2}\\}\\)를 제공한다. 모든 \\(k\\) 반복 후, 우리는 새로운 프레임 \\(I_{i}\\)를 추가하고 \\(P_{i-1}\\)에 의해 포즈 \\(P_{i}\\)를 초기화한다. 모든 프레임이 등록될 때까지 포즈(\\{P_{i}\\}_{i=1}^{N}\\)와 NeRF \\(\\Theta\\)에 대한 학습률을 동결한다. 모든 \\(N\\) 이미지가 추가된 후에 학습률 붕괴 일정이 적용될 수 있다.\n' +
      '\n' +
      '### _Confidence-Based Optimization_\n' +
      '\n' +
      '증가적 등록 과정은 카메라 포즈에 대한 좋은 초기화를 제공하는 것을 목표로 한다. 그러나 광학적 손실을 사용하여 포즈와 NeRF를 최적화하는 것은 매우 비코넥스이며 많은 국소 최소 Yen-Chen et al.(2021), Lin et al.(2023)를 포함한다. 또한 잘못 최적화 된 포즈는 NeRF에 잘못된 학습 신호를 제공하여 포즈가 이미 등록된 시점(**오버링 등록** 무화과 3)에서 잘못 등록될 가능성을 높일 수 있다.\n' +
      '\n' +
      '이를 해결하기 위해 신뢰 유도 최적화 스키마를 제안합니다. 직관은 간단하며, 포즈 \\(P_{i}\\)가 자신 있는 경우 학습된 NeRF \\(f(\\Theta)\\(f)를 개선하기 위해 더 신뢰해야 하며, \\(P_{i}\\)에서 샘플링된 광선이 자신 있는 위치를 포함할 때 포즈 조절을 위해 가중치가 더 많이 있어야 한다. 신뢰도 그림 3: ** 3의 공동 포즈 및 NeRF 최적화의 주요 실패 모드인 단편화, 분지 구제 및 중복 등록**를 제공한다. 색색 포즈는 예측이며 회색 포즈는 접지 진실입니다. *** 절편**: 포스와 NeRF가 떨어져 분리되고 상호 보이지 않는 방사 영역을 생성한다. 여기서 토트루플의 튜브가 생성되며, 각각은 다음을 방해한다. 이 튜브 플립북 스타일의 파이는 각각 단일 토트리트럭을 볼 수 있습니다. 도표. 3공간의 다른 영역에서 완전히 독립적인 재구성이 발생하는 1. *** 기본 구제**: RGB 재구성의 고유한 모호성에 대한 모델은 테이블 내부에 오목한 사과를 생성하여 "구호"를 구성하므로 카메라 궤적이 180도 반전된다. ** 과매핑 등록**: 포즈 궤적의 2개의 하위 집합이 국소 최소에 갇혀 복사장의 동일한 부분을 잘못 관찰하여 흐릿한 렌더링과 빈 복셀을 유발한다. 여기서 토스터의 한쪽은 조회수가 겹쳐 흐릿한 반면, 다른 한쪽은 시야가 없고 공석이다.\n' +
      '\n' +
      'drops dramatically for a new frame, it is likely that the pose got stuck in a local minima, so we perform a restart to re-register this pose. This is similar to the trial and error strategy of COLMAP Schonberger and Frahm (2016). We next describe how we measure confidence for each pose \\(P_{i}\\) and each point/viewing direction \\((\\mathbf{x},\\mathbf{d})\\) in 3D.\n' +
      '\n' +
      '**Encoding confidence in 3D**. We construct a Neural Confidence Field on top of NeRF: given an input 3D location and direction \\((\\mathbf{x},\\mathbf{d})\\), NeRF \\(f\\) also predicts confidence \\(\\zeta_{(\\mathbf{x},\\mathbf{d})}\\). We add one fully-connected layer on top of the features, followed by a sigmoid, similar to the color prediction head.\n' +
      '\n' +
      '레이 \\(\\mathbf{r}\\)에 대한 신뢰는 불투명도 렌더링과 유사한 체적 응집을 통해 집계된다.\n' +
      '\n' +
      '}} (1)\\math{r} (z)\\math{r}}\\math{dz,{dz)\\.\n' +
      '\n' +
      '\\(\\mathcal{P}(z)=T(z)\\sigma(\\mathbf{r}(z))\\. 우리는 픽셀이 불투명할 때 첫 번째 용어가 더 두드러지는 반면 후자는 투명 픽셀에 대해 더 두드러진다는 점에 주목한다.\n' +
      '\n' +
      '자신감***를 측정합니다. 우리는 광계 오차를 통해 2D에서 픽셀 재프로세싱이 얼마나 잘 되는지 자신감을 측정한다. 우리는 광선(\\mathcal{L}_{\\text{r}})과 자신감을 감안할 때 \\(\\|e^{L}_{\\text{L}=\\|e^{-\\mathcal{E}/\\tau}/\\tau}-\\zeta_{\\mathbf{r}}})를 최소화하며, 여기서 \\(\\mathcal{L})는 NeRF 및 \\(\\mathcal{L}/\\|e^{mathcal{f{r}}/\\|tathcal{mathcal{f{r}/\\tathcal{mathcal{r}/\\tcal{mathcal{r}/\\tau}/\\tau}-\\tathcal{r}/\\tathcal{r}-\\tathcal{r}/\\tathcal{r}/\\tathcal{r}/\\tathcal{r}/\\tathcal{r}-\\tathcal{r}/\\tau}-\\tathcal{r}/\\t (\\mathcal{L}_{\\text{conf}}\\)는 신뢰 헤드를 훈련시키는 데만 사용되며 NeRF 매개변수 \\(\\Theta\\) 또는 포즈 이전에 구배가 중단된다.\n' +
      '\n' +
      '**Pose 신뢰****입니다. 우리는 \\(P_{i}\\)에서 샘플링된 광선에 대한 자신감을 집합함으로써 포즈 \\(P_{i}\\)에 대한 신뢰(\\zeta_{P_{i}}\\)를 계산한다. 시작 시 \\(P_{1}\\)는 신뢰 \\(1\\)를 가지며, 다른 사람들은 신뢰 \\(0\\)를 갖는다. 훈련 중 운동량 일정을 사용하여\\(B\\) 광선을 훈련하고(P_{i}}) 포즈(P_{i}}_{j}})에서\\(\\{mathbf{i}_{j1}^{t}})를 신뢰로 업데이트한다.\n' +
      '\n' +
      '}^{t_{i}}\\山_{mathbf{r}.\n' +
      '\n' +
      'The momentum \\(\\beta\\) is \\(0.9\\) in our experiments.\n' +
      '\n' +
      '자신감***에 의한 손실량**. 직관적 y: 우리는 토칼리브트 e\\(\\mathcal {L}\\)를 사용하여 토칼리브트 e\\(\\mathcal {L}\\)를 사용한다.\n' +
      '\n' +
      '* NeRF 매개변수 \\(\\Theta\\)에 대한 구배를 계산할 때, 손실은 \\(\\{\\zeta_{P_{i}}\\}\\)에 의해 가중된다.\n' +
      '* 포즈 \\(\\{P_{i}\\}\\)에 대한 구배를 계산할 때, 선당 손실은 광선 신뢰인 \\(\\{\\zeta_{\\mathbf{r}}\\}\\)에 의해 가중된다.\n' +
      '\n' +
      '각 단계에서 우리는 \\(P_{i}\\)의 광선 \\(\\{\\mathbf{r}_{j}^{i}\\}_{j=1}^{B}\\)를 샘플한다. 손실이요.\n' +
      '\n' +
      '(\\math{i})\\math{i}(\\math{{i})\\math{i}(\\math{{i})\\math{i}(\\math{{i})\n' +
      '\n' +
      '**Pose 재인격****입니다. 증가형 SfM Schonberger와 Frahm(2016)에서 시험 및 오류 등록 메커니즘에 의해 영감을 받아 새로운 이미지가 등록되지 않으면 이전 포즈로부터 재설정한다. (K\\_{i}},\\{P_{i}) 이전의 평균의 표준 편차(\\{\\q\\text{mean},\\{zeta_{j}},\\{zeta_{j}})는 \\(\\{zeta_{j}},\\_{j})를 등록하고 나면 실패한다. 우리는 실험 전반에 걸쳐 \\(\\lambda=2\\)와 \\(K=10\\)를 사용한다.\n' +
      '\n' +
      '유공자.\n' +
      '\n' +
      'Bas-relief ambiguity Belhumeur et al. (1999), and the related "hollow-face" optical illusion, are examples of fundamental ambiguity in recovering an object\'s 3D structure when objects that differ in shape produce identical images, perhaps under differing photometric conditions like lighting or shadow. For example, a surface with a round convex bump lit from the left may appear identical to the same surface with an concavity lit from the right. We refer generically to such situations as "Bas-Relief" solutions. Human visual systems are known to employ strong priors (e.g. favoring convexity) to select a particular solution among multiple possibilities.\n' +
      '\n' +
      'We observe this phenomenon when jointly optimizing camera poses and NeRF, especially early in optimization when total camera motion is small. The model becomes stuck in a local minimum and cannot escape. For example, a concave version of the scene may be reconstructed when the groundtruth is a convex scene (see **Bas Relief** in Fig. 3). In this example, the camera movement is off by 180 degrees and moves in opposite directions compared to the groundtruth trajectory. We believe that simple priors, using cues like coarse depth, could help produce more human-like interpretations of natural scenes. However, for this study we avoid crafting priors, and remark that our confidence-based calibration of losses helps reduce this issue (16% to 9%).\n' +
      '\n' +
      '우리는 또한 잘못된 분지 구제 솔루션이 일반적으로 더 높은 오류와 더 낮은 확신을 가지고 있음을 관찰하며, 구제 솔루션은 제한된 견해 세트에 유효하고 더 넓은 관점이 일치하지 않는다. 따라서 우리는 증분 SfM에서 재시작 전략을 채택하여 일반 솔루션을 제안한다. 예를 들어, COLMAP는 최종 재구성이 특정 기준(예: 등록된 이미지의 비율)을 충족하지 않는 경우 다른 초기 쌍을 식별하기 위해 재시작한다. 우리에게, 우리는 \\(K\\)를 독립적으로 발사하고 고정된 수의 반복 후에 자신감을 측정한다. 저희는 자신감이 가장 높은 것을 선택합니다. 실무상 3점을 출시하여 훈련의 10%에서 자신감을 측정합니다.\n' +
      '\n' +
      '기반의 기하학적 제약.\n' +
      '\n' +
      '최근의 작품인 정(2021)에 이어 트롱 등(2023년)은 최적화에 기하학적 제약을 더한다. 레이거리 손실 정주(2021년)와 깊이 일관성 상실 투롱 등(2023년)과는 다른 왕 등(2023년)과 유사하게 삼손 거리 하틀리와 지셔만(2003년)을 채택한다. 프레임과 이웃 사이의 대응 관계를 추출합니다. 우리는 주로 COLMAP와의 공정한 비교를 위해 SIFT Lowe(1999) 기능을 사용한다. 훈련 시간에 각 포즈 \\(P_{i}\\)에 대해 이웃에서 포즈 \\(P_{j}\\)를 샘플링한 다음 Sampson 거리를 계산한다.\n' +
      '\n' +
      '}_{text{j}}.^{x_{j}}.^{1}(x_{i})\n' +
      '\n' +
      'Hf(F\\)가 \\(P_{i}\\)와 \\(P_{j}\\) 사이의 기본 매트릭스인 경우(P_{j}\\)와 \\((x_{i}F)^{k}\\)는 \\(k\\) 요소를 나타낸다.\n' +
      '\n' +
      '신뢰에 의한 손실 보정은 기하학적 신호가 초기 최적화 풍경을 제한하는 데 도움이 되지만, 대응 쌍은 특히 질감이 적은 객체에 대해 잘못된 및/또는 픽셀 정확하지 않을 수 있다. 이로 인해 기하적 제약은 정확한 포즈와 재구성을 얻기 위해 ICON에 해로울 수 있다. 우리는 Sampson 거리를 중량화하기 위해 신뢰 \\(\\zeta_{P_{i}}\\)에 의존하는데, 한 쌍의 포즈 \\(P_{i}\\) 및 \\(P_{z}\\)의 경우, \\(1-\\min(\\zeta_{P_{i}}},\\zeta_{P_{j}}},\\)의 체중)에 의존한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '우리는 물체의 턴테이블 스타일 영상으로 구성된 대규모 데이터세트인 3D v2(**CO3D**) 데이터세트 레이젠슈타인(2021)의 공통 대상에 대한 연구를 집중한다. 라운드 진리 포즈는 COLMAP를 통해 얻어진다. 우리는 변형되지 않은 이미지 프레임(객체 및 배경 가시)을 사용하는 **full-scene**와 전경 객체 픽셀만 남기는 배경을 제거하는 *** 오브젝트 전용**의 두 버전으로 훈련한다. 우리는 객체 전용 버전이 더 도전적이면서도 의미 있는 평가 세트라고 믿습니다; 풀스크렌에서는 COLMAP가 포즈를 성공적으로 추출할 수 있는 질감 있는 배경에 물체를 배치하는 경우가 많다. 이것은 암묵적으로 객체 포즈와 카메라 포즈와 동일하며, 이러한 가정은 객체와 카메라가 모두 움직이는 동적 장면에서 파손된다. 우리는 대칭(객체 전용 설정에서 부패할 수 없는)으로 인해 "시나제"와 "돈트"가 제거된 일탈 집합에 의해 지정된 18개의 범주를 사용한다. 카메라 포즈 평가를 위해 높은 COLMAP 포즈 자신감을 가진 장면을 선택합니다. 양씨 등 양씨(2023년)를 사용하여 마스크를 청소하고 원래 마스크에 대한 결과가 보충에 존재한다. 동적 객체에 대한 성능을 입증하기 위해 우리는 또한 카메라 포즈 추적 및 합성 품질을 평가하기 위해 목적 **HO3D*Hampali 등 v2를 재목적했다. HO3D는 사람의 손에 의해 조작된 동적 객체를 캡처하는 정적 카메라 RGBD 영상으로 구성된다. 우리는 ICON에 RGB 프레임만 사용하고 8개의 비디오에서 8개의 클립(약 200 프레임 정도)을 선택하여 각각 다른 오브젝트를 커버한다. 마지막으로, 우리는 특히 NeRF에 대해 현장 수준의 새로운 뷰 합성에 일반적으로 사용되는 8개의 전향 장면을 갖는 데이터 세트인 **LLFF**Mildenhall 등(2019)에 대한 결과를 보여준다.\n' +
      '\n' +
      '***건축 및 Lchitect and Losses** 우리의 아키텍처는 NeRF Mildenhall et al.(2020)(계층적 샘플링 없음)를 따르고 이미지의 더 긴 가장자리를 640으로 설정했으며 NeRF의 표준 MSE 손실을 사용한다. Sampson 거리를 사용할 때 \\(10^{-4}\\)에 의해 가중치가 된다. 객체 마스크를 사용할 수 있는 CO3D 및 HO3D의 객체 전용 설정의 경우 MSE 손실을 사용하여 불투명도를 감독한다. HO3D의 경우 폐색 지역의 샘플링 광선을 피하기 위해 제공된 경우(8개의 클립 중 7개) 핸드 마스크를 사용한다.\n' +
      '\n' +
      '**Training**. We use BARF Lin et al. (2021) settings and train for 200k iterations. For CO3D and HO3D, we skip every other frame to reduce training time, producing sequences around 100 frames. For ICON and its variants, we add a new frame every 1k iterations (CO3D/HO3D) / 500 iterations (LLFF) and freeze the learning rate (100k iterations for HO3D and CO3D, 30k for LLFF). Following BARF, we do not use positional encodings during registration and apply coarse-to-fine positional encoding after registration.\n' +
      '\n' +
      '**Evaluation**. Following Lin et al. (2021), we evaluate on the last part (typically 10%) of each sequence. We measure camera pose quality with Absolute Trajectory Error (ATE) Zhang and Scaramuzza (2018), performing Umeyama alignment Umeyama (1991) of predicted camera centers with ground truth. ATE consists of a translation (ATE) and rotation (ATE\\({}_{\\text{rot}}\\)) component, evaluating \\(l2\\)-distance between camera centers and angular distance between aligned cameras, respectively. For novel view synthesis, we run an additional test-time pose refinement, following standard practices in previous works Lin et al. (2021); Wang et al. (2021); Yen-Chen et al. (2021); Truong et al. (2023). We use PSNR, LPIPS Zhang et al. (2018), and SSIM as metrics.\n' +
      '\n' +
      '** 기본*****입니다. 우리는 **BARF**Lin 등(2021) 위에 ICON을 구축하고 관절 포즈 및 NeRF 최적화를 위해 BARF와 비교한다. 새로운 뷰 합성을 위해 우리는 지상 진리 포즈로 NeRF를 훈련시킨다. 포즈의 경우, 우리는 확률적 포즈 확산 프레임워크 내에서 **PoseDiff*Wang et al.(2023) 모델 SfM을 비교하고, 추정된 3D 장면 흐름에서 동시 작업 **FlowCam**FlowCAM Smith et al.(2023) 해결 포즈는 SOTA 말단 학습 기반 SLAM 시스템인 **DROID**SLAM 시스템이다. 또한 예측된 포즈를 사용하여 NeRF를 초기화하고 훈련합니다. 또한, 물체 단독 CO3D 평가에서 우리는 학습 기반 기능인 슈퍼포인트 데톤(2017)+슈퍼Glue Sarlin et al.(**COLMAP+SPSG**)을 사용하여 최첨단 SfM 파이프라인 **COLMAP**Schonberger 및 Frahm(2016) 및 COLMAP Sarlin et al.(2019)의 증강 버전을 평가하고 있다. ICON은 _RGB_만을 사용하지만, 우리는 그라운드 진리 깊이 입력이 있는 DROID, **BundleTrack**Wen 및 Bekris(2021) 및 최첨단 **BundleSDF**Wen 등 HO3D에 인기 있는 _RGB-D_ 방법을 포함한다(2023).\n' +
      '\n' +
      'CO3D 풀 장면.\n' +
      '\n' +
      'C O3D의 정전기적격물질은 ICON과 아넬린소 nf, aullC O3Ds cenesi nTOH*ICON을 전체-scene CO3D에서 강력하게 비교합니다.\n' +
      '\n' +
      'Figure 4: **Novel view synthesis visualization of ICON without poses and NeRF trained with GT poses. Despite having no pose priors, ICON renders novel views at comparable or higher quality. Results are taken from LLFF and CO3D.**\n' +
      '\n' +
      'in camera pose variation that significantly exceeds the threshold after which BARF\'s performance collapses, with an \\(\\text{ATE}_{\\text{rot}}\\) exceeding 100 degrees. In contrast, ICON\'s incremental approach recovers significantly more precise camera poses (ATE of 0.137 and \\(\\text{ATE}_{\\text{rot}}\\) of 1.20), while also achieving better visual fidelity, both qualitatively and quantitatively, as measured by PSNR, SSIM, and LPIPS. Interestingly, ICON still outperforms BARF _even if BARF is provided with the ground truth poses at initialization_. We originally proposed this setting as an upper bound, but we believe this result reflects instability in early iterations of BARF training: CO3D sequences are challenging compared to BARF benchmark scenes (e.g. synthetic dataset from Mildenhall et al. (2020)/forward facing LLFF). Camera coverage is sparser, with more drastic lighting changes, and motion blur. Among the 18 scenes, BARF suffers from \\(\\geq\\) 10 degree \\(\\text{ATE}_{\\text{rot}}\\) in 4, dragging down the overall performance.\n' +
      '\n' +
      'We also make several comparisons with NeRF Mildenhall et al. (2020) and pose prediction methods. We provide NeRF with poses predicted by DROID-SLAM, FLOW-CAM, and PoseDiff, which rely on annotated poses to train or additional signals such as optical flow Teed and Deng (2020). However, our joint NeRF and pose training produces better pose estimates (as measured by ATE and \\(\\text{ATE}_{\\text{rot}}\\)), and as a result, NeRF\'s novel view synthesis suffers in comparison. Even given CO3D\'s ground truth poses, ICON can outperform NeRF. While this may at first seem surprising, we point out that even the "ground truth" poses in CO3D are not true ground truth; they are generated with COLMAP, which is not perfect. Additionally, in contrast to COLMAP, ICON\'s joint learning of NeRF and poses means that the estimated poses are specifically optimized to also maximize NeRF quality. We hypothesize that this leads to poses more compatible for learning a NeRF, as reflected by the better performance we observe. Similar observations were presented in prior works Jeong et al. (2021); Meng et al. (2021).\n' +
      '\n' +
      'CO3D 전용\n' +
      '\n' +
      '6DoF pose is inherently tricky to annotate, so past datasets often restrict motion to either the object or the camera; in the latter case, visually distinct backgrounds (e.g., specially designed patterns, such as QR codes around the object) are often used to make pose trajectory reconstruction easier. These strategies however do not generalize to more in-the-wild video, especially when both an object and the background (or camera) are moving. For this reason, we also perform evaluations on CO3D with the background masked out; in such a setting, algorithms are forced to only rely on object-based visual signal for estimating pose (Table 4.2).\n' +
      '\n' +
      'In this challenging setting, we again observe that BARF fails to estimate accurate poses, as the camera trajectory changes beyond what BARF can correct. Additionally, the difficulty of this setting produces further deterioration of BARF\'s novel view synthesis. However, we observe that ICON can still handle such videos, even without signal from the background. This implies ICON is viable for joint pose estimation and 3D object reconstruction on more general videos, when the background cannot be relied on.\n' +
      '\n' +
      'As with our full-scene CO3D experiments, we compare with methods for estimating pose, and how well those poses work when fed to a NeRF. We observe that without being able to leverage the background, these methods struggle magntily. Pose prediction ATE and \\(\\text{ATE}_{\\text{rot}}\\) from DROID-SLAM in particular shoot up from 0.431 to 5.903 and 8.92 to 90.25, respectively. With poorer pose, the quality of the learned NeRFs are also correspondingly worse.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c}  & ATE & \\(\\text{ATE}_{rot}\\) & PSNR & SSIM & LPIPS \\\\ \\hline Pose Source + NeRF & & & & & \\\\ DROID & 0.431 & 8.92 & 17.19 & 0.526 & 0.541 \\\\ FLOW-CAM & 2.681 & 91.28 & 14.40 & 0.441 & 0.689 \\\\ PoseDiff & 1.973 & 27.25 & 18.82 & 0.563 & 0.520 \\\\ \\hline Groundtruth & - & - & 21.03 & 0.575 & 0.629 \\\\ \\hline Joint Pose + NeRF optimization & & & & & \\\\ BARF & 6.215 & 114.63 & 12.77 & 0.401 & 0.871 \\\\ GT-Pose+BARF & 0.417 & 3.77 & 19.33 & 0.558 & 0.647 \\\\ \\hline ICON (Ours) & **0.138** & **1.16** & **22.24** & **0.654** & **0.428** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: CO3D 레이젠슈타인 등의 비교 (2021) 전체 이미지 장면. 기준선 BARF는 전반적으로 더 큰 카메라 동작으로 인해 CO3D에 실패할 수 있지만 ICON은 포즈를 매우 정확하게 추정하고 GT 포즈로 훈련된 NeRF보다 품질 유사하거나 더 나은 품질로 새로운 뷰를 렌더링할 수 있다.\n' +
      '\n' +
      'For pose in particular, we additionally evaluate COLMAP and its variant COLMAP-SPSG, which replaces SIFT Lowe (1999) with SuperPoint-SuperGlue DeTone et al. (2017); Sarlin et al. (2020), on how they predict pose from just the foreground objects of CO3D. We observe that COLMAP performs significantly worse when it cannot rely on background cues, far worse than ICON. We believe this finding to be especially significant, as COLMAP is often considered the gold standard for camera pose alignment, and is often treated as "ground truth" (as in CO3D). This suggests our incrementally learned joint pose and NeRF optimization represents a promising new alternative for posing moving foreground objects, even if the background or camera is also moving.\n' +
      '\n' +
      '핸드폰은 HO3D에서 동적 객체를 제공합니다.\n' +
      '\n' +
      'Understanding handheld objects is of particular importance to many applications, as the very nature of interaction often implies importance, and hands are often the source of object motion. Pose and 3D reconstructions are key components of understanding objects, so the ability to generate them from videos of handheld interactions is of high utility. We show results on HO3D Hampali et al. (2020) in Table 3.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c|c c c c c c c c c c c c c c c c c}  & & & \\multicolumn{5}{c|}{CO3D-FullImg} & \\multicolumn{5}{c|}{CO3D-No Background} & \\multicolumn{5}{c}{HO3D} \\\\ \\cline{3-19} \\multicolumn{1}{c|}{} & \\multirow{2}{*}{Ince} & \\multirow{2}{*}{Geo.} & \\multirow{2}{*}{Calib.} & \\multirow{2}{*}{Restart} & \\multicolumn{2}{c}{ATE} & \\multicolumn{2}{c}{ATE\\({}_{rot}\\)} & \\multicolumn{2}{c|}{PSNR} & \\multicolumn{2}{c|}{SSIM} & \\multicolumn{2}{c}{LPIPS} \\\\ \\cline{3-19} \\multicolumn{1}{c|}{} & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & **0.138** & **1.16** & **22.24** & **0.654** & **0.428** & **0.215** & **1.80** & **22.34** & **0.893** & **0.132** & **0.035** & **8.07** & **16.24** & **0.865** & **0.164** \\\\ \\cline{3-19} \\multicolumn{1}{c|}{} & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & 0.714 & 25.40 & 20.48 & 0.632 & 0.486 & 0.224 & 1.86 & **22.47** & 0.829 & **0.132** & 0.035 & 27.23 & 15.02 & 0.873 & 0.670 \\\\ \\cline{3-19} \\multicolumn{1}{c|}{} & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & & 0.691 & 28.95 & 18.66 & 0.565 & 0.556 & 0.340 & 3.91 & 21.92 & 0.887 & 0.140 & 0.032 & 19.19 & 14.51 & 0.866 & 0.184 \\\\ \\cline{3-19} \\multicolumn{1}{c|}{} & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & & 1.283 & 36.82 & 19.05 & 0.567 & 0.562 & 0.972 & 15.94 & 21.03 & 0.875 & 0.163 & 0.046 & 30.50 & 12.86 & 0.863 & 0.290 \\\\ \\cline{3-19} \\multicolumn{1}{c|}{} & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & & 3.075 & 78.49 & 14.38 & 0.454 & 0.816 & 0.890 & 8.05 & 20.67 & 0.850 & 0.187 & 0.076 & 32.26 & 12.51 & 0.870 & 0.189 \\\\ \\cline{3-19} \\multicolumn{1}{c|}{} & \\multirow{2}{*}{} & & & 6.215 & 114.63 & 12.77 & 0.401 & 0.871 & 6.522 & 114.97 & 8.22 & 0.772 & 0.370 & 0.307 & 131.16 & 7.45 & 0.82 & 0.29 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '가능한 경우 구성 요소를 제거하여 표 4: ** 증폭 연구를 한다. 우리는 모든 설계된 구성 요소가 ICON에 중요하다고 말했습니다. 또한, 우리는 CO3D 대상만이 (배경 없음) 장면에 대한 바지 구제를 관찰하지 않았기 때문에 유당의 효과는 미미하다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c}  & ATE & ATE\\({}_{rot}\\) & PSNR & SSIM & LPIPS \\\\ \\hline Pose Source + NeRF & & & & & \\\\ DROID & 5.903 & 90.25 & 14.54 & 0.181 & 0.818 \\\\ FLOW-CAM & 6.700 & 120.52 & 13.08 & 0.127 & 0.886 \\\\ PoseDiff & 4.601 & 64.24 & 15.42 & 0.508 & 0.492 \\\\ Groundtruth & - & - & 20.77 & 0.718 & 0.301 \\\\ \\hline COLMAP variants & & & & & \\\\ COLMAP(11) & 1.177 & 13.62 & & & & \\\\ COLMAP-SPSG(11) & 2.815 & 38.37 & & - & \\\\ COLMAP-SPSG & 3.616 & 43.74 & & & & \\\\ \\hline Joint Pose + NeRF optimization & & & & & \\\\ GT-Pose+BARF & 2.055 & 17.00 & 15.65 & 0.802 & 0.277 \\\\ BARF & 6.522 & 114.97 & 8.22 & 0.772 & 0.370 \\\\ \\hline ICON (Ours) & **0.215** & **1.80** & **22.45** & **0.893** & **0.132** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **는 CO3D 레이젠슈타인 등(2021) 객체 전용 장면을 배경 없이 비교한 것이다. 다른 방법의 배경 제거 및 실패에 대한 도전에도 불구하고 ICON은 높은 정밀도에서 포즈를 얻고 고품질에서 새로운 견해를 만들 수 있다. COLMAP는 11개 객체에 50% 이상의 프레임만 성공적으로 등록했기 때문에 비교를 위해 “(11)로 표시하였다. SPSG 버전의 COLMAP는 모든 장면을 등록하며 바닐라 COLMAP가 성공하는 11개의 장면 하위 집합에 데이터포인트를 포함한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c c}  & Input & ATE & ATE\\({}_{rot}\\) & Trans & PSNR \\\\ \\hline BARF & RGB & 0.135 & 122.38 & 0.580 & 5.72 \\\\ ICON & & 0.033 & 8.07 & 0.049 & **16.24** \\\\ \\hline \\end{tabular} \\begin{tabular}{c|c|c|c c c}  & Input & ATE & ATE\\({}_{rot}\\) & Trans & PSNR \\\\ \\hline BARF & RGB & 0.135 & 122.38 & 0.580 & 5.72 \\\\ ICON & & 0.033 & 8.07 & 0.049 & **16.24** \\\\ \\hline \\end{tabular}\n' +
      '\\begin{tabular}{c|c|c c c}  & Input & ATE & ATE\\({}_{rot}\\) & Trans & PSNR \\\\ \\hline Basculines & & & & & \\\\ \\hline DROID & RGB & 0.187 & 114.71 & 0.548 & & \\\\ DROID & & 0.105 & 51.93 & 0.262 & & \\\\ BundleTrack & RGB-D & 0.046 & 29.45 & 0.158 & & \\\\ BundleSDF & & **0.021** & **6.82** & **0.030** & & \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: **Comparison on HO3D Hampali et al. (2020). ICON works robustly against faster motion (vs CO3D), hand occlusion and lack of background information. In fact, despite only using RGB inputs, ICON can track poses at similar precision as SOTA RGB-D BundleSDF.**gain, we primarily compare against BARF for joint object pose estimation and NeRF learning. Similar to CO3D object-only version, background is masked out since it moves differently than object. In addition, HO3D presents challenges with hand-occlusion and faster pose changes than CO3D. As with CO3D, we observe that BARF struggles to properly learn pose, especially with more drastic camera motion across nearby frames. On the other hand, ICON can perform well with these challenges: poses are predicted accurately (Tab 3) and textures are rendered properly in novel views (Fig. 5)\n' +
      '\n' +
      'Several existing works Wen and Bekris (2021); Wen et al. (2023) addressing this problem additionally use depth, which provides a powerful signal for 3D object reconstruction and pose. On the other hand, depth requires additional sensors and is not always available, and most visual media on the internet is RGB-only. Interestingly, we find that our results with ICON are competitive with state-of-the-art methods like BundleSDF which do require depth. In addition, although we don\'t design or optimize ICON for mesh generation, we include a comparison on mesh by running an off-the-shelf MarchingCube Lorensen and Cline (1987) algorithm. We follow the evaluation protocol in Wen et al. (2023), use ICP for alignment Besl and McKay (1992) and report Chamfer distnace. Despite not using depth signals, we found ICON provides competitive mesh quality (0.7cm) compared to BundleSDF (0.77cm). We remark that BundleSDF\'s reconstruction performed poorly on one scene (2.39 cm); removing one worst scene for both method, BundleSDF and ICON achieved 0.54cm and 0.56cm. We believe that this represents the potential of monocular RGB-only methods for object pose estimation and 3D reconstruction.\n' +
      '\n' +
      '### Ablation studies\n' +
      '\n' +
      '**What are the key components in ICON?** We perform ablation studies to gain deeper insight why our proposed methodology leads to such significant improvements in Table 4, examining the impact of incremental frame registration ("Incre."), as well as confidence-based geometric constraint ("Geo."), loss calibration through confidence ("Calib."), and restarts ("Restart"). Note that the top row, with all options enabled, corresponds to our proposed ICON, while the bottom row (with none) is equivalent to BARF. We find all the proposed techniques to be essential\n' +
      '\n' +
      '**Whil emucho는 nthdexperim nth echallengings에 영향을 미치며 nth echallengings가 이온 및 N eR Frepresentatio ns,w edonotenfo rce. Ctapi O3D, BAR Fatatn.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E\n' +
      '\n' +
      '그림 5: HO3D에 대한 ICON 새로운 뷰 합성의 표준화를 보여준다. ICON은 모양과 질감을 정확하게 회복할 수 있습니다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '우리는 증분 설정에서 관절 포즈와 NeRF 최적화를 연구하고 이 환경에서 흥미롭고 중요한 문제를 강조했다. 이를 해결하기 위해 새로운 신뢰 기반 최적화 절차인 ICON을 설계했습니다. 다중 데이터 세트에 걸친 강력한 경험적 성능은 ICON이 공통 비디오에서 포즈 초기화의 요구 사항을 본질적으로 제거함을 시사한다. 우리의 초점은 객체 중심 시나리오에 있지만 다른 설정을 배제한 사제나 휴리스틱은 없다. ICON의 LLFF와 풀스코인 CO3D 결과는 강하며 이동 카메라(예: 예를 들어, 중심성 그레이만 등)로부터의 장면 재구성과 같은 보다 일반적인 유형의 비디오 입력에 대한 약속을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c}  & ATE & ATE\\({}_{\\text{rot}}\\) & PSNR & SSIM & LPIPS \\\\ \\hline GT-Pose+NeRF & - & - & 22.06 & 0.648 & 0.294 \\\\ BARF & 0.498 & 0.896 & 23.89 & 0.721 & 0.240 \\\\ ICON & **0.459** & **0.806** & **24.23** & **0.731** & **0.221** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: ** LLFF Mildenhall et al.(2019) 데이터세트**에 대한 비교. 카메라 포즈가 경미하거나 가벼운 움직임을 보일 때, BARF는 정체성 포즈 초기화와 잘 작용하고 ICON은 약간 더 나은 성능을 발휘한다. ATE는 100으로 확장됩니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Azinovic et al. [2022] Dejan Azinovic, Ricardo Martin-Brualla, Dan B Goldman, Matthias Niessner, and Justus Thies. Neural rgb-d surface reconstruction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6290-6301, June 2022.\n' +
      '* Belhumeur et al. [1999] Peter N Belhumeur, David J Kriegman, and Alan L Yuille. The bas-relief ambiguity. _International journal of computer vision_, 1999.\n' +
      '* 베슬과 맥케이[1992] 폴 제블과 닐 디 맥케이. 3-d 형상 등록 방법. __ 3-d 형상 등록 방법. IEEE는 패턴 분석 및 기계 정보_, 14(2):239-256, 1992. 도이: 10.1109/34.121791[https://doi/10.1109/34.121791](https://doi/10.1109/34.121791).\n' +
      '* Bian et al. [2023] Wenjing Bian, Zirui Wang, Kejie Li, Jiawang Bian, and Victor Adrian Prisacariu. Nope-nerf: Optimising neural radiance field with no pose prior. 2023.\n' +
      '* Boss et al. [2022] Mark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen Li, Deqing Sun, Jonathan T. Barron, Hendrik P.A. Lensch, and Varun Jampani. SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* Caron et al. [2021] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* Cheng et al. [2023] Shuo Cheng, Caelan Garrett, Ajay Mandlekar, and Danfei Xu. NOD-TAMP: Multi-step manipulation planning with neural object descriptors. In _Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition @ CoRL2023_, 2023. [https://openreview.net/forum?id=43MSbj5mSS](https://openreview.net/forum?id=43MSbj5mSS).\n' +
      '* Chng et al. [2022] Shin-Fang Chng, Sameera Ramasinghe, Jamie Sherrah, and Simon Lucey. Gaussian activated neural radiance fields for high fidelity reconstruction and pose estimation. In _The European Conference on Computer Vision: ECCV_, 2022.\n' +
      '* Dai et al. [2017] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niessner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. _Computer Vision and Pattern Recognition (CVPR)_, pages 5828-5839, 2017. doi: 10.1109/CVPR.2017.618. [http://www.scan-net.org/](http://www.scan-net.org/).\n' +
      '* 다비슨[2003] 다비슨이요. 실시간 동시 로컬화 및 단일 카메라로 매핑됩니다. 컴퓨터 비전_ 페이지는 1403-1410. IEEE, 2003. IEEE에 대한 _프로플레이션 Ninth IEEE 국제 콘퍼런스입니다.\n' +
      '* Davison et al. [2007] Andrew J Davison, Ian D Reid, Nicholas D Molton, and Olivier Stasse. Monoslam: Real-time single camera slam. _IEEE transactions on pattern analysis and machine intelligence_, 29(6):1052-1067, 2007.\n' +
      '* DeTone et al. [2017] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest point detection and description. _2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, pages 337-33712, 2017. [https://api.semanticscholar.org/CorpusID:4918026](https://api.semanticscholar.org/CorpusID:4918026).\n' +
      '* Engel et al. [2014] Jakob Engel, Thomas Schops, and Daniel Cremers. Lsd-slam: Large-scale direct monocular slam. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part II 13_, pages 834-849. Springer, 2014.\n' +
      '* Engel et al. [2017] Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. _IEEE transactions on pattern analysis and machine intelligence_, 40(3):611-625, 2017.\n' +
      '* Grauman et al. [2017] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcug Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abraham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttkeysa Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video. In _Computer Vision and Pattern Recognition_, 2022.\n' +
      '* Ghahahramani et al. [2017]Shreyas Hampali, Mahdi Rad, Markus Oberweger, and Vincent Lepetit. Honnotate: A method for 3d annotation of hand and object poses. In _Computer Vision and Pattern Recognition_, 2020.\n' +
      '* Hartley and Zisserman (2003) Richard Hartley and Andrew Zisserman. _Multiple View Geometry in Computer Vision_. Cambridge University Press, USA, 2 edition, 2003. ISBN 0521540518.\n' +
      '* Jeong et al. (2021) Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Anima Anandkumar, Minsu Cho, and Jaesik Park. Self-calibrating neural radiance fields. In _International Conference on Computer Vision_, 2021.\n' +
      '* Kappler et al. (2018) Daniel Kappler, Franziska Meier, Jan Issac, Jim Mainprice, Cristina Garcia Cifuentes, Manuel Wuthrich, Vincent Berenz, Stefan Schaal, Nathan Ratliff, and Jeannette Bohg. Real-time perception meets reactive motion generation. _IEEE Robotics and Automation Letters_, 3(3):1864-1871, 2018. doi: 10.1109/LRA.2018.2795645.\n' +
      '* Klein and Murray (2007) Georg Klein and David Murray. Parallel tracking and mapping for small ar workspaces. In _2007 6th IEEE and ACM international symposium on mixed and augmented reality_, pages 225-234. IEEE, 2007.\n' +
      '* Kuang et al. (2022) Zhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng Huang, Panos Achlioptas, and Sergey Tulyakov. Neroic: Neural rendering of objects from online image collections. _ACM Trans. Graph._, 41(4), jul 2022. ISSN 0730-0301. doi: 10.1145/3528223.3530177. [https://doi.org/10.1145/3528223.3530177](https://doi.org/10.1145/3528223.3530177).\n' +
      '* Labbe et al. (2020) Yann Labbe, Justin Carpentier, Mathieu Aubry, and Josef Sivic. Cosypose: Consistent multi-view multi-object 6d pose estimation. In _European Conference on Computer Vision_, 2020.\n' +
      '* Labbe et al. (2022) Yann Labbe, Lucas Manuelli, Arsalan Mousavian, Stephen Tyree, Stan Birchfield, Jonathan Tremblay, Justin Carpentier, Mathieu Aubry, Dieter Fox, and Josef Sivic. Megapose: 6d pose estimation of novel objects via render & compare. _arXiv preprint arXiv:2212.06870_, 2022.\n' +
      '* Lin et al. (2021) Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural radiance fields. In _IEEE International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* Lin et al. (2023) Yunzhi Lin, Thomas Muller, Jonathan Tremblay, Bowen Wen, Stephen Tyree, Alex Evans, Patricio A. Vela, and Stan Birchfield. Parallel inversion of neural radiance fields for robust pose estimation. In _ICRA_, 2023.\n' +
      '* Liu et al. (2022) Yuan Liu, Yilin Wen, Sida Peng, Cheng Lin, Xiaoxiao Long, Taku Komura, and Wenping Wang. Gen6d: Generalizable model-free 6-dof object pose estimation from rgb images. In _European Conference on Computer Vision_, pages 298-315. Springer, 2022.\n' +
      '* Lorensen and Cline (1987) William E. Lorensen and Harvey E. Cline. Marching cubes: A high-resolution 3d surface construction algorithm. _Computer Graphics_, 21(4):163-169, 1987. doi: 10.1145/37402.37422. [https://doi.org/10.1145/37402.37422](https://doi.org/10.1145/37402.37422).\n' +
      '* 로이(1999D avidG.O jectr 생태인지)는 ale-i n 변이체 s.__를 계산합니다. 내부 회의 nceonCompu te rVision( ICCV)_,pages11 50-11 57,1999.도 i:10.1109/ICCV1999.790410[280540 6005617117513][https://www.cs.c.c.C.99pdf].\n' +
      '* Marchand et al. (2015) Eric Marchand, Hideaki Uchiyama, and Fabien Spindler. Pose estimation for augmented reality: A hands-on survey. _IEEE Transactions on Visualization and Computer Graphics_, 22(12):2633-2651, 2016. doi: 10.1109/TVCG.2015.2513408.\n' +
      '* McCormac et al. (2018) John McCormac, Ronald Clark, Michael Bloesch, Andrew Davison, and Stefan Leutenegger. Fusion++: Volumetric object-level slam. In _2018 international conference on 3D vision (3DV)_, pages 32-41. IEEE, 2018.\n' +
      '* Meng et al. (2021) Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, and Jingyi Yu. GNeRF: GAN-based Neural Radiance Field without Posed Camera. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* Merrill et al. (2022) Nathaniel Merrill, Yuliang Guo, Xingxing Zuo, Xinyu Huang, Stefan Leutenegger, Xi Peng, Liu Ren, and Guoquan Huang. Symmetry and uncertainty-aware object slam for 6dof object pose estimation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14901-14910, 2022.\n' +
      'Cen Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima 크라디미 칼란타리, Ravi 람모에키R enN g,andA bhishekK ar.L ocall hishekK 아르.P acticalvi ewsy ntstrongwiwitopa 에스펠린:P acticalvi ewsy ntodeCE 에스블레이밍구 메슬링구 아이솔:P 액티컬 이셴드프 펜드프 펜스프레드프 프레이온:P acticalvsy L. CMTrsacton Gr aphics(T OG)_,38(4):1-14,20 19.\n' +
      '* Mildenhall et al. (2020) Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In _European Conference on Computer Vision_, 2020.\n' +
      '* Muller et al. (2021) Norman Muller, Yu-Shiang Wong, Niloy J Mitra, Angela Dai, and Matthias Niessner. Seeing behind objects for 3d multi-object tracking in rgb-d sequences. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6071-6080, 2021.\n' +
      '* Mur-Ar ta lan dTardos2017 ]RaulMur-아트 alandJuanDT ardos. IEEEtra nsactionso nr 망상 _,33(5):12 55-1262,201 7.\n' +
      '* Mur-Artal and Tardos [2017] Raul Mur-Artal and Juan D Tardos. Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras. _IEEE transactions on robotics_, 33(5):1255-1262, 2017.\n' +
      '* Mur-Artal et al. [2015] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. Orb-slam: a versatile and accurate monocular slam system. _IEEE transactions on robotics_, 31(5):1147-1163, 2015.\n' +
      '* Oechsle et al. [2021] Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In _International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* Park et al. [2020] Keunhong Park, Arsalan Mousavian, Yu Xiang, and Dieter Fox. Latentfusion: End-to-end differentiable reconstruction and rendering for unseen object pose estimation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10710-10719, 2020.\n' +
      '* 파우웰과 크라이즈[2015] 칼 파우웨인과 다니카 크레이크. 심트랙: 확장 가능한 실시간 객체를 위한 시뮬레이션 기반 프레임워크는 검출 및 추적을 제공한다. 2015년 지능형 로봇 및 시스템_에 관한 _국제회의에서.\n' +
      '* Qi et al. [2023] Haozhi Qi, Brent Yi, Sudharshan Suresh, Mike Lambeta, Yi Ma, Roberto Calandra, and Jitendra Malik. General in-hand object rotation with vision and touch. In _7th Annual Conference on Robot Learning_, 2023. [https://openreview.net/forum?id=R1N00jfIV-X](https://openreview.net/forum?id=R1N00jfIV-X).\n' +
      '* Reizenstein et al. [2021] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In _International Conference on Computer Vision_, 2021.\n' +
      '* Rosinol et al. [2022] Antoni Rosinol, John J Leonard, and Luca Carlone. Nerf-slam: Real-time dense monocular slam with neural radiance fields. _arXiv preprint arXiv:2210.13641_, 2022.\n' +
      '* Runz et al. [2018] Martin Runz, Maud Buffier, and Lourdes Agapito. Maskfusion: Real-time recognition, tracking and reconstruction of multiple moving objects. In _2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)_, pages 10-20. IEEE, 2018.\n' +
      '* Salas-Moreno et al. [2013] Renato F Salas-Moreno, Richard A Newcombe, Hauke Strasdat, Paul HJ Kelly, and Andrew J Davison. Slam++: Simultaneous localisation and mapping at the level of objects. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1352-1359, 2013.\n' +
      '* Sarlin et al. [2019] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine: Robust hierarchical localization at large scale. In _CVPR_, 2019.\n' +
      '* Sarlin et al. [2020] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4938-4947, 2020.\n' +
      '* 소촌베르거와 프래름[2016] 요하네스 로촌베르거와 얀-미카엘 프라흐름. 구조로부터 모션이 재방출되었습니다. 컴퓨터 비전 및 패턴 인식_ 페이지 4104-4113에 대한 IEEE 회의의 _발표에서 2016년 페이지 4104-4113.\n' +
      '* Sharma et al. [2021] Akash Sharma, Wei Dong, and Michael Kaess. Compositional and scalable object slam. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 11626-11632. IEEE, 2021.\n' +
      '* Smith et al. [2023] Cameron Smith, Yilun Du, Ayush Tewari, and Vincent Sitzmann. Flowcam: Training generalizable 3d radiance fields without camera poses via pixel-aligned scene flow, 2023.\n' +
      '* Stoiber et al. [2022] Manuel Stoiber, Martin Sundermeyer, and Rudolph Triebel. Iterative corresponding geometry: Fusing region and depth for highly efficient 3d tracking of textureless objects. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6855-6865, 2022.\n' +
      '* Sun et al. [2021] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. _CVPR_, 2021.\n' +
      '* Sundermeyer et al. [2018] Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker, and Rudolph Triebel. Implicit 3d orientation learning for 6d object detection from rgb images. In _Proceedings of the european conference on computer vision (ECCV)_, pages 699-715, 2018.\n' +
      '2020: 제16회 유럽 회의: 글라스고, 영국, 8월 23-28일, 2020년 8월 23-28일, 프로리딩, 파트 II_, 페이지 402-419, 페이지 402-419, 페이지 402_, 페이지 402-419,* ECCV 2020: 제16회 유럽 회의: 글라스고우, 영국, 2020년 8월 23-28일.\n' +
      '베를린, 하이델베르크, 2020년 스프링거-베를래그. ISBN 978-3-078-030-030-58535-8.\n' +
      '* 테드 앤 덩[2021] 자차 테드 앤 지아 다이. DROID-SLAM: Monocular, Stereo 및 RGB-D Cameras의 딥 비주얼 SLAM. __ 뉴럴 정보 처리 시스템_ 2021의 발전.\n' +
      '* Truong et al. [2023] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, and Federico Tombari. Sparf: Neural radiance fields from sparse and noisy poses. In _Computer Vision and Pattern Recognition_, 2023.\n' +
      '* Tschernezki et al. [2021] Vadim Tschernezki, Diane Larlus, and Andrea Vedaldi. NeuralDiff: Segmenting 3D objects that move in egocentric videos. In _Proceedings of the International Conference on 3D Vision (3DV)_, 2021.\n' +
      '* 우미야마[1991] 신지 우미야마. 두 점 패턴 사이의 변환 파라미터의 정지 제곱 추정. __ 2개의 점 패턴 간의 형질전환 파라미터의 평균 제곱 추정. __리바스트 제곱 추정. IEEE 전환은 1991년 패턴 분석 및 기계 정보_, 13(04):376-380에 관한 것이다.\n' +
      '* Wang et al. [2019] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas. Normalized object coordinate space for category-level 6d object pose and size estimation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2642-2651, 2019.\n' +
      '* Wang et al. [2023] Jianyuan Wang, Christian Rupprecht, and David Novotny. Posediffusion: Solving pose estimation via diffusion-aided bundle adjustment. In _International Conference on Computer Vision_, 2023.\n' +
      '* Wang et al. [2021a] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. _arXiv preprint arXiv:2106.10689_, 2021a.\n' +
      '* Wang et al. [2020] Wenshan Wang, Yaoyu Hu, and Sebastian Scherer. Tartanov: A generalizable learning-based vo. 2020.\n' +
      '* 왕 등[2021b] 지의왕, 샹제우, 위디 제이, 민첸, 빅토르 아드리안 프리스카리루. NeRF\\(-\\)-: 알려져 있는 카메라 파라미터가 없는 신경 방사선 분야. __알려진 카메라 파라미터가 없다. arXiv 프리프린트 arXiv:2102.07064_ 2021b.\n' +
      '* Wen and Bekris [2021] Bowen Wen and Kostas Bekris. Bundletrack: 6d pose tracking for novel objects without instance or category-level 3d models. In _2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, page 8067-8074. IEEE Press, 2021. doi: 10.1109/IROS51168.2021.9635991. [https://doi.org/10.1109/IROS51168.2021.9635991](https://doi.org/10.1109/IROS51168.2021.9635991).\n' +
      '* Wen et al. [2022a] 보웬 위넨, 위즈하오 리안, 코스타스 베크리스 및 스테판 슈알라. Catgrasp: 시뮬레이션에서 배설물에서 학습 범주 수준 과제 관련 파악은 시뮬레이션에서이다. __ 학습 범주 수준 과제 관련 파악이다. ICRA 2022_, 2022a.\n' +
      '* Wen et al. [2022b] Bowen Wen, Wenzhao Lian, Kostas E. Bekris, and Stefan Schaal. You only demonstrate once: Category-level manipulation from single visual demonstration. _ArXiv_, abs/2201.12716, 2022b. [https://api.semanticscholar.org/CorpusID:246430152](https://api.semanticscholar.org/CorpusID:246430152).\n' +
      '* Wen et al. [2023] Bowen Wen, Jonathan Tremblay, Valts Blukis, Stephen Tyree, Thomas Muller, Alex Evans, Dieter Fox, Jan Kautz, and Stan Birchfield. Bundlesdf: Neural 6-dof tracking and 3d reconstruction of unknown objects. _Computer Vision and Pattern Recognition_, 2023.\n' +
      '* Xia et al. [2022] Yitong Xia, Hao Tang, Radu Timofte, and Luc Van Gool. Sinerf: Sinusoidal neural radiance fields for joint pose estimation and scene reconstruction. In _33rd British Machine Vision Conference 2022, BMVC 2022, London, UK, November 21-24, 2022_. BMVA Press, 2022. [https://bmvc2022.mpi-inf.mpg.de/0131.pdf](https://bmvc2022.mpi-inf.mpg.de/0131.pdf).\n' +
      '* Xiang et al. [2018] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. In _Robotics: Science and Systems (RSS)_, 2018.\n' +
      '* Yang et al. [2023] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos, 2023.\n' +
      '* Yariv et al. [2020] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. _Advances in Neural Information Processing Systems_, 33, 2020.\n' +
      '* Yariv et al. [2021] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. In _Thirty-Fifth Conference on Neural Information Processing Systems_, 2021.\n' +
      '* Yen-Chen et al. [2021] Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, and Tsung-Yi Lin. iNeRF: Inverting neural radiance fields for pose estimation. In _IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, 2021.\n' +
      '* Yu et al. [2021] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance fields from one or few images. In _CVPR_, 2021.\n' +
      '유 등. [2021]조슨 Y]. 장, 풍산양, 슈바햄 툴시안, 데바 라만. NeRS: 야생에서 희소뷰 3d 재구성을 위한 신경 반사면. 2021년 신경 정보 처리 시스템__Conference에서.\n' +
      '* Zhang et al. [2018] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 586-595, 2018. doi: 10.1109/CVPR.2018.00068.\n' +
      '* 장과 스카라무자[2018] 지차오 장과 다빈드 스카라무자]. 시각적 (-inertial) 악취 측정에 대한 정량적 궤적 평가에 대한 튜토리얼이다. i_2018 IEEE/RSJ 국제 지능형 로봇 및 시스템(IROS)_, 페이지 7244-7251. IEEE 2018.\n' +
      '* Zhao et al. [2022] Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild. In _European conference on computer vision (ECCV)_, 2022.\n' +
      '* Zubizarreta et al. [2020] Jon Zubizarreta, Iker Aguinaga, and Jose Maria Martinez Montiel. Direct sparse mapping. _IEEE Transactions on Robotics_, 36(4):1363-1370, 2020.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:18]\n' +
      '\n' +
      '다른 CO3D 범주의 ICO.\n' +
      '\n' +
      'In this section, we supplement the results reported in the main paper on CO3D Reizenstein et al. (2021). We add a study using all the remaining 33 categories from CO3D and evaluate on the full scene. This makes it possible for us to include symmetric objects such as vase whose poses are indistinguishable in the object-only evaluation. Since no official subset is specified for these categories, we take top-4 instances from each category with highest camera pose confidence and randomly sample one instance for each category. It is worth noting that the "ground-truth" camera poses are estimated by COLMAP, and may not be 100% accurate, especially these categories are not part of the official benchmarking sets. We use the same (hyper-)parameters as the main paper benchmarking on the 18 categories.\n' +
      '\n' +
      'We report the results in Tab 9. We observe that most objects achieve similar results as Tab 6. However, there are a few objects where ICON yields imprecise poses, dragging down the average metrics. We believe there are two causes. First, ICON relies on photometric loss and may suffer from changes in the scenes. Many of the scenes where ICON has \\(\\geq 3\\) degree rotation error have moving shadows (either object or human), strong lighting change (from the builtin flash of the camera) or reflective surfaces. We show a few examples here in Fig. 6. Second, the groundtruth poses used to evaluate the trajectory are generated by COLMAP, which may not be accurate, especially the categories not included in the official benchmarking sets.\n' +
      '\n' +
      '스칸넷에 대한 평가 금액.\n' +
      '\n' +
      'ICON은 CO3D 및 HO3D와 같은 객체 중심 영상에 대한 연구에 중점을 둔다. 그러나 ICON은 다른 유형의 비디오에서 작동하는 것을 방지하는 오브젝트에 맞춘 특정 디자인을 적용하지 않는다. 여기에서 스카넷 Dai et al.(2017)에 ICON을 벤치마킹하여 예비 연구를 포함한다. 스칸넷 테스트 세트의 20개 장면 중 10개를 무작위로 샘플하고 카메라 포즈에서 NaN 값이 있는 2. 스켄의 보이드가 있는 200개 프레임의 클립을 사용하면 장면을 샘플링할 때 제거된다.\n' +
      '\n' +
      '전작 Zhao et al.(2022)에 따른 카메라 포즈 품질을 회전 및 번역에 대한 절대 추적 유도(ATE (m))에 대해 연관 Pose Error(RPE)를 사용하여 보고한다. 스카넷의 일부 궤적은 번역이 매우 작고 궤적을 정렬한 다음 회전을 평가하는 것이 신뢰할 수 없기 때문에 Zhao et al.(2022)를 따르지 않는다.\n' +
      '\n' +
      'We do not change _any_ (hyper-)parameters used in CO3D full scene training for ICON to stress test the system on the significantly different scenarios in ScanNet. We include four methods designed to work well on ScanNet for comparison: TartanVO Wang et al. (2020), COLMAP Schonberger and Frahm (2016), DROID-SLAM Teed and Deng (2021) and current state-of-the-art method ParticleSfM Zhao et al. (2022). We note that COLMAP and ParticleSfM may fail to perform well when running only on the short clip, so we run them on the entire video and report the results on the clip. In addition, as noted in Zhao et al. (2022), since COLMAP often fail on many ScanNet scenes, we use a tuned version following Tschernezki et al. (2021).\n' +
      '\n' +
      'We report results in Tab 10. Despite having no tuning or change when transferring from CO3D, ICON achieves strong performance on ScanNet compared to the state-of-the-art methods designed to work well on\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c}  & ATE & \\(\\text{ATE}_{rot}\\) & Trans & PSNR & CD(cm) \\\\ \\hline SiS1 & 0.028 & 3.80 & 0.017 & 19.13 & 0.23 \\\\ MC1 & 0.019 & 5.90 & 0.049 & 14.24 & 0.41 \\\\ ABF13 & 0.064 & 10.67 & 0.094 & 11.79 & 1.72 \\\\ GPMF12 & 0.029 & 11.23 & 0.056 & 16.27 & 0.38 \\\\ ND2 & 0.027 & 7.18 & 0.015 & 20.06 & 0.50 \\\\ SM2 & 0.026 & 5.56 & 0.032 & 13.51 & 0.85 \\\\ SMu1 & 0.017 & 13.19 & 0.081 & 14.46 & 1.02 \\\\ AP13 & 0.058 & 7.06 & 0.046 & 20.42 & 0.50 \\\\ \\hline Avg & 0.033 & 8.07 & 0.049 & 16.24 & 0.70 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'HO3D 평가에 대한 ICON의 퍼센 성능은 표 8::였다. CD는 메쉬 품질을 측정하는 Chamfer Distance를 의미합니다.\n' +
      '\n' +
      '그림 6: ICON이 더 큰 오류를 생성하는 스켄이다. ICON은 주로 광계 손실이 일관되지 않은 감독을 생성하는 장면으로 고통받고 있다. 자동차 예는 인간의 그림자와 자동차 상의 반사면을 움직이는 것으로 구성된다. 와인글라스 예는 투명 표면 및 광 반사를 포함한다. 도넛 예는 일관되지 않은 조명을 포함하며, 여기서 카메라로부터의 플래시는 전면에서 더 밝은 색을 생성하고 뒷부를 어둡게 한다. 다른 관점에서 이러한 불일치는 ICON이 부정확한 카메라를 생성하게 한다.\n' +
      '\n' +
      '캔넷 스타일 비디오. 우리는 이것이 ICON이 다른 유형의 영상에 일반화되고 적응될 수 있다는 개념 증명이라고 믿는다.\n' +
      '\n' +
      '## 부록 D 임장 및 향후 방향\n' +
      '\n' +
      'ICON은 포즈와 NeRF를 공동으로 최적화하기 위해 강력한 성능을 달성하지만 몇 가지 한계를 가지고 있다. 첫째, ICON은 NeRF와 포즈 모두에 대한 감독으로서 광학적 손실에 크게 의존한다. 이것은 색상이 서로 다른 관점에 걸쳐 적당히 일관된다는 가정에 의존한다. 그러나 이러한 가정은 실제 세계에서 깨질 수 있다. ICON은 광계 손실이 일치하지 않는 부피에 대한 자신감을 사용하지만 모호성으로 인해 부정확한 포즈(5~10도 회전 오류)를 생성할 것이다. Tab 9 및 Fig 6에서 볼 수 있듯이 ICON은 움직임, 반사 표면, 투명도 및 강력한 조명 변화를 겪는다. DINO 카온(2021)과 같은 이러한 변화에 강력한 기능을 활용하는 것이 이 문제를 완화하는 데 도움이 될 수 있다고 믿는다.\n' +
      '\n' +
      '또한 ICON은 훈련하는 데 몇 시간이 걸리는 NeRF 마덴힐 등(2020)을 통한 구배 기반 최적화에 달려 있다. 우리는 ICON을 3 공간의 보다 효율적인 모델링과 결합시키는 것이 PixelNeRF 유(2021)와 FLOW-CAM 스미스 등 유망한 방향이 될 것이라고 믿는다(2023).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l r r r r r r} \\hline Category & Scene & \\multicolumn{3}{c}{ATE} & \\multicolumn{1}{c}{ATE\\({}_{rot}\\)} & \\multicolumn{1}{c}{PSNR} & \\multicolumn{1}{c}{SSIM} & \\multicolumn{1}{c}{IPIPS} \\\\ \\hline backpack & 506\\_72977 & 141839 & 0.060 & 0.42 & 20.74 & 0.59 & 0.42 \\\\ banana & 612\\_97867 & 196978 & 1.691 & 11.23 & 13.04 & 0.15 & 0.81 \\\\ baseball & 375 42661 & 85494 & 0.791 & 7.83 & 13.92 & 0.61 & 0.68 \\\\ baseball glove & 350\\_36909 & 69272 & 0.054 & 0.72 & 20.52 & 0.43 & 0.62 \\\\ bicycle & 62\\_4324 & 10701 & 0.700 & 5.94 & 15.22 & 0.19 & 0.69 \\\\ bottle & 589\\_88280 & 175252 & 0.098 & 1.18 & 29.59 & 0.76 & 0.38 \\\\ car & 439 62880 & 124254 & 0.765 & 4.43 & 11.40 & 0.32 & 0.87 \\\\ carrot & 372\\_40937 & 81628 & 0.873 & 2.17 & 20.86 & 0.63 & 0.44 \\\\ cellphone & 76\\_7569 & 15872 & 4.725 & 19.55 & 13.26 & 0.30 & 0.85 \\\\ chair & 455\\_64283 & 126636 & 0.009 & 0.28 & 22.77 & 0.73 & 0.27 \\\\ couch & 427\\_59830 & 115190 & 0.140 & 1.64 & 25.67 & 0.84 & 0.29 \\\\ cup & 44\\_2241 & 6750 & 0.453 & 2.47 & 23.50 & 0.60 & 0.49 \\\\ donut & 403\\_52964 & 103416 & 2.248 & 11.89 & 17.60 & 0.74 & 0.57 \\\\ frisbee & 339\\_35238 & 64092 & 0.738 & 3.75 & 22.34 & 0.43 & 0.66 \\\\ hairdryer & 378\\_44249 & 88180 & 0.022 & 0.16 & 25.84 & 0.82 & 0.33 \\\\ handbag & 406\\_54390 & 105616 & 0.273 & 2.32 & 26.51 & 0.89 & 0.26 \\\\ hotdog & 618\\_100797 & 202003 & 2.600 & 7.23 & 19.78 & 0.45 & 0.78 \\\\ keyboard & 375\\_42606 & 85350 & 1.596 & 7.04 & 18.54 & 0.46 & 0.60 \\\\ kite & 428\\_60143 & 116852 & 0.029 & 0.36 & 18.01 & 0.30 & 0.74 \\\\ laptop & 378\\_44295 & 88252 & 1.128 & 7.92 & 15.04 & 0.36 & 0.59 \\\\ microwave & 504\\_72519 & 140728 & 0.023 & 0.45 & 21.17 & 0.61 & 0.42 \\\\ motorcycle & 367\\_39692 & 77422 & 0.006 & 0.14 & 26.52 & 0.78 & 0.30 \\\\ parkingmeter & 483\\_69196 & 135585 & 0.136 & 2.48 & 17.24 & 0.56 & 0.56 \\\\ pizza & 372\\_41288 & 82251 & 0.036 & 0.26 & 27.70 & 0.69 & 0.42 \\\\ sandwich & 366\\_39376 & 76719 & 0.411 & 1.67 & 19.74 & 0.53 & 0.51 \\\\ stopsign & 617\\_99969 & 199015 & 3.229 & 13.81 & 13.99 & 0.40 & 0.72 \\\\ toilet & 605\\_94579 & 188112 & 0.252 & 5.48 & 18.53 & 0.69 & 0.41 \\\\ toybus & 273\\_29204 & 56363 & 0.057 & 0.40 & 23.34 & 0.65 & 0.60 \\\\ toylplane & 405\\_53880 & 105088 & 0.020 & 0.12 & 22.20 & 0.53 & 0.53 \\\\ tv & 48\\_27428095 & 0.097 & 0.81 & 26.32 & 0.81 & 0.39 \\\\ umbrella & 191\\_20630 & 39388 & 1.115 & 5.73 & 17.35 & 0.44 & 0.60 \\\\ vase & 374\\_41862 & 83720 & 0.100 & 1.27 & 29.25 & 0.85 & 0.28 \\\\ wineglass & 401\\_51903 & 101703 & 1.191 & 7.80 & 21.43 & 0.58 & 0.53 \\\\ \\hline Avg & & 0.778 & 4.21 & 20.57 & 0.57 & 0.53 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: CO3D 전장 평가에서 ICON의 퍼센 성능이 다른 33개 범주에 대해 나타났다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c}  & \\multicolumn{2}{c}{TartanVO} & \\multicolumn{1}{c}{DROID} & \\multicolumn{1}{c}{COLMAP} & \\multicolumn{1}{c}{ParticleSIM} & \\multicolumn{1}{c}{ICON} \\\\ \\hline RPE(degree) & 1.41 & 0.56 & 0.67 & 0.34 & 0.47 \\\\ ATE(m) & 0.198 & 0.066 & 0.091 & 0.053 & 0.092 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '스카넷에 대한 카메라 포즈 평가는 Table 10과 같다. SanNet 시나리오에 최적화되지 않았음에도 불구하고 ICON은 경쟁 성과를 달성하여 RPE에서 2위, ATE에서는 3위를 차지하고 있다. ICON과 최첨단 방식의 차이는 매우 작다(회전에 0.13도, 번역 시 0.039m).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>