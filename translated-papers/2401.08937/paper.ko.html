<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '공동 포스와 라메스 필드 최적화 ICON: 공동 CON.\n' +
      '\n' +
      ' 우리야오왕.\n' +
      '\n' +
      '메타FAIR에서 MetaFAIR.\n' +
      '\n' +
      'Pierre Gleize\n' +
      '\n' +
      'Hao Tang\n' +
      '\n' +
      'Xingyu Chen\n' +
      '\n' +
      '메타FAIR에서 MetaFAIR.\n' +
      '\n' +
      '상상 케빈.\n' +
      '\n' +
      '메타FAIR에서 MetaFAIR.\n' +
      '\n' +
      'Matt Feiszli\n' +
      '\n' +
      '메타FAIR에서 MetaFAIR.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '신경방사성 필드(NeRF)는 2D 이미지 세트를 감안할 때 노벨뷰 합성(NVS)에 대해 놀라운 성능을 나타낸다. 그러나 NeRF 훈련은 일반적으로 구조로부터 모션(SfM) 파이프라인에 의해 얻어지는 각각의 입력 뷰에 대한 정확한 카메라 포즈를 필요로 한다. 최근 작품들은 이러한 제약을 완화하려고 시도했지만, 여전히 그들이 다듬을 수 있는 괜찮은 초기 포즈에 의존하는 경우가 많다. 여기에서 포즈 초기화에 대한 요구 사항을 제거하는 것을 목표로 한다. 2D 비디오 프레임에서 NeRF를 트레이닝하기 위한 최적화 절차인 인스테이션 CONfidence(ICON)을 제시한다. ICON은 포즈에 대한 초기 추측을 추정하기 위해 부드러운 카메라 동작만을 가정한다. 또한 ICON은 구배를 동적으로 재체중화하는 데 사용되는 모델 품질의 적응적 측정인 "신뢰"를 소개한다. ICON은 NeRF를 배우기 위한 고신력 포즈, 포즈를 배우기 위해 고신력 3D 구조(NRF로 인코딩된)에 의존한다. 우리는 사전 포즈 초기화가 없는 ICON이 SfM 포즈를 사용하는 방법에 비해 CO3D와 HO3D 모두에서 우수한 성능을 달성한다는 것을 보여준다.\n' +
      '\n' +
      '미야오위앙@meta.com; mdf@meta.com. mdf@metang@meta.com에서 마트페리졸리.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '2D 비디오에서 3D로 물체를 쉽게 들어올리는 것은 광범위한 애플리케이션으로 인해 어려운 문제이다. 예를 들어, 가상, 혼합 및 증강 현실(2016)의 발전은 가상 3D 객체와의 새로운 상호 작용을 해제하고 있으며, 3D 객체 이해는 로봇 공학(예: 조작 카펠러 등(2018년), Wen et al.(2022년), Qi et al.(2023년) 및 학습별 Wen et al.(2023년)에도 중요하다.\n' +
      '\n' +
      '3D로 물체를 가져오는 것은 3D 구조를 추출하고 6DoF 포즈를 추적하는 것을 모두 필요로 하지만 기존 접근법은 한계가 있다. 많은 원과 베크리스(2021); 아지노비치 등(2022); 원 등은 깊이에 의존하며, 이는 3D 추론에 대한 강력한 신호이다. 그러나 정확한 깊이는 일반적으로 장치에 비용, 중량 및 전력 소비를 추가하는 추가 센서(예: 스테레오, LiDAR)를 필요로 하므로 널리 이용 가능하지 않는 경우가 많다. 이 깊이 신호가 없으면 이러한 방법은 종종 실패한다. 문제를 절반만 해결하는 것도 일반적이며, 3D 객체 재구성 방법은 종종 마덴힐 등의 포즈를 취한다고 가정하며(2020년), 리젠슈타인 등은 마찬가지이다.\n' +
      '\n' +
      '그림 1: ** 새로운 뷰와 초기 포즈가 없을 때 ICON 및 BARF의 시각적 포즈를 제공한다. CO3D 레이젠슈타인 등(2021)의 책 비행 영상을 훈련합니다. BARF 궤적은 단편화를 나타내며 카메라 포즈는 두 개의 전향 클러스터로 분할되어 두 개의 책을 생성한다. ICON은 고품질 뷰 합성을 제공하고 포즈를 매우 정확하게 복구합니다. 색색 삼각형 메서는 ICON 예측된 포즈를 나타내며 회색은 지상진성을 나타내며, 여기서 우리는 단일 단안 RGB 비디오에서 암묵적인 3D 표현과 프레임당 카메라 포즈를 모두 학습하면서 공동으로 두 문제를 다루는 것을 목표로 한다. 우리는 3D 표현을 2D 입력 프레임에 투영하여 조밀한 광학적 손실로 6DoF 포즈와 재구성을 모두 감독한다. 구체적으로, 우리는 2D 렌더링을 얻기 위해 신경 방사선 필드(NeRF) 마덴힐 등(2020)로서 오브젝트/센을 나타낸다.\n' +
      '\n' +
      '이 환경에서 어느 정도 포즈가 학습될 수 있다는 것을 보여주었는데(2021년), 린 등(2021년), 알(2021년), 린 등(2021년)은 초기 포즈들을 적당한 소음으로 다듬는 데 사용될 때 가장 효과적이다. 예를 들어 왕 등(2021)은 포즈 노이즈가 약 20도 회전오차를 초과할 때 실패하기 시작하며, 더 복잡한 궤적은 찾을 수 없다. 실제로, 이러한 방법은 또한, 예를 들어 객체(Sec. 4)의 전체 360도 비행에 대해 중간 정도의 완전 궤적에도 실패한다. 이는 SfM 전처리가 라디턴스 필드를 구성하기 위한 전제조건으로 남아 있음을 의미한다.\n' +
      '\n' +
      '한 가지 앞으로 더 큰 포즈 변화를 해결하기 위해 노력하는 대규모 노이즈 케이스에 초점을 맞추는 것이 될 것이다. 이것은 유망한 멍 등(2021년)이지만, 여기서 우리는 다른 길을 가고 증분 사건에 초점을 맞춘다. 이것은 비디오가 입력된 실제 설정, 예를 들어 구현 AI에서 자연스럽게 발생한다. 증가적 SfM Schonberger와 Frahm(2016), SLAM 다비슨(2003), 훈련 포즈와 NeRF를 증분적 설정에서 공동으로 영감을 얻는다. 이 설정에서 모델은 한 번에 하나씩 비디오 프레임의 스트림을 취한다. 이전에는 모션 미스큐리티를 조작하면서 이전 프레임의 포즈로 들어오는 프레임을 초기화한다. 프레임 간의 정보는 NeRF로부터의 뷰 합성을 통해 교환된다.\n' +
      '\n' +
      '3D 구조와 포즈 사이의 상호 의존성에서 주요 과제는 비롯되며, 높은 광측정 오차는 좋은 포즈에도 불구하고 불량한 3D 모델에 기인하거나 좋은 모델에도 불구하고 포즈에서 큰 오류가 발생할 수 있다. 우리는 파편화, 고전적 분지-구호 모호성 벨호뮤 등 일반화, 중복 등록(그림 3 참조)을 포함한 몇 가지 흥미로운 실패 모드를 관찰하고 분석한다.\n' +
      '\n' +
      '어려움을 해결하기 위해 ICON(증가된 CONfidence)을 제안한다. 직관이 간단합니다(그림 2). "포즈가 좋으면 NeRF를 배우고, NeRF가 좋을 때, 광학적 오류로부터 얻은 신뢰의 측정을 사용하고, 3공간에 대한 자신감을 저장하기 위해 NeRF 스타일의 "신경 진증 필드"를 유지하는 이 두 체제 사이를 ICON이 보간한다. 신뢰는 최적화를 안내하는 신호로도 사용되며, 특히 로컬 최소의 식별(및 탈출)에 도움이 될 수 있다.\n' +
      '\n' +
      'CO3D Reizenstein et al.(2021), HO3D Hampali et al.(2020), LLFF Mildenhall et al.(2019)에 대한 ICON의 정량적 평가를 수행한다. 공동 포즈 및 3D 바젤린은 종종 치명적인 영향을 받지 않지만 ICON은 COLMAP Schonberger 및 Frahm(2016)에 대해 훈련된 NeRF와 비교할 수 있는 CO3D에 대한 강력한 성능을 달성하며 DROID-SLAM 테헤드 및 덩(2021) 및 PoseD확산 왕 등 다양한 바젤을 능가한다. 또한, 배경이 제거된 CO3D 비디오에 대해 평가합니다;\n' +
      '\n' +
      '그림 2: **ICON 개요***. ICON은 각 3D 위치에 대한 신뢰 \\(\\zeta\\)를 인코딩하기 위해 NeRF 위에 신경 진증 분야를 구성한다. 그런 다음 신뢰는 최적화 프로세스를 안내하는 데 사용됩니다.\n' +
      '\n' +
      '배경 텍스처가 카메라를 쉽게 추출하기 때문에 어려움을 크게 증가시킨다. 우리는 이 경우(분리되는 단일 마스킹 오브젝트)가 상당히 가치가 있다는 점에 주목하며, 여기에서의 성공은 카메라가 움직이고 있는지, 객체가 이동하는지, 또는 둘 다 작동하는지 여부를 의미한다. ICON은 NeRF+COLMAP 포즈와 바젤린의 광범위한 선택에 대해 우수한 성능을 달성하며, 핀란드 ICON은 RGB 바셀린을 능가하며 HO3D의 동적 휴대용 객체에 대해 SOTA RGB-D 방법 연방(SDF Wen et al.(2023)과 비슷하다.\n' +
      '\n' +
      '요약하자면, 우리는 다음과 같은 기여를 합니다.\n' +
      '\n' +
      '1. 우리는 공동 포즈 및 NeRF 최적화에 대한 증분 등록을 제안한다. 이 설정은 일반적인 비디오 설정에서 포즈 초기화를 위한 요구 사항을 제거한다.\n' +
      '2. 우리는 이 증분 설정을 체계적으로 연구하고 몇 가지 과제를 발견합니다. 관찰에 기초하여 공간 위치와 포즈에 대한 신뢰를 기반으로 하는 최적화 프로토콜인 ICON을 제안한다.\n' +
      '3. 물체 중심 데이터셋에 중점을 두고 ICON을 평가한다. ICON은 RGB 전용 방법 중 SOTA이며 SOTA RGB-D 방법에서도 경쟁력이 있다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '*** 신경 방사선 필드***(NeRF) 마덴힐 등(2020)은 새로운 뷰 합성을 위해 제기된 2D 이미지로부터 3D를 나타내는 강력한 기술이다. NRF의 한 가지 주요 한계는 정확한 카메라 포즈 요구 사항에 있다. Nerf- Wang et al(2021), BARF Lin et al.(2021), SCNeRF Lin et al.(2021), SiNeRF Xia et al al.(2022), NeuROIC Kuang et al al.(2022), IDR Yariv et al.(2020), GARF Chng et al.(2022), SPARF Truong et al.(2023) 등 최근 작품은 포즈와 NeRF를 공동 최적화하여 이 요구 사항을 완화하려고 시도했다. 유망한 방향에도 불구하고 시끄러운 초기 포즈를 정제할 때 가장 잘 작동하며 초기 포즈 추정 방법의 견고성에 의해 제한된다. 포즈에 대한 의존성을 더욱 줄이기 위해 커뮤니티의 한 가지 방향은 GAN 멍 등(2021), SLAM 로신올 등(2022), 모양 제사 장(2021), 깊이 Bian et al.(2023) 및 거친 주석 보스(2022)와 같은 초기 포즈 추정에 대한 추가 구성 요소 또는 신호를 추가하는 것이다. 우리는 이 문제를 다른 각도에서 해결하고 관절 NeRF의 증분 설정을 제안하고 최적화를 제기한다. 우리의 제안된 방법 ICON은 추가 신호를 사용하지 않으며 카메라 포즈가 얻기 어려울 때 도전 시나리오에 대한 강력한 성능을 달성한다.\n' +
      '\n' +
      '**Pose 추정(객체)***는 이미지 프레임으로부터 객체의 6가지 자유(DoF) 포즈를 추론하는 것을 목표로 한다. 작품의 선은 이미지 포즈 추정 시랑(2018)과 랩베(2020) 및 비디오 포즈 추적 뮬러 등(2021)의 두 가지 주요 범주로 분류될 수 있으며, 전자에서는 대부분 희박한 프레임에서 포즈를 추론하는 데 초점을 맞추고 후자는 시간적 정보를 고려하게 된다. 그러나 비디오 또는 이미지 포즈 추정의 많은 방법은 대상 CAD 모델 샤랑(2018), 랩베(2020, 2022), 선더마이어(2018) 등 알려진 사례 또는 범주 수준의 객체 표현을 가정하며, 왕 등은 알(2018), 왕 등은 알(2019), 스투버(2021), 또는 미리 캡처된 참조 뷰는 공지된 포즈 류(2022) 등을 포함한다(2020). 최근 연방정부 추적 위엔과 베크리스(2021)는 이러한 객체 책임자들의 필요성을 제거함으로써, 새로운 사물에 대한 포즈 추적을 일반화하고, 연방회(SDF Wen et al.2023)는 오브젝트에 대한 신경 표현을 구성하여 포즈 추적을 개선한다. 그러나 둘 다 깊이 정보가 필요하여 애플리케이션을 제한합니다.\n' +
      '\n' +
      '**SLAM(동시 지역화 및 매핑)***는 무아탈 등(2015)을 지도하는 자체 위치, 무아탈과 타르도스(2017), 다비슨 등(2007), 엔젤(2014년, 2017년), 클레인과 머레이(2007년), 주비자레타 등(2020년)를 동시에 결정하면서 환경 지도를 구축한다. 대부분의 SLAM 방법은 정적 환경에서 카메라를 이해하는 데 초점을 맞춘 반면, 물체 중심 SLAM McCormac et al.(2018); Merrill et al.(2022); Runz et al.(2018); Salas-더노 et al.(2013); Sharma et al.(2021)은 동적 환경에서 학습 대상에 초점을 맞추고 있다. 그러나 이러한 방법의 대부분은 깊이 신호 Runz et al.(2018); McCormac et al.(2018); Merrill et al.(2022) 및 큰 폐색 또는 갑작스러운 운동 Wen et al.(2023)으로 투쟁을 필요로 한다.\n' +
      '\n' +
      'Method\n' +
      '\n' +
      'ICON은 RGB 비디오 프레임을 입력으로 스트리밍하고 3D 재구성 및 카메라 포즈 추정치를 생성한다. ICON은 신뢰에 의해 유도되는 3D 재구성을 최적화하기 위해 각 입력 프레임을 증분 등록한다: 3D 재구성은 높은 신뢰 포즈가 있는 프레임에서 더 많이 학습되고 포즈는 3D 재구성의 더 높은 신뢰 영역에서 3D-2D 재주입에 의존한다.\n' +
      '\n' +
      '라메니컬 라일스 펜스__프라이날리.\n' +
      '\n' +
      '(\\mathb{R}}\\mathb{f}}\\mathf{d\\)\\ a\\b{R}\\math{d}를 나타내는\\math{f}}\\math{d\\(\\math{d\\)는 입력 \\(\\math{d\\)\\b}}\\s{d\\(\\math{d\\:\\s{d\\)\\b}}\\b\\b}\\s{d\\)\\b\\b}\\s{d\\s{d\\s{d\\.\\b}\\s{d\\)\\b}\\b}\\s{d\\b}\\b}\\s{d\\)\\b}\\b}\\b}\\b}\\b}\\d\\d\\d\\)\\b}\\b}\\b}\\d\\d\\d\\I}를 구성하고\\d\\d\\)\\b}\\b}\\d\\d\\(\\)\\b}\\b}\\d\\d\\(\\)\\b}\\d\\d\\d\\d\\d\\d\\)\\ P_{i}\\(P_{i}\\) 카메라 포즈(P_{i}\\)에서 이미지\\(\\hat{I}_{i}\\)에서 장면의 2D 렌더링을 생성하기 위해, NeRF는 카메라 센터(p=(u,v)에서 광선을 따라 방사선을 집계하는 렌더링 함수 \\(\\)를 사용한다.\n' +
      '\n' +
      '}}}(z)\\mathf{\\mathf{r}}(z))\\mathf{\\mathsf{c}}(z),ddz(\\math{s})\\math{R}(p,P_{i\\)\n' +
      '\n' +
      '>{\\int_{\\mathsf{r}}}}(z)){\\mathsf{r}}}(z))\\)는 광선을 따라 축적된 투과율이며, \\(\\mathbf{\\mathsf{r}}(z)=o_{i}+zd\\)은 카메라 포즈 \\(P_{i}\\)에 의해 결정된 바와 같이 \\(f{nath_{f{naths}}}}. NeRF는 광선을 따라 샘플링된 지점을 통해 적분을 근사화하여 \\(\\mathcal{R}\\)를 구현하며, 모든 이미지(i=1, N\\)에 대해 접지 진위 뷰 \\(I_{i}\\)와 렌더링된 뷰 \\(\\hat{I}_{i}\\) 사이의 광학적 손실을 통해 훈련된다.\n' +
      '\n' +
      '타\\{*=\\arg\\et{L}_{p}(\\hat{I}},\\hat{I}_{p},\\hat{I})\n' +
      '\n' +
      '구축 프레임 등록.\n' +
      '\n' +
      '이러한 관절 포즈와 NeRF 최적화 방법에 대한 주요 한계는 좋은 초기 포즈에 대한 요구 사항이다. I\\(\\{P_{i}\\}\\)가 다양한 관점을 포함하고 있으며 모든 관점을 동일성에서 초기화한다면 이러한 방법은 종종 붕괴된다. 예를 들어, 단순하지만 일반적인 붕괴 용액은 단편화이며, 각 프레임은 자체 단편화된 3D 표현을 생성하며, 모두 다른 뷰들(** 프래그먼트** 무화과 3)에 상호 보이지 않는다. 실제로, BARF Lin et al.(2021)는 포즈 \\(\\{P_{i}\\}\\)가 닫힌 루프 플라이밍(Tab. 1 참조)으로 구성될 때 CO3D 데이터셋의 모든 서열에서 붕괴된다. 왕 등(2021)에서 살펴본 바와 같이, 포즈 이전이 제공되지 않는 경우 전체 궤적에 대해 20도 회전차이의 파괴점이 관찰된다.\n' +
      '\n' +
      '이 문제를 해결하기 위해 단순하면서도 효과적인 직관에 의존하며, 비디오의 카메라 동작은 매끄럽습니다. 따라서 비디오에서 프레임 \\(I_{i}\\)를 감안할 때, 카메라 포즈 \\(P_{i}\\)는 \\(P_{i-1}\\)에 가까울 가능성이 높다. 우리는 이 관찰을 레버리지하고 시간적 순서에 따라 프레임들을 확장적으로 등록할 것을 제안한다.\n' +
      '\n' +
      '** 구현*** 훈련 시작 시, 우리는 처음 두 프레임 \\(\\{I_{1},I_{2}\\}\\)에서 NeRF 매개변수 \\(\\{P_{1}, P_{2}\\}\\)를 공동으로 최적화하고 \\(\\{P_{1},I_{2}\\}\\)를 제공한다. 모든 \\(k\\) 반복 후, 우리는 새로운 프레임 \\(I_{i}\\)를 추가하고 \\(P_{i-1}\\)에 의해 포즈 \\(P_{i}\\)를 초기화한다. 모든 프레임이 등록될 때까지 포즈(\\{P_{i}\\}_{i=1}^{N}\\)와 NeRF \\(\\Theta\\)에 대한 학습률을 동결한다. 모든 \\(N\\) 이미지가 추가된 후에 학습률 붕괴 일정이 적용될 수 있다.\n' +
      '\n' +
      '### _Confidence-Based Optimization_\n' +
      '\n' +
      '증가적 등록 과정은 카메라 포즈에 대한 좋은 초기화를 제공하는 것을 목표로 한다. 그러나 광학적 손실을 사용하여 포즈와 NeRF를 최적화하는 것은 매우 비코넥스이며 많은 국소 최소 Yen-Chen et al.(2021), Lin et al.(2023)를 포함한다. 또한 잘못 최적화 된 포즈는 NeRF에 잘못된 학습 신호를 제공하여 포즈가 이미 등록된 시점(**오버링 등록** 무화과 3)에서 잘못 등록될 가능성을 높일 수 있다.\n' +
      '\n' +
      '이를 해결하기 위해 신뢰 유도 최적화 스키마를 제안합니다. 직관은 간단하며, 포즈 \\(P_{i}\\)가 자신 있는 경우 학습된 NeRF \\(f(\\Theta)\\(f)를 개선하기 위해 더 신뢰해야 하며, \\(P_{i}\\)에서 샘플링된 광선이 자신 있는 위치를 포함할 때 포즈 조절을 위해 가중치가 더 많이 있어야 한다. 신뢰도 그림 3: ** 3의 공동 포즈 및 NeRF 최적화의 주요 실패 모드인 단편화, 분지 구제 및 중복 등록**를 제공한다. 색색 포즈는 예측이며 회색 포즈는 접지 진실입니다. *** 절편**: 포스와 NeRF가 떨어져 분리되고 상호 보이지 않는 방사 영역을 생성한다. 여기서 토트루플의 튜브가 생성되며, 각각은 다음을 방해한다. 이 튜브 플립북 스타일의 파이는 각각 단일 토트리트럭을 볼 수 있습니다. 도표. 3공간의 다른 영역에서 완전히 독립적인 재구성이 발생하는 1. *** 기본 구제**: RGB 재구성의 고유한 모호성에 대한 모델은 테이블 내부에 오목한 사과를 생성하여 "구호"를 구성하므로 카메라 궤적이 180도 반전된다. ** 과매핑 등록**: 포즈 궤적의 2개의 하위 집합이 국소 최소에 갇혀 복사장의 동일한 부분을 잘못 관찰하여 흐릿한 렌더링과 빈 복셀을 유발한다. 여기서 토스터의 한쪽은 조회수가 겹쳐 흐릿한 반면, 다른 한쪽은 시야가 없고 공석이다.\n' +
      '\n' +
      '새로운 프레임에 대해 극적으로 감소하면 포즈가 국부적인 최소치에 갇힐 가능성이 높기 때문에 이 포즈를 재등록하기 위해 재시작한다. 이는 COLMAP Schonberger와 Frahm(2016)의 시행착오 전략과 유사하다. 다음으로 3D에서 각 포즈 \\(P_{i}\\) 및 각 점/뷰 방향 \\((\\mathbf{x},\\mathbf{d})에 대한 자신감을 측정하는 방법을 설명한다.\n' +
      '\n' +
      '**는 3D**에 대한 자신감을 향상시킵니다. 우리는 입력 3D 위치와 방향 \\((\\mathbf{x},\\mathbf{d})\\), NeRF \\(f\\)도 신뢰 \\(\\zeta_{(\\mathbf{x},\\mathbf{d})}\\를 예측한다. 우리는 특징 위에 하나의 완전히 연결된 층을 첨가한 다음 색상 예측 헤드와 유사한 S자형이다.\n' +
      '\n' +
      '레이 \\(\\mathbf{r}\\)에 대한 신뢰는 불투명도 렌더링과 유사한 체적 응집을 통해 집계된다.\n' +
      '\n' +
      '}} (1)\\math{r} (z)\\math{r}}\\math{dz,{dz)\\.\n' +
      '\n' +
      '\\(\\mathcal{P}(z)=T(z)\\sigma(\\mathbf{r}(z))\\. 우리는 픽셀이 불투명할 때 첫 번째 용어가 더 두드러지는 반면 후자는 투명 픽셀에 대해 더 두드러진다는 점에 주목한다.\n' +
      '\n' +
      '자신감***를 측정합니다. 우리는 광계 오차를 통해 2D에서 픽셀 재프로세싱이 얼마나 잘 되는지 자신감을 측정한다. 우리는 광선(\\mathcal{L}_{\\text{r}})과 자신감을 감안할 때 \\(\\|e^{L}_{\\text{L}=\\|e^{-\\mathcal{E}/\\tau}/\\tau}-\\zeta_{\\mathbf{r}}})를 최소화하며, 여기서 \\(\\mathcal{L})는 NeRF 및 \\(\\mathcal{L}/\\|e^{mathcal{f{r}}/\\|tathcal{mathcal{f{r}/\\tathcal{mathcal{r}/\\tcal{mathcal{r}/\\tau}/\\tau}-\\tathcal{r}/\\tathcal{r}-\\tathcal{r}/\\tathcal{r}/\\tathcal{r}/\\tathcal{r}/\\tathcal{r}-\\tathcal{r}/\\tau}-\\tathcal{r}/\\t (\\mathcal{L}_{\\text{conf}}\\)는 신뢰 헤드를 훈련시키는 데만 사용되며 NeRF 매개변수 \\(\\Theta\\) 또는 포즈 이전에 구배가 중단된다.\n' +
      '\n' +
      '**Pose 신뢰****입니다. 우리는 \\(P_{i}\\)에서 샘플링된 광선에 대한 자신감을 집합함으로써 포즈 \\(P_{i}\\)에 대한 신뢰(\\zeta_{P_{i}}\\)를 계산한다. 시작 시 \\(P_{1}\\)는 신뢰 \\(1\\)를 가지며, 다른 사람들은 신뢰 \\(0\\)를 갖는다. 훈련 중 운동량 일정을 사용하여\\(B\\) 광선을 훈련하고(P_{i}}) 포즈(P_{i}}_{j}})에서\\(\\{mathbf{i}_{j1}^{t}})를 신뢰로 업데이트한다.\n' +
      '\n' +
      '}^{t_{i}}\\山_{mathbf{r}.\n' +
      '\n' +
      '운동량 \\(\\beta\\)은 우리 실험에서 \\(0.9\\)이다.\n' +
      '\n' +
      '자신감***에 의한 손실량**. 우리는 \\(\\mathcal{L}\\)를 보정하기 위해 자신감을 사용한다. 직관적으로.\n' +
      '\n' +
      '* NeRF 매개변수 \\(\\Theta\\)에 대한 구배를 계산할 때, 손실은 \\(\\{\\zeta_{P_{i}}\\}\\)에 의해 가중된다.\n' +
      '* 포즈 \\(\\{P_{i}\\}\\)에 대한 구배를 계산할 때, 선당 손실은 광선 신뢰인 \\(\\{\\zeta_{\\mathbf{r}}\\}\\)에 의해 가중된다.\n' +
      '\n' +
      '각 단계에서 우리는 \\(P_{i}\\)의 광선 \\(\\{\\mathbf{r}_{j}^{i}\\}_{j=1}^{B}\\)를 샘플한다. 손실이요.\n' +
      '\n' +
      '(\\math{i})\\math{i}(\\math{{i})\\math{i}(\\math{{i})\\math{i}(\\math{{i})\n' +
      '\n' +
      '**Pose 재인격****입니다. 증가형 SfM Schonberger와 Frahm(2016)에서 시험 및 오류 등록 메커니즘에 의해 영감을 받아 새로운 이미지가 등록되지 않으면 이전 포즈로부터 재설정한다. (K\\_{i}},\\{P_{i}) 이전의 평균의 표준 편차(\\{\\q\\text{mean},\\{zeta_{j}},\\{zeta_{j}})는 \\(\\{zeta_{j}},\\_{j})를 등록하고 나면 실패한다. 우리는 실험 전반에 걸쳐 \\(\\lambda=2\\)와 \\(K=10\\)를 사용한다.\n' +
      '\n' +
      '유공자.\n' +
      '\n' +
      '기준구호 모호성 벨호메루 등(1999)과 관련 "저면" 착시성은 모양이 다른 객체가 조명이나 그림자와 같은 다양한 광계 조건에서 동일한 이미지를 생성할 때 물체의 3D 구조를 회복하는 데 근본적인 모호성의 예이다. 일 예로, 좌측에서 둥근 볼록 범프가 점등된 표면은 오른쪽에서 돌출된 간결함으로 동일한 표면과 동일하게 나타날 수 있다. 우리는 "기준 구제" 솔루션과 같은 상황을 아낌없이 참조한다. 인간 시각 시스템은 강력한 제사(예를 들어 볼록함을 선호)를 사용하여 여러 가능성 중에서 특정 솔루션을 선택하는 것으로 알려져 있다.\n' +
      '\n' +
      '우리는 카메라 포즈 및 NeRF를 공동으로 최적화할 때, 특히 총 카메라 동작이 작을 때 최적화가 초기에 이러한 현상을 관찰한다. 모델은 로컬 최소값에 갇혀 탈출할 수 없습니다. 예컨대, 접지진우가 볼록한 장면인 경우, 오목한 버전의 장면이 재구성될 수 있다(그림 3의 ** 기본구호** 참조). 이 예에서, 카메라 움직임은 180도 오프되어 접지진 궤적에 비해 반대 방향으로 이동한다. 우리는 단순한 사제들이 거친 깊이와 같은 신호를 사용하는 것이 자연 장면에 대한 더 인간의 것과 같은 해석을 생성하는 데 도움이 될 수 있다고 믿는다. 그러나 본 연구의 경우 크래프팅 사제들을 피하고, 신뢰 기반 손실 보정이 이 문제를 줄이는 데 도움이 된다고 말한다(16%에서 9).\n' +
      '\n' +
      '우리는 또한 잘못된 분지 구제 솔루션이 일반적으로 더 높은 오류와 더 낮은 확신을 가지고 있음을 관찰하며, 구제 솔루션은 제한된 견해 세트에 유효하고 더 넓은 관점이 일치하지 않는다. 따라서 우리는 증분 SfM에서 재시작 전략을 채택하여 일반 솔루션을 제안한다. 예를 들어, COLMAP는 최종 재구성이 특정 기준(예: 등록된 이미지의 비율)을 충족하지 않는 경우 다른 초기 쌍을 식별하기 위해 재시작한다. 우리에게, 우리는 \\(K\\)를 독립적으로 발사하고 고정된 수의 반복 후에 자신감을 측정한다. 저희는 자신감이 가장 높은 것을 선택합니다. 실무상 3점을 출시하여 훈련의 10%에서 자신감을 측정합니다.\n' +
      '\n' +
      '기반의 기하학적 제약.\n' +
      '\n' +
      '최근의 작품인 정(2021)에 이어 트롱 등(2023년)은 최적화에 기하학적 제약을 더한다. 레이거리 손실 정주(2021년)와 깊이 일관성 상실 투롱 등(2023년)과는 다른 왕 등(2023년)과 유사하게 삼손 거리 하틀리와 지셔만(2003년)을 채택한다. 프레임과 이웃 사이의 대응 관계를 추출합니다. 우리는 주로 COLMAP와의 공정한 비교를 위해 SIFT Lowe(1999) 기능을 사용한다. 훈련 시간에 각 포즈 \\(P_{i}\\)에 대해 이웃에서 포즈 \\(P_{j}\\)를 샘플링한 다음 Sampson 거리를 계산한다.\n' +
      '\n' +
      '}_{text{j}}.^{x_{j}}.^{1}(x_{i})\n' +
      '\n' +
      'Hf(F\\)가 \\(P_{i}\\)와 \\(P_{j}\\) 사이의 기본 매트릭스인 경우(P_{j}\\)와 \\((x_{i}F)^{k}\\)는 \\(k\\) 요소를 나타낸다.\n' +
      '\n' +
      '신뢰에 의한 손실 보정은 기하학적 신호가 초기 최적화 풍경을 제한하는 데 도움이 되지만, 대응 쌍은 특히 질감이 적은 객체에 대해 잘못된 및/또는 픽셀 정확하지 않을 수 있다. 이로 인해 기하적 제약은 정확한 포즈와 재구성을 얻기 위해 ICON에 해로울 수 있다. 우리는 Sampson 거리를 중량화하기 위해 신뢰 \\(\\zeta_{P_{i}}\\)에 의존하는데, 한 쌍의 포즈 \\(P_{i}\\) 및 \\(P_{z}\\)의 경우, \\(1-\\min(\\zeta_{P_{i}}},\\zeta_{P_{j}}},\\)의 체중)에 의존한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '우리는 물체의 턴테이블 스타일 영상으로 구성된 대규모 데이터세트인 3D v2(**CO3D**) 데이터세트 레이젠슈타인(2021)의 공통 대상에 대한 연구를 집중한다. 라운드 진리 포즈는 COLMAP를 통해 얻어진다. 우리는 변형되지 않은 이미지 프레임(객체 및 배경 가시)을 사용하는 **full-scene**와 전경 객체 픽셀만 남기는 배경을 제거하는 *** 오브젝트 전용**의 두 버전으로 훈련한다. 우리는 객체 전용 버전이 더 도전적이면서도 의미 있는 평가 세트라고 믿습니다; 풀스크렌에서는 COLMAP가 포즈를 성공적으로 추출할 수 있는 질감 있는 배경에 물체를 배치하는 경우가 많다. 이것은 암묵적으로 객체 포즈와 카메라 포즈와 동일하며, 이러한 가정은 객체와 카메라가 모두 움직이는 동적 장면에서 파손된다. 우리는 대칭(객체 전용 설정에서 부패할 수 없는)으로 인해 "시나제"와 "돈트"가 제거된 일탈 집합에 의해 지정된 18개의 범주를 사용한다. 카메라 포즈 평가를 위해 높은 COLMAP 포즈 자신감을 가진 장면을 선택합니다. 양씨 등 양씨(2023년)를 사용하여 마스크를 청소하고 원래 마스크에 대한 결과가 보충에 존재한다. 동적 객체에 대한 성능을 입증하기 위해 우리는 또한 카메라 포즈 추적 및 합성 품질을 평가하기 위해 목적 **HO3D*Hampali 등 v2를 재목적했다. HO3D는 사람의 손에 의해 조작된 동적 객체를 캡처하는 정적 카메라 RGBD 영상으로 구성된다. 우리는 ICON에 RGB 프레임만 사용하고 8개의 비디오에서 8개의 클립(약 200 프레임 정도)을 선택하여 각각 다른 오브젝트를 커버한다. 마지막으로, 우리는 특히 NeRF에 대해 현장 수준의 새로운 뷰 합성에 일반적으로 사용되는 8개의 전향 장면을 갖는 데이터 세트인 **LLFF**Mildenhall 등(2019)에 대한 결과를 보여준다.\n' +
      '\n' +
      '***건축 및 Lchitect and Losses** 우리의 아키텍처는 NeRF Mildenhall et al.(2020)(계층적 샘플링 없음)를 따르고 이미지의 더 긴 가장자리를 640으로 설정했으며 NeRF의 표준 MSE 손실을 사용한다. Sampson 거리를 사용할 때 \\(10^{-4}\\)에 의해 가중치가 된다. 객체 마스크를 사용할 수 있는 CO3D 및 HO3D의 객체 전용 설정의 경우 MSE 손실을 사용하여 불투명도를 감독한다. HO3D의 경우 폐색 지역의 샘플링 광선을 피하기 위해 제공된 경우(8개의 클립 중 7개) 핸드 마스크를 사용한다.\n' +
      '\n' +
      '** 훈련****입니다. 우리는 BARF Lin et al.(2021) 설정을 사용하고 200k 반복을 위해 훈련을 합니다. CO3D와 HO3D의 경우 훈련 시간을 줄이기 위해 다른 모든 프레임을 건너뛰어 100 프레임 내외의 서열을 생성한다. ICON 및 그 변이체의 경우 1k 반복(CO3D/HO3D)/500 반복(LLFF)마다 새로운 프레임을 추가하고 학습 속도(HO3D 및 CO3D에 대한 100k 반복, LLFF에 대해 30k 반복)를 동결한다. BARF 후 등록 중 위치 인코더를 사용하지 않고 등록 후 거친 위치 인코딩을 적용한다.\n' +
      '\n' +
      '*** 평가***입니다. 린 등(2021년)에 이어 각 서열의 마지막 부분(유형적으로 10%)을 평가한다. 우리는 Abs절대 추적 유도(ATE) 장과 Scaramuzza(2018)로 카메라 포즈 품질을 측정하고, 우미야마 정렬 우미야마(1991)를 지상이 있는 예측 카메라 센터의 수행합니다. ATE는 번역(ATE)과 회전(ATE\\({}_{\\text{rot}}\\)) 성분으로 구성되며, 카메라 센터 간의 \\(l2\\)-거리 및 정렬된 카메라 간의 각도 거리를 각각 평가한다. 새로운 관점에서 합성을 위해 이전 작업인 린(2021)에서 표준 관행에 따라 추가 테스트 시간 포즈를 개선한다. 우리는 PSNR, LPIPS 장 등(2018) 및 SSIM을 메트릭으로 사용한다.\n' +
      '\n' +
      '** 기본*****입니다. 우리는 **BARF**Lin 등(2021) 위에 ICON을 구축하고 관절 포즈 및 NeRF 최적화를 위해 BARF와 비교한다. 새로운 뷰 합성을 위해 우리는 지상 진리 포즈로 NeRF를 훈련시킨다. 포즈의 경우, 우리는 확률적 포즈 확산 프레임워크 내에서 **PoseDiff*Wang et al.(2023) 모델 SfM을 비교하고, 추정된 3D 장면 흐름에서 동시 작업 **FlowCam**FlowCAM Smith et al.(2023) 해결 포즈는 SOTA 말단 학습 기반 SLAM 시스템인 **DROID**SLAM 시스템이다. 또한 예측된 포즈를 사용하여 NeRF를 초기화하고 훈련합니다. 또한, 물체 단독 CO3D 평가에서 우리는 학습 기반 기능인 슈퍼포인트 데톤(2017)+슈퍼Glue Sarlin et al.(**COLMAP+SPSG**)을 사용하여 최첨단 SfM 파이프라인 **COLMAP**Schonberger 및 Frahm(2016) 및 COLMAP Sarlin et al.(2019)의 증강 버전을 평가하고 있다. ICON은 _RGB_만을 사용하지만, 우리는 그라운드 진리 깊이 입력이 있는 DROID, **BundleTrack**Wen 및 Bekris(2021) 및 최첨단 **BundleSDF**Wen 등 HO3D에 인기 있는 _RGB-D_ 방법을 포함한다(2023).\n' +
      '\n' +
      'CO3D 풀 장면.\n' +
      '\n' +
      '**ICON은 풀스코어 CO3D***의 강합니다. 우리는 표 1의 전체 CO3D 장면에서 ICON과 기저부를 비교하며 이전 지식과는 달리 BARF는 모든 카메라 포즈를 동일성으로 초기화해야 한다. 객체 결과물인 COCO3D의 비행 캡쳐입니다.\n' +
      '\n' +
      '그림 4: ** 새로운 뷰 합성 시각화는 포즈 없이 ICON의 시각화 및 GT 포즈로 훈련된 NeRF이다. 스타일러가 없음에도 불구하고 ICON은 새로운 견해를 비슷하거나 더 높은 품질로 만든다. 결과는 LLFF 및 CO3D***에서 발생한다.\n' +
      '\n' +
      'in 카메라는 임계치를 크게 초과하는 변이를 나타내며, 그 후 BARF의 성능이 붕괴되며 \\(텍스트{ATE}_{\\text{rot}}\\)는 100도를 초과한다. 대조적으로, ICON의 증분 접근법은 1.20의 더 정확한 카메라 포즈(0.137의ATE 및 \\(텍스트{ATE}_{\\text{rot}}\\)를 회복하는 동시에 PSNR, SSIM 및 LPIPS에 의해 측정된 바와 같이 질적으로나 정량적으로 더 나은 시각적 충실도를 달성한다. 흥미롭게도 ICON은 BARF가 초기화_에서 근거 진리를 제공한다면 여전히 BARF _even을 능가한다. 원래 이 설정을 상부 결합으로 제안했지만 이 결과는 BARF 훈련의 초기 반복의 불안정성을 반영한다고 믿습니다. 카메라는 더 과감한 조명 변화와 모션 블러와 함께 스파커입니다. 18개의 장면 중 BARF는 4에서 \\(\\geq\\) 10도(\\text{ATE}_{\\text{rot}}\\)를 겪으며 전체 성능을 끌어내린다.\n' +
      '\n' +
      '또한 NeRF 마덴힐 등(2020)과 포즈 예측 방법과 여러 비교를 한다. NRF에는 DROID-SLAM, FLOW-CAM 및 PoseDiff에 의해 예측된 포즈가 제공되며, 이는 광학 흐름 테드 및 댑(2020)과 같은 추가적인 신호를 훈련하거나 훈련시키기 위한 주석이 달린 포즈에 의존한다. 그러나, 우리의 공동 NeRF와 포즈 훈련은 더 나은 포즈 추정치(ATE 및 \\(텍스트{ATE}_{\\text{rot}}\\)를 생성하며, 결과적으로 NeRF의 새로운 뷰 합성이 비교된다. CO3D의 그라운드 진실이 주어지더라도 ICON은 NeRF를 능가할 수 있다. 이것은 처음에는 놀라운 것처럼 보일 수 있지만 CO3D에서 "지하 진실" 포즈조차도 진정한 근거 진리가 아니며 완벽한 것이 아닌 COLMAP로 생성된다는 점을 지적한다. 또한, COLMAP와 달리 ICON의 NeRF와 포즈에 대한 공동 학습은 추정된 포즈가 NeRF 품질을 최대화하기 위해 특별히 최적화된다는 것을 의미한다. 우리는 이것이 우리가 관찰하는 더 나은 성능에 의해 반영되는 바와 같이 NeRF 학습에 더 호환될 수 있다고 가정한다. 앞의 작품인 정(2021)에서 유사한 관찰이 제시되었고, 멍 등은 (2021)였다.\n' +
      '\n' +
      'CO3D 전용\n' +
      '\n' +
      '6DoF 포즈는 본질적으로 주석에 까다롭기 때문에 과거의 데이터 세트는 종종 객체 또는 카메라에 대한 움직임을 제한하며, 후자의 경우 시각적으로 구별되는 배경(예를 들어, 객체 주변의 QR 코드와 같은 특별히 설계된 패턴)은 종종 포즈 궤적 재구성을 더 쉽게 만드는 데 사용된다. 그러나 이러한 전략은 특히 객체와 배경(또는 카메라)이 모두 이동 중인 경우 보다 풍성한 영상으로 일반화되지 않는다. 이러한 이유로 배경 마스킹으로 CO3D에 대한 평가도 수행하는데, 이러한 설정에서 알고리즘은 포즈를 추정하기 위한 객체 기반 시각적 신호에만 의존할 수밖에 없다(표 4.2).\n' +
      '\n' +
      '이러한 도전 환경에서 BARF가 교정할 수 있는 것을 넘어 카메라 궤적이 변화함에 따라 BARF가 정확한 포즈를 추정하지 못한다는 것을 다시 관찰하였다. 또한 이 설정의 어려움은 BARF의 새로운 뷰 합성의 추가 악화를 생성한다. 그러나 ICON이 배경의 신호 없이 여전히 그러한 비디오를 처리할 수 있다는 것을 관측한다. 이는 ICON이 공동 포즈 추정에 실행 가능하고 배경을 의지할 수 없는 경우 더 일반적인 비디오에 대한 3D 객체 재구성을 의미한다.\n' +
      '\n' +
      '풀스코어 CO3D 실험과 마찬가지로 포즈 추정 방법과 NeRF에 먹였을 때 어떻게 잘 작동하는지 비교한다. 우리는 배경을 레버리지 못한 채 이러한 방법이 마법하게 투쟁한다는 것을 관찰한다. Pose 예측 ATE 및 \\(텍스트{ATE}_{\\text{E}}}}\\)는 특히 DROID-SLAM에서 각각 0.431에서 5.903 및 8.92에서 90.25까지 새싹을 달성했다. 더 나쁜 포즈로 학습된 NeRF의 품질도 상응하는 더 나쁘다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c}  & ATE & \\(\\text{ATE}_{rot}\\) & PSNR & SSIM & LPIPS \\\\ \\hline Pose Source + NeRF & & & & & \\\\ DROID & 0.431 & 8.92 & 17.19 & 0.526 & 0.541 \\\\ FLOW-CAM & 2.681 & 91.28 & 14.40 & 0.441 & 0.689 \\\\ PoseDiff & 1.973 & 27.25 & 18.82 & 0.563 & 0.520 \\\\ \\hline Groundtruth & - & - & 21.03 & 0.575 & 0.629 \\\\ \\hline Joint Pose + NeRF optimization & & & & & \\\\ BARF & 6.215 & 114.63 & 12.77 & 0.401 & 0.871 \\\\ GT-Pose+BARF & 0.417 & 3.77 & 19.33 & 0.558 & 0.647 \\\\ \\hline ICON (Ours) & **0.138** & **1.16** & **22.24** & **0.654** & **0.428** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: CO3D 레이젠슈타인 등의 비교 (2021) 전체 이미지 장면. 기준선 BARF는 전반적으로 더 큰 카메라 동작으로 인해 CO3D에 실패할 수 있지만 ICON은 포즈를 매우 정확하게 추정하고 GT 포즈로 훈련된 NeRF보다 품질 유사하거나 더 나은 품질로 새로운 뷰를 렌더링할 수 있다.\n' +
      '\n' +
      '특히 포즈의 경우 SIFT Lowe(1999)를 슈퍼포인트-슈퍼Glue DeTone et al.(2017)로 대체하는 COLMAP 및 그 변이체 COLMAP-SPSG, 사린 등은 CO3D의 전경 물체에서만 포즈를 예측하는 방법에 대해 추가로 평가한다. 우리는 COLMAP가 ICON보다 훨씬 나쁜 배경 신호에 의존할 수 없을 때 유의하게 더 나쁘다고 관찰한다. COLMAP가 종종 카메라 포즈 정렬의 금 표준으로 간주되고 종종 "그라운드 진실"(CO3D에서)으로 취급되기 때문에 이 발견은 특히 중요할 것으로 믿는다. 이것은 우리의 증분적으로 학습된 관절 포즈를 시사하며 NeRF 최적화는 배경이나 카메라도 이동하더라도 움직이는 전경 객체를 포밍하기 위한 유망한 새로운 대안을 나타낸다.\n' +
      '\n' +
      '핸드폰은 HO3D에서 동적 객체를 제공합니다.\n' +
      '\n' +
      '핸드헬드 객체를 이해하는 것은 상호 작용의 매우 성격이 종종 중요성을 의미하며 손이 종종 객체 운동의 근원이기 때문에 많은 응용 분야에 특히 중요하다. 포스와 3D 재구성은 대상을 이해하는 핵심 구성 요소이므로 핸드헬드 상호 작용의 영상으로부터 이를 생성하는 능력은 높은 효용성을 가진다. 표 3에서 HO3D Hampali et al.(2020)에 대한 결과를 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c|c c c c c c c c c c c c c c c c c}  & & & \\multicolumn{5}{c|}{CO3D-FullImg} & \\multicolumn{5}{c|}{CO3D-No Background} & \\multicolumn{5}{c}{HO3D} \\\\ \\cline{3-19} \\multicolumn{1}{c|}{} & \\multirow{2}{*}{Ince} & \\multirow{2}{*}{Geo.} & \\multirow{2}{*}{Calib.} & \\multirow{2}{*}{Restart} & \\multicolumn{2}{c}{ATE} & \\multicolumn{2}{c}{ATE\\({}_{rot}\\)} & \\multicolumn{2}{c|}{PSNR} & \\multicolumn{2}{c|}{SSIM} & \\multicolumn{2}{c}{LPIPS} \\\\ \\cline{3-19} \\multicolumn{1}{c|}{} & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & **0.138** & **1.16** & **22.24** & **0.654** & **0.428** & **0.215** & **1.80** & **22.34** & **0.893** & **0.132** & **0.035** & **8.07** & **16.24** & **0.865** & **0.164** \\\\ \\cline{3-19} \\multicolumn{1}{c|}{} & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & 0.714 & 25.40 & 20.48 & 0.632 & 0.486 & 0.224 & 1.86 & **22.47** & 0.829 & **0.132** & 0.035 & 27.23 & 15.02 & 0.873 & 0.670 \\\\ \\cline{3-19} \\multicolumn{1}{c|}{} & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & & 0.691 & 28.95 & 18.66 & 0.565 & 0.556 & 0.340 & 3.91 & 21.92 & 0.887 & 0.140 & 0.032 & 19.19 & 14.51 & 0.866 & 0.184 \\\\ \\cline{3-19} \\multicolumn{1}{c|}{} & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & & 1.283 & 36.82 & 19.05 & 0.567 & 0.562 & 0.972 & 15.94 & 21.03 & 0.875 & 0.163 & 0.046 & 30.50 & 12.86 & 0.863 & 0.290 \\\\ \\cline{3-19} \\multicolumn{1}{c|}{} & \\multirow{2}{*}{\\(\\check{\\check{}}\\)} & & & 3.075 & 78.49 & 14.38 & 0.454 & 0.816 & 0.890 & 8.05 & 20.67 & 0.850 & 0.187 & 0.076 & 32.26 & 12.51 & 0.870 & 0.189 \\\\ \\cline{3-19} \\multicolumn{1}{c|}{} & \\multirow{2}{*}{} & & & 6.215 & 114.63 & 12.77 & 0.401 & 0.871 & 6.522 & 114.97 & 8.22 & 0.772 & 0.370 & 0.307 & 131.16 & 7.45 & 0.82 & 0.29 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '가능한 경우 구성 요소를 제거하여 표 4: ** 증폭 연구를 한다. 우리는 모든 설계된 구성 요소가 ICON에 중요하다고 말했습니다. 또한, 우리는 CO3D 대상만이 (배경 없음) 장면에 대한 바지 구제를 관찰하지 않았기 때문에 유당의 효과는 미미하다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c}  & ATE & ATE\\({}_{rot}\\) & PSNR & SSIM & LPIPS \\\\ \\hline Pose Source + NeRF & & & & & \\\\ DROID & 5.903 & 90.25 & 14.54 & 0.181 & 0.818 \\\\ FLOW-CAM & 6.700 & 120.52 & 13.08 & 0.127 & 0.886 \\\\ PoseDiff & 4.601 & 64.24 & 15.42 & 0.508 & 0.492 \\\\ Groundtruth & - & - & 20.77 & 0.718 & 0.301 \\\\ \\hline COLMAP variants & & & & & \\\\ COLMAP(11) & 1.177 & 13.62 & & & & \\\\ COLMAP-SPSG(11) & 2.815 & 38.37 & & - & \\\\ COLMAP-SPSG & 3.616 & 43.74 & & & & \\\\ \\hline Joint Pose + NeRF optimization & & & & & \\\\ GT-Pose+BARF & 2.055 & 17.00 & 15.65 & 0.802 & 0.277 \\\\ BARF & 6.522 & 114.97 & 8.22 & 0.772 & 0.370 \\\\ \\hline ICON (Ours) & **0.215** & **1.80** & **22.45** & **0.893** & **0.132** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **는 CO3D 레이젠슈타인 등(2021) 객체 전용 장면을 배경 없이 비교한 것이다. 다른 방법의 배경 제거 및 실패에 대한 도전에도 불구하고 ICON은 높은 정밀도에서 포즈를 얻고 고품질에서 새로운 견해를 만들 수 있다. COLMAP는 11개 객체에 50% 이상의 프레임만 성공적으로 등록했기 때문에 비교를 위해 “(11)로 표시하였다. SPSG 버전의 COLMAP는 모든 장면을 등록하며 바닐라 COLMAP가 성공하는 11개의 장면 하위 집합에 데이터포인트를 포함한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c c}  & Input & ATE & ATE\\({}_{rot}\\) & Trans & PSNR \\\\ \\hline BARF & RGB & 0.135 & 122.38 & 0.580 & 5.72 \\\\ ICON & & 0.033 & 8.07 & 0.049 & **16.24** \\\\ \\hline \\end{tabular} \\begin{tabular}{c|c|c|c c c}  & Input & ATE & ATE\\({}_{rot}\\) & Trans & PSNR \\\\ \\hline BARF & RGB & 0.135 & 122.38 & 0.580 & 5.72 \\\\ ICON & & 0.033 & 8.07 & 0.049 & **16.24** \\\\ \\hline \\end{tabular}\n' +
      '\\begin{tabular}{c|c|c c c}  & Input & ATE & ATE\\({}_{rot}\\) & Trans & PSNR \\\\ \\hline Basculines & & & & & \\\\ \\hline DROID & RGB & 0.187 & 114.71 & 0.548 & & \\\\ DROID & & 0.105 & 51.93 & 0.262 & & \\\\ BundleTrack & RGB-D & 0.046 & 29.45 & 0.158 & & \\\\ BundleSDF & & **0.021** & **6.82** & **0.030** & & \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: ** HO3D Hampali 등(2020)에 대한 비교이다. ICON은 더 빠른 움직임(vs CO3D), 손 폐색 및 배경 정보의 부족에 대해 강력하게 작용한다. 사실, RGB 입력만 사용함에도 불구하고 ICON은 SOTA RGB-D의회leSDF**gain과 유사한 정밀도로 포즈를 추적할 수 있지만, 공동 객체 포즈 추정 및 NeRF 학습에 대한 BARF와 주로 비교된다. CO3D 객체 전용 버전과 유사하게 물체와 다르게 움직이기 때문에 배경은 마스킹된다. 또한 HO3D는 CO3D보다 손 배제와 더 빠른 포즈 변화에 대한 도전을 제시한다. CO3D와 마찬가지로 BARF가 포즈, 특히 인근 프레임 전체에 걸쳐 더 과감한 카메라 동작으로 포즈를 제대로 배우기 위해 투쟁한다는 것을 관찰했다. 한편, ICON은 이러한 도전과 관련하여 잘 수행할 수 있으며, 포즈들은 정확하게 예측(Tab 3)되고 텍스처는 새로운 뷰(그림 5)에서 적절하게 렌더링된다.\n' +
      '\n' +
      '기존의 여러 작품인 원과 베크리스(2021)와 이 문제를 다룬 원 등은 깊이도 추가로 이용하는데, 이는 3D 객체 재구성 및 포즈에 강력한 신호를 제공한다. 반면에 깊이는 추가적인 센서를 필요로 하고 항상 사용할 수 없으며 인터넷의 대부분의 시각적 매체는 RGB 전용이다. 흥미롭게도 ICON에 대한 결과는 깊이가 필요한 연방정부DF와 같은 최첨단 방법으로 경쟁력이 있다는 것을 발견했다. 또한, 메쉬 생성을 위해 ICON을 설계하거나 최적화하는 것은 아니지만, 우리는 오프 스테프 3월 큐브 로렌센 및 체인(1987) 알고리즘을 실행하여 메쉬에 대한 비교를 포함한다. 우리는 Wen et al.(2023)의 평가 프로토콜을 따르고 정렬 베슬과 맥케이(1992)에 ICP를 사용하고 Chamfer 증류소를 보고한다. 깊이 신호를 사용하지 않았음에도 불구하고 ICON이 연방정부(0.77cm)에 비해 경쟁적인 메쉬 품질(0.7cm)을 제공하는 것을 발견했다. 의회의 재구성은 한 장면(2.39cm)에 대해 제대로 수행되지 않았으며, 두 방법 모두에서 최악의 장면 1개를 제거했으며, 연방회SDF와 ICON은 0.54cm와 0.56cm를 달성했다고 지적했다. 이것은 객체 포즈 추정 및 3D 재구성을 위한 단안 RGB 전용 방법의 잠재력을 나타낸다고 믿는다.\n' +
      '\n' +
      '### Ablation studies\n' +
      '\n' +
      '*** ICON의 핵심 구성 요소는 더 깊은 통찰력을 얻기 위해 절제 연구를 수행하며, 제안된 방법론은 표 4에서 이러한 상당한 개선으로 이어지며, 증분 프레임 등록("증가"), 신뢰 기반 기하학적 제약("거북", 신뢰(칼리브)" 및 재시작("한정)"의 영향을 조사한다. 모든 옵션이 가능한 상단 행은 제안된 ICON에 해당하는 반면 하단 행(없음)은 BARF에 해당한다. 우리는 제안된 모든 기술이 필수적이라는 것을 발견했다.\n' +
      '\n' +
      '**ICON은 사소한 카메라 운동으로 전방 대면 장면에서 작동하는데*** 목표 중심 포즈 추정 및 NeRF 표현의 도전 설정에 대한 동기 및 실험 센터의 많은 부분이 우리의 방법에서 어떤 객체 특정 사제도 강제하지 않는다. 따라서 우리의 접근법은 또한 광범위한 NeRF 커뮤니티가 사용하는 공통 벤치마크인 LLFF Mildenhall 등의 장면 이미지(2019)에 일반화한다. CO3D 또는 HO3D의 비디오 유형에 비해 LLFF의 이미지는 전향적인 경향이 있으며, 각 이미지에 대한 카메라 포즈들은 경미한 차이만 갖는다. 더 쉬운데, 그러한 환경에서 카메라 포즈를 복구할 수 있는 것은 더 넓은 적용 가능성을 위해 여전히 중요하다. 우리는 LLFF의 카메라 포즈가 제한된 변화만을 가지고 있기 때문에 동일성으로 초기화된 BARF가 좋은 포즈를 회복하고 좋은 PSNR, SSIM 및 LPIPS를 달성할 수 있다는 것을 발견했다. 그러나 ICON은 그라운드 진실이 제공되는 BARF와 표준 NeRF를 모두 능가한다.\n' +
      '\n' +
      '그림 5: HO3D에 대한 ICON 새로운 뷰 합성의 표준화를 보여준다. ICON은 모양과 질감을 정확하게 회복할 수 있습니다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '우리는 증분 설정에서 관절 포즈와 NeRF 최적화를 연구하고 이 환경에서 흥미롭고 중요한 문제를 강조했다. 이를 해결하기 위해 새로운 신뢰 기반 최적화 절차인 ICON을 설계했습니다. 다중 데이터 세트에 걸친 강력한 경험적 성능은 ICON이 공통 비디오에서 포즈 초기화의 요구 사항을 본질적으로 제거함을 시사한다. 우리의 초점은 객체 중심 시나리오에 있지만 다른 설정을 배제한 사제나 휴리스틱은 없다. ICON의 LLFF와 풀스코인 CO3D 결과는 강하며 이동 카메라(예: 예를 들어, 중심성 그레이만 등)로부터의 장면 재구성과 같은 보다 일반적인 유형의 비디오 입력에 대한 약속을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c}  & ATE & ATE\\({}_{\\text{rot}}\\) & PSNR & SSIM & LPIPS \\\\ \\hline GT-Pose+NeRF & - & - & 22.06 & 0.648 & 0.294 \\\\ BARF & 0.498 & 0.896 & 23.89 & 0.721 & 0.240 \\\\ ICON & **0.459** & **0.806** & **24.23** & **0.731** & **0.221** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: ** LLFF Mildenhall et al.(2019) 데이터세트**에 대한 비교. 카메라 포즈가 경미하거나 가벼운 움직임을 보일 때, BARF는 정체성 포즈 초기화와 잘 작용하고 ICON은 약간 더 나은 성능을 발휘한다. ATE는 100으로 확장됩니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Azinovic et al. [2022] Dejan Azinovic, Ricardo Martin-Brualla, Dan B Goldman, Matthias Niessner, and Justus Thies. Neural rgb-d surface reconstruction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6290-6301, June 2022.\n' +
      '* Belhumeur et al. [1999] Peter N Belhumeur, David J Kriegman, and Alan L Yuille. The bas-relief ambiguity. _International journal of computer vision_, 1999.\n' +
      '* 베슬과 맥케이[1992] 폴 제블과 닐 디 맥케이. 3-d 형상 등록 방법. __ 3-d 형상 등록 방법. IEEE는 패턴 분석 및 기계 정보_, 14(2):239-256, 1992. 도이: 10.1109/34.121791[https://doi/10.1109/34.121791](https://doi/10.1109/34.121791).\n' +
      '* Bian et al. [2023] Wenjing Bian, Zirui Wang, Kejie Li, Jiawang Bian, and Victor Adrian Prisacariu. Nope-nerf: Optimising neural radiance field with no pose prior. 2023.\n' +
      '* Boss et al. [2022] Mark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen Li, Deqing Sun, Jonathan T. Barron, Hendrik P.A. Lensch, and Varun Jampani. SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* Caron et al. [2021] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* Cheng et al. [2023] Shuo Cheng, Caelan Garrett, Ajay Mandlekar, and Danfei Xu. NOD-TAMP: Multi-step manipulation planning with neural object descriptors. In _Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition @ CoRL2023_, 2023. [https://openreview.net/forum?id=43MSbj5mSS](https://openreview.net/forum?id=43MSbj5mSS).\n' +
      '* Chng et al. [2022] Shin-Fang Chng, Sameera Ramasinghe, Jamie Sherrah, and Simon Lucey. Gaussian activated neural radiance fields for high fidelity reconstruction and pose estimation. In _The European Conference on Computer Vision: ECCV_, 2022.\n' +
      '* Dai et al. [2017] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niessner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. _Computer Vision and Pattern Recognition (CVPR)_, pages 5828-5839, 2017. doi: 10.1109/CVPR.2017.618. [http://www.scan-net.org/](http://www.scan-net.org/).\n' +
      '* 다비슨[2003] 다비슨이요. 실시간 동시 로컬화 및 단일 카메라로 매핑됩니다. 컴퓨터 비전_ 페이지는 1403-1410. IEEE, 2003. IEEE에 대한 _프로플레이션 Ninth IEEE 국제 콘퍼런스입니다.\n' +
      '* Davison et al. [2007] Andrew J Davison, Ian D Reid, Nicholas D Molton, and Olivier Stasse. Monoslam: Real-time single camera slam. _IEEE transactions on pattern analysis and machine intelligence_, 29(6):1052-1067, 2007.\n' +
      '* DeTone et al. [2017] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest point detection and description. _2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, pages 337-33712, 2017. [https://api.semanticscholar.org/CorpusID:4918026](https://api.semanticscholar.org/CorpusID:4918026).\n' +
      '* Engel et al. [2014] Jakob Engel, Thomas Schops, and Daniel Cremers. Lsd-slam: Large-scale direct monocular slam. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part II 13_, pages 834-849. Springer, 2014.\n' +
      '* Engel et al. [2017] Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. _IEEE transactions on pattern analysis and machine intelligence_, 40(3):611-625, 2017.\n' +
      '* Grauman et al. [2017] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcug Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abraham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttkeysa Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video. In _Computer Vision and Pattern Recognition_, 2022.\n' +
      '* 가하람마니[2017] 시레이아스햄팔리, 마흐디 라드, 마르쿠스 오베르웨거, 빈센트 레핏. 혼례: 손과 물체의 3d 주석을 위한 방법은 포즈를 취하고 있다. 2020년 _컴퓨터 비전 및 패턴 인식_에서.\n' +
      '__* 하틀리와 Zisserman(2003) 리처드 하틀리와 앤드류 Zisserman. __. artley and Zisserman (2003) 리처드 하틀리와 앤드류 Zisserman. 컴퓨터 비전_의 다중 뷰 지오메트릭입니다. 미국 캄브리지대 프레스, 2판, 2003. ISBN 0521540518.\n' +
      '* 정씨(2021) 윤우정, 석준안, 크리스토퍼초이, 안마안단두마, 민수조, 재익공원 등이 있다. 자기열화 신경 방사선 촬영 필드입니다. 2021년 컴퓨터 비전_국제 회의에서.\n' +
      '* 카플러 등은 (2018) 다니엘 카펠러, 프랑지스카 메이어, 얀 이셔크, 짐 메인프, 크리스티나 가르시아 케이프, 마누엘 웨트리히, 빈센트 브렌츠, 스테판 슈알라, 나탄 라틀리프, 제만네트 보허 등을 들 수 있다. 실시간 인식은 반응성 움직임 생성을 충족한다. __ 실시간 인식은 반응성 움직임 생성을 충족한다. IEEE 로봇 및 오토메이션 레터_, 3(3):1864-1871, 2018. 도이: 10.1109/LRA.2018.2795645.\n' +
      '* 클라인과 머레이(2007) 게르그 클라인과 데이비드 머레이가 있다. 작은 아르 작업 공간을 위한 병렬 추적 및 매핑입니다. _2007 제6차 IEEE 및 ACM 국제 심포지엄에서 혼합 및 증강 현실_ 페이지 225-234. IEEE 2007.\n' +
      '* 쿠랑 등은(2022) 정페이 쿠랑, 카일 올스즈웨스키, 멍글레이 차이, 풍황, 판노스 아틀리토마스, 세르게이 투르사코프가 있다. 네로릭: 온라인 이미지 컬렉션의 객체들의 신경 렌더링. __Neroic: 온라인 이미지 컬렉션의 객체들의 신경 렌더링이다. 그래픽.___ACM 트랜즈. 10.1145/3528223.3530177[https://doi/10.1145/3528223.3530177](https://doi/10.1145/3528223.3530223.3530177).\n' +
      '*랩베 등은 (2020) 얀랩베, 저스틴 카펜티어, 마티누 아우브리, 호세프 시비치 등이 있다. 목적: 지속 가능한 멀티 뷰 멀티 오브젝트 6d 포즈 추정. 2020년 컴퓨터 비전_에 관한 _유럽 회의에서.\n' +
      '*랩베 등은 (2022) 야나랩베, 루카스 마누엘리, 아르살란 무스카비, 스티븐 타이리, 스탠 비르치필드, 조나단 트렘블레이, 저스틴 카펜티어, 마티누 아우브리, 디에테르 폭스, 호세프 시비치 등을 들 수 있다. 메가포즈: 메가포즈: 6d 포즈 추정은 렌더링 & 비교를 통해 새로운 물체의 __d 포즈 추정이다. arXiv 프리프린트 arXiv:2212.06870_, 2022.\n' +
      '* Lin 등은 (2021) Chen-Hsuan Lin, Wei-Chiu Ma, 안토니오 토랄바, 시몬 루시 등이 있다. 바프: 연방 조정 신경 방사선 분야. 2021년 컴퓨터 비전(ICCV)__IEEE 국제 회의.\n' +
      '* 린 등은 (2023) 윤지린, 토마스 뮐러, 조나단 트렘블레이, 보웬 위엔, 스티븐 타이리, 알렉스 에반스, 패트리시오 아벨라, 스탠 비르치필드 등이다. 강력한 포즈 추정을 위해 신경 영상의 역전을 병행합니다. 1974년 _ICRA_에서.\n' +
      '* 류 등은 (2022)유안 류, 요린 위넨, 사이다 펑, 청린, 샤오시아오 롱, 다쿠 코무라, 원핑 왕이다. 유전자6d: 일반 가능한 모델이 없는 6-dof 객체는 rgb 이미지로부터 추정을 제기한다. 컴퓨터 비전_에 관한 _유럽 회의에서 298-315 페이지는 2022년 스프링거이다.\n' +
      '* 로렌센과 Cline(1987) 윌리엄 E. 로렌센과 하비 E. 콜라인. 정육면체: 고해상도의 3d 표면 구성 알고리즘. __고해상도의 3d 표면 구성 알고리즘. 컴퓨터 그래픽_, 21(4):163-169, 1987. 도이: 10.1145/37402.37422[https://doi/10.1145/37402.37422](https://doi/10.1145/37402.37402.37422).\n' +
      '* 로우(1999) 데이비드 G. 로이. 지역 스케일-불변 특징으로부터의 객체 인식 __ 지역 스케일-불변 특징으로부터의 객체 인식. 컴퓨터 비전(ICCV)_, 페이지 1150-1157, 페이지 10.1109/ICCV.1999.790410[https://www.cs.cs.cca/~lowe/illecv99pdf] (https://www.cs.c.c.C.pdf].\n' +
      '*3월과 (2015) 에릭 3월, 히데키 우치야마, 파비엔 스핀들러. 증강현실에 대한 포즈 추정: 증강현실에 대한 선택적 추정: A 실습 조사. IEEE 거래 표준화 및 컴퓨터 그래픽_, 22(12):2633-2651, 2016. 도이: 10.2513408/TVCG.2015.2513408.\n' +
      '* McCormac et al. (2018) 존 매코믹, 로널드 클락, 마이클 블레쉬, 앤드류 다비슨, 스테판 레텐거. 융합++: 볼로미컬 객체 레벨 슬램입니다. 3D 비전(3DV)__2018 국제회의에서는 2018년 32-41. IEEE 페이지이다.\n' +
      '* 멍 등은 (2021) 취안멍, 안페이 첸, 하임린 루노, 민예우, 하오수, 란Xu, 제밍허, 징이유. GNeRF: GAN 기반 신경 광선 필드, Posed Camera가 없다. 2021년 컴퓨터 비전(ICCV)_에 대한 IEEE/CVF 국제 회의의 _검토에서.\n' +
      '* 메릴 등 (2022) 나타니엘 메릴, 유리랑 구오, 꽝싱 주오, 신유 황, 스테판 리텐거, 시펑, 류 르, 구오만 황 등이 있다. 6dof 오브젝트에 대한 기호 및 불확실성 인식 오브젝트 슬램은 추정 결과를 제공한다. 컴퓨터 비전 및 패턴 인식_ 페이지 14901-14910에 대한 IEEE/CVF 회의의 _발표에서 2022년 페이지 14901-14910.\n' +
      '* 마덴힐 등은 (2019) 벤 마덴힐, 프라틀 피시니바산, 로드리고 오르티즈-캐온, 니마 카다미 칼란타리, 라비 라마모에키, 르네그, 압스헤크 카 등이 있다. 지역 광장 융합: 기술 전 샘플링 지침을 사용한 실제 뷰 합성. __ 국소 광장 융합: 기술 표집 지침을 사용한 실제 뷰 합성. ACM 서비스는 그래픽스(TOG)_, 38(4):1-14, 2019.\n' +
      '* 마덴힐 등 (2020) 벤 마덴힐, 프라틀 피시니바산, 마테워 투키, 조나단 타브론, 라비 라마모에키, 르네네 등이 있다. Nerf: 합성을 보기 위한 신경 영상의 영역으로서 장면을 제시한다. 2020년 컴퓨터 비전_에 관한 _유럽 회의에서.\n' +
      '* 뮐러 등 (2021) 노먼 뮬러, 유시앙 웡, 닐이 J 미트라, 앙헬라 다이, 마티아스 니스너. rgb-d 서열에서 3d 다중 객체 추적을 위해 오브젝트 뒤에 있는 것을 볼 수 있다. 컴퓨터 비전 및 패턴 인식_ 페이지 6071-6080에 대한 IEEE/CVF 회의의 _발표에서 2021년 페이지.\n' +
      '* 뮐러 등은 알(2020) 자콥 무크버그, 조온 하셀그렌, 톈창 선, 준가오, 위즈정 첸, 알렉스 에반스, 토마스 뮐러, 산자 피들러 등이다. 3D 모델, 재료 및 파이팅을 이미지로부터 추출합니다. 컴퓨터 비전 및 패턴 인식(CVPR)_ 페이지 8280-8290, 2022년 6월 IEEE/CVF 회의의 _검토에서.\n' +
      '* 무아탈과 타르도[2017] 라울 머아탈과 후안 다르도스. Orb-slam2: 단안, 스테레오 및 rgb-d 카메라를 위한 An 오픈 소스 슬램 시스템. 2017년 로봇 공학_, 33(5):1255-1262에 대한 IEEE 거래.\n' +
      '* Mur-Artal et al. [2015] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. Orb-slam: a versatile and accurate monocular slam system. _IEEE transactions on robotics_, 31(5):1147-1163, 2015.\n' +
      '* Oechsle et al. [2021] Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In _International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* Park et al. [2020] Keunhong Park, Arsalan Mousavian, Yu Xiang, and Dieter Fox. Latentfusion: End-to-end differentiable reconstruction and rendering for unseen object pose estimation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10710-10719, 2020.\n' +
      '* 파우웰과 크라이즈[2015] 칼 파우웨인과 다니카 크레이크. 심트랙: 확장 가능한 실시간 객체를 위한 시뮬레이션 기반 프레임워크는 검출 및 추적을 제공한다. 2015년 지능형 로봇 및 시스템_에 관한 _국제회의에서.\n' +
      '* Qi et al. [2023] Haozhi Qi, Brent Yi, Sudharshan Suresh, Mike Lambeta, Yi Ma, Roberto Calandra, and Jitendra Malik. General in-hand object rotation with vision and touch. In _7th Annual Conference on Robot Learning_, 2023. [https://openreview.net/forum?id=R1N00jfIV-X](https://openreview.net/forum?id=R1N00jfIV-X).\n' +
      '* Reizenstein et al. [2021] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In _International Conference on Computer Vision_, 2021.\n' +
      '* Rosinol et al. [2022] Antoni Rosinol, John J Leonard, and Luca Carlone. Nerf-slam: Real-time dense monocular slam with neural radiance fields. _arXiv preprint arXiv:2210.13641_, 2022.\n' +
      '* Runz et al. [2018] Martin Runz, Maud Buffier, and Lourdes Agapito. Maskfusion: Real-time recognition, tracking and reconstruction of multiple moving objects. In _2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)_, pages 10-20. IEEE, 2018.\n' +
      '* Salas-Moreno et al. [2013] Renato F Salas-Moreno, Richard A Newcombe, Hauke Strasdat, Paul HJ Kelly, and Andrew J Davison. Slam++: Simultaneous localisation and mapping at the level of objects. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1352-1359, 2013.\n' +
      '* Sarlin et al. [2019] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine: Robust hierarchical localization at large scale. In _CVPR_, 2019.\n' +
      '* Sarlin et al. [2020] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4938-4947, 2020.\n' +
      '* 소촌베르거와 프래름[2016] 요하네스 로촌베르거와 얀-미카엘 프라흐름. 구조로부터 모션이 재방출되었습니다. 컴퓨터 비전 및 패턴 인식_ 페이지 4104-4113에 대한 IEEE 회의의 _발표에서 2016년 페이지 4104-4113.\n' +
      '* Sharma et al. [2021] Akash Sharma, Wei Dong, and Michael Kaess. Compositional and scalable object slam. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 11626-11632. IEEE, 2021.\n' +
      '* Smith et al. [2023] Cameron Smith, Yilun Du, Ayush Tewari, and Vincent Sitzmann. Flowcam: Training generalizable 3d radiance fields without camera poses via pixel-aligned scene flow, 2023.\n' +
      '* Stoiber et al. [2022] Manuel Stoiber, Martin Sundermeyer, and Rudolph Triebel. Iterative corresponding geometry: Fusing region and depth for highly efficient 3d tracking of textureless objects. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6855-6865, 2022.\n' +
      '* Sun et al. [2021] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. _CVPR_, 2021.\n' +
      '* Sundermeyer et al. [2018] Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker, and Rudolph Triebel. Implicit 3d orientation learning for 6d object detection from rgb images. In _Proceedings of the european conference on computer vision (ECCV)_, pages 699-715, 2018.\n' +
      '2020: 제16회 유럽 회의: 글라스고, 영국, 8월 23-28일, 2020년 8월 23-28일, 프로리딩, 파트 II_, 페이지 402-419, 페이지 402-419, 페이지 402_, 페이지 402-419,* ECCV 2020: 제16회 유럽 회의: 글라스고우, 영국, 2020년 8월 23-28일.\n' +
      '베를린, 하이델베르크, 2020년 스프링거-베를래그. ISBN 978-3-078-030-030-58535-8.\n' +
      '* 테드 앤 덩[2021] 자차 테드 앤 지아 다이. DROID-SLAM: Monocular, Stereo 및 RGB-D Cameras의 딥 비주얼 SLAM. __ 뉴럴 정보 처리 시스템_ 2021의 발전.\n' +
      '* Truong et al. [2023] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, and Federico Tombari. Sparf: Neural radiance fields from sparse and noisy poses. In _Computer Vision and Pattern Recognition_, 2023.\n' +
      '* Tschernezki et al. [2021] Vadim Tschernezki, Diane Larlus, and Andrea Vedaldi. NeuralDiff: Segmenting 3D objects that move in egocentric videos. In _Proceedings of the International Conference on 3D Vision (3DV)_, 2021.\n' +
      '* 우미야마[1991] 신지 우미야마. 두 점 패턴 사이의 변환 파라미터의 정지 제곱 추정. __ 2개의 점 패턴 간의 형질전환 파라미터의 평균 제곱 추정. __리바스트 제곱 추정. IEEE 전환은 1991년 패턴 분석 및 기계 정보_, 13(04):376-380에 관한 것이다.\n' +
      '* Wang et al. [2019] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas. Normalized object coordinate space for category-level 6d object pose and size estimation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2642-2651, 2019.\n' +
      '* Wang et al. [2023] Jianyuan Wang, Christian Rupprecht, and David Novotny. Posediffusion: Solving pose estimation via diffusion-aided bundle adjustment. In _International Conference on Computer Vision_, 2023.\n' +
      '* 왕 등[2021a] 펑왕, 링지 류, 원 류, 기독교 테오발트, 다쿠 코무라, 원핑 왕. 네우스: 학습 신경망 암묵면은 멀티뷰 재구성을 위한 볼륨 렌더링에 의한 볼륨 렌더링에 의해, __Neus: 학습 신경망 암묵면이다. arXiv 프리프린트 arXiv:2106.10689_ 2021a.\n' +
      '* Wang et al. [2020] Wenshan Wang, Yaoyu Hu, and Sebastian Scherer. Tartanov: A generalizable learning-based vo. 2020.\n' +
      '* 왕 등[2021b] 지의왕, 샹제우, 위디 제이, 민첸, 빅토르 아드리안 프리스카리루. NeRF\\(-\\)-: 알려져 있는 카메라 파라미터가 없는 신경 방사선 분야. __알려진 카메라 파라미터가 없다. arXiv 프리프린트 arXiv:2102.07064_ 2021b.\n' +
      '* 위엔과 베크리스[2021] 보웬 위엔과 코스타스 베크리스. 분들레트라크: 6d는 인스턴스 또는 카테고리 레벨 3d 모델 없이 새로운 객체들을 추적한다. 지능형 로봇 및 시스템에 관한 _2021 IEEE/RSJ 국제 회의(IROS)_, 페이지 8067-8074 IEEE 프레스, 2021. 도이: 10.1109/IROS51168.2021.9635991[https://doi/10.1109/IROS51168.2021.979/IROS51168.2021.9635991](https://doi/IROS51168.2021.979/IROS511.2021.979/IROS5119/IROS511.2021.2021.2021.2021.2021.2021.2021.2021.2021.2021.2021.2021.2021.2021.930.2021.2021.930.2021.930.930.2021.2021.2021.2021.979/IROS511.2021.979/IROS511.2021.2021.979/IROS511.2021.2021.979/IROS511.2021.2021.979/IROS511.2021.979/IROS511.2021.979/IROS511.2021.979/IROS511.2021.9635991]]\n' +
      '* Wen et al. [2022a] 보웬 위넨, 위즈하오 리안, 코스타스 베크리스 및 스테판 슈알라. Catgrasp: 시뮬레이션에서 배설물에서 학습 범주 수준 과제 관련 파악은 시뮬레이션에서이다. __ 학습 범주 수준 과제 관련 파악이다. ICRA 2022_, 2022a.\n' +
      '* Wen et al. [2022b] 보웬 위넨, 위즈하오 리안, 코스타스 E. 베크리스 및 스테판 슈알라. 단일 시각 시연의 카테고리 수준 조작, _1회만 보여주세요. 참조균류콜라.org/CorpusID:246430152].12716, 2022b.[https://apisemanticscholar.org/CorpusID:246430152].12716[https://apisemanticscholar.org/CorpusID:246430152]. (https://apisemanticscholar.org/CorpusID:246430152)\n' +
      '* Wen et al. [2023] Bowen Wen, Jonathan Tremblay, Valts Blukis, Stephen Tyree, Thomas Muller, Alex Evans, Dieter Fox, Jan Kautz, and Stan Birchfield. Bundlesdf: Neural 6-dof tracking and 3d reconstruction of unknown objects. _Computer Vision and Pattern Recognition_, 2023.\n' +
      '* Xia et al. [2022] Yitong Xia, Hao Tang, Radu Timofte, and Luc Van Gool. Sinerf: Sinusoidal neural radiance fields for joint pose estimation and scene reconstruction. In _33rd British Machine Vision Conference 2022, BMVC 2022, London, UK, November 21-24, 2022_. BMVA Press, 2022. [https://bmvc2022.mpi-inf.mpg.de/0131.pdf](https://bmvc2022.mpi-inf.mpg.de/0131.pdf).\n' +
      '* Xiang et al. [2018] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. In _Robotics: Science and Systems (RSS)_, 2018.\n' +
      '* Yang et al. [2023] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos, 2023.\n' +
      '* Yariv et al. [2020] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. _Advances in Neural Information Processing Systems_, 33, 2020.\n' +
      '* Yariv et al. [2021] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. In _Thirty-Fifth Conference on Neural Information Processing Systems_, 2021.\n' +
      '* Yen-Chen et al. [2021] Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, and Tsung-Yi Lin. iNeRF: Inverting neural radiance fields for pose estimation. In _IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, 2021.\n' +
      '* Yu et al. [2021] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance fields from one or few images. In _CVPR_, 2021.\n' +
      '유 등. [2021]조슨 Y]. 장, 풍산양, 슈바햄 툴시안, 데바 라만. NeRS: 야생에서 희소뷰 3d 재구성을 위한 신경 반사면. 2021년 신경 정보 처리 시스템__Conference에서.\n' +
      '* Zhang et al. [2018] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 586-595, 2018. doi: 10.1109/CVPR.2018.00068.\n' +
      '* 장과 스카라무자[2018] 지차오 장과 다빈드 스카라무자]. 시각적 (-inertial) 악취 측정에 대한 정량적 궤적 평가에 대한 튜토리얼이다. i_2018 IEEE/RSJ 국제 지능형 로봇 및 시스템(IROS)_, 페이지 7244-7251. IEEE 2018.\n' +
      '* Zhao et al. [2022] Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild. In _European conference on computer vision (ECCV)_, 2022.\n' +
      '* Zubizarreta et al. [2020] Jon Zubizarreta, Iker Aguinaga, and Jose Maria Martinez Montiel. Direct sparse mapping. _IEEE Transactions on Robotics_, 36(4):1363-1370, 2020.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:18]\n' +
      '\n' +
      '다른 CO3D 범주의 ICO.\n' +
      '\n' +
      '이 절에서는 CO3D 레이젠슈타인 등(2021)에 대한 본고에서 보고된 결과를 보완한다. 우리는 CO3D로부터 나머지 33개 범주를 모두 사용한 연구를 추가하고 전체 장면을 평가한다. 이를 통해 포즈가 객체 전용 평가에 구별할 수 없는 꽃병과 같은 대칭 객체를 포함할 수 있다. 이러한 카테고리에 대해 공식 하위 집합이 지정되지 않기 때문에, 우리는 가장 높은 카메라를 가진 각 카테고리에서 상위 4개의 인스턴스를 취하므로 각 카테고리에 대해 자신감과 랜덤하게 하나의 샘플을 제공한다. "지상-진실" 카메라 포즈는 COLMAP에 의해 추정되며 100% 정확하지 않을 수 있으며, 특히 이러한 범주는 공식 벤치마킹 세트의 일부가 아니라는 점에 주목할 필요가 있다. 18개 범주의 주요 논문 벤치마킹과 동일(하이퍼-)모수)를 사용합니다.\n' +
      '\n' +
      '우리는 Tab 9에서 결과를 보고한다. 우리는 대부분의 객체가 Tab 6과 유사한 결과를 달성하지만 ICON이 부정확한 포즈를 산출하여 평균 메트릭을 끌어내는 몇 가지 객체가 있음을 관찰한다. 우리는 두 가지 원인이 있다고 믿습니다. 먼저 ICON은 광학적 손실에 의존하며 장면의 변화에 시달릴 수 있다. ICON이 \\(\\geq 3\\) 정도의 회전 오차가 있는 많은 장면들은 이동 그림자(객체 또는 인간)와 강한 조명 변화(카메라의 내장 플래시로부터) 또는 반사 표면들을 갖는다. 우리는 그림 6의 몇 가지 예를 보여주는데, 두 번째, 궤적을 평가하는 데 사용되는 지상 진실 포즈는 COLMAP에 의해 생성되며, 이는 정확하지 않을 수 있으며, 특히 공식 벤치마킹 세트에 포함되지 않은 범주이다.\n' +
      '\n' +
      '스칸넷에 대한 평가 금액.\n' +
      '\n' +
      'ICON은 CO3D 및 HO3D와 같은 객체 중심 영상에 대한 연구에 중점을 둔다. 그러나 ICON은 다른 유형의 비디오에서 작동하는 것을 방지하는 오브젝트에 맞춘 특정 디자인을 적용하지 않는다. 여기에서 스카넷 Dai et al.(2017)에 ICON을 벤치마킹하여 예비 연구를 포함한다. 스칸넷 테스트 세트의 20개 장면 중 10개를 무작위로 샘플하고 카메라 포즈에서 NaN 값이 있는 2. 스켄의 보이드가 있는 200개 프레임의 클립을 사용하면 장면을 샘플링할 때 제거된다.\n' +
      '\n' +
      '전작 Zhao et al.(2022)에 따른 카메라 포즈 품질을 회전 및 번역에 대한 절대 추적 유도(ATE (m))에 대해 연관 Pose Error(RPE)를 사용하여 보고한다. 스카넷의 일부 궤적은 번역이 매우 작고 궤적을 정렬한 다음 회전을 평가하는 것이 신뢰할 수 없기 때문에 Zhao et al.(2022)를 따르지 않는다.\n' +
      '\n' +
      '우리는 ICON에 대한 CO3D 전체 장면 훈련에 사용된 _any_(하이퍼-) 파라미터는 ScanNet에서 유의미한 다른 시나리오에 대한 시스템을 테스트하는 데 변경되지 않는다. 비교를 위해 스칸넷에서 잘 작동하도록 설계된 4가지 방법에는 타탄VO 왕(2020), COLMAP Schonberger 및 Frahm(2016), DROID-SLAM Teed and Deng(2021), 현재 최첨단 방식 참여 SfM Zhao et al.(2022). 우리는 COLMAP와 참여형 SfM이 짧은 클립에서만 실행 시 잘 작동하지 않을 수 있으므로 전체 비디오에서 실행하여 결과를 클립에 보고한다는 점에 주목한다. 또한 Zhao et al.(2022)에서 언급한 바와 같이 COLMAP는 종종 많은 ScanNet 장면에 실패하기 때문에 Tschernezki et al.(2021)에 따라 튜닝된 버전을 사용한다.\n' +
      '\n' +
      '우리는 Tab 10에서 결과를 보고하는데, CO3D로부터 이전할 때 튜닝이나 변화가 없음에도 불구하고 ICON은 Tab 10에서 잘 작동하도록 설계된 최첨단 방법에 비해 ScanNet에 대한 강력한 성능을 달성합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c}  & ATE & \\(\\text{ATE}_{rot}\\) & Trans & PSNR & CD(cm) \\\\ \\hline SiS1 & 0.028 & 3.80 & 0.017 & 19.13 & 0.23 \\\\ MC1 & 0.019 & 5.90 & 0.049 & 14.24 & 0.41 \\\\ ABF13 & 0.064 & 10.67 & 0.094 & 11.79 & 1.72 \\\\ GPMF12 & 0.029 & 11.23 & 0.056 & 16.27 & 0.38 \\\\ ND2 & 0.027 & 7.18 & 0.015 & 20.06 & 0.50 \\\\ SM2 & 0.026 & 5.56 & 0.032 & 13.51 & 0.85 \\\\ SMu1 & 0.017 & 13.19 & 0.081 & 14.46 & 1.02 \\\\ AP13 & 0.058 & 7.06 & 0.046 & 20.42 & 0.50 \\\\ \\hline Avg & 0.033 & 8.07 & 0.049 & 16.24 & 0.70 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'HO3D 평가에 대한 ICON의 퍼센 성능은 표 8::였다. CD는 메쉬 품질을 측정하는 Chamfer Distance를 의미합니다.\n' +
      '\n' +
      '그림 6: ICON이 더 큰 오류를 생성하는 스켄이다. ICON은 주로 광계 손실이 일관되지 않은 감독을 생성하는 장면으로 고통받고 있다. 자동차 예는 인간의 그림자와 자동차 상의 반사면을 움직이는 것으로 구성된다. 와인글라스 예는 투명 표면 및 광 반사를 포함한다. 도넛 예는 일관되지 않은 조명을 포함하며, 여기서 카메라로부터의 플래시는 전면에서 더 밝은 색을 생성하고 뒷부를 어둡게 한다. 다른 관점에서 이러한 불일치는 ICON이 부정확한 카메라를 생성하게 한다.\n' +
      '\n' +
      '캔넷 스타일 비디오. 우리는 이것이 ICON이 다른 유형의 영상에 일반화되고 적응될 수 있다는 개념 증명이라고 믿는다.\n' +
      '\n' +
      '## 부록 D 임장 및 향후 방향\n' +
      '\n' +
      'ICON은 포즈와 NeRF를 공동으로 최적화하기 위해 강력한 성능을 달성하지만 몇 가지 한계를 가지고 있다. 첫째, ICON은 NeRF와 포즈 모두에 대한 감독으로서 광학적 손실에 크게 의존한다. 이것은 색상이 서로 다른 관점에 걸쳐 적당히 일관된다는 가정에 의존한다. 그러나 이러한 가정은 실제 세계에서 깨질 수 있다. ICON은 광계 손실이 일치하지 않는 부피에 대한 자신감을 사용하지만 모호성으로 인해 부정확한 포즈(5~10도 회전 오류)를 생성할 것이다. Tab 9 및 Fig 6에서 볼 수 있듯이 ICON은 움직임, 반사 표면, 투명도 및 강력한 조명 변화를 겪는다. DINO 카온(2021)과 같은 이러한 변화에 강력한 기능을 활용하는 것이 이 문제를 완화하는 데 도움이 될 수 있다고 믿는다.\n' +
      '\n' +
      '또한 ICON은 훈련하는 데 몇 시간이 걸리는 NeRF 마덴힐 등(2020)을 통한 구배 기반 최적화에 달려 있다. 우리는 ICON을 3 공간의 보다 효율적인 모델링과 결합시키는 것이 PixelNeRF 유(2021)와 FLOW-CAM 스미스 등 유망한 방향이 될 것이라고 믿는다(2023).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l r r r r r r} \\hline Category & Scene & \\multicolumn{3}{c}{ATE} & \\multicolumn{1}{c}{ATE\\({}_{rot}\\)} & \\multicolumn{1}{c}{PSNR} & \\multicolumn{1}{c}{SSIM} & \\multicolumn{1}{c}{IPIPS} \\\\ \\hline backpack & 506\\_72977 & 141839 & 0.060 & 0.42 & 20.74 & 0.59 & 0.42 \\\\ banana & 612\\_97867 & 196978 & 1.691 & 11.23 & 13.04 & 0.15 & 0.81 \\\\ baseball & 375 42661 & 85494 & 0.791 & 7.83 & 13.92 & 0.61 & 0.68 \\\\ baseball glove & 350\\_36909 & 69272 & 0.054 & 0.72 & 20.52 & 0.43 & 0.62 \\\\ bicycle & 62\\_4324 & 10701 & 0.700 & 5.94 & 15.22 & 0.19 & 0.69 \\\\ bottle & 589\\_88280 & 175252 & 0.098 & 1.18 & 29.59 & 0.76 & 0.38 \\\\ car & 439 62880 & 124254 & 0.765 & 4.43 & 11.40 & 0.32 & 0.87 \\\\ carrot & 372\\_40937 & 81628 & 0.873 & 2.17 & 20.86 & 0.63 & 0.44 \\\\ cellphone & 76\\_7569 & 15872 & 4.725 & 19.55 & 13.26 & 0.30 & 0.85 \\\\ chair & 455\\_64283 & 126636 & 0.009 & 0.28 & 22.77 & 0.73 & 0.27 \\\\ couch & 427\\_59830 & 115190 & 0.140 & 1.64 & 25.67 & 0.84 & 0.29 \\\\ cup & 44\\_2241 & 6750 & 0.453 & 2.47 & 23.50 & 0.60 & 0.49 \\\\ donut & 403\\_52964 & 103416 & 2.248 & 11.89 & 17.60 & 0.74 & 0.57 \\\\ frisbee & 339\\_35238 & 64092 & 0.738 & 3.75 & 22.34 & 0.43 & 0.66 \\\\ hairdryer & 378\\_44249 & 88180 & 0.022 & 0.16 & 25.84 & 0.82 & 0.33 \\\\ handbag & 406\\_54390 & 105616 & 0.273 & 2.32 & 26.51 & 0.89 & 0.26 \\\\ hotdog & 618\\_100797 & 202003 & 2.600 & 7.23 & 19.78 & 0.45 & 0.78 \\\\ keyboard & 375\\_42606 & 85350 & 1.596 & 7.04 & 18.54 & 0.46 & 0.60 \\\\ kite & 428\\_60143 & 116852 & 0.029 & 0.36 & 18.01 & 0.30 & 0.74 \\\\ laptop & 378\\_44295 & 88252 & 1.128 & 7.92 & 15.04 & 0.36 & 0.59 \\\\ microwave & 504\\_72519 & 140728 & 0.023 & 0.45 & 21.17 & 0.61 & 0.42 \\\\ motorcycle & 367\\_39692 & 77422 & 0.006 & 0.14 & 26.52 & 0.78 & 0.30 \\\\ parkingmeter & 483\\_69196 & 135585 & 0.136 & 2.48 & 17.24 & 0.56 & 0.56 \\\\ pizza & 372\\_41288 & 82251 & 0.036 & 0.26 & 27.70 & 0.69 & 0.42 \\\\ sandwich & 366\\_39376 & 76719 & 0.411 & 1.67 & 19.74 & 0.53 & 0.51 \\\\ stopsign & 617\\_99969 & 199015 & 3.229 & 13.81 & 13.99 & 0.40 & 0.72 \\\\ toilet & 605\\_94579 & 188112 & 0.252 & 5.48 & 18.53 & 0.69 & 0.41 \\\\ toybus & 273\\_29204 & 56363 & 0.057 & 0.40 & 23.34 & 0.65 & 0.60 \\\\ toylplane & 405\\_53880 & 105088 & 0.020 & 0.12 & 22.20 & 0.53 & 0.53 \\\\ tv & 48\\_27428095 & 0.097 & 0.81 & 26.32 & 0.81 & 0.39 \\\\ umbrella & 191\\_20630 & 39388 & 1.115 & 5.73 & 17.35 & 0.44 & 0.60 \\\\ vase & 374\\_41862 & 83720 & 0.100 & 1.27 & 29.25 & 0.85 & 0.28 \\\\ wineglass & 401\\_51903 & 101703 & 1.191 & 7.80 & 21.43 & 0.58 & 0.53 \\\\ \\hline Avg & & 0.778 & 4.21 & 20.57 & 0.57 & 0.53 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: CO3D 전장 평가에서 ICON의 퍼센 성능이 다른 33개 범주에 대해 나타났다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c}  & \\multicolumn{2}{c}{TartanVO} & \\multicolumn{1}{c}{DROID} & \\multicolumn{1}{c}{COLMAP} & \\multicolumn{1}{c}{ParticleSIM} & \\multicolumn{1}{c}{ICON} \\\\ \\hline RPE(degree) & 1.41 & 0.56 & 0.67 & 0.34 & 0.47 \\\\ ATE(m) & 0.198 & 0.066 & 0.091 & 0.053 & 0.092 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '스카넷에 대한 카메라 포즈 평가는 Table 10과 같다. SanNet 시나리오에 최적화되지 않았음에도 불구하고 ICON은 경쟁 성과를 달성하여 RPE에서 2위, ATE에서는 3위를 차지하고 있다. ICON과 최첨단 방식의 차이는 매우 작다(회전에 0.13도, 번역 시 0.039m).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>