<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '영상 제작 시스템+# DiffusionGPT+LLM-Driven Text\n' +
      '\n' +
      'Jie Qin\\({}^{1}\\,{}^{1}\\) 위펑첸\\({}^{1}\\,{}^{1}\\) 유시아 리\\({}^{1}\\)\n' +
      '\n' +
      '헤펑우\\({}^{2}\\,{}^{1}\\) 루이 왕\\({}^{1}\\) 샤일리 위니\\({}^{1}\\)\n' +
      '\n' +
      '경쟁 기업({}^{2}\\).\n' +
      '\n' +
      '프로젝트 페이지: [https://DiffusionGPT.githubio](https://DiffusionGPT.githubio) (https://DiffusionGPT.githubio)\n' +
      '\n' +
      'Equal contribution.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '확산 모델은 이미지 생성 분야에 대한 새로운 방법을 열어 오픈 소스 플랫폼에서 공유되는 고품질 모델의 확산을 가져왔다. 그러나 현재 텍스트 대 이미지 시스템에서 주요 과제는 종종 다양한 입력을 처리할 수 없거나 단일 모델 결과에 국한된다. 현재의 통일된 시도는 종종 입력 단계에서 i) 파세 디버스 프롭트에 대한 두 가지 직교 측면에 속하며 ii)는 전문가 모델을 활성화하여 출력한다. 두 세계 모두의 최고를 결합하기 위해 다양한 유형의 프롬프트를 원활하게 수용하고 도메인-퍼블리셔 모델을 통합할 수 있는 통일된 생성 시스템을 제공하기 위해 대형 언어 모델(LLM)을 활용하는 **DiffusionGPT**를 제안한다. DiffusionGPT는 사전 지식을 기반으로 다양한 생성 모델에 대한 도메인 특이적 트리를 구성한다. 투입이 제공되었을 때, LLM은 프롬프트를 파싱하고 트리를 사용하여 적절한 모델의 선택을 안내함으로써 입력 제약을 완화하고 다양한 도메인에 걸쳐 탁월한 성능을 보장한다. 더욱이, 우리는 모델 선택 과정을 인간의 선호와 정렬하여 인간 피드백이 풍부한 아반파지 데이터베이스를 소개한다. 광범위한 실험과 비교를 통해 DiffusionGPT의 효과를 입증하며 다양한 영역에서 이미지 합성의 경계를 밀어낼 수 있는 가능성을 보여준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 몇 년 동안 이미지 생성 작업에서 확산 모델[6]의 유병률을 목격하고 이미지 편집, 스타일화 및 기타 관련 과제에 혁명을 가져왔다. DALLE-2[14]와 Imagen[17] 모두 텍스트 프롬프트에서 이미지를 생성하는 데 매우 능숙하다. 그러나 그들의 비개방적 소스 특성은 광범위한 대중화와 그에 상응하는 생태학적 개발을 방해했다. 급속히 인기를 얻고 널리 사용했던 스테이블 디퓨전(SD)[16]으로 알려진 첫 오픈소스 텍스트 대 이미지 확산 모델이다. 제어넷[27], 로라와 같은 SD에 맞춘 다양한 기술이 SD의 개발을 위한 길을 더 포장하고 다양한 응용 프로그램으로의 통합을 촉진했다. 10]은 최신 영상 생성 모델입니다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:2]\n' +
      '\n' +
      '비언어를 위한 작업용 대용량 모델(LLM)\n' +
      '\n' +
      '자연어 처리 분야(NLP)는 대화 인터페이스를 통해 인간 상호작용에서 괄목할 만한 숙련도를 보여준 대형 언어 모델(LLM)[3, 9, 21]의 등장으로 상당한 전환을 목격했다. LLM의 능력을 더욱 높이기 위해 사상(CoT) 프레임워크[7, 22, 28, 29]가 도입되었다. 이 프레임워크는 LLM이 우수한 최종 답변을 목표로 단계별 답변을 생성할 수 있도록 안내한다. 최근의 연구는 외부 도구나 모델을 LLM[11, 18, 19, 20, 23]과 통합하여 혁신적인 접근법을 탐구해 왔다. 예를 들어, Toolformer[18]는 API 태그를 통해 외부 툴에 접근할 수 있는 능력으로 LLM을 촉진한다. 시각적 ChatGPT[23]과 Hugging-GPT[19]는 언어 경계를 넘어서는 복잡한 작업을 처리하기 위해 다른 모델을 레버리지할 수 있게 함으로써 LLM의 능력을 확장한다. 유사하게, 비주얼 프로그램[5] 및 ViperGPT[20]은 프로그래밍 언어를 사용하여 시각적 질문을 파싱함으로써 시각적 객체를 처리하는 LLM의 잠재력을 활용한다. 이러한 노력의 영감을 얻으려면 LLM의 개념을 다재다능한 도구로 수용하고 이 패러다임을 레버리지하여 T2I 모델이 고품질 이미지를 생성할 수 있도록 안내합니다.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '디확산GPT는 다양한 입력 프롬프트들을 위해 고품질 이미지를 생성하도록 특별히 설계된 올인원 시스템이다. 그 주요 목적은 입력 프롬프트를 파싱하고 가장 최적의 결과를 생성하는 생성 모델을 식별하는 것이며, 이는 높은 일반화와 고이용성, 편리한 것이다.\n' +
      '\n' +
      '디확산GPT는 오픈소스 커뮤니티(_e.g_ Hugging 페이스, 민사ai)의 대형 언어 모델(LLM) 및 다양한 도메인-전문가 생성 모델을 구성한다. LLM은 핵심 제어기의 역할을 가정하고 시스템의 전체 워크플로우를 유지하는데, 이 시스템은 프롭 파세, 빌딩 및 검색 모델 트리 어포던스, 인간 피드백을 사용한 모델 선택 및 세대 실행의 네 단계로 구성된다. 디확산GPT의 전체 파이프라인은 그림 2와 같다.\n' +
      '\n' +
      '### Prompt Parse\n' +
      '\n' +
      '프로빗 파라세젠트는 대규모 언어 모델(LLM)을 활용하여 입력 프롬프트에서 두드러진 텍스트 정보를 분석하고 추출함에 따라 우리의 방법론에서 중추적인 역할을 한다. 프롬프트의 정확한 파싱은 사용자 입력의 고유한 복잡성을 감안할 때 원하는 콘텐츠를 효과적으로 생성하는 데 중요하다. 이 에이전트는 신속한 기반, 지시 기반, 영감 기반, 가설 기반, _etc_를 포함하여 다양한 유형의 프롬프트에 적용할 수 있다.\n' +
      '\n' +
      '** 촉진 기반**: 전체 입력은 생성의 프롬프트로 사용된다. 예를 들어, 입력이 _"a개"_세대에 사용되었던 프롬프트가 _"a개"_일 것이다.\n' +
      '\n' +
      '명령어 기반**: 명령어의 핵심 부분은 생성의 프롬프트로 추출된다. 예를 들어, 입력이 _"개의 이미지를 감소시키면 인식된 프롬프트는 _"개의 이미지"_가 될 것이다.\n' +
      '\n' +
      '** 증산 기반**: 욕망의 목표 주체를 추출하여 생성 프롬프트(예: 인풋: _"나는 해변"_, 인지: _"a 해변"_)로 사용한다.\n' +
      '\n' +
      '** 가설 기반**: 가설 조건(_"만약 xxx, I-xx"_)과 신진 행위의 대상을 생성의 신속성으로 추출하는 것을 포함한다. 예를 들어, 입력이 _"인 경우 장난감을 주시면 매우 행복하게 웃을 것"_, 인식된 프롬프트는 _"a 장난감이자 웃는 얼굴"_이 될 것이다.\n' +
      '\n' +
      '이러한 형태의 프롬프트를 식별함으로써 프롬프트 파라세입니다.\n' +
      '\n' +
      '그림 2: 디퓨전GPT를 참조하세요. 디퓨전GPT의 워크플로우는 빌딩 및 검색 모델, 모델 선택 및 실행 세대의 프롭트 파라세, 트리의 어포던트 4단계로 구성된다. 4단계는 왼쪽에서 오른쪽으로 표시되고 LLM과 연속적으로 상호작용한다. 상측은 각 단계의 세부 과정을 보여준다. 하측은 전체 워크플로우의 예를 보여준다.\n' +
      '\n' +
      'Agent는 DiffusionGPT가 사용자가 생성하고자 하는 핵심 콘텐츠를 정확하게 인식하면서 시끄러운 텍스트의 영향력을 완화시킬 수 있게 한다. 이 과정은 적절한 생성 모델을 선택하고 고품질의 생성 결과를 달성하는 데 중요하다.\n' +
      '\n' +
      '메텔은 메텔로 지정되었습니다.\n' +
      '\n' +
      '신속한 파싱 단계에 이어, 후속 단계는 원하는 이미지를 생성하기 위해 광범위한 모델 라이브러리에서 적절한 생성 모델을 선택하는 것을 포함한다. 그러나 이용 가능한 모델의 수가 많은 것을 고려할 때, 선택을 위해 모든 모델을 대형 언어 모델(LLM)에 동시에 입력할 수 없다. 추가적으로 서로 다른 모델이 그들의 생성 공간에서 유사성을 나타낼 수 있으므로 전체 모델 라이브러리에 걸쳐 단일 퍼지 일치를 통해 가장 적합한 모델을 정확하게 식별하는 것이 어려워진다. 이 문제를 해결하고 최적의 모델을 식별하기 위해 모델 트리의 검색 능력(TOT)을 기반으로 모델 트리의 활용을 제안함으로써 모델들의 후보 세트를 좁히고 모델 선택 과정의 정확도를 높일 수 있다.\n' +
      '\n' +
      'TOT***를 사용하여 모델 트리를 구성하는 모델 빌딩 아젠트의 트리는 모든 모델의 태그 속성을 기반으로 모델 트리를 자동으로 구축하는 데 사용된다. 모든 모델의 태그 속성을 입력함으로써\n' +
      '\n' +
      '그림 3: ChatGPT[9]와의 상호작용 동안 프롬프트의 소매. ChatGPT에 입력되기 전에 그림의 슬롯 \'\\(\\{\\}\\)\'를 해당 텍스트 값으로 균일하게 대체한다.\n' +
      '\n' +
      '에이전트는 하위객체 도민과 스타일 도메인에서 파생된 잠재적 범주를 분석하고 요약한다. 그런 다음 스타일 범주는 하위 범주 내에서 하위 범주로 통합되어 2층 계층적 트리 구조를 확립한다. 이어서, 모든 모델이 그 속성에 기초하여 적합한 리프 노드에 할당되어 포괄적인 모델 트리 구조를 완료한다. 아래 그림은 모델 트리의 시각적 표현을 보여준다. 모델 트리가 에이전트에 의해 자동으로 구성됨에 따라, 이 접근법은 새로운 모델을 통합하기 위한 편리한 확장성을 보장한다. 새로운 모델이 추가될 때마다 에이전트는 속성을 기반으로 모델 트리 내에서 적절한 위치에 원활하게 배치한다.\n' +
      '\n' +
      'TOT**를 사용하여 모델 트리를 검색하는** 모델 검색 어젠트의 트리를 기반으로 모델 트리 내의 검색 프로세스는 주어진 프롬프트와 밀접하게 일치하는 모델들의 후보 세트를 식별하는 것을 목표로 한다. 이 검색 방법은 각 리프 노드에서 최고의 하위 범주를 체계적으로 평가하는 폭 우선 접근 방식을 사용한다. 각 수준에서 카테고리는 가장 가까운 일치를 나타내는 카테고리를 결정하기 위해 입력 프롬프트와 비교된다. 이 반복 과정은 후속 리프 노드에 대한 후보 세트를 계속 도출하고, 모델들의 후보 세트가 얻어지는 최종 노드에 도달할 때까지 검색이 진행된다. 이 모델 후보 세트는 후속 단계에서 모델 선택의 기초가 된다.\n' +
      '\n' +
      '### Model Selection\n' +
      '\n' +
      '모델 선택 단계는 이전 단계에서 획득된 후보 세트로부터 원하는 이미지를 생성하기 위한 가장 적합한 모델을 식별하는 것을 목적으로 한다. 이 후보 세트는 입력 프롬프트와 상대적으로 높은 매칭도를 나타내는 모델로 구성된 전체 모델 라이브러리의 서브세트를 나타낸다. 그러나 오픈소스 커뮤니티에서 이용 가능한 제한된 속성 정보는 대형 언어 모델(LLM)에 상세 모델 정보를 제공하면서 최상의 모델을 정확하게 결정하는 데 어려움을 초래한다. 이를 해결하기 위해 인간 피드백을 활용하고 우위 데이터베이스 기술을 활용하여 모델 선택 과정을 인간의 선호와 정렬시키는 모델 선택 에이전트를 제안한다.\n' +
      '\n' +
      '우위 데이터베이스의 경우, 점수 정보를 저장하는 10,000 프롬프트의 코퍼스 기반의 모든 모델 생성 결과에 대한 점수를 계산하기 위해 보상 모델을 사용한다. 입력 프롬프트를 받으면 입력 프롬프트와 10,000 프롬프트 사이의 의미 유사도를 계산하여 유사도가 가장 높은 상위 5 프롬프트를 식별한다. 이어서, 모델 선택 어젠트는 오프라인 데이터베이스에서 이러한 프롬프트들에 대한 각각의 모델의 사전 연산된 성능을 검색하고 선택된 프롬프트 각각에 대해 상위 5개의 모델을 선택한다. 이 과정은 5x5 모델의 후보 세트를 생성한다.\n' +
      '\n' +
      '그런 다음 에이전트는 모델 세트와 모델 스테이지 TOT에서 얻은 모델 후보 세트를 교차하며, 발생 확률이 높고 순위가 상대적으로 높은 모델에 초점을 맞추고 있다. 이러한 모델은 궁극적으로 모델 생성을 위한 최종 선택으로 선택된다.\n' +
      '\n' +
      '세대\n' +
      '\n' +
      '가장 적합한 모델이 선택되면 선택된 생성 모델을 사용하여 획득된 코어 프롬프트를 사용하여 원하는 이미지를 생성한다.\n' +
      '\n' +
      '**촉진 확장*** 생성 과정에서 프롬프트의 품질을 높이기 위해 프롬프트 확장제를 사용하여 프롬프트를 증가시킨다. 이 에이전트는 선택된 모델로부터의 프롬프트 예를 강조하여 입력 프롬프트를 자동으로 풍부하게 한다. 예제 프롬프트와 입력 프롬프트는 모두 인텍스트 학습 패러다임에서 LLM으로 전송된다. 특히, 이 에이전트는 예시적인 프롬프트의 문장 패턴에 따른 입력 프롬프트에 풍부한 설명과 상세한 어휘를 통합한다. 꽃과 새, 날카로운 초점, 야간, 목걸이, 중국어 신화, 절단, 미디밴드, 미디브, 스콜레이즈, 미디비전, 미디렉션, 미디렉션, 스릴리언트 아브레이크, 미디엄의 사진 초상화(1.2), 꽃과 발톱, 미디얼, 미디엄, 스포트라스트, 퍼펙트, 미디브, 미디비전, 미디비전, 파란, 미디비전, 미디렉션, 스릴리언트, 미디비전, 미디비전, 미디비전, 미디렉션, 미디비전, 미디렉션, 미디브러시, 미디브레이팅, 미디비전, 미디렉션, 미디비전, 미디렉션, 미디브레이팅, 미디브레이팅, 미디비전, 프레이팅, 미디비전, 미디렉션, 미디렉션, 미디렉션, 미디렉션, 미디비전, 프레이팅, 프레이팅, 프롭스, 프레이팅, 프레이팅, 프레이팅, 퍼펙트, 스웨이트, 페이드, 스 이러한 증강은 생성된 출력의 품질을 크게 향상시킵니다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Settings\n' +
      '\n' +
      '우리의 실험 설정에서 사용된 1차 대형 언어 모델(LLM) 제어기는 ChatGPT[9]였으며, 특히 OpenAI API를 통해 접근할 수 있는 텍스트-다빈치-003 버전을 사용하였다. LLM 반응의 지침을 용이하게 하기 위해 생성된 출력을 효과적으로 제어하고 지시한 LangChain 프레임워크를 채택했다. 실험에 사용된 생성 모델의 경우 Civitai 및 Hugging 페이스 커뮤니티에서 제공하는 다양한 모델을 선택했다. 선택 과정은 이러한 플랫폼에서 사용할 수 있는 다양한 유형 또는 스타일에 걸쳐 가장 인기 있는 모델을 선택하는 것과 관련이 있다.\n' +
      '\n' +
      '### Qualitative Results\n' +
      '\n' +
      'SDG1.5 Versionion.5 Version######## 4.2.1 Vualization SD1.5 Versionion.5 Versionion.######### 4.2.1 Vualization.\n' +
      '\n' +
      '시스템 효능을 평가하기 위해 기준 방법 SD 1.5[16]에 대한 생성 성능을 비교하여 포괄적인 평가를 수행했다. SD 1.5는 다양한 전문 커뮤니티 모델의 발굴 모델 역할을 한다. 비교 결과는 그림 4에 제시되어 있으며, 네 가지 별개의 프롬프트 유형에 대한 세부 분석을 실시하여 의미 정렬과 이미지 미학의 두 가지 핵심 차원을 따라 비교하였다.\n' +
      '\n' +
      '결과를 주의 깊게 조사한 결과, 기저 모형에 주목할 만한 두 가지 이슈인 i)애니메이션 링: 베이스 모델의 생성된 이미지는 입력 프롬프트에서 파생된 특정 시맨틱 클래스에 제한된 초점을 나타내어 전체 시맨틱 정보를 불완전하게 캡처한다. 이러한 한계는 모든 종류의 프롬프트 유형에서 특히 분명하며, 기본 모델은 _"만, 요리사, 어린이 및 토일랜드"_와 관련된 물체를 효과적으로 생성하기 위해 투쟁한다. 인간 관련 대상에 대한 성능 저하: 염기 모델은 인간 관련 객체에 대한 정확한 얼굴 및 신체 세부 정보를 생성하는 데 어려움을 겪으며 하위 이미지 품질을 초래한다. 이 결핍은 _"걸과 부부"_를 묘사한 이미지의 심미적 특성을 비교할 때 명백해진다.\n' +
      '\n' +
      '대조적으로, 디퓨전GPT는 이러한 한계를 효과적으로 다룬다. 우리 시스템에 의해 생성된 이미지는 대상 영역의 비교적 완전한 표현을 보여주며, 이는 전체 입력 프롬프트를 포함하는 의미 정보를 성공적으로 캡처한다. 아이들이 눈사람을 쌓고 눈볼 싸움을 하는 ‘눈덩어리’_\'를 휘둘리는 __"만"과 같은 예들은 우리 체제의 광범위한 맥락을 포괄할 수 있는 능력을 보여준다. 또한, 우리의 시스템은 인간과 관련된 객체에 대해 더 상세하고 정확한 이미지를 생성하는 데 탁월합니다. 이는 프롬프트 _" 로맨틱 커플이 스타리 하늘 아래 압통한 순간을 공유하는 것으로 예시된다.\n' +
      '\n' +
      '#### 4.2.2 Visualization SDXL Version.\n' +
      '\n' +
      '공공적으로 이용 가능한 보편적 세대 모델의 발전으로 새롭게 개선된 방법 SD XL[10]이 유망한 접근법으로 등장하여 우수한 세대 결과를 보여주고 있다. 시스템 개선을 위해 SD XL를 기반으로 다양한 오픈소스 커뮤니티 모델을 통합하여 업그레이드된 버전을 개발했습니다. 시스템 성능을 평가하기 위해서는 <그림 5>와 같이 SD XL와 비교하여 시스템의 성능을 평가하기 위해서는 <그림 5>와 같이 SD XL에 비유하는 것이 중요하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c} \\hline \\hline Method & Image-reward & Aes score \\\\ \\hline SD15 & 0.28 & 5.26 \\\\ Random & 0.45 & 5.50 \\\\ DiffusionGPT wo HF & 0.56 & 5.62 \\\\ DiffusionGPT & **0.63** & **5.70** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: ** 정량적 결과**: 이미지 위상 및 심미적 점수로 심미적 점수를 평가한다. "SD15"와 "랜덤"에 대한 호환은 출력하기 위해 전문가 모델을 선택하는데, 나무와 인간 피드백은 모두 생성된 이미지와 인간 정렬의 미학을 향상시킬 수 있다.\n' +
      '\n' +
      '그림 4: SD1.5 기반 DiffusionGPT와 SD15[16]을 비교할 때 DiffusionGPT는 인간과 장면과 같은 범주에 대해 미세화된 수준에서 더 현실적인 결과를 생성하는 데 탁월함을 관찰할 수 있다. 생성된 이미지는 SD15에 비해 시각 충실도가 향상되고 더 미세한 디테일을 캡처하고 더 고수준의 리얼리즘을 나타낸다.\n' +
      '\n' +
      '모든 출력 이미지가 1024x1024의 해상도를 가지며 비교 목적으로 4가지 별개의 유형의 프롬프트가 생성되었음을 알 수 있다. 조심하게 분석하면 SD XL가 특정 사례에서 간혹 의미 정보의 부분적 손실을 나타냄이 명백해진다. 예를 들어, 지시 기반 카테고리에서 프롬프트 기반 카테고리 또는 _"페이싱 자동차"_에서 _"3D 호랑이"_를 포함하는 프롬프트에 대한 생성된 결과는 정확한 표현이 부족할 수 있다. 대조적으로, 우리의 시스템은 보다 정확하고 시각적으로 매력적인 표현을 생산하는 데 탁월합니다. 주목할 만한 예로는 만화 고양이가 있는 _"a 화이트 타월의 생성과 _"스타리 하늘"_의 묘사가 있다.\n' +
      '\n' +
      '### Quantitative Results\n' +
      '\n' +
      '표 1에 제시된 사용자 선호도와 정량적 발견 사이의 정렬은 디퓨전GPT의 견고성과 효과에 대한 강력한 증거 역할을 한다. 생성된 다른 결과를 추가로 평가하기 위해 심미적 예측 변수와 인간 피드백 관련 보상 모델을 사용했다. 기본 버전과 기준 모델 SD1.5의 효과를 비교함으로써 표 1의 결과는 전체 틀이 이미지 위상 및 미적 점수 측면에서 SD15를 능가하여 각각 0.35% 및 0.44%의 개선을 달성했음을 보여준다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '사상과 휴먼 피드백###### 4.4.1목 4.4.1목이다.\n' +
      '\n' +
      '설계된 구성 요소의 효과를 검증하기 위해 그림 8과 같이 다양한 모듈을 통합하여 달성된 성능에 대한 시각적 분석을 수행했으며 그림에서 "랜덤"으로 표지된 변이체는 무작위 샘플링 모델을 나타낸다. 특히, 랜덤 선택 모델은 입력 프롬프트와 정렬되지 않고 의미 정합성이 부족한 상당한 수의 이미지를 생성한다. 그러나, 우리는 점차적으로 트리와 인간 피드백(HF) 모듈을 우리 시스템에 통합함에 따라 생성된 이미지의 품질이 크게 향상되는 것이 분명해진다. TOT 및 HF 모듈의 포함으로 생성된 이미지는 다양하게 나타난다.\n' +
      '\n' +
      '그림 5: SDXL 버전의 DiffusionGPT와 기준 SDXL[10]의 비교이다. 생성된 모든 iamges는 1024\\(\\tot\\)1024 픽셀이다.\n' +
      '\n' +
      '그림 6: 디퓨전GPT-Xl과 염기 모델의 비교이다.\n' +
      '\n' +
      '그림 7: ** 사용자 연구: DiffusionGPT와 SD1.5를 비교하는**. Users는 10개 범주의 프롬프트 측면에서 기준선에서 DiffusionGPT가 선택한 전문가 모델을 강력하게 선호한다.\n' +
      '\n' +
      '강화 현실주의, 입력 프롬프트와의 의미 정렬, 더 높은 심미적 호소. 이 시각적 분석은 TOT 및 HF 성분의 통합을 통해 우수한 모델을 선택하는 데 있어 시스템의 이점을 보여준다.\n' +
      '\n' +
      '4.4.2 프롭 확장 확장기##### 4.4.2 프롭 확장기####\n' +
      '\n' +
      '신속 확장 에이전트의 효과를 평가하기 위해 그림 9와 같이 원래의 프롬프트와 확장된 프롬프트를 입력으로 사용하여 생성 결과를 비교했으며 확장된 프롬프트는 원하는 이미지에 대한 더 풍부하고 자세한 설명을 제공하도록 설계되었다. 분석 시, 확장된 프롬프트가 생성된 이미지의 미학과 세부 수준의 주목할만한 개선을 산출한다는 것을 관찰했다. 확장된 프롬프트에 추가 정보를 포함시키면 보다 시각적으로 매력적이고 예술적으로 향상된 이미지를 생성할 수 있다.\n' +
      '\n' +
      '### User Study\n' +
      '\n' +
      '생성된 이미지에 대한 실제 인간의 선호도를 얻기 위해 기준 모델에 대한 모델을 비교하는 사용자 연구를 수행했다. 우리는 파트프로젝션[25], 무작위로 100개의 프롬프트를 선택하고 각 프롬프트에 대해 4개의 이미지를 생성하는 이미지 캡처를 활용했다. 이어서 이미지의 우월성이나 평등성을 높이도록 요청받은 20명의 사용자들로부터 피드백을 수집하였다. 이 과정은 각 베이스 모델(SD15 및 SD XL)에 대해 약 400표를 생성했다. 그림 7과 그림 6에서 볼 수 있듯이 사용자 연구 결과는 기준선에 걸쳐 우리의 모델에 대한 명확한 선호도를 일관되게 보여주었다. 사용자는 우리의 모델에 의해 생성된 이미지에 대해 일관되게 뚜렷한 선호도를 나타내었고, 이는 그들이 기준선에 비해 더 높은 품질 또는 우월하다고 인식했음을 나타낸다.\n' +
      '\n' +
      '임무 및 보호 작업\n' +
      '\n' +
      '디퓨전GPT는 양질의 이미지를 생성하는 능력을 입증했지만 여전히 몇 가지 한계가 있으며 향후 계획은 다음과 같다.\n' +
      '\n' +
      '***Feedback-Driven Optimization***는 LLM의 최적화 과정에 피드백을 직접 통합하여 보다 정제된 pompt 파스와 모델 선택을 가능하게 하는 것을 목표로 한다.\n' +
      '\n' +
      '모델 캔디테이트***의 확장**는 모델 생성 공간을 더욱 풍부하게 하고 더 인상적인 결과를 얻기 위해 사용 가능한 모델의 레퍼토리를 확장할 것입니다.\n' +
      '\n' +
      '-영상 과제***를 넘어 제어 가능한 생성, 스타일 이동, 속성 편집 등을 포함한 광범위한 작업 세트에 대한 통찰력을 적용하고자 한다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '우리는 우수한 생성 모델을 원활하게 통합하고 다양한 프롬프트를 효율적으로 파싱하는 1회용 프레임워크인 디퓨전-GPT를 제안한다. 대량 레이트를 레버리징합니다.\n' +
      '\n' +
      '도 8:디퓨전GPT에 대한 복제 연구는 다음과 같다. 랜덤 선택은 이미지를 생성하기 위한 기본 방법이다. TOT 또는 TOT+HF는 다른 제제의 성능을 나타낸다.\n' +
      '\n' +
      '그림 9: 프롭트 확장에 대한 구조화 연구가 있다. 확장은 더 높은 품질의 이미지를 생성하는 부자 프롬프트를 제공하는 것을 목표로 한다.\n' +
      '\n' +
      '구이 모델(LLM), 디퓨전-GPT는 입력 프롬프트의 의도에 대한 통찰력을 획득하고, 사상(ToT) 구조에서 가장 적합한 모델을 선택한다. 이 프레임워크는 다양한 프롬프트와 도메인에 걸쳐 다양성 및 탁월한 성능을 제공하는 동시에 아도파지 데이터베이스를 통한 인간 피드백을 통합한다. 요약하면, 디퓨전-GPT는 훈련이 없고 플러그 앤 플레이 솔루션으로 쉽게 통합될 수 있으며 이 분야에서 커뮤니티 개발을 위한 효율적이고 효과적인 경로를 제공한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning.\n' +
      '* [2] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, WilliamT Freeman, Michael Rubinstein, Yuanzhen Li, Dilip Krishnan, and Google Research. Muse: Text-to-image generation via masked generative transformers.\n' +
      '* [3] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, HyungWon Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, and Liam Fe. Palm: Scaling language modeling with pathways.\n' +
      '* [4] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hierarchical transformers.\n' +
      '* [5] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training.\n' +
      '* [6] Jonathan Ho, Ajay Jain, Pieter Abbeel, and UC Berkeley. Denoising diffusion probabilistic models.\n' +
      '* [7] Takeshi Kojima, Shane Shixiang, Gu Gu, Mached Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners.\n' +
      '* [8] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and ShixiangShane Gu. Aligning text-to-image models using human feedback.\n' +
      '* [9] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback.\n' +
      '* [10] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* [11] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, YiRen Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Sihhuo Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models.\n' +
      'Cornell 대학교 대학교 arXiv, Cornell 대학교 대학교 arXiv, Cornell 대학교 대학교 arXiv, Cornell 대학교 대학교 arXiv, Cornell 대학교 대학교 arXiv, Cornell 대학교 대학교 arXiv, Cornell University* arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv 및 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv* arXiv, Cornell 대학교 arXiv.\n' +
      '* [13] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and PeterJ. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _arXiv: Learning,arXiv: Learning_, 2019.\n' +
      '* [14] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents.\n' +
      'Cornell 대학교 대학교 arXiv, Cornell 대학교 대학교 arXiv, Cornell 대학교 대학교 arXiv, Cornell 대학교 대학교 arXiv, Cornell 대학교 대학교 arXiv, Cornell 대학교 대학교 arXiv, Cornell University* arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv 및 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv, Cornell 대학교 arXiv* arXiv, Cornell 대학교 arXiv.\n' +
      '2016년 arXiv_.\n' +
      '* [16] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [17] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar, Seyed Ghasemipour, Burcu Karagol, SSara Mahdavi, RaphaGontijo Lopes, Tim Salimans, Jonathan Ho, DavidJ Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding.\n' +
      '* [18] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raineanu, Maria Lomeli, Luke Zettlemoyer, Nicola Canceda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools.\n' +
      '* [19] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, Zhejiang University, and Microsoft Research. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.\n' +
      '* [20] Didac Suris, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. _arXiv preprint arXiv:2303.08128_, 2023.\n' +
      '* [21] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\'ee Lacroix, Baptiste Rozi\'ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillume Lample. Llama: Open and efficient foundation language models.\n' +
      '* [22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models.\n' +
      '* [23] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatppt: Talking, drawing and editing with visual foundation models.\n' +
      '* [24] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation.\n' +
      '* [25] Jiahui Yu, Yuanzhong Xu, JingYu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, BurcuKaragol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation.\n' +
      '* [26] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In _2017 IEEE International Conference on Computer Vision (ICCV)_, 2017.\n' +
      '* [27] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.\n' +
      '* [28] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models.\n' +
      '* [29] Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models.\n' +
      '* [\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>