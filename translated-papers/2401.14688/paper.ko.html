<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'Taiyi-Diffusion-XL: 대용량 시각언어 모델 지원을 통한 2개국어 텍스트-이미지 생성\n' +
      '\n' +
      'Xiaojun Wu\\({}^{\\blacktriangledown}\\)\n' +
      '\n' +
      '**Dixiang Zhang\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Ruyi Gan\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Junyu Lu\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Ziwei Wu\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Renliang Sun\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Jiaxing Zhang\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Pingjian Zhang\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Yan Song\\({}^{\\blacktriangle}\\)**\n' +
      '\n' +
      '남중국공과대학 국제디지털경제학원({}^{\\blacklozenge}\\)\n' +
      '\n' +
      '중국과학기술대학교({}^{\\blacklozenge}\\)\n' +
      '\n' +
      '{wuxiaojun, zhangdixiang, ganruyi, lujunyu, wuzwei, sunrenliang}@idea.edu.cn zhangjiaxing@idea.edu.cn pjzhang@scut.edu.cn clksong@gmail.com\n' +
      '\n' +
      '동등한 기여.프로젝트 리더.교신 작성자.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '텍스트 대 이미지 모델의 최근 발전은 이미지 생성 기능을 크게 향상시켰지만, 이중 언어 또는 중국어 지원에서는 오픈 소스 모델의 현저한 격차가 지속된다. 이를 해결하기 위해 본 논문에서는 이중언어 연속 사전교육 과정을 통해 CLIP와 Stable-Diffusion-XL의 기능을 확장하여 개발된 새로운 중국어와 영어 이중언어 텍스트-이미지 모델인 Taiyi-Diffusion-XL을 제시한다. 이 접근법은 가장 자주 사용되는 한자를 CLIP의 토큰화기와 임베딩 레이어에 통합하고 절대 위치 인코딩 확장과 결합하여 어휘의 효율적인 확장을 포함한다. 또한, 큰 시각 언어 모델에 의해 텍스트 프롬프트를 풍부하게 하여 더 나은 이미지 캡션을 유도하고 더 높은 시각적 품질을 보유한다. 이러한 향상은 후속적으로 다운스트림 텍스트 대 이미지 모델에 적용된다. 본 연구의 결과는 개발된 CLIP 모델이 이중언어 이미지-텍스트 검색에 탁월하다는 것을 보여준다. 또한, 타이이-디퓨전-XL의 이중언어 이미지 생성 능력은 이전 모델을 능가한다. 이 연구는 타이이-확산-XL 모델의 개발 및 오픈소싱으로 이어지며, 특히 중국어 응용 분야에서 이미지 생성 분야의 주목할 만한 발전을 나타낸다. 모델과 시연은 [https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/](https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/)에서 공개되어 이 분야의 추가 연구와 협력을 육성한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'SD(Stable Diffusion)에 제시된 것(Rombach et al., 2022; Podell et al., 2023), DALL-E(Ramesh et al., 2022; Betker et al., 2023), Imagen(Sahara et al., 2022) 및 Deepfloyd-IF(Shonenkov et al., 2023)와 같은 확산 모델의 최근 발전은 텍스트 설명으로부터 고품질 이미지를 생성하는 데 그들의 잠재력을 보여주었다. 그러나 현재 오픈 소스 텍스트 대 이미지 모델의 대부분은 주로 영어를 지원하며 중국어와 영어 모두에 대한 이중 언어 지원을 제공하는 경우는 거의 없다는 점에 유의하는 것이 중요하다. 이러한 발전은 영어 중심 모델로 후속 이미지 생성을 위해 중국 텍스트를 영어로 변환하기 위해 번역 소프트웨어를 사용하는 기존의 방법론과 다르다. 특히, 타이이-디퓨전(Zhang et al., 2022), 파이-디퓨전(Wang et al., 2023) 및 알트-디퓨전(Ye et al., 2023)과 같은 연구는 중국 시나리오에 대한 텍스트-이미지 모델 적응에 상당한 진전을 이루었으며, 이러한 모델에서 모국어 지원의 실현 가능성과 중요성을 보여준다. 이러한 모델은 언어별 표현을 능숙하게 처리하여 원래 의미의 보존과 번역 과정에서 손실될 수 있는 감정적 뉘앙스를 보장한다. 이러한 모델들은 종종 다국어 텍스트 인코더(Radford et al., 2021; Devlin et al., 2019)를 대체하고 unet(Ronneberger et al., 2015)을 유지함으로써 중국어 이해 능력을 획득하는 반면, 이 방법론은 원래의 영어 이해 능력을 폐기할 것이다.\n' +
      '\n' +
      '이러한 발전을 바탕으로, 우리의 작품인 타이이-디퓨전-XL(Taiyi-Diffusion-XL)은 특히 중국어 텍스트-이미지 생성을 위해 이러한 모델을 보완하는 데 중점을 두고 있으며, 이중언어 언어의 고유한 언어적, 문화적 측면을 다루고 있다. 요약하면, 번역 도구는 교차 언어 애플리케이션에 대해 일정 수준의 편의를 제공하는 반면, 모델, 특히 중국어와 같은 언어에 대한 모국어 지원은 이해, 정확성 및 효율성 측면에서 뚜렷한 이점을 제공한다. 우리의 기여는 이러한 능력을 향상시켜 연구 커뮤니티를 위한 보다 효과적이고 포괄적인 도구를 제공하는 것을 목표로 한다. 우리의 연구는 세 가지 중요한 방식으로 이 진화하는 분야에 기여한다.\n' +
      '\n' +
      '* _Efficient Algorithms for Bilingual Expansion_: We developed algorithms for expansion vocabulary and position encoding in text-to-image models tailored for bilingual context. 이러한 발전은 보다 정확하고 문화적으로 조정된 이미지 생성을 용이하게 한다.\n' +
      '* _ Large Vision-Language Models에 의한 텍스트 프롬프트의 풍부화_: 텍스트 프롬프트를 풍부하게 하기 위해 Large Vision-Language 모델을 채용한다. 이 접근법은 복잡한 텍스트 설명을 해석하고 시각화하는 모델의 능력에 상당한 향상을 나타낸다.\n' +
      '* _Bilingual Models의 생성_: 멀티모달 기반 모델의 기능을 활용하여 텍스트 대 이미지 모델인 Taiyi-XL을 개발하고 오픈 소스하여 이중언어 텍스트 대 이미지 모델의 연구와 적용을 크게 발전시킨다.\n' +
      '\n' +
      '## 2 Methodology\n' +
      '\n' +
      '특히 확산 모델을 사용한 텍스트 대 이미지 생성을 위한 우리의 방법론은 데이터 세트 준비 및 모델 학습에 초점을 맞춘 두 가지 주요 단계를 포함한다.\n' +
      '\n' +
      '그림 1: 다양한 스타일과 프롬프트에 따라 텍스트-이미지 생성 결과를 보여주는 타이이-XL의 그림이다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:3]\n' +
      '\n' +
      '순수한 노이즈로\\(T\\)를 가지며, 모델은 다음에 기술된 바와 같이 깨끗한 이미지인 \\(x_{0}\\)에 수렴하면서 입력을 반복적으로 잡음 제거한다:\n' +
      '\n' +
      '\\[x_{t-1}=x_{t}-\\epsilon_{\\theta}(x_{t},t,\\tau_{\\theta}(y)),\\quad\\lim_{t\\to 0}x_{t}=x_{0}\\tag{3}\\]\n' +
      '\n' +
      '##3 실험 및 분석\n' +
      '\n' +
      '**Training Settings.** We base our Taiyi-XL model on the pre-trained Stable Diffusion XL (SD-XL)(Podell et al., 2023) checkpoint, provides strong foundation for image generation. 효율성을 높이고 GPU 메모리 사용을 관리하기 위해 BFLOAT16 형식을 채택했다. 우리의 훈련 접근법은 안정적인 학습을 위한 준비 단계로 시작하여 모델을 미세 조정하고 정제하기 위한 코사인 감쇠 일정이 뒤따르는 1e-5의 학습 속도를 포함한다. 이러한 전략은 훈련 속도와 모델 성능의 균형을 맞추는 데 필수적이다.\n' +
      '\n' +
      '**평가 프로토콜.** 우리의 평가 프레임워크는 모델의 성능에 대한 포괄적인 이해를 제공하기 위해 기계 및 인간 평가를 모두 포함한다. 기계 평가 메트릭은 이미지 대 텍스트 검색 및 텍스트 대 이미지 검색을 통한 CLIP 성능 평가; 생성된 이미지와 텍스트 설명 간의 의미 정렬을 측정하는 CLIP 유사성(CLIP Sim); 인셉션 스코어(IS), 이미지의 품질 및 다양성을 평가하는 단계; 및 프레쳇 인셉션 거리(FID), 생성된 이미지와 실제 이미지의 분포 사이의 거리를 평가하는 단계를 포함한다. 텍스트-이미지 생성에 대한 인간 평가의 맥락에서, 그러한 평가는 본질적으로 어느 정도의 주관성을 갖는다는 것이 인정된다. 결과적으로, 이 연구는 주로 다른 모델에 의해 생성된 이미지 생성 결과의 고유한 특성을 식별하고 명확하게 하기 위해 사례 분석 접근법을 사용한다. 모델 간의 우열을 묘사하는 직접적인 정량적 결과를 제공하기보다는 이미지 생성 작업에서 각 모델의 고유한 속성과 성능 뉘앙스를 강조하는 정성적 검사에 초점을 맞추고 있다.\n' +
      '\n' +
      '비교 분석을 위해 SD-XL(Podell et al., 2023), Midjourney1, DALL-E\\(3\\)2(Betker et al., 2023), 그리고 우리의 이전 작업인 Taiyi-v0.1(Wang et al., 2022), Alt-Diffusion(Ye et al., 2023) 및 Pai-Diffusion(Wang et al., 2023)과 같은 다른 오픈 소스 모델과 함께 몇 가지 기본 모델을 포함한다. 혁신적인 텍스트 대 이미지 기능을 인정받은 DLL-E 3은 텍스트 설명으로부터 양질의 이미지를 생성하는데 높은 기준을 설정한다. SD-XL,\n' +
      '\n' +
      '그림 2: 데이터 전처리, 이미지-텍스트 대조 학습 및 다중 해상도 잡음 제거 훈련 과정을 포함하는 타이이-디퓨전-XL(Taiyi-XL) 훈련 과정의 개요.\n' +
      '\n' +
      '안정적인 확산 모델의 변수는 복잡한 이미지 합성 작업에서 탁월합니다. 타이이-XL을 이러한 모델과 비교함으로써, 우리는 특히 이중 언어 이미지 생성 및 텍스트 프롬프트에 대한 충실도에서 접근법의 발전과 효능을 보여주는 것을 목표로 한다.\n' +
      '\n' +
      '### Machine Evaluation\n' +
      '\n' +
      'CLIP 모델 평가.우리의 CLIP 모델의 성능은 영-샷 이미지-텍스트 검색 결과에 의해 입증된 바와 같이 영어 및 중국어 데이터 세트 모두에서 모범적이다. 원래의 CLIP 모델(Radford et al., 2021)은 기초적인 이해를 확립하면서 Flickr(Young et al., 2014) 및 MSCOCO 데이터 세트(Lin et al., 2014)에서 적당한 검색률을 나타낸다. 이 결과는 언어 간 전이 학습과 관련된 고유한 문제를 강조한다. 대조적으로, Alt-CLIP(Chen et al., 2022) 및 우리의 향상된 CLIP 모델은 상당한 개선을 보여주며, 우리의 모델은 대부분의 평가 메트릭에서 가장 높은 재현율을 달성한다. 특히 주목할 점은 Flickr-CN(Young et al., 2014)과 MSCOCO-CN 데이터세트(Li et al., 2019)의 Text \\(\\rightarrow\\) 이미지 검색 태스크에서 R@1에서 각각 88.1%와 69.7%의 재현율을 달성하는 모델의 성능이다. 이러한 결과는 텍스트 프롬프트와 시각적 콘텐츠 간의 강력한 정렬을 나타내며 CLIP의 언어 간 성능을 향상시키는 데 있어 맞춤형 수정의 효과를 강조한다. 표 1에 제시된 결과는 멀티모달 AI 애플리케이션 내에서 다양한 언어적 맥락을 처리하는 데 전문화된 모델의 잠재력을 보여준다. 특히 이중 언어 맥락에서 CLIP 모델의 우수한 성능은 타이이-XL 모델의 능력을 크게 강화한다. 이러한 향상은 사용자-입력 프롬프트들에 대한 보다 미묘한 이해를 가능하게 하여, 주어진 프롬프트들을 보다 정확하게 반영하는 이미지들의 생성으로 이어진다. 결과는 고급 멀티모달 애플리케이션을 위한 모델에서 강력한 이중 언어 이해 능력 개발의 중요성을 확인한다.\n' +
      '\n' +
      '확산 모델 평가.표 2에 제시된 데이터를 바탕으로 이중언어 이미지 생성 작업에서 다양한 모델의 성능을 종합적으로 분석하면 상당한 통찰력이 드러난다. 이 분석에 사용되는 평가 메트릭으로는 CLIP Sim(CLIP Similarity), IS(Inception Score) 및 FID(Frechet Inception Distance)가 있으며, 이는 이미지 품질, 다양성 및 텍스트 설명과의 정렬 측면에서 모델 성능의 강력한 평가를 일괄적으로 제공한다. 영어 데이터셋(COCO)에서, 우리의 Taiyi-XL 모델은 모든 메트릭에 걸쳐 우수한 성능을 보여주며, 특히 CLIP Sim 점수가 가장 높고 IS가 가장 높으며 FID가 가장 유리한 것을 보여준다. 이러한 결과는 타이이-XL이 주어진 텍스트 프롬프트와 밀접하게 정렬된 이미지를 생성할 뿐만 아니라 높은 이미지 품질과 다양성을 보장한다는 것을 나타낸다. 이 모델은 Alt-Diffusion, SD-v1.5, SD-XL과 같은 다른 경쟁자들보다 뛰어난 성능을 보여 이미지 생성 작업에서 영어 프롬프트를 처리하는 데 효과가 있음을 강조한다. 마찬가지로 중국 데이터세트(COCO-CN)에서도 타이이-XL이 다시 눈에 띄어 CLIP Sim 점수, IS, FID로 최상의 결과를 얻었다. 타이이-v0.1, Alt-Diffusion 및 Pai-Diffusion과 같은 다른 모델에 비해 타이이-XL은 중국 텍스트 설명과 잘 정렬된 고품질 이미지를 생성하는 놀라운 능력을 보여준다. 이 성능은 모델의 강력한 이중 언어 기능을 강조하여 다양한 언어 입력에서 고충실도 이미지 생성이 필요한 응용 프로그램에 특히 적합하다.\n' +
      '\n' +
      '전반적으로 두 데이터 세트의 결과는 이중 언어 이미지 생성 작업에서 타이이-XL 모델의 유효성을 확인한다. 영어 및 중국어 텍스트의 내용을 정확하게 반영하는 고품질의 다양한 이미지를 일관되게 생산하는 능력은 이를 선도적인 솔루션으로 포지셔닝한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c} \\hline \\hline  & \\multicolumn{6}{c}{Fickr30K} & \\multicolumn{6}{c}{MSCOCO} \\\\  & \\multicolumn{3}{c}{Image \\(\\rightarrow\\) Text} & \\multicolumn{3}{c}{Text \\(\\rightarrow\\) Image} & \\multicolumn{3}{c}{Image \\(\\rightarrow\\) Text} & \\multicolumn{3}{c}{Text \\(\\rightarrow\\) Image} \\\\ Model & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\\\ \\hline CLIP (Radford et al., 2021) & 85.1 & 97.3 & 99.2 & 65.0 & 87.1 & 92.2 & 56.4 & 79.5 & 86.5 & 36.5 & 61.1 & 71.1 \\\\ AltCLIP (Chen et al., 2022) & 86.0 & 98.0 & 99.1 & 72.5 & 91.6 & 95.4 & 58.6 & 80.6 & 87.8 & 42.9 & 68.0 & 77.4 \\\\ Our-CLIP & **88.4** & **95.8** & **99.9** & **75.7** & **93.8** & **96.9** & **61.2** & **84.8** & **90.3** & **49.2** & **70.3** & **79.6** \\\\ \\hline  & \\multicolumn{6}{c}{Fickr30K-CN} & \\multicolumn{6}{c}{MSCOCO-CN} \\\\  & \\multicolumn{3}{c}{Image \\(\\rightarrow\\) Text} & \\multicolumn{3}{c}{Text \\(\\rightarrow\\) Image} & \\multicolumn{3}{c}{Image \\(\\rightarrow\\) Text} & \\multicolumn{3}{c}{Text \\(\\rightarrow\\) Image} \\\\ CLIP (Radford et al., 2021) & 2.3 & 8.1 & 12.6 & 0 & 2.4 & 4.0 & 0.6 & 4.1 & 7.1 & 1.8 & 6.7 & 11.9 \\\\ AltCLIP (Chen et al., 2022) & 69.8 & 89.9 & 94.7 & 84.8 & 97.4 & 98.8 & 63.9 & 87.2 & 93.9 & 62.8 & 88.8 & 95.5 \\\\ Our-CLIP & **73.2** & **90.3** & **96.5** & **88.1** & **98.2** & **99.1** & **66.0** & **91.1** & **96.6** & **69.7** & **91.3** & **96.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: Flickr30K, MSCOCO, Flickr30K-CN, MSCOCO-CN 데이터셋에 대한 제로샷 이미지-텍스트 검색 결과. 가장 좋은 결과는 **볼드**로 표시되어 있습니다.\n' +
      '\n' +
      '멀티모달 AI 응용 분야 이러한 이중언어 맥락에서 타이이-XL의 우수한 성능은 이미지 생성 작업 내에서 다양한 언어 환경의 복잡성을 탐색하는 데 있어 전문 모델의 잠재력을 강조한다.\n' +
      '\n' +
      '########## 인간의 기호도 평가\n' +
      '\n' +
      '우리의 종합적인 분석에서 중국어와 영어 텍스트 대 이미지 생성에서 다양한 모델의 성능을 보여주는 그림 3과 4에 묘사된 바와 같이 몇 가지 주요 관찰과 결론이 나타났다. SD-XL 및 Taiyi-XL과 같은 모델의 XL 버전은 SD-v1.5 및 Alt-Diffusion과 같은 1.5 버전보다 상당한 개선을 나타내어 모델 매개변수, 기본 알고리즘 및 훈련 방법론의 규모 향상을 나타낸다. DALL-E 3은 간혹 지나치게 선명한 색상을 생성하지만 예외적인 프롬프트 팔로우 기능이 두드러져 주어진 텍스트 설명과 밀접하게 일치하는 이미지를 생성하는 데 높은 벤치마크를 설정한다. 사진 스타일로 특징지어지는 우리의 모델은 특히 미학적 매력에서 미드저니의 성능과 매우 유사하다. 그러나 주목할 만한 차이점은 다양한 언어 맥락에서 특히 가치가 있는 특징인 이중 언어(중국어와 영어) 텍스트 대 이미지 생성에 대한 우리 모델의 향상된 지원에 있다. 이 기능은 생성 모델의 영역에서 언어 다양성의 중요성을 강조합니다.\n' +
      '\n' +
      '이 분석에서 도출된 최종 결론은 우리의 모델이 아직 상용 모델의 성능과 일치하지 않을 수 있지만 현재 이중 언어 오픈 소스 모델을 크게 능가한다는 것이다. 상업적 모델과의 격차는 주로 훈련에 사용되는 이미지 텍스트 데이터의 양, 품질 및 다양성의 차이에 기인한다. 우리의 모델은 저작권 준수 이미지 텍스트 데이터에 대해서만 훈련되어 텍스트 대 이미지 및 AI 생성 콘텐츠(AIGC) 모델에서 저작권 문제에 대한 지속적인 도전을 강조한다. 이 측면은 생성 모델의 개발 및 개선에 중요한 요소로 남아 있으며, 저작권 제약의 복잡성을 탐색하면서 다양하고 고품질 데이터 세트에 대한 액세스 필요성을 강조한다.\n' +
      '\n' +
      '또한, 영상 생성 과정을 가속화하기 위해 LCM(Latent Consistency Models)을 사용하는 경우의 영향을 평가하였다 (Song et al., 2023; Luo et al., 2023;b). 이러한 테스트에서 주목할 만한 관찰 5는 추론 단계의 감소와 그에 따른 이미지 품질 저하 사이의 상관 관계이다. 구체적으로, 생성이 단일 단계로 제한될 때, 결과 이미지들은 주로 기본적인 윤곽들만을 나타내고 더 미세한 세부사항들이 부족하다. 그러나, 생성 프로세스를 8 단계로 확장하는 것은 생성된 이미지의 상당히 더 높은 품질을 보장한다. 이 발견은 LCM이 생성 프로세스를 효과적으로 가속화할 수 있지만 단계 수와 원하는 이미지 품질 사이에 균형이 맞춰져야 함을 시사한다. 테스트에서 8단계와 같은 최소 단계를 유지하는 것은 만족스러운 수준의 세부 사항과 전체 이미지 충실도를 유지하는 데 중요한 것으로 판단된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Model & CLIP Sim(\\(\\uparrow\\)) & FID(\\(\\downarrow\\)) & IS(\\(\\uparrow\\)) \\\\ \\hline \\multicolumn{4}{c}{English Dataset (COCO)} \\\\ \\hline Alt-Diffusion(Ye et al., 2023) & 0.220 & 27.600 & 31.577 \\\\ SD-v1.5(Rombach et al., 2022) & 0.225 & 25.342 & 32.876 \\\\ SD-XL(Podell et al., 2023) & 0.231 & 23.887 & 33.793 \\\\ Taiyi-XL & **0.254** & **22.543** & **35.465** \\\\ \\hline \\multicolumn{4}{c}{Chinese Dataset (COCO-CN)} \\\\ \\hline Taiyi-v0.1(Wang et al., 2022) & 0.197 & 69.226 & 21.060 \\\\ Alt-Diffusion(Ye et al., 2023) & 0.220 & 68.488 & 22.126 \\\\ Pai-Diffusion(Wang et al., 2023) & 0.196 & 72.572 & 19.145 \\\\ Taiyi-XL & **0.225** & **67.675** & **22.965** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 영어(COCO) 및 중국어(COCO-CN) 데이터 세트에 걸친 CLIP Sim, IS 및 FID를 기반으로 하는 상이한 모델의 비교. 가장 좋은 결과는 **볼드**로 표시되어 있습니다.\n' +
      '\n' +
      '##4 관련 업무\n' +
      '\n' +
      '영상 생성 및 확산 모델에서의### 발전\n' +
      '\n' +
      '최근 몇 년 동안 텍스트 대 이미지 생성 분야에서 상당한 발전이 있었다. 이 작업은 GANs(Generative Adversarial Networks)(Goodfellow et al., 2014; Arjovsky et al., 2017), VAEs(Variational Autoencoders)(Kingma and Welling, 2013), Flow-based 모델(Rezende and Mohamed, 2015), 및 자기회귀 모델(Ramesh et al., 2021; Ding et al., 2021; 2022)과 같은 전통적인 접근법으로부터 분기되며, 대신에 보다 진보된 확산 모델에 초점을 맞춘다. 확산 이론과 기술의 진화 및 개선(Vincent, 2011; Ho et al., 2020; Song et al., 2020; Cao et al., 2022)은 확산 모델을 이미지 생성의 선도 기술로 위치시켰다. 이 분야에서 주목할 만한 발전은 계층적 네트워크(Dall-E 2)를 활용하는 Dall-E 2(Ramesh et al., 2022), 계층적 네트워크(Dall-E 3) 등이 있다. 제안된 접근법은 확산 모델을 사용하여 확산 모델을 생성하는 것이다.\n' +
      '\n' +
      '그림 4: 영어 텍스트-이미지 생성 성능에서 서로 다른 모델의 비교.\n' +
      '\n' +
      '그림 3: 중국 텍스트-이미지 생성 성능에서 서로 다른 모델의 비교.\n' +
      '\n' +
      'CLIP 잠복기를 사용한 텍스트 설명에 기초하여 이미지를 생성하기 위한 아치형 접근법. 유사하게, Imagen(Saharia et al., 2022) 및 Deepfloyd-IF(Shonenkov et al., 2023)는 딥 언어 이해를 강조하면서 텍스트로부터 사실적 이미지를 생성하는 확산 모델의 능력을 입증한다. 안정-확산-v1-5, 안정-확산-2-1, 안정-확산-xl(Podell et al., 2023) 등의 작업을 포괄하는 잠재 확산 모델(Rombach et al., 2022)은 이 기술의 최전방을 나타낸다. 이러한 모델은 주로 텍스트 특징 추출을 위해 CLIP 텍스트 모델을 활용하며, 이러한 특징을 잠재 확산 프로세스에 통합하여 계산 오버헤드와 메모리 요구 사항을 줄인다.\n' +
      '\n' +
      '이중언어 문맥에서### 텍스트-이미지 모델\n' +
      '\n' +
      '이중 언어 시나리오, 특히 중국어에서 텍스트 대 이미지 생성의 요구 사항에 대응하여 연구자들은 상당한 기여를 했다. 초기에는 CLIP 텍스트 인코더를 중국어 특화 인코더로 대체한 후 중국어 데이터셋에서 텍스트-이미지 매칭을 위한 사전 훈련을 수행한다. 이 영역의 핵심 저작물로는 타이이-CLIP(Zhang et al., 2022), 중국-CLIP(Yang et al., 2022), Alt-CLIP(Chen et al., 2022) 등이 있다. 이어서, 안정적인 확산의 텍스트 인코더를 교체하고, 텍스트-이미지 생성 능력을 향상시키기 위해 중국어 텍스트-이미지 데이터세트에 대한 추가 교육을 실시한다. 이는 타이이-확산(Zhang et al., 2022), Alt-확산(Ye et al., 2023) 및 파이-확산(Wang et al., 2023)과 같은 중국 버전의 확산 이미지 생성 모델의 개발로 이어진다. 그러나, CLIP 텍스트 인코더를 교체하는 것은 모델에서 영어 능력의 손실을 초래할 수 있고, 트레이닝 프로세스가 리소스 집약적일 수 있다는 점은 주목할 만하다.\n' +
      '\n' +
      '텍스트 이미지 데이터 셋의 역할\n' +
      '\n' +
      '데이터 세트는 텍스트-이미지 매칭 및 텍스트-이미지 생성 모두에서 중추적이다. 영어의 COCO(Lin et al., 2014) 및 Flickr(Young et al., 2014)과 중국어의 COCO-CN(Li et al., 2019) 및 Flickr-CN(Li et al., 2016)과 같은 전통적인 이미지 캡션 데이터 세트는 기초적인 훈련 기반을 제공하지만 일반적으로 100만 엔트리 미만으로 크기가 제한된다. 결과적으로, 라이온(Schuhmann et al., 2021)(주로 영어로), 우콩(Gu et al., 2022)(주로 중국어로)과 같은 웹 크롤링된 데이터 세트는 최대 1억 또는 50억의 크기를 자랑하며 확산 텍스트-이미지 모델을 훈련하는 데 더 중요한 데이터 소스로 부상했다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '우리의 연구는 이중 언어 지원을 텍스트 대 이미지 모델에 통합함으로써 중국 맥락에서 멀티모달 연구를 크게 발전시킨 지대한 영향을 보여준다. 타이이-CLIP 및 타이이-XL 모델의 개발은 확장된 어휘와 위치 인코딩을 통해 이미지 텍스트 검색 및 이미지 생성의 현저한 발전을 나타낸다. 이 모델들은 기초를 다졌다.\n' +
      '\n' +
      '그림 5: 잠재 일치성 모델을 가진 Taiyi-XL 생성 예제\n' +
      '\n' +
      '이중 언어 다중 모드 연구의 미래 혁신을 위한 것입니다. 또한 텍스트 프롬프트를 풍부하게 하기 위해 대형 비전 언어 모델을 사용하면 보다 정확하고 상세한 이미지 생성이 가능하여 사용자 의도와 밀접하게 일치했다. 이 접근법은 텍스트 대 이미지 생성에서 정확하고 복잡한 언어 이해의 중요성을 강조한다. 연구 결과와 모델을 계속 오픈소싱함에 따라 협업과 추가 탐구를 초대하여 인공지능 연구에서 보다 포괄적이고 언어적으로 다양한 미래에 기여합니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Arjovsky et al. (2017) Martin Arjovsky, Soumith Chintala, and Leon Bottou. 와세르스타인 생성적 적대 네트워크 In _International conference on machine learning_, pp. 214-223. PMLR, 2017.\n' +
      '* Betker et al. (2023) James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. 더 나은 캡션으로 이미지 생성을 개선합니다. _ openai cdn.openai.com/papers/dall-e-3.pdf_, 2023.\n' +
      '* Cao et al. (2022) Hanqun Cao, Cheng Tan, Zhangyang Gao, Guangyong Chen, Pheng-Ann Heng, and Stan Z Li. 생성확산모형에 관한 연구 arXiv preprint arXiv:2209.02646_, 2022.\n' +
      '* Chen et al. (2023) Yihao Chen, Xianbiao Qi, Jianan Wang, and Lei Zhang. Disco-clip: 메모리 효율적인 클립 트레이닝을 위한 분산 대조 손실. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 22648-22657, 2023.\n' +
      '* Chen et al. (2022) Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, and Ledell Wu. Alt-clip: 확장된 언어 기능을 위해 클립으로 언어 인코더를 변경하는 단계. _ ARXiv 프리프린트 arXiv:2211.06679_, 2022.\n' +
      '* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: 언어 이해를 위한 딥 양방향 변압기의 사전 훈련. In _Proceedings of the 2019 Conference of the North American chapter of the Computational Linguistics: Human Language Technologies, Volume 1(Long and Short Papers)_, pp. 4171-4186, 2019.\n' +
      '* Ding et al. (2021) Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Transformers를 통한 텍스트-투-이미지 생성을 마스터링하는 것; _ 신경 정보 처리 시스템_, 34:19822-19835, 2021에서의 발전.\n' +
      '* Ding et al. (2022) Ming Ding, Wendi Zheng, Wenyi Hong, 및 Jie Tang. Cogview2: 계층적 트랜스포머를 통한 더 빠르고 더 나은 텍스트-이미지 생성. _ 신경 정보 처리 시스템_, 35:16890-16902, 2022에서의 발전.\n' +
      '* Gan et al. (2023) Ruyi Gan, Ziwei Wu, Renliang Sun, Junyu Lu, Xiaojun Wu, Dixiang Zhang, Kunhao Pan, Ping Yang, Qi Yang, Jiaxing Zhang, et al. Ziya2: Data-centric learning is all l lms need. _ arXiv preprint arXiv:2311.03301_, 2023.\n' +
      '* Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 생성적 적대적 그물 신경 정보 처리 시스템_, 27, 2014의 발전.\n' +
      '* Gu et al. (2022) Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, et al. Wukong: A 1억 개의 대규모 중국어 크로스모달 사전 훈련 벤치마크 _ 신경 정보 처리 시스템_, 35:26418-26431, 2022에서의 발전.\n' +
      '* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 확산 확률 모델의 잡음제거 신경 정보 처리 시스템_, 33:6840-6851, 2020에서의 발전.\n' +
      '* Kingma and Welling (2013) Diederik P Kingma and Max Welling. 자동 인코딩 변량 베이지안 arXiv preprint arXiv:1312.6114_, 2013.\n' +
      '* Li 등(2016) Xirong Li, Weiyu Lan, Jianfeng Dong, Hailong Liu. 이미지에 중국어 캡션을 추가합니다. In _Proceedings of the 2016 ACM on international conference on multimedia retrieval_, pp. 271-275, 2016.\n' +
      '\n' +
      '* Li 등(2019) Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang, 및 Jieping Xu. 언어 간 이미지 태깅, 캡션 및 검색을 위한 Coco-cn. _ IEEE Transactions on Multimedia_, 21(9):2347-2360, 2019.\n' +
      '* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. 마이크로소프트 코코: 맥락상 흔한 물건들. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pp. 740-755. Springer, 2014.\n' +
      '* Lu et al. (2023a) Junyu Lu, Ruyi Gan, Dixiang Zhang, Xiaojun Wu, Ziwei Wu, Renliang Sun, Jiaxing Zhang, Pingjian Zhang, and Yan Song. 가사: 의미 인식 시각 객체를 통한 세밀한 언어-비전 정렬 및 이해 촉진 arXiv preprint arXiv:2312.05278_, 2023a.\n' +
      '* Lu et al. (2023b) Junyu Lu, Dixiang Zhang, Xiaojun Wu, Xinyu Gao, Ruyi Gan, Jiaxing Zhang, Yan Song, 및 Pingjian Zhang. Ziya-vi: 다중 작업 명령어 튜닝을 통한 이중 언어 대형 비전 언어 모델 _ arXiv preprint arXiv:2310.08166_, 2023b.\n' +
      '* Luo et al.(2023a) Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. 잠재적 일관성 모델: 소수의 추론으로 고해상도 이미지를 합성하는 것, 2023a.\n' +
      '* Luo et al. (2023b) Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinario Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: 범용 안정확산 가속 모듈. _ arXiv preprint arXiv:2311.05556_, 2023b.\n' +
      '* Podell et al. (2023) Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: 고해상도 영상 합성을 위한 잠재 확산 모델 개선. _ arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.\n' +
      '* Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 제로 샷 텍스트 대 이미지 생성 In _International Conference on Machine Learning_, pp. 8821-8831. PMLR, 2021.\n' +
      '* Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 클립 래턴트를 사용한 계층적 텍스트 조건 이미지 생성. _ arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '* Rezende and Mohamed (2015) Danilo Rezende and Shakir Mohamed. 흐름을 정규화하는 변형 추론입니다. In _International conference on machine learning_, pp. 1530-1538. PMLR, 2015.\n' +
      '* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성 In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 10684-10695, June 2022.\n' +
      '* Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: 생체 의학 영상 분할을 위한 컨볼루션 네트워크. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pp. 234-241. Springer, 2015.\n' +
      '* Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Hwang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _ 신경 정보 처리 시스템_, 35:36479-36494, 2022에서의 발전.\n' +
      '* Schuhmann et al. (2021) Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: 클립 필터링된 4억 개의 이미지-텍스트 쌍들의 오픈 데이터세트 _ arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* Shonenkov et al. (2023) Alex Shonenkov, Misha Konstantinov, Daria Bakshandaeva, Christoph Schuhmann, Ksenia Ivanova, and Nadiia Klokova. 만약: 리포지토리의 제목, 2023.\n' +
      '* Shonenkov et al. (2021)* Song et al. (2020) Jiaming Song, Chenlin Meng, and Stefano Ermon. 디노이징 확산 암시적 모델 arXiv preprint arXiv:2010.02502_, 2020.\n' +
      '* Song et al. (2023) Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. 일관성 모델 2023년\n' +
      '* Vincent(2011) Pascal Vincent. 스코어 매칭과 노이즈 제거 오토인코더 사이의 연결 Neural computation_, 23(7):1661-1674, 2011.\n' +
      '* Wang et al. (2023) Chen규 Wang, Zhongjie Duan, Bingyan Liu, Xinyi Zou, Cen Chen, Kui Jia, and Jun Huang. 유급확산: 클라우드에서 텍스트-이미지 합성을 위한 열린 중국 확산 모델 패밀리의 구성 및 서비스. _ arXiv preprint arXiv:2309.05534_, 2023.\n' +
      '* Wang et al. (2022) Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen, Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Ziasojun Zeng, Chongpei Chen, Ruyi Gan, 및 Jiaxing Zhang. 펑셴방 1.0: 중국 인지 지능의 기초가 되는 것. _ CoRR_, abs/2209.02970, 2022.\n' +
      '* Yang et al. (2022) An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, and Chang Zhou. 중국 클립: 중국어에서의 대비적 시각 언어 사전 훈련. _ ArXiv:2211.01335_, 2022.\n' +
      '* Ye et al. (2023) Fulong Ye, Guangyi Liu, Xinya Wu, and Ledell Yu Wu. Altdiffusion: 다국어 텍스트-이미지 확산 모델. _ ArXiv_, abs/2308.09991, 2023. URL[https://api.semanticscholar.org/CorpusID:261048720](https://api.semanticscholar.org/CorpusID:261048720)\n' +
      '* Young et al. (2014) Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 이미지 설명부터 시각적 표현까지: 이벤트 설명에 대한 의미론적 추론을 위한 새로운 유사성 메트릭 _ 2014년 2:67-78, 계산 언어학 협회 거래.\n' +
      '* Zhang et al. (2022) Jiaxing Zhang, Ruyi Gan, Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, et al. Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence. _ arXiv preprint arXiv:2209.02970_, 2022.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>