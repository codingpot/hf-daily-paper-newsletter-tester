<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '슬로 및 디퓨전 기반 유전체 모델# SiT\n' +
      '\n' +
      '사용 가능한 인터폴란트 전송 장치\n' +
      '\n' +
      '난예마 마크 골드슈타인  마이클 S. 알버고 니콜라스 M. 비비야.\n' +
      '\n' +
      'Eric Vanden-Eijnden  Saining Xie\n' +
      '\n' +
      'New York University\n' +
      '\n' +
      'Code: [https://github.com/willsma/SiT](https://github.com/willsma/SiT)\n' +
      '\n' +
      'Equal advising.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '우리는 디확산 트랜스포머(DiT)의 백본에 구축된 생성 모델 계열인 스칼러블 인터폴란트 트랜스포머(SiT)를 제시한다. 표준 확산 모델보다 더 유연한 방식으로 두 분포를 연결할 수 있는 보간 프레임워크는 역동적 수송에 구축된 생성 모델에 영향을 미치는 다양한 설계 선택에 대한 모듈식 연구를 가능하게 한다. 지속적인 시간 학습, 모델이 학습할 목적을 결정하고, 분포를 연결하는 보간체를 선택하고, 결정론적 또는 확률적 샘플러를 배치한다. 위 성분을 조심스럽게 도입하여 SiT는 동일한 백본, 파라미터 수 및 GFLOP를 사용하여 조건부 이미지넷 256x256 벤치마크에서 모델 크기에 걸쳐 획일적으로 DiT를 능가한다. 학습과 별도로 조정 가능한 다양한 확산 계수를 탐색하여 SiT는 FID-50K 점수 2.06을 달성한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '이미지 생성의 일시적인 성공은 알고리즘 발전 및 모델 아키텍처의 개선 및 스케일링 신경망 모델 및 데이터의 진행의 조합에서 비롯되었다. 최첨단 확산 모델[25, 51]은 데이터를 이산 또는 연속 시간에 특정할 수 있는 반복 확률적 공정에 의해 규정된 가우시안 노이즈로 확장 변환함으로써 진행된다. 추상적인 수준에서 이러한 부패 과정은 원본 데이터 분포로부터 반복적으로 평활화되는 시간 의존적 분포를 표준 정규 분포로 정의하는 것으로 볼 수 있다. 확산 모델은 이러한 부패 과정을 역전시키고 이러한 연결을 따라 가우시안 노이즈를 뒤로 밀어 데이터 샘플을 얻기 위해 학습한다. 이러한 변형을 수행하기 위해 학습된 대상은 종래 부패 과정[25]에서 소음을 예측하거나 자료와 가우시안[62]를 연결하는 분포의 점수를 예측하는 것이지만 이러한 선택의 대안은 [27, 54]가 존재한다.\n' +
      '\n' +
      '이러한 객체들을 나타내는 데 사용되는 신경망 아키텍처는 다양한 작업들에 대해 잘 수행되는 것으로 나타났다. 확산 모델은 원래 U-Net 백본[25, 52]에 구축된 반면, 최근 연구는 비전 트랜스포머(ViT)[21]과 같은 비전의 건축 발전이 성능[48]을 개선하기 위해 표준 확산 모델 파이프라인에 통합될 수 있음을 강조했다. [48]의 목표는 알고리즘과 모델의 이중성의 모델 측면에 대한 개선을 추진하는 것이었다.\n' +
      '\n' +
      '오토돈적으로, 유의미한 연구 노력은 성취 과정의 구조를 탐구하는 데 들어갔는데, 이는 성과 혜택[32, 35, 36, 58]으로 이어지는 것으로 나타났다. 그러나 이러한 노력의 대부분은 지나가겠다는 개념을 지나서 움직이지 않는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Model & Params(M) & Training Steps & FID \\(\\downarrow\\) \\\\ \\hline DiT-S & 33 & 400K & 68.4 \\\\ SiT-S & 33 & 400K & **57.6** \\\\ \\hline DiT-B & 130 & 400K & 43.5 \\\\ SiT-B & 130 & 400K & **33.5** \\\\ \\hline DiT-L & 458 & 400K & 23.3 \\\\ SiT-L & 458 & 400K & **18.8** \\\\ \\hline DiT-XL & 675 & 400K & 19.5 \\\\ SiT-XL & 675 & 400K & **17.2** \\\\ \\hline DiT-XL & 675 & 7M & 9.6 \\\\ SiT-XL & 675 & 7M & **8.6** \\\\ \\hline DiT-XL (cfg=1.5) & 675 & 7M & 2.27 \\\\ SiT-XL (cfg=1.5) & 675 & 7M & **2.06** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **Scalable 인터폴란트 트랜스퍼러** 우리는 생성 모델의 다음과 같은 측면을 체계적으로 달리한다. 연속 시간**, 모델 예측, 보간, **ch******** **샘플러의***입니다. 생성된 Scalable 인터폴란트 트랜스포머(SiT) 모델은 동일한 훈련 계산 하에서 256x256 이미지넷 이미지를 생성하는 데 있어서 일관되게 디확산 트랜스포머(DiT)를 능가한다. 모든 모델은 2의 패치 크기를 사용하는데, 이 작업은 __성과 이득의 원천이 무엇인지 질문한다. 데이터와 가우시안 사이의 제한된 유형의 연결인 평형 분포를 갖는 확산 과정을 통한 데이터이다. 최근 도입된 _줄기간 보간제_[3]는 이러한 제약을 해제하고 소음-데이터 연결에서 더 많은 유연성을 도입한다. 본 논문에서는 대규모 영상 생성에서 그 성과를 더욱 탐색한다.\n' +
      '\n' +
      'Intuitively, we expect that the difficulty of the _learning problem_ can be related to both the specific connection chosen and the object that is learned. Our aim is to clarify these design choices, in order to simplify the learning problem and improve performance. To glean where potential benefits arise in the learning problem, we start with Denoising Diffusion Probabilistic Models (DDPMs) and sweep through adaptations of: (i) which object to learn, and (ii) which interpolant to choose to reveal best practices.\n' +
      '\n' +
      'In addition to the learning problem, there is a _sampling problem_ that must be solved at inference time. It has been acknowledged for diffusion models that sampling can be either deterministic or stochastic [61], and the choice of sampling method can be made after the learning process. Yet, the diffusion coefficients used for stochastic sampling are typically presented as intrinsically tied to the forward noising process, which need not be the case in general.\n' +
      '\n' +
      'Throughout this paper, we explore how the design of the interpolant and the use of the resulting model as either a deterministic or a stochastic sampler impact performance. We gradually transition from a typical denoising diffusion model to an interpolant model by taking a series of orthogonal steps in the design space. As we progress, we carefully evaluate how each move away from the diffusion model impacts the performance. In summary, our **main contributions** are:\n' +
      '\n' +
      '* By moving from **discrete to continuous time**, changing the model prediction, **interpolant**, and the **choice of sampler**, we observe a consistent performance improvement over the Diffusion Transformer (DiT).\n' +
      '* We systematically study where these improvements come from by addressing these factors one by one: learning in continuous time; learning a _velocity_ as compared to a _score_; changing the interpolant connecting the the two distributions; and using the velocity in an SDE sampler with particular choices of diffusion coefficients.\n' +
      '* We show that the SDE for the interpolant can be instantiated using just a velocity model, which we use to push the performance of these methods beyond previous results.\n' +
      '\n' +
      'Figure 1: Selected samples from SiT-XL models trained on ImageNet [53] at \\(512\\times 512\\) and \\(256\\times 256\\) resolution with cfg = 4.0, respectively.\n' +
      '\n' +
      '사용 가능한 인터폴란트 변환기 2 SiT\n' +
      '\n' +
      '흐름 기반 및 확산 기반 생성 모델을 구축하기 위한 주요 성분을 회상하는 것으로 시작한다.\n' +
      '\n' +
      '### Flows and diffusions\n' +
      '\n' +
      '최근 몇 년 동안 노이즈\\(\\mathbf{\\varepsilon}\\ason\\mathsf{N}(0,\\mathbf{I})\\를 데이터 \\(\\mathbf{x}_{*}\\ason p(\\mathbf{x})로 전환시키는 데 기반한 유연한 수준의 생성 모델이 도입되었다. 이 모델은 시간 의존적 프로세스를 사용한다.\n' +
      '\n' +
      '\\[\\mathbf{x}_{t}=\\alpha_{t}\\mathbf{x}_{*}+\\sigma_{t}\\mathbf{\\varepsilon}, \\tag{1}\\]\n' +
      '\n' +
      '\\(알파_{t}\\)가 \\(t\\)의 감소 함수이고 \\(\\sigma_{t}\\)는 \\(t\\)의 증가 함수이다. 알파_{0}=1\\), \\(알파_{1}=1\\), \\(\\sigma_{0}=0\\), \\(\\sigma_{0}=0\\) 및 \\(\\mathbf{x}_{t} <0\\)에서, \\(\\bf{x}_{0}=0\\)를 설정함으로써 \\(\\bf{x}. 대조적으로, 점수 기반 확산 모델[32, 36, 62]은 \\(\\alpha_{t}\\) 및 \\(\\sigma_{t}\\)를 모두 평형 분포로서 \\(\\mathsf{N}(0,\\mathbf{I})를 갖는 확률적 차등 방정식(SDE)의 상이한 제형을 통해 간접적으로 설정했다. 더욱이, 그들은 \\(\\mathbf{x}_{t}\\)가 가우시안 분포를 근사할 만큼 충분히 큰 \\(T\\)를 갖는 간격([0,T]\\)에서 프로세스 \\(\\mathbf{x}_{t}\\)를 고려한다.\n' +
      '\n' +
      '확률 흐름 및 점수 기반 확산 모델 모두에 대해 일반적으로는 프로세스 \\(\\mathbf{x}_{t}\\)가 SDE 또는 확률 흐름 일반 미분 방정식(ODE)을 사용하여 동적으로 샘플링될 수 있다는 관찰이다. (1)의\\(\\mathbf{x}_{t}\\)의 확률 흐름 ODE 분포와 보다 정확하게는 한계 확률 분포 \\(p_{t}(\\mathbf{x})와 일치한다.\n' +
      '\n' +
      '\\[\\dot{\\mathbf{X}}_{t}=\\mathbf{v}(\\mathbf{X}_{t},t), \\tag{2}\\]\n' +
      '\n' +
      '\\(\\mathbf{v},\\mathbf{x},t)는 조건부 기대치에 의해 주어진다.\n' +
      '\n' +
      '\\[\\begin{split}\\mathbf{v}(\\mathbf{x},t)&=\\mathbb{E}[ \\dot{\\mathbf{x}}_{t}|\\mathbf{x}_{t}=\\mathbf{x}],\\\\ &=\\dot{\\alpha}_{t}\\mathbb{E}[\\mathbf{x}_{*}|\\mathbf{x}_{t}= \\mathbf{x}]+\\dot{\\sigma}_{t}\\mathbb{E}[\\mathbf{\\varepsilon}|\\mathbf{x}_{t}= \\mathbf{x}].\\end{split} \\tag{3}\\]\n' +
      '\n' +
      'Equation (3) is derived in Appendix A.1. By solving the probability flow ODE (2) backwards in time from \\(\\mathbf{X}_{T}=\\mathbf{\\varepsilon}\\sim\\mathsf{N}(0,\\mathbf{I})\\), we can generate samples from \\(p_{0}(\\mathbf{x})\\), which approximates the ground-truth data distribution \\(p(\\mathbf{x})\\). We refer to (2) as a _flow-based_ generative model.\n' +
      '\n' +
      '(\\mathbf{x}_{t}_{t})\\(\\mathbf{x}_{t})\\(p_{t}(\\mathbf{x})\\)의 시간 의존적 확률 분포(p_{t}(\\mathbf{x})도 역전 시간 SDE[5]시간 SDE의 분포와 일치한다.\n' +
      '\n' +
      '}} <\\mathbf{t>}(\\mathbf{f}_{t})\\mathbf{v}(\\mathbf{f}_{t})\n' +
      '\n' +
      '\\(w_{t}}<{t}>0\\)는 임의의 시간 의존적 확산 계수, \\(\\mathbf{v}) 즉,\\(\\mathbf{v}(\\mathbf{x},t)는 (3)에서 정의된 속도이며, 여기서 \\(\\mathbf{s}(\\mathbf{x},\\)는 \\(\\mathbf{v})는 \\(\\mathbf{f{v}(\\mathbf{v})는 \\(\\mathbf{v}(\\mathbf{v})는 (\\mathbf{v}(\\mathbf{v}(\\mathbf{v}(\\mathbf{v},\\mathbf{f{v},\\)는 \\)는 (\\mathbf{v}(\\mathbf{v}(\\mathbf{v}(\\mathbf{x},\\)는 (\\mathbf{v} \\(\\mathbf{v}}\\)와 유사하게 이 점수는 조건부 기대치에 의해 주어진다.\n' +
      '\n' +
      '\\[\\mathbf{s}(\\mathbf{x},t)=-\\sigma_{t}^{t}^{bb{E}[\\mathbf{\\bf{{E}[\\mathbf{\\varepsilon}|\\mathbf{f{x}_{t}=\\mathbf{x})]=\\mathbf{x} <\\mathbf{x}.\n' +
      '\n' +
      '이 방정식은 a\\(\\mathbf{X}_{T}=\\mathbf{\\bf{T}=\\mathbf{varepsilon}\\ason\\mathsf{N}(0,\\mathbf{I})\\에서 시간이 지남에 따라 파생된 것으로, 근사 데이터 분포 \\(p_{0}(\\mathbf{x})\\심 p(\\mathbf{x})\\(\\mathbf{x})\\(\\mathbf{X})\\(\\mathbf{x})\\(\\mathbf{x}(\\mathbf{x})\\)\\(\\mathbf{x}(\\mathbf{x}(\\mathbf{x},\\mathbf{x},\\mathbf{x},\\mathbf{x},\\mathbf{x},\\mathbf{x},\\mathbf{x},\\mathbf{x})\\ 우리는 (2)를 _확산 기반_생성 모델로 지칭한다.\n' +
      '\n' +
      '디자인 선택 스코어 기반 확산 모델은 일반적으로 \\(\\alpha_{t}\\), \\(\\sigma_{t}\\), \\(\\sigma_{t}\\), \\(w_{t}\\)의 선택을 \\(\\mathbf{x}_{t}\\)를 생성하는 정방향 SDE에서 사용되는 드리프트 및 확산 계수에 (아래 (10 참조)로 묶는다. 확률적 인터폴란트 프레임워크는 정방향 SDE에서 \\(\\mathbf{x}_{t}\\)의 제형을 탈색하고 \\(\\alpha_{t}\\), \\(\\sigma_{t}\\), \\(w_{t}\\)의 선택에 더 유연성이 있음을 보여준다. 아래에서는 이러한 유연성을 활용하여 이미지 생성 작업에서 표준 벤치마크에 대한 점수 기반 확산 모델을 능가하는 생성 모델을 구성할 것이다.\n' +
      '\n' +
      '#####는 점수와 속도를 계산하면 점수와 속도를 계산합니다.\n' +
      '\n' +
      '확률 흐름 ODE(2) 및 역시간 SDE(4)를 생성 모델로 실용적인 사용은 속도 \\(\\mathbf{v}(\\mathbf{x},t) 및 점수 \\(\\mathbf{s}(\\mathbf{x},t)를 추정하는 능력에 의존한다. 점수 기반 확산 모델에서 이루어진 주요 관찰은 점수를 \\(\\mathbf{s}_{\\theta}(\\mathbf{x},t)로 모수적으로 추정할 수 있다는 것이다.\n' +
      '\n' +
      '\\{L}(\\mathcal{L}_{\\text{L})=\\mathbb}^{T}[\\|\\sigma_{t}[\\|\\sigma_{t}\\|\\sigma_{t}\\mathbf{s}_{ \\mathbf{s}}_{ \\mathbf{s}(\\mathbf{s}:\\mathbf{t},\\mathbf{t},\\mathbf{t},\\mathbf{t},\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t} <{t}:\\mathbf{t}:\\mathbf{f{f{s}_{t}\n' +
      '\n' +
      '이러한 손실은 조건부 기대의 표준 성질과 함께 (5)를 이용하여 도출할 수 있다. 유사하게, 유사하게,\n' +
      '\n' +
      '그림 2: **SiT는 모든 모델 크기에 걸쳐 FID의 개선을 관찰했으며*** DiT와 SiT 모두에 대한 훈련 반복보다 FID-50K를 보여준다. 모든 결과는 250개의 통합 단계를 사용하여 오일러 마누야마 샘플러에 의해 생성된다. 모든 모델 크기에 걸쳐 SiT는 훨씬 더 빠르게 수렴한다.\n' +
      '\n' +
      'velocity in (3) can be estimated parametrically as \\(\\mathbf{v}_{\\theta}(\\mathbf{x},t)\\) via the loss\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\mathbf{v}}(\\theta)=\\int_{0}^{T}\\mathbb{E}[\\|\\mathbf{v}_{\\theta}( \\mathbf{x}_{t},t)-\\dot{\\alpha}_{t}\\mathbf{x}_{*}-\\dot{\\sigma}_{t}\\boldsymbol{ \\varepsilon}\\|^{2}]\\mathrm{d}t. \\tag{7}\\]\n' +
      '\n' +
      'We note that any time-dependent weight can be included under the integrals in both (6) and (7). These weight factors are key in the context of score-based models when \\(T\\) becomes large [35]; in contrast, with stochastic interpolants where \\(T=1\\) without any bias, these weights are less important and might impose numerical stability issue (see Appendix B).\n' +
      '\n' +
      '모델 예측은 두 가지 양 \\(\\mathbf{s}_{\\theta}(\\mathbf{x},t)\\)과 \\(\\mathbf{v}_{\\theta}(\\mathbf{x},t)\\) 중 하나만 실제 추정해야 함을 관찰했다. 이는 제약 조건으로부터 직접적으로 뒤따른다.\n' +
      '\n' +
      '}.\n' +
      '\n' +
      '속도(3)의 측면에서 점수(5)를 재표현하기 위해 사용될 수 있는 속도(3)와 같이 속도(5)의 측면에서 점수(5)를 재표현하는 데 사용될 수 있다.\n' +
      '\n' +
      '<\\{t_{t}\\mathbf{t}>=\\mathbf{t}\\mathbf{t}}\\mathbf{v}}(\\mathbf{x},t)-\\mathbf{x}_{t},\\math{f{x}_{t}}}}{mathbf{t}:\\mathbf{t}.\n' +
      '\n' +
      '이 관계를 사용하여 ** 모델 예측**를 지정합니다. 반대로, 우리는 \\(\\mathbf{v}(\\mathbf{x},t)\\(\\mathbf{s}(\\mathbf{x},t)\\) 측면에서 \\(\\mathbf{v}(\\mathbf{x},t)\\도 표현할 수 있다. 우리의 실험에서 우리는 보통 속도 필드 \\(\\mathbf{v}(\\mathbf{x},t)를 배우고 샘플링을 위해 SDE를 사용할 때 점수 \\(\\mathbf{s}(\\mathbf{x},t)를 표현하는데 사용한다. 우리는 부록 A.4에 상세한 도출을 포함한다.\n' +
      '\n' +
      'Note that by our definitions \\(\\dot{\\alpha}_{t}<0\\) and \\(\\dot{\\sigma}_{t}>0\\), so that the denominator of (9) is never zero. Yet, \\(\\sigma_{t}\\) vanishes at \\(t=0\\), making the \\(\\sigma_{t}^{-1}\\) in (9) appear to cause a singularity there1. This suggests the choice \\(w_{t}=\\sigma_{t}\\) in (4) to cancel this singularity (see Appendix A.3), for which we will explore the performance in the numerical experiments.\n' +
      '\n' +
      '그러나, 오데오 리피닌 이눌리}_{t}<0\\>에 의해 노베타트(스피디{\\t}<0\\)를 통해 트라코(티시 gma_{t}<0\\)는 매미(티시::{t} <0\\)의 모반이다.\n' +
      '\n' +
      '###은 보간 공정을 유추한다.\n' +
      '\n' +
      'Score-based diffusion.In Score-Based Diffusion Models (SBDM), the choice of \\(\\alpha_{t}\\) and \\(\\sigma_{t}\\) in (1) is typically determined by the choice of the forward SDE used to define this process, though recent work has tried to reconsider this [32, 35]. For example, if we use the standard variance-preserving (VP) SDE [62]\n' +
      '\n' +
      '\\[\\mathrm{d}\\mathbf{X}_{t}=-\\frac{1}{2}\\beta_{t}\\mathbf{X}_{t}\\mathrm{d}t+ \\sqrt{\\beta_{t}}\\mathrm{d}\\mathbf{W}_{t} \\tag{10}\\]\n' +
      '\n' +
      'for some \\(\\beta_{t}>0\\), it can be shown (see Appendix B) that the solution to (10) has the same probability distribution \\(p_{t}(\\mathbf{x})\\) as the process \\(\\mathbf{x}_{t}\\) defined in (1) for the choice\n' +
      '\n' +
      '\\[\\text{VP: }\\alpha_{t}=e^{-\\frac{1}{2}\\int_{0}^{t}\\beta_{s}\\mathrm{d}s}, \\qquad\\sigma_{t}=\\sqrt{1-e^{-\\int_{0}^{t}\\beta_{s}\\mathrm{d}s}}. \\tag{11}\\]\n' +
      '\n' +
      '(11)의 유일한 설계 유연성은 \\(\\alpha_{t}\\)와 \\(\\sigma_{t}\\)2를 모두 결정하기 때문에 \\(\\sigma_{t}\\)의 선택에서 비롯되며, 예를 들어 \\(\\beta_{t}=1\\)를 설정하면 \\(\\beta_{t}=e^{t}=e^{t}=e^{t}\\)와 \\(\\sigma_{t}\\) 및 \\(\\sigma_{t}<\\_{t}1\\)는 \\(\\_{t}1\\)는 \\(\\_{t}=1\\)는 \\(\\_{t}=1\\)는 \\(\\_{t}=e^_{t}=e^{t}=e^{t}=e^{t}=e^{t}=e^{t}=e^{t}=e^{t}=e^{t}=e^{t}=e^{t}=e^{ 이 선택은 \\(T\\_{t}\\)의 충분히 큰 선택[25] 또는 더 적절한 선택을 찾기 위해 \\(\\beta_{t}\\)[62, 58, 16]을 필요로 하며, 이는 SDE(10)에 대한 솔루션만이 \\(B\\boldsymbol{\\varepsilon}\\simchathsf{N}(0,\\mathbf{I})로 수렴한다는 사실만으로 수렴한다는 사실에 의해 유도된 편향을 감소시켜야 한다.\n' +
      '\n' +
      'Footnote 2: VP is the only linear scalar SDE with an equilibrium distribution [58]: interpolants extend beyond \\(\\alpha_{t}^{2}+\\sigma_{t}^{2}=1\\) by foregoing the requirement of an equilibrium distribution.\n' +
      '\n' +
      'General interpolants.In the stochastic interpolant framework, the process (1) is defined explicitly and without any reference to a forward SDE, creating more flexibility in the choice of \\(\\alpha_{t}\\) and \\(\\sigma_{t}\\). Specifically, any choice satisfying:\n' +
      '\n' +
      '1. \\(\\alpha_{t}^{2}+\\sigma_{t}^{2}>0\\) for all \\(t\\in[0,1]\\);\n' +
      '2. \\(\\alpha_{t}\\) and \\(\\sigma_{t}\\) are differentiable for all \\(t\\in[0,1]\\);\n' +
      '3. \\(\\alpha_{1}=\\sigma_{0}=0\\), \\(\\alpha_{0}=\\sigma_{1}=1\\);\n' +
      '\n' +
      'gives a process that interpolates without bias between \\(\\mathbf{x}_{t=0}=\\mathbf{x}_{*}\\) and \\(\\mathbf{x}_{t=1}=\\boldsymbol{\\varepsilon}\\). In our numerical experiments, we exploit this design flexibility to test, in particular, the choices\n' +
      '\n' +
      '\\[\\begin{split}\\text{Linear:}&\\alpha_{t}=1-t, \\qquad\\qquad\\sigma_{t}=t,\\\\ \\text{GVP:}&\\alpha_{t}=\\cos(\\tfrac{1}{2}\\pi t), \\qquad\\sigma_{t}=\\sin(\\tfrac{1}{2}\\pi t),\\end{split} \\tag{12}\\]\n' +
      '\n' +
      'GVP가 동일한 분산을 갖는 모든 종말점 분포에 대해 시간에 걸쳐 일정한 분산을 갖는 일반화된 VP를 의미한다. 우리는 현장 \\(\\mathbf{v}(\\mathbf{x},t)}(\\mathbf{s})와 \\(\\mathbf{x},t)\\(\\mathbf{x},t)\\(2) 및 (4)로 들어가는 분야는 현재 학습3 전에 \\(\\alpha_{t}\\) 및 \\(\\mathbf{x}) 및 \\(\\mathbf{x}(\\mathbf{x}(\\mathbf{x}(\\mathbf{x},t)의 선택에 따라 달라지고(\\mathbf{x})\\)의 선택에 의존하며, B\\(\\mathbf{x}(\\)\\)의 선택에 따라 달라지고(\\)\\(\\)\\(\\)\\(\\)\\(\\)\\(\\)\\(\\)의 선택에 의존하며, a\\(\\)\\(\\)\\(\\)\\(\\)\\)를 지정해야 하며, B\\(\\)\\(\\)\\(\n' +
      '\n' +
      '부츠 3: 훈련 시간에 \\(\\alpha_{t},\\sigma_{t}\\)에 의해 지정된 하나의 경로 선택하에 학습 및 샘플을 배워야 할 요구 사항은 완화될 수 있으며 [2]에서 탐색된다.\n' +
      '\n' +
      '확산 계수 측정##\n' +
      '\n' +
      '앞서 밝힌 바와 같이 역 SDE(4)에서 사용되는 SBDM 확산 계수는 통상적으로 정방향 SDE(10)와 일치하도록 취해진다. 즉, 하나의 세트 \\(w_{t}=\\beta_{t}\\)이다. 확률적 보간제 프레임워크에서 이러한 선택은 다시 더 큰 유연성을 적용하며 _any_\\(w_{t}>0\\)를 사용할 수 있다. 흥미롭게도 이 선택은 속도 \\(\\mathbf{v}(\\mathbf{x},t) 또는 점수 \\(\\mathbf{s}(\\mathbf{x},t)\\에 영향을 미치지 않기 때문에 _후에_학습을 할 수 있다. 우리의 실험에서 우리는 표 2에 나열된 선택을 고려하여 이러한 유연성을 사용한다.\n' +
      '\n' +
      '시간 탈분화 및 DDPM과의 연결 및 연결.\n' +
      '\n' +
      '추론 과정에서 확률 흐름 ODE(2)와 역시간 SDE(4)를 풀 때 연속 시간 모델을 폐기해야 한다. 이를 통해 DDPM[25]과 링크를 만들 수 있습니다.\n' +
      '\n' +
      '각 격자점(0=t_{t}}} <\\mathbf{x} <\\mathbf{x}_{t_{i}>}=\\mathbf{x}_{t_{i})을 사용하여 시간을 판별할 수 있다고 가정한다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{s}}^{N}(\\theta) =\\sum_{i=0}^{N}\\mathbb{E}[\\|\\sigma_{t_{i}}\\mathbf{s}_{\\theta}( \\mathbf{x}_{t_{i}},t_{i})+\\boldsymbol{\\varepsilon}\\|^{2}], \\tag{13}\\] \\[\\mathcal{L}_{\\text{v}}^{N}(\\theta) =\\sum_{i=0}^{N}\\mathbb{E}[\\|\\mathbf{v}_{\\theta}(\\mathbf{x}_{t_{i }},t_{i})-\\dot{\\alpha}_{t_{i}}\\mathbf{x}_{*}-\\dot{\\sigma}_{t_{i}}\\boldsymbol{ \\varepsilon}\\|^{2}]. \\tag{14}\\]\n' +
      '\n' +
      '더욱이, 학습된 \\(\\mathbf{s}_{\\theta}(\\mathbf{x},t_{i}) 또는 \\(\\mathbf{v}_{\\theta}_{\\theta}(\\mathbf{x},t_{i})\\)만이 확률 흐름 ODE(2) 및 역시간 SDE(t_{i})를 동일한 그리드에서 통합하기 위해 필요하다. 우리가 격자에 반복적으로 \\(\\mathbf{x}_{t}\\)를 정의하는 결과 절차는 DDPM의 일반화이다. \\(i\\geq 0\\),}(\\mathbf{x}_{t_{0}=\\mathbf{x}_{*})를 시작했다.\n' +
      '\n' +
      '\\[\\mathbf{x}_{t_{i+1}}=\\sqrt{1-h\\beta_{t_{i}}}\\mathbf{x}_{t_{i}}+\\sqrt{h\\beta_{ t_{i}}}\\boldsymbol{\\varepsilon}_{t_{i}}, \\tag{15}\\]\n' +
      '\n' +
      'HH(h=t_{i+1}-t_{i}\\)와 그리드가 균일하다고 가정하는 곳. H\\(\\sqrt{1-h\\beta_{t_{i}}=1-\\frac{1}{2}h\\beta_{t_{i}}+o(h)\\)이기 때문에 (15)가 정방향 SDE(10)의 일관된 시간 분비물임을 쉽게 알 수 있다. 우리의 결과는 (15)를 사용하여 시간 폐기 프로세스 \\(\\mathbf{x}_{t_{i}}\\)를 특정할 필요가 없지만 대신 시간 격자에 직접 (1) 사용할 수 있음을 보여준다.\n' +
      '\n' +
      '인터폴란트 트랜스퍼는 아키텍처 개발\n' +
      '\n' +
      '생성 모델의 백본 아키텍처 및 용량은 또한 고품질 샘플을 생산하는 데 중요하다. 어떤 교란 요인을 제거하고 우리의 탐사에 초점을 맞추기 위해 표준 디퓨전 트랜스포머(DiT) [48] 및 그 구성을 엄격하게 따른다. 이렇게 하면 다양한 모델 크기에 걸쳐 모델의 확장성도 테스트할 수 있습니다.\n' +
      '\n' +
      'Here we briefly introduce the model design. Generating high-resolution images with diffusion models can be computationally expensive. Latent diffusion models (LDMs) [51] address this by first downsampling images into a smaller latent embedding space using an encoder \\(E\\), and then training a diffusion model on \\(z=E(x)\\). New images are created by sampling \\(z\\) from the model and decoding it back to images using a decoder \\(x=D(z)\\).\n' +
      '\n' +
      'Similarly, SiT is also a latent generative model and we use the same pre-trained VAE encoder and decoder models originally used in Stable Diffusion [51]. SiT processes a spatial input \\(z\\) (shape \\(32\\times 32\\times 4\\) for \\(256\\times 256\\times 3\\) images) by first \'patchifying\' it into \\(T\\) linearly embedded tokens of dimension \\(d\\). We always use a patch size of 2 in these models as they achieve the best sample quality. We then\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c} \\hline \\hline\n' +
      '**Expression for \\(w_{t}\\)** \\\\ \\hline \\(\\beta_{t}=-2\\sigma_{t}(\\dot{\\sigma}_{t}-\\frac{\\sigma_{t}\\dot{\\sigma}_{t}}{ \\alpha_{t}})\\) \\\\ \\(\\sigma_{t}\\) \\\\ \\(1-t\\) \\\\ \\(\\sin^{2}(\\pi t)\\) \\\\ \\((\\cos(\\pi t)+1)^{2}\\) \\\\ \\((\\cos(\\pi t)-1)^{2}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 2>:< **D확산 계수.**>는 성능을 극대화하기 위해 학습 후 특정될 수 있으며, <표 2>:< **D확산 계수.**. \\는 학습 후 특정할 수 있다. 첫 번째 행의 (w_{t}=\\beta_{t}\\)은 SBDM(Eq 참조)에 해당한다. 전방 공정과 결합된 (11) 세부 도출은 부록 B. \\(w_{t}=\\sigma_{t}\\)에서 제공되며, 두 번째 행의 특이성은 Sec. 2.2의 끝에서 설명 후 \\(t=0\\)에서 특이점을 제거하기 위해 사용된다.\n' +
      '\n' +
      '그림 3: ** 상승 변압기 크기는 표본 품질*****. __<그림 3>: ** 증가 변압기 크기는 표본 품질****를 증가시킨다. 가장 크게 줌인_을 보았다. 동일한 잠재 소음 및 클래스 라벨을 사용하여 400K 훈련 단계 후에 SiT 모델(SiT-S, SiT-B, SiT-L 및 SiT-XL)의 모든 \\(4\\)에서 샘플을 채취했다.\n' +
      '\n' +
      '이 토큰에 대한 표준 ViT[21] 정현 위치 임베딩이 적용되었습니다. 우리는 각각 숨겨진 치수 \\(d\\)를 가진 일련의 \\(N\\) SiT 변압기 블록을 사용한다.\n' +
      '\n' +
      '우리의 모델 구성들(SiT-{S,B,L,XL})은 모델 크기(파라미터) 및 계산(플롭스)에서 변화하여 모델 스케일링 분석을 허용한다. 이미지넷에서 수업조건부 생성을 위해 AdaLN-Zero 블록[48]을 사용하여 추가적인 조건부 정보(시간 및 등급 라벨)를 처리한다. SiT 건축 세부 정보는 표 3에 나열되어 있다.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      '타브에서 제기된 질문에 대한 더 자세한 답변을 제공하기 위해요. Dt와 SiT를 1로 공정하게 비교하고 DiT 모델(탈환, 점수 예측, VP 보간제)에서 다음 4개의 하위 섹션에서 SiT 모델(연속, 속도 예측, 라인 인터폴란트)으로 점진적으로 전환하고 성능에 미치는 영향을 제시한다. 각 하위 섹션에서 실험 전반에 걸쳐 400K 훈련 단계에서 DiT-B 모델을 백본으로 사용한다. ODE(2)를 해결하기 위해 고정된 허네이터를 채택했으며 SDE(4)를 해결하기 위해 오일러 마누야마 통합기를 사용했다. 두 솔버 선택 모두 DiT에 사용된 샘플링 단계의 수와 일치하도록 기능 평가 수(NFE)를 \\(250\\)로 제한한다. 다음 섹션에 제시된 모든 숫자는 이미지넷256 훈련 세트에 대해 평가된 FID-50K 점수이다.\n' +
      '\n' +
      '날짜타임으로 옮겨요.\n' +
      '\n' +
      '연속 시간 대 이산 시간 모델의 역할을 이해하기 위해 점수를 추정하면서 연속 시간 SBDM-VP에 대해 이산 시간 DDPM을 연구했다. 결과는 표 4에 나와 있으며, 이산 시간 데노저에서 연속 시간 점수 분야로 갈 때 FID 점수의 한계 개선을 발견했다.\n' +
      '\n' +
      '### Model parameterizations\n' +
      '\n' +
      'SBDM-VP의 맥락에서 모델 파라미터화의 역할을 명확히 하기 위해 (i) 학습(i)을 (6), (ii) 가중 점수 모델 (부록 A.3 참조), 또는 (iii)를 이용하여 속도 모델을 비교한다. 결과는 가중 점수 모델 또는 속도 모델을 학습하여 유의한 성능 향상을 얻는 표 5에 나와 있다.\n' +
      '\n' +
      '도대체 보간물.\n' +
      '\n' +
      '섹션 2는 보간제(1)의 정의에서 \\(\\alpha_{t}\\)와 \\(\\sigma_{t}\\)의 선택을 달리하여 데이터 분포와 가우시안 사이의 연결을 구축할 수 있는 많은 방법이 있음을 강조한다. 이 선택의 역할을 이해하기 위해 우리는 이제 일반적으로 사용되는 SBDM-VP 설정에서 멀어지는 이점을 연구한다. 우리는 Gaussian와 데이터 분포 사이의 보간을 \\([0,1]\\)에서 정확하게 만드는 라인 및 GVP 보간제와 속도 모델 \\(\\mathbf{v}(\\mathbf{x},t)를 학습한다(12). 우리는 표 6의 SBDM-VP에 대해 이러한 모델을 벤치마킹하며, 여기서 우리는 GVP와 라인 인터폴렌트가 모두 상당히 향상된 성능을 얻는다는 것을 발견했다.\n' +
      '\n' +
      '이 관찰에 대한 한 가지 가능한 설명은 그림 1에 나와 있다. 4, SBDM-VP에서 GVP 또는 라인르로 변경될 때 경로 길이(수송 비용)가 감소한다는 것을 알 수 있다. 우리는 또한 SBDM-VP, \\(\\dot{\\sigma}_{t}=\\beta_{t}e^{{-\\int_{0}^{t}\\beta_{s}/(2\\sigma_{t})에서 특이하게 된다는 점에 주목한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & _Model_ & _Objective_ & FID \\\\ \\hline DDPM & Noise & \\(\\mathcal{L}_{\\mathrm{s}}^{N}\\) & 44.2 \\\\ SBDM-VP & Score & \\(\\mathcal{L}_{\\mathrm{s}}\\) & **43.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **DDPM 대 표. SBDM*******SBDM*****.\n' +
      '\n' +
      '<그림 4> <\\mathbb{E> (v)=\\mathbb{E}[|\\mathbf{v}[|\\mathbf{v}[\\mathbf{x}_{t},t)|^{2}]는 SBDM(VP), 라인 및 GVP로 간주되는 다양한 모델에 대한 다양한 훈련 단계에서 속도 필드에서 발생하는 경로 길이 \\*Path{C}[\\mathbf{v}[\\mathbf{C}[\\mathbf{v}[\\mathbf{v}[\\mathbf{v}[\\mathbf{v}[\\mathbf{v}.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Model & Layers \\(N\\) & Hidden size \\(d\\) & Heads \\\\ \\hline SiT-S & 12 & 384 & 6 \\\\ SiT-B & 12 & 768 & 12 \\\\ SiT-L & 24 & 1024 & 16 \\\\ SiT-XL & 28 & 1152 & 16 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: ** SiT 모델의 상세 상세* (S), 베이스(B), 대형(L) 및 X 대형(XL) 모델 구성에 대한 DiT[48]을 따른다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '확산 모델의 훈련 및 샘플링은 [25, 59, 62]에서 발생했으며 변성 방법[28, 29, 57]과 밀접한 역사적 관계를 가지고 있다. DDPM[60] 및 SBDM[32, 61]의 맥락에서 이러한 방법 뒤에 샘플링 알고리즘을 개선하는 데 다양한 노력이 들어갔으며, 이는 또한 우리의 연구와 직교하며 향후 작업에서 더 나은 성능을 추진하기 위해 결합될 수 있다. 개선된 확산 ODE[71]는 또한 ODE를 샘플링하기 위한 모델 매개변수화(벨로시티 대 노이즈) 및 경로(VP 대 라인)의 여러 조합을 연구하며, 더 원활한 확률 흐름을 갖는 속도 모델에 대해 최상의 결과를 보고하며, 더 낮은 차원 실험, 가능성이 있는 벤치마크에 초점을 맞추고 SDE 샘플링을 고려하지 않는다. 우리의 연구에서 VP, 라인 및 GVP 인터폴트 간의 변화와 점수 및 속도 매개변수가 심층적으로 변화하는 효과를 탐색하고 이러한 선택이 더 큰 스케일 이미지넷256에 대한 성능을 개별적으로 개선하는 방법을 보여준다. 또한 확산 계수의 선택에 의해 인덱싱된 블랙박스 ODE 및 SDE를 포함한 샘플링 알고리즘의 계열과 관련하여 FID가 어떻게 변화하는지 문서화하고 최고의 계수 선택이 모델 및 보간체에 의존할 수 있음을 보여준다. 이는 [3]의 샘플링의 유연성과 절충에 대한 관찰을 실천으로 가져온다.\n' +
      '\n' +
      'Interpolants and flow matching.Velocity field parameterizations using the Linear interpolant were also studied in [39, 42], and were generalized to the manifold setting in [6]. A trade-off in bounds on the KL divergence between the target distribution and the model arises when considering sampling with SDEs versus ODE; [3] shows that minimizing the objectives presented in this work controls KL for SDEs, but not for ODEs. Error bounds for SDE-based sampling with score-based diffusion models are studied in [13, 14, 37, 38]. Error bounds on the ODE are also explored in [7, 15], in addition to the Wasserstein bounds provided in [1].\n' +
      '\n' +
      'Other related works make improvements by changing how noise and data are sampled during training. [64, 50] compute mini-batch optimal couplings between the Gaussian and data distribution to reduce the transport cost and gradient variance; [4] instead build the coupling by flowing directly from the conditioning variable to the data for image-conditional tasks such as super-resolution and in-painting. Finally, various work considers learning a stochastic bridge connecting two arbitrary distributions [41, 18, 56, 49]. These directions are compatible with our investigations; they specify the learning problem for which one can then vary the choices of model parameterizations, interpolant schedules, and sampling algorithms.\n' +
      '\n' +
      '잠복 공간(65, 51])의 생성 모델링은 고차원 데이터를 모델링하기 위한 관능적 접근법이다. 속도 훈련된 모델에 대해 아직 탐구되고 유망한 응용 영역인 비디오 생성 [8]에 영상을 넘어 접근법을 적용했다. 전 훈련된 스테이블 디확산 VAE의 잠재 공간에서의 발소 열차 속도 모델이다. 그들은 최종 FID-50K가 4.46인 DiT-B 백본에 대한 유망한 결과를 보여주는데, 그들의 연구는 이러한 모델의 어떤 측면이 DiT보다 성능의 이익에 기여하는지에 관한 이 작업에서 조사에 대한 하나의 동기였다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline _Interpolant_ & _Model_ & _Objective_ & \\(w_{t}=\\beta_{t}\\) & \\(w_{t}=\\sigma_{t}\\) & \\(w_{t}=\\sin^{2}(\\pi t)\\) \\\\ \\hline SBDM-VP & velocity & \\(\\mathcal{L}_{\\mathrm{v}}\\) & 37.8 & 38.7 & 39.2 \\\\  & score & \\(\\mathcal{L}_{\\mathrm{s}_{\\mathrm{x}}}\\) & 35.7 & 37.1 & 37.7 \\\\ \\hline GVP & velocity & \\(\\mathcal{L}_{\\mathrm{v}}\\) & **32.9** & 33.4 & 33.6 \\\\  & score & \\(\\mathcal{L}_{\\mathrm{s}}\\) & 38.0 & 33.5 & 33.2 \\\\ \\hline Linear & velocity & \\(\\mathcal{L}_{\\mathrm{v}}\\) & 33.6 & 33.5 & 33.3 \\\\  & score & \\(\\mathcal{L}_{\\mathrm{s}}\\) & 41.0 & 35.3 & 34.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: **Evaluation of our SDE samplers.** All results in the table are FID-50K scores produced by an SiT-B model at 400K training steps. The last three columns specify different diffusion coefficients \\(w_{t}\\) detailed in Tab. 2. To make the SBDM-VP competitive when learning a score, we use a weighted score given in Appendix A.4, as per the remarks below (7).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline\n' +
      '**Class-Conditional ImageNet 256x256** & & & & & \\\\ \\hline Model & FID\\(\\downarrow\\) & sFID\\(\\downarrow\\) & IS\\(\\uparrow\\) & Precision\\(\\uparrow\\) & Recall\\(\\uparrow\\) \\\\ \\hline BigGAN-deep[10] & 6.95 & 7.36 & 171.4 & **0.87** & 0.28 \\\\ StyleGAN-XL[55] & 2.30 & **4.02** & 265.12 & 0.78 & 0.53 \\\\ \\hline Mask-GIT[12] & 6.18 & - & 182.1 & - & - \\\\ \\hline ADM[19] & 10.94 & 6.02 & 100.98 & 0.69 & 0.63 \\\\ ADM-G, ADM-U & 3.94 & 6.14 & 215.84 & 0.83 & 0.53 \\\\ \\hline CDM[26] & 4.88 & - & 158.71 & - & - \\\\ \\hline RIN[30] & 3.42 & - & 182.0 & - & - \\\\ \\hline Simple Diffusion(U-Net)[27] & 3.76 & - & 171.6 & - & - \\\\ Simple Diffusion(U-ViT, L) & 2.77 & - & 211.8 & - & - \\\\ \\hline VDM++[25] & 2.12 & - & 267.7 & - & - \\\\ \\hline DiT-XL(cfg = 1.5)[48] & 2.27 & 4.60 & 278.24 & 0.83 & 0.57 \\\\ \\hline\n' +
      '**SiT-XL(cfg = 1.5, ODE)** & 2.15 & 4.60 & 258.09 & 0.81 & 0.60 \\\\\n' +
      '**SiT-XL(cfg = 1.5, SDE:\\(\\sigma_{t}\\))** & **2.06** & 4.50 & 270.27 & 0.82 & 0.59 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: **Bench마크 클래스-조건 이미지 생성은 ImageNet 256x256.** SiT-XL에서 샘플러, ODE 또는 SDE 기반일 때 FID에서 DiT-XL을 능가한다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '이 작업에서 이미지 생성 작업을 위한 간단하고 강력한 프레임워크인 스칼러블 인터폴란트 트랜스포머를 제시했다. 프레임워크 내에서 연속 또는 이산 시간 모델의 선택, 보간제의 선택, 모델 예측의 선택 및 확산 계수의 선택이라는 다수의 주요 설계 선택 사이의 트레이드오프를 조사했다. 우리는 각 선택의 장점과 단점을 강조했고, 얼마나 신중한 결정이 상당한 성과 개선으로 이어질 수 있는지 보여주었다. 많은 동시 작품[23, 31, 40, 45]은 매우 다양한 하류 작업에서 유사한 접근법을 탐색하고 미래의 작업에 대해 SiT를 이러한 작업에 적용하는 것을 맡기게 된다.\n' +
      '\n' +
      '아키스토우스를 통해 애드리샤 이이어, 세이 샤이타 아쿨라, 프레드 루, 지타오 구, 에드윈 파이어 게버에게 도움이 되는 논의와 피드백에 감사드린다. 이 연구는 부분적으로 구글 TRC 프로그램에 의해 뒷받침된다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In _ICLR_, 2023.\n' +
      '* [2] Michael S Albergo, Nicholas M Boffi, Michael Lindsey, and Eric Vanden-Eijnden. Multimarginal generative modeling with stochastic interpolants. _arXiv preprint arXiv:2310.03695_, 2023.\n' +
      '* [3] Michael S. Albergo, Nicholas M. Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. _arXiv preprint arXiv:2303.08797_, 2023.\n' +
      '* [4] Michael S Albergo, Mark Goldstein, Nicholas M Boffi, Rajesh Ranganath, and Eric Vanden-Eijnden. Stochastic interpolants with data-dependent couplings. _arXiv preprint arXiv:2310.03725_, 2023.\n' +
      '* [5] Brian D.O. Anderson. Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 1982.\n' +
      '* [6] Heli Ben-Hamu, Samuel Cohen, Joey Bose, Brandon Amos, Aditya Grover, Maximilian Nickel, Ricky TQ Chen, and Yaron Lipman. Matching normalizing flows and probability paths on manifolds. In _ICML_, 2022.\n' +
      '* [7] Joe Benton, George Deligiannidis, and Arnaud Doucet. Error bounds for flow matching methods. _arXiv preprint arXiv:2305.16860_, 2023.\n' +
      '* [8] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In _CVPR_, 2023.\n' +
      '* [9] Nicholas M Boffi and Eric Vanden-Eijnden. Deep learning probability flows and entropy production rates in active matter. _arXiv preprint arXiv:2309.12991_, 2023.\n' +
      '* [10] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. In _ICLR_, 2019.\n' +
      '* [11] Abel Chandra, Laura Tunnermann, Tommy Lofstedt, and Regina Gratz. Transformer-based deep learning for predicting protein properties in the life sciences. _Elife_, 12:e82819, 2023.\n' +
      '* [12] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. In _CVPR_, 2022.\n' +
      '* [13] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In _ICML_, 2023.\n' +
      '* [14] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In _ICLR_, 2023.\n' +
      '* [15] Sitan Chen, Giannis Daras, and Alex Dimakis. Restoration-degradation beyond linear diffusions: A non-asymptotic analysis for DDIM-type samplers. In _ICML_, 2023.\n' +
      '* [16] Ting Chen. On the importance of noise scheduling for diffusion models. _arXiv preprint arXiv:2301.10972_, 2023.\n' +
      '* [17] Quan Dao, Hao Phung, Binh Nguyen, and Anh Tran. Flow matching in latent space. _arXiv preprint arXiv:2307.08698_, 2023.\n' +
      '* [18] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrodinger bridge with applications to score-based generative modeling. In _NeurIPS_, 2021.\n' +
      '* [19] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. In _NeurIPS_, 2021.\n' +
      '* [20] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Score-based generative modeling with critically-damped langevin diffusion. In _ICLR_, 2022.\n' +
      '* [21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.\n' +
      '* [22] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strong image synthesizer. _arXiv preprint arXiv:2303.14389_, 2023.\n' +
      '* [23] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models. _arXiv preprint arXiv:2312.06662_, 2023.\n' +
      '* [24] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _NeurIPS_, 2020.\n' +
      '* [26] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. _arXiv preprint arXiv:2106.15282_, 2021.\n' +
      '* [27] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In _ICML_, 2023.\n' +
      '* [28] Aapo Hyvarinen. Estimation of non-normalized statistical models by score matching. _JMLR_, 2005.\n' +
      '***[29] 아포 히바린넨. 파세 코드 수축: 최대 우도 추정에 의한 비 가우시안 데이터의 분해: 최대 우도 추정에 의한 비 가우시안 데이터의 수축이다. 신경 컴퓨팅_ 1999.\n' +
      '* [30] Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. In _ICML_, 2023.\n' +
      '* [31] Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rupprecht, and Andrea Vedaldi. Farm3D: Learning articulated 3d animals by distilling 2d diffusion. In _3DV_, 2024.\n' +
      '* [32] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In _NeurIPS_, 2022.\n' +
      '* [33] Patrick Kidger. _On Neural Differential Equations_. PhD thesis, University of Oxford, 2021.\n' +
      '* [34] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.\n' +
      '* [35] Diederik P Kingma and Ruiqi Gao. Understanding the diffusion objective as a weighted integral of elbos. _arXiv preprint arXiv:2303.00848_, 2023.\n' +
      '* [36] Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In _NeurIPS_, 2021.\n' +
      '* [37] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. In _NeurIPS_, 2022.\n' +
      '* [38] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. In _ALT_, 2023.\n' +
      '* [39] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In _ICLR_, 2023.\n' +
      '* [40] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In _ICCV_, 2023.\n' +
      '* [41] Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us build bridges: Understanding and extending diffusion generative models. _arXiv preprint arXiv:2208.14699_, 2022.\n' +
      '* [42] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In _ICLR_, 2023.\n' +
      '* [43] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _ICLR_, 2019.\n' +
      '* [44] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. In _NeurIPS_, 2022.\n' +
      '* [45] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In _ICLR_, 2022.\n' +
      '* [46] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _ICML_, 2021.\n' +
      '* [47] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image Transformer. In _ICML_, 2018.\n' +
      '* [48] William Peebles and Saining Xie. Scalable diffusion models with transformers. In _ICCV_, 2023.\n' +
      '* [49] Stefano Peluchetti. Non-denoising forward-time diffusions. In _ICLR_, 2022.\n' +
      '* [50] Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky T. Q. Chen. Multisample flow matching: Straightening flows with minibatch couplings. In _ICML_, 2023.\n' +
      '* [51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022.\n' +
      '* [52] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _MICCAI_, 2015.\n' +
      '* [53] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _IJCV_, 2015.\n' +
      '* [54] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In _ICLR_, 2022.\n' +
      '* [55] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In _SIGGRAPH_, 2022.\n' +
      '* [56] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schrodinger bridge matching. In _NeurIPS_, 2023.\n' +
      '* [57] Eero P. Simoncelli and Edward H. Adelson. Noise removal via bayesian wavelet coring. In _ICIP_, 1996.\n' +
      '* [58] Raghav Singhal, Mark Goldstein, and Rajesh Ranganath. Where to diffuse, how to diffuse, and how to get back: Automated learning for multivariate diffusions. In _ICLR_, 2023.\n' +
      '* [59] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _ICML_, 2015.\n' +
      '* [60] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _ICLR_, 2021.\n' +
      '* [61] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. In _NeurIPS_, 2021.\n' +
      '* [62] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _ICLR_, 2021.\n' +
      '* [63] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In _ICML_, 2023.\n' +
      '* [64] Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. In _ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems_, 2023.\n' +
      '* [65] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. In _NeurIPS_, 2021.\n' +
      '* [66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, 2017.\n' +
      '* [67] Ingrid von Glehn, James S. Spencer, and David Pfau. A Self-Attention Ansatz for Ab-initio Quantum Chemistry. In _ICLR_, 2023.\n' +
      '\n' +
      '* [68] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S Chao. Learning deep transformer models for machine translation. In _ACL_, 2019.\n' +
      '* [69] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudu Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big Bird: Transformers for Longer Sequences. In _NeurIPS_, 2020.\n' +
      '* [70] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. _arXiv preprint arXiv:2306.09305_, 2023.\n' +
      '* [71] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Improved techniques for maximum likelihood estimation for diffusion odes. In _ICML_, 2023.\n' +
      '\n' +
      '## Appendix\n' +
      '\n' +
      '### Proofs\n' +
      '\n' +
      '아래 모든 증명에서 우리는 점 제품에 \\(\\cdot\\)를 사용하고 모든 과감한 미션(\\(\\mathbf{x}\\), \\(\\mathbf{\\varepsilon}\\)을 가정하며 \\(\\mathbb{R}^{d}\\)에서 실제 평가 벡터이다. 대부분의 증명은 알베르고 등[3]에서 파생된다.\n' +
      '\n' +
      'Eq의 속도를 갖는 확률 흐름 ODE(2)의### Proof. >3개.\n' +
      '\n' +
      'Eq에서 정의한 \\(\\mathbf{x}_{t}=\\alpha_{t}\\mathbf{x}_{*}\\mathbf{x}_{*}+\\sigma_{t}\\mathbf{\\varepsilon}\\)의 시간 의존적 확률 밀도 함수(PDF) \\(p_{t}(p_{t},\\mathbf{t})\\-의존 확률 밀도 함수(p_{t},\\mathbf{t},\\mathbf{t})\\-의존 확률 밀도 함수(p_{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf (1) 정의에 따라\\(\\hat{p}_{t}}^mathbf{k}}\\mathbf{x})가 제공합니다.\n' +
      '\n' +
      '\\[\\hat{p}_{t}(\\mathbf{k})=\\mathbb{E}[e^{i\\mathbf{k}\\cdot\\mathbf{x}_{t}}] \\tag{16}\\]\n' +
      '\n' +
      'HH(\\mathbb{E}\\)는 \\(\\mathbf{x}_{*}\\) 및 \\(\\mathbf{\\varepsilon}\\)에 대한 기대를 나타낸다. 양측에서 시간 유도체를 취하며 조건부 기대의 타워 속성을 이용하여 양면의 시간 유도체를 취하고 있다.\n' +
      '\n' +
      '}}{dath{}\n' +
      '\n' +
      '{t}[\\mathbf{x}} <\\mathbf{t} <\\mathbf{t}]]\\mathbf{t}(\\mathbf{t} <\\mathb{t} <\\mathb{t} <\\mathb{t} <\\mathb{t} <\\math{f}>{t} <\\mathb{t} <\\mathbf{t} <\\mathbf{t} <\\mathbf{t} <\\mathbf{t} <\\mathbf{t} <\\math{f{t} <\\math{f{t} <\\math{f{t} <\\math{f{t} <\\math{f{t} <\\math{f{t} <\\math{f{t}>{t} <\\math{f{t} <\\math{f{t} <\\math{f{t}>{t} <\\math{f{t} <\\m >3개. 예외적으로 Eq. (21) 읽기 읽기 읽기 읽기 판독(21)(21)\n' +
      '\n' +
      '}}.\n' +
      '\n' +
      '우리가 그것을 알고 있다.\n' +
      '\n' +
      '}\\math{d}\\math{d}.\n' +
      '\n' +
      'r\\(\\nabla_{\\mathbf{x}}\\cdot[\\mathbf{v}p_{t}]=\\sum_{i =1}^{d}\\frac{TPd}{ \\frac{\\ial}{ \\frac{i}}}[v_{i}p_{t}]\\)는 분기 연산자이며 두 번째 평등을 얻기 위해 부품에 의한 통합을 활용하고 분산을 사용한다. 푸리에 변환의 특성에 따라 Eq. (23)은 \\(p_{t}(\\mathbf{x})가 수송식을 만족한다는 것을 의미한다(23).\n' +
      '\n' +
      '\\[\\partial_{t}p_{t}(\\mathbf{x})+\\nabla_{\\mathbf{x}}\\cdot(\\mathbf{v}(\\mathbf{x}, t)p_{t}(\\mathbf{x}))=0. \\tag{24}\\]\n' +
      '\n' +
      '이 식을 특성 방법으로 제거하는 것은 확률 흐름 ODE(2)로 이어진다.\n' +
      '\n' +
      'SPSE(4)의### Proof.\n' +
      '\n' +
      '우리는 SDE(4)가 한계 밀도 \\(p_{t}(\\mathbf{x})\\(w_{t}\\geq 0\\)를 가지고 있음을 보여준다. 이를 위해 SDE에 대한 솔루션을 리콜한다.\n' +
      '\n' +
      '\\[d\\mathbf{X}_{t}=[\\mathbf{v}(\\mathbf{X}_{t},t)+\\frac{1}{2}w_{t}\\mathbf{s}( \\mathbf{X}_{t},t)]dt+\\sqrt{w_{t}}d\\bar{\\mathbf{W}}_{t}\\]\n' +
      '\n' +
      'has a PDF that satisfies the Fokker-Planck equation\n' +
      '\n' +
      '}\\mathbf{x} (\\mathbf{x})\\mathbf{v}}(\\mathbf{x})+\\mathbf{x}.\n' +
      '\n' +
      'r\\(\\Delta_{\\mathbf{x}})는 \\(\\Delta_{\\mathbf{x}=\\nabla_{\\mathbf{x}}},\\nabla_{\\mathbf{x}}},\\cdot\\natha_{\\mathbf{x}},\\cdot\\natha_{\\mathbf{x}}},\\cdot\\nathbf{mathbf{mathbf{mathbf{mathbf{mathbf{x}}}},\\cdot\\nathbf{mathbf{mathbf{matha_{mathbf{x}},\\cot\\natha_{mathbf{x}:{matha_{mathbf{x}_{mathbf{x}:{matha_{mathbf{x} <{mathbf{x}_{mathbf{x }}}(\\mathbf{x}) 식(\\mathbf{x})의 정의는\\nathbf{s}(\\mathbf{x},\\mathbf{x})=p_{t}(\\mathbf{x})를 가지고 있다.\n' +
      '\n' +
      '\\[\\mathbf{x}) =\\[\\mathbf{t}(\\mathbf{t}_{t}-\\underbabla_{\\mathbf{x}}}\\cdot[\\mathbf{v}(\\mathbf{v},t){f{v}(\\mathbf{x},\\mathbf{x})}. \\mathbf{v},\\mathbf{v},\\mathbf{v},\\mathbf{v},\\mathbf{v},\\mathbf{v},\\mathbf{v},\\mathbf{v},t{x},t:\\mathbf{v}:\\mathbf{v}:\\mathbf{v}:\\mathbf{v}:\\mathbf{v}:\\mathbf{v}:\\mathbf{v}:\\mathbf{v}} Lplace 연산자의 정의에 의해, 마지막 방정식은 모든 \\(w_{t}\\geq 0\\)에 대해 유지된다. i\\(w_{t}=0\\) 때, Fokker-Planck 방정식은 연속 방정식으로 감소하며, SDE는 ODE로 감소하므로 연결은 3배 증가한다.\n' +
      '\n' +
      'Eq의 점수에 대한 발현의### Proof. (5)\n' +
      '\n' +
      '우리는 \\(\\mathbf{s}(\\mathbf{x},t)=-\\sigma_{t}^{t}^{bb{E}[\\boldsbol{\\varepsilon}| \\mathbf{x}_{t}=\\mathbf{x}]\\)를 보여준다. 우리는\\(\\hat{f}\\mathbf{k},\\mathbf{k)를 포함한다.\n' +
      '\n' +
      '}\\nathbf}}\\mathbf{k}\\mathbf{k}\\mathbol{k}\\mathbf{k}\n' +
      '\n' +
      '\\(히스볼{\\varepsilon}\\sim\\mathsf{I},0,\\mathbf{I})를 얻기 위해 명시적으로 기대를 계산할 수 있다.\n' +
      '\n' +
      '}}\\nath{k}\\math{f}}^{t}^{f{.\n' +
      '\n' +
      '\\(\\mathbf{x}_{*}\\) 및 \\(\\bathbol{\\varepsilon}\\)는 독립적인 확률 변수이기 때문에 우리는 가지고 있다.\n' +
      '\n' +
      '\\[\\mathbb{E}[\\boldsymbol{\\varepsilon}e^{i\\mathbf{k}\\cdot\\mathbf{x}_{t}}]=\\hat{f }(\\mathbf{k},t)\\mathbb{E}[e^{i\\alpha_{t}\\mathbf{k}\\cdot\\mathbf{x}_{*}}]=i \\sigma_{t}\\mathbf{k}\\underbrace{e^{-\\frac{1}{2}\\sigma_{t}^{2}|\\mathbf{k}|^{2} }\\mathbb{E}[e^{i\\alpha_{t}\\mathbf{k}\\cdot\\mathbf{x}_{*}}]}_{\\text{combine this}}=i \\sigma_{t}\\mathbf{k}\\hat{p}_{t}(\\mathbf{k}) \\tag{31}\\]\n' +
      '\n' +
      '\\(\\{p}_{t}(\\mathbf{k})\\)가 자신의 혼란스러운 racteristicfun ctionof\\(\\ 수학bf{x}_{t}=\\alpha_{t}\\mathbf{x}_{*}_{*}+\\sigma_{t}\\sigma_{t}\\buffbol{varepsilon}:\\{t},\\{t}_{t} 및\\mathbf{t}_{t} <\\mathbf{t}_{t} <\\mathbf{t}_{t}<\\mathbf{t}_{t}_{t} +\\mathbf{x}_{t}+\\sigma_{t}+\\sigma_{t}{t}{t}:{t}:{t}:{t}:{t}:{t}+\\sigma_{t}:{t}:{t}:{t}:{t}}+\\sigma_{t} d-사이드소프트 hi는 디사이드소프트가 디온칸 알프드 리 타텐타스: 아피온칸에서 격리하는 것보다 리프(16)를 격리한다.\n' +
      '\n' +
      '}}{ml}.\n' +
      '\n' +
      '오른손측이 어디에 있든 오른손측이.\n' +
      '\n' +
      '}\\math{d}\\math{d}\\math{f}.\n' +
      '\n' +
      '세 번째 평등을 얻기 위해 부품을 통해 정리와 통합을 다시 사용한 곳, 그리고 다시 점수의 정의를 통해 마지막을 얻을 수 있었다. Eq를 비교해 보세요. (33)와 Eq. (37)은 \\(모험_{t} 헨네크 0\\)를 할 때 이를 추론한다.\n' +
      '\n' +
      '>\\mathbf{x}(\\mathbf{x})\n' +
      '\n' +
      'Further, setting \\(w_{t}\\) to \\(\\sigma_{t}\\) in Eq. (4) gives\n' +
      '\n' +
      '\\[\\frac{1}{2}w_{t}\\mathbf{s}(\\mathbf{x}_{t},t)=-\\frac{1}{2}\\mathbb{E}[\\boldsymbol {\\varepsilon}|\\mathbf{x}_{t}=\\mathbf{x}] \\tag{39}\\]\n' +
      '\n' +
      'for all \\(t\\in[0,1]\\). This bypass the constraint of \\(\\sigma_{t}\\neq 0\\) and effectively eliminate the singularity at \\(t=0\\).\n' +
      '\n' +
      '### Eq 프로포즈. (9)\n' +
      '\n' +
      '우리는 \\(\\mathbf{v}(\\mathbf{x},t)\\)와 \\(\\mathbf{s}(\\mathbf{x},t)\\ 사이에 간단한 연결이 존재한다는 점에 주목한다. Eq부터. (1), 우리는 (1), (1) 가지고 있다.\n' +
      '\n' +
      '}}\\math{{.\n' +
      '\n' +
      '우리가 정의된 대로\n' +
      '\n' +
      '\\[\\lambda_{t}=\\dot{\\sigma}_{t}-\\frac{\\dot{\\alpha}_{t}\\sigma_{t}}{\\alpha_{t}} \\tag{45}\\]\n' +
      '\n' +
      'Eq를 감안할 때. (44)는 \\(mathbf{s}\\) 측면에서 선형이며, 이는 우여곡절 에드트 oE q를 포기한다. >9개.\n' +
      '\n' +
      'Note that we can also plug Eq. (44) into the loss \\(\\mathcal{L}_{\\mathbf{v}}\\) in Eq. (7) to deduce that\n' +
      '\n' +
      '}}{{.\n' +
      '\n' +
      '가중 점수 목표\\(\\mathcal{L}_{\\mathbf{s}_{1}}(\\theta)\\)를 정의한다. 이 관찰은 서로 다른 단조 가중치 함수를 가진 점수 목표가 서로 다른 모델 매개변수에 대한 손실과 일치한다는 킹마 및 가오[35]에서 이루어진 주장과 일치한다. 부록 B에서 \\(\\lambda_{t}\\)가 송 등 알에서 제안된 최대 가능성 가중치의 제곱에 해당한다는 것을 보여준다[61] 및 Vahdat 등[65].\n' +
      '\n' +
      '<##>는 Score 기반 디퓨전 B와 관련이 있다.\n' +
      '\n' +
      '송 등에 나타난 바와 같이, [62], Eq의 역시간 SDE. (10)는 (10)이고, (10)은 (10))\n' +
      '\n' +
      '}\\mathbf{d}}\\mathbf{t}}\\mathbf{t}.\n' +
      '\n' +
      '이 SDE는 Eq입니다. 특정 선택 \\(w_{t}=\\beta_{t}\\)에 대한 (4) 이를 위해 용액 \\(\\mathbf{X}_{t}\\)을 Eq에 통지한다. 고정된 것은 초기 조건 \\(\\mathbf{X}_{t=0}=\\mathbf{x}_{*}\\)에 대해 각각 주어진 평균과 분산으로 분포된 가우시안이다.\n' +
      '\n' +
      '}.\n' +
      '\n' +
      'Eq를 사용합니다. 따라서 점수 기반 확산 모델의 속도는 점수 기반 확산 모델의 속도(44)로 표현될 수 있으므로 점수 기반 확산 모델의 속도를 (44)로 나타낼 수 있다.\n' +
      '\n' +
      '}}\\math{d}\\math{2}\\math{d}\\math{.\n' +
      '\n' +
      '우리는 \\(2\\lambda_{t}\\sigma_{t}\\)가 정확하게 \\(\\beta_{t}\\)임을 알 수 있으며, \\(\\lambda_{t}\\)는 송(62]에서 제안된 최대 가능성 가중치의 제곱에 해당한다. 또한 Eq를 꽂으면요. (55) Eq로. (w_{t}=\\beta_{t}\\)가 있는 (4) Eq에 도착합니다. (51)\n' +
      '\n' +
      'A useful observation for choosing velocity versus noise model.We see that in the velocity model, all of the path-dependent terms (\\(\\alpha_{t}\\), \\(\\sigma_{t}\\)) are inside the squared loss, and in the score model, the terms are pulled out (apart from the necessary \\(\\sigma_{t}\\) in score matching loss) and get squared due to coming out of the norm. So which is more stable depends on the interpolant. In the paper we see that for SBDM-VP, due to the blowing up behavior of \\(\\dot{\\sigma}_{t}\\) near \\(t=0\\), both \\(\\mathcal{L}_{\\mathrm{v}}\\) and \\(\\mathcal{L}_{\\mathrm{s}_{\\lambda}}\\) are unstable.\n' +
      '\n' +
      'Yet, shown in Tab. 5, we observed better performance with \\(\\mathcal{L}_{\\mathrm{s}_{\\lambda}}\\) for SBDM-VP, as the blowing up \\(\\lambda_{t}\\) near \\(t=0\\) will compensate for the diminishing gradient inside the squared norm, where \\(\\mathcal{L}_{\\mathrm{v}}\\) would simply experience gradient explosion resulted from \\(\\dot{\\sigma}_{t}\\). The behavior is different for the Linear and GVP interpolant, where the source of instability is \\(\\alpha_{t}^{-1}\\) near \\(t=1\\). We note \\(\\mathcal{L}_{\\mathrm{v}}\\) is stable since \\(\\alpha_{t}^{-1}\\) gets cancelled out inside the squared norm, while in \\(\\mathcal{L}_{\\mathrm{s}_{\\lambda}}\\) it remains in \\(\\lambda_{t}\\) outside the norm.\n' +
      '\n' +
      '도지도 C 샘플링.\n' +
      '\n' +
      'Let \\(p_{t}(\\mathbf{x}|\\mathbf{y})\\) be the density of \\(\\mathbf{x}_{t}=\\alpha_{t}\\mathbf{x}_{*}+\\sigma_{t}\\boldsymbol{\\varepsilon}\\) conditioned on some extra variable \\(\\mathbf{y}\\). By argument similar to the one given in Appendix A.1, it is easy to see that \\(p_{t}(\\mathbf{x}|\\mathbf{y})\\) satisfies the transport equation (compare Eq. (24))\n' +
      '\n' +
      '\\[\\partial_{t}p_{t}(\\mathbf{x}|\\mathbf{y})+\\nabla_{\\mathbf{x}}\\cdot(\\mathbf{v}( \\mathbf{x},t|\\mathbf{y})p_{t}(\\mathbf{x},|\\mathbf{y}))=0, \\tag{56}\\]\n' +
      '\n' +
      '그곳(화합물 Eq) (3))))((3))))((3))))))((3))))))((3))))))(((3))))))(((3))))))((((3))))))((((((3))))))(((((((((3))))))((((((((((((3))))))(((((((((((3))))((((((((((((((((((((((((((3))))))(((((((((((((((((((((3))))))(((((((((((((((((3))))))))((((((((((((3))))))))((((((((((((3))))))))))))(((((((((((3))))))))(((((((((((3))))))))))((((((((\n' +
      '\n' +
      '\\[\\mathbf{v}(\\mathbf{x},t|\\mathbf{y})=\\mathbb{E}[\\dot{\\mathbf{x}}_{t}|\\mathbf{ x}_{t}=\\mathbf{x},\\mathbf{y}]=\\dot{\\alpha}_{t}\\mathbb{E}[\\mathbf{x}_{*}| \\mathbf{x}_{t}=\\mathbf{x},\\mathbf{y}]+\\dot{\\sigma}_{t}\\mathbb{E}[\\boldsymbol{ \\varepsilon}|\\mathbf{x}_{t}=\\mathbf{x},\\mathbf{y}] \\tag{57}\\]\n' +
      '\n' +
      'Proceeding as in Appendix A.3 and Appendix A.4, it is also easy to see that the score \\(\\mathbf{s}(\\mathbf{x},t|\\mathbf{y})=\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x}| \\mathbf{y})\\) is given by (compare Eq. (5))\n' +
      '\n' +
      '\\[\\mathbf{s}(\\mathbf{x},t|\\mathbf{y})=-\\sigma_{t}^{-1}\\mathbb{E}[\\boldsymbol{ \\varepsilon}|\\mathbf{x}_{t}=\\mathbf{x},\\mathbf{y}] \\tag{58}\\]\n' +
      '\n' +
      'H\\\\mathbf{v}(\\mathbf{x},t|\\mathbf{y})\\) nd\\(\\mathbf{s}(\\mathbf{x},t|\\mathbf{y}) a\\)가 latedvi(c ompareEq)를 재조명한다. 4 4))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))\n' +
      '\n' +
      '}}\\mathbf{t}}\\mathbf{x}\\mathbf{y}\n' +
      '\n' +
      'Consider now\n' +
      '\n' +
      '\\[\\mathbf{s}^{\\zeta}(\\mathbf{x},t|\\mathbf{y}) \\equiv(1-\\zeta)\\mathbf{s}(\\mathbf{x},t)+\\zeta\\mathbf{s}(\\mathbf{ x},t|\\mathbf{y}) \\tag{60}\\] \\[=\\nabla\\log p_{t}(\\mathbf{x})-\\zeta\\nabla\\log p_{t}(\\mathbf{x})+ \\zeta\\nabla\\log p_{t}(\\mathbf{x}|\\mathbf{y})\\] (61) \\[=\\nabla\\log p_{t}(\\mathbf{x})-\\zeta\\nabla\\log p_{t}(\\mathbf{x})+ \\left(\\zeta\\nabla\\log p_{t}(\\mathbf{y}|\\mathbf{x})+\\zeta\\nabla\\log p_{t}( \\mathbf{x})\\right)\\] (62) \\[=\\nabla\\log p_{t}(\\mathbf{x})+\\zeta\\nabla\\log p_{t}(\\mathbf{y}| \\mathbf{x})\\] (63) \\[=\\nabla\\log[p_{t}(\\mathbf{x})p_{t}^{\\zeta}(\\mathbf{y}|\\mathbf{x})] \\tag{64}\\]\n' +
      '\n' +
      'where we have used the fact \\(\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x}|\\mathbf{y})=\\nabla_{\\mathbf{x}}\\log p _{t}(\\mathbf{y}|\\mathbf{x})+\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x})\\) that follows from \\(p_{t}(\\mathbf{x}|\\mathbf{y})p(\\mathbf{y})=p_{t}(\\mathbf{y}|\\mathbf{x})p_{t}( \\mathbf{x})\\), and \\(\\zeta\\) to be some constant greater than \\(1\\). Eq. (64) shows that using the score mixture \\(\\mathbf{s}^{\\zeta}(\\mathbf{x},t|\\mathbf{y})=(1-\\zeta)\\mathbf{s}(\\mathbf{x},t )+\\zeta\\mathbf{s}(\\mathbf{x},t|\\mathbf{y})\\), and the velocity mixture associated with it, namely,\n' +
      '\n' +
      '(\\mathf})\\math{x}(\\math{f},\\math{f},\\math{f},\\math{f})\\math{}.\n' +
      '\n' +
      '분류기 지침 [19]에 따라 강화 분포 \\(p_{t}(\\mathbf{x}_{t})p_{t}^{\\zeta}(\\mathbf{y}|\\mathbf{x}_{t})\\)를 샘플링하는 생성 모델을 구성하도록 할 것이다. p_{t}(\\mathbf{x})p(\\mathbf{t}}^{t}(\\mathbf{y})\\propto p_{t}^{\\o p_{t}^{\\zeta}( \\mathbf{t} \\mathbf{f{x})\\(\\mathbf{t}:\\mathbf{t}:\\mathbf{t})\\propto p_{t}.{t}^{f{t}d{t}^{t}^{t}^{f{t}^{t}^{f{t}^{t}^{t}^{f{t}^{f{t}^{t}^{f{t}^{\\zeta p_{t}^{t}^{\\zeta p_{t}^{\\zeta p_{t}^{\\zeta p_{t}^{\\mathbf{t}^{\\zeta}:^{\\ 실증적으로 Tab에서 볼 수 있듯이 분류기 무료 지침을 적용하여 상당한 성능 부스트를 관찰한다. 1, Tab. 9.\n' +
      '\n' +
      'ODE 및 SDE와 함께 실험할 수 있습니다.\n' +
      '\n' +
      'In the main body of the paper, we used a Heun integrator for solving the ODE in Eq. (2) and an Euler-Maruyama integrator for solving the SDE in Eq. (4). We summarize all results in Tab. 1, and present the implementations below.\n' +
      '\n' +
      '```\n' +
      'procedureHeunSampler(\\(\\mathbf{v}_{\\theta}(\\mathbf{x},t,\\mathbf{y}),t_{i\\in\\{0,\\cdots,N\\}},\\alpha_{t},\\sigma_{t}\\)) sample\\(\\mathbf{x}_{0}\\sim\\mathsf{N}(0,\\mathbf{I})\\)\\(\\triangleright\\) Generate initial sample\\(\\Delta t\\gets t_{1}-t_{0}\\)\\(\\triangleright\\) Determine fixed step size for\\(i\\in\\{0,\\cdots,N-1\\}\\)do\\(\\mathbf{d}_{i}\\leftarrow\\mathbf{v}_{\\theta}(\\mathbf{x}_{i},t_{i},\\mathbf{y})\\)\\(\\tilde{\\mathbf{x}}_{i+1}\\leftarrow\\mathbf{x}_{i}+\\Delta t\\mathbf{d}_{i}\\)\\(\\triangleright\\) Euler Step at \\(t_{i}\\)\\(\\mathbf{d}_{i+1}\\leftarrow\\mathbf{v}_{\\theta}(\\tilde{\\mathbf{x}}_{i+1},t_{i+1}, \\mathbf{y})\\)\\(\\triangleright\\) Euler Step at \\(t_{i+1}\\)\\(\\mathbf{x}_{i+1}\\leftarrow\\mathbf{x}_{i}+\\frac{\\Delta t}{2}[\\mathbf{d}_{i}+ \\mathbf{d}_{i+1}]\\)\\(\\triangleright\\) Explicit trapezoidal rule at \\(t_{i+1}\\) endfor return\\(\\mathbf{x}_{N}\\) endprocedure\n' +
      '```\n' +
      '\n' +
      '단골.\n' +
      '\n' +
      '```\n' +
      '(\\mathbf}})\\(\\mathf{t}},\\mathf{t})\\(\\mathf{t})\\(\\mathf{t},\\mathf{t})\\(\\mathf{t})\\(\\mathf{t})\\(\\mathf{t} <\\math{t},\\math{t},\\math{t}},\\math{t})\\. (\\math{_\\i})\\.\n' +
      '```\n' +
      '\n' +
      '원주 오일러-마누야마 샘플**\n' +
      '\n' +
      '속도 모델 \\(\\mathbf{v}_{\\theta}\\) 또는 점수 모델 \\(\\mathbf{s}_{\\theta}\\)를 상기 두 샘플러를 적용하는 데 사용할 수 있다. 만약 점수를 학습하면, 스코어를 학습하게 된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Model & Training Steps(K) & FID\\(\\downarrow\\) & sFID\\(\\downarrow\\) & IS\\(\\uparrow\\) & Precision\\(\\uparrow\\) & Recall\\(\\uparrow\\) \\\\ \\hline SiT-S & 400 & 58.97 / 57.64 & 8.95 / 9.05 & 23.34 / 24.78 & 0.40 / 0.41 & 0.59 / 0.60 \\\\ \\hline SiT-B & 400 & 34.84 / 33.45 & 6.59 / 6.46 & 41.53 / 43.71 & 0.52 / 0.53 & 0.64 / 0.63 \\\\ \\hline SiT-L & 400 & 20.01 / 18.79 & 5.31 / 5.29 & 67.76 / 72.02 & 0.62 / 0.64 & 0.64 / 0.64 \\\\ \\hline SiT-XL & 400 & 18.04 / 17.19 & 5.17 / 5.07 & 73.90 / 76.52 & 0.63 / 0.65 & 0.64 / 0.63 \\\\ SiT-XL & 7000 & 9.35 / 8.61 & 6.38 / 6.32 & 126.06 / 131.65 & 0.67 / 0.68 & 0.68 / 0.67 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'ODE 및 SDE**에 의해 생성된 표 1: **FID-50K 점수는 모든 모델 크기에 걸쳐 ODE와 SDE 사이의 비교를 보여준다. 모든 통계는 분류기 무료 안내 없이 생산됩니다. 표의 각 셀은 [ODE 결과]/[SDE 결과]를 보여주고 있다. 모든 모델 크기에서 관찰된 SDE의 더 나은 성능은 [3]에서 주어진 결합과 일치하며 ODE는 그림과 같이 더 낮은 NFE 영역에서 이점이 있다는 점에 주목한다. \\(\\dot{\\sigma}_{t}\\), \\(\\alpha_{t}^{t}\\)에 잠재적인 수치 불안정성(\\alpha_{t}^{t}\\)이 존재하지만, 결정론적 하이틀러(\\lambda_{t}\\)을 사용하여\\(\\mathbf{v}{t}\\)은 항상 \\(\\mathbf}{t}.{t}<\\_{t}\\)에서 \\(\\bf{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}\\_{t}\\_{t}\\_{t}\\_{t}\\_{t}\\_{t}\\_{t}\\ 확률 샘플러의 경우 통합에서 \\(\\mathbf{v}_{\\theta}\\)와 \\(\\mathbf{s}_{\\theta}\\)를 모두 가져야 하므로 다른 하나를 얻기 위해서는 항상 하나의(학습 속도 또는 점수)에서 변환해야 한다. 이 시나리오에서 부록 A.4의 수치 문제는 \\(t=0\\) 근처에서 시간 간격을 클릭하기만 하면 피할 수 있다. 실증적으로 우리는 \\(h=0.04\\)에 의해 간격을 닫고 \\(t=0.04\\)에서 \\(0\\)까지 긴 마지막 단계를 하는 것을 발견했는데, 이는 성능에 큰 도움이 될 수 있다. 부록 E에는 샘플러 구성에 대한 상세한 요약이 제공된다.\n' +
      '\n' +
      '또한, 우리는 \\(\\mathbf{v}_{\\theta}\\)와 \\(\\mathbf{v}_{\\theta}_{\\theta}_{\\theta}\\)를 두 샘플러의 입력으로 대체하고 부록 C(\\mathbf{s}_{\\{\\zeta}_{\\i}_{\\)에 제시된\\(\\mathbf{v}_{\\bf{v}_{\\bf{\\s}_{\\bf{\\)와 \\(\\mathbf{v}_{\\theta}_{\\)와 \\(\\mathbf{\\dta}_{\\{\\ta}_{\\{\\) 및 \\(\\mathbf{\\FAFA}_{\\{\\ta}_{\\{\\ta}_{\\:\\)와 \\(\\mathbf{\\t}_{\\theta}_{\\{\\ta}_{\\{\\ta}^{\\ta}^{\\ta}^{\\ 지도는 단일 단계에서 조건부 모델과 무조건적인 모델 출력을 모두 평가해야 하므로 샘플링 시 계산 비용의 2배를 부과하게 된다.\n' +
      '\n' +
      'Comparison between DDPM and Euler-MaruyamaWe primarily investigate and report the performance comparison between DDPM and Euler-Maruyama samplers. We set our Euler sampler\'s number of steps to be 250 to match that of DDPM during evaluation. This comparison is made direct and fair, as the DDPM method is equivalent to a discretized Euler\'s method.\n' +
      '\n' +
      'DDIM과 Heun 간의 비교도 DiT와 모델 간의 결정론적 샘플러에 의해 생성된 성능 차이를 조사한다. 그림에서. 1, 우리는 DDIM으로 샘플링된 DiT 모델과 Heun로 샘플링된 SiT 모델 모두에 대한 FID-50K 결과를 보여준다. 우리는 DDIM이 첫 번째 주문 오일러의 방법의 폐기된 버전으로 볼 수 있는 반면, 우리는 연속 시간에 오일러의 방법과 큰 폐기 오류로 인해 SiT 모델을 샘플링하는 데 두 번째 주문 하이군의 방법을 사용할 수 있기 때문에 이것은 사과 대애플 비교가 아니라는 점에 주목한다. 그럼에도 불구하고 우리는 DDIM(250개의 샘플링 단계)과 Heun(250개의 NFE) 모두에 대해 NFE를 제어한다.\n' +
      '\n' +
      'Higher order solversThe performances of an adaptive deterministics dopri5 solver and a second order stochastic Heun Sampler [32] are also tested. For dopri5, we set atol and rtol to le-6 and le-3, respectively; for Heun, we again maintain the NFE to be 250 to match that of DDPM. In both solvers we do not observe performance increment; under the CFG scale of \\(\\zeta=1.5\\), dopri5 and stochastic Heun gives FID-50K of 2.15 and 2.07, respectively.\n' +
      '\n' +
      'We also note that our models are compatible with other samplers [36, 44] specifically tuned to diffusion models as well as sampling distillation [63, 54]. We do not include the evaluations of those methods in our work for the sake of apples-to-apples comparison with the DDPM model, and we leave the investigation of potential performance improvements to future work.\n' +
      '\n' +
      '자료 적용\n' +
      '\n' +
      'We implemented our models in JAX following the DiT PyTorch codebase by Peebles and Xie [48]4, and referred to Albergo et al. [3]5, Song et al. [62]6, and Dockhorn et al. [20]7 for our implementation of the Euler-Maruyama sampler. For the Heun sampler, we directly used the one from diffrax[33]8, a JAX-based numerical differential equation solver library.\n' +
      '\n' +
      '유도 4: [https://github.com/페이스북리서치/DiT] (https://github.com/facebookresearch/DiT)\n' +
      '\n' +
      'Footnote 5: [https://github.com/malbergo/stochastic-interpolants](https://github.com/malbergo/stochastic-interpolants)\n' +
      '\n' +
      '폐경 5: [유동성://github.com/말버고/탄력성-상호결합소https://github.com/말버고/말버고/안정성-상호흡착제)\n' +
      '\n' +
      '폐경 7: [https://github.com/nv-talabs/CLD-SGM] (https://github.com/nv-talabs/CLD-SGM)\n' +
      '\n' +
      '우리는 0T[48]에서 유지된 동일한 구조 및 하이퍼모수 후에 모든 모델을 훈련했다: [https://github.com/frick-kidger/diffrax]. 모든 모델의 최적지로 AdamW[34, 43]를 사용했다. 우리는 \\(1\\t10^{-4}\\)의 일정한 학습률과 \\(256\\)의 배치 크기를 사용한다. 데이터 증강에서 \\(0.5\\) 확률로 무작위 수평 플립을 사용했다. 학습률, 붕괴/지연 일정, AdamW 매개변수, 학습_ 동안 추가 데이터 증강 또는 구배 클램핑을 사용하지 않았다. 우리의 가장 큰 모델인 SiT-XL은 위의 구성에 따라 TPU v4-64 팟에서 약 \\(6.8\\) 계층/sec의 열차이다. 이 속도는 동일한 설정 하에서 \\(~{}6.4\\) 계층/sec에서 훈련하는 DiT-XL에 비해 약간 더 빠르다. 또한 다른 모델 크기의 훈련 속도를 수집하고 아래를 요약합니다.\n' +
      '\n' +
      '샘플링 실시예들은 \\(0.9999\\)의 붕괴로 훈련을 통해 모든 모델 가중치들의 지수 이동 평균(EMA)을 유지한다. 모든 결과는 EMA 체크포인트에서 샘플링되며, 이는 더 나은 성능을 생성하는 것으로 경험적으로 관찰된다. 우리는 각 \\(t_{0}\\) 및 \\(t_{N}\\)가 성능을 최적화하고 통합 중에 수치 불안정성을 피하기 위해 조심스럽게 조정된 아래 서로 다른 보간체를 갖는 결정론적 및 확률적 샘플러의 시작과 종점을 요약한다.\n' +
      '\n' +
      'FID calculationWe calculate FID scores between generated images (10K or 50K) and all available real images in ImageNet training dataset. We observe small performance variations between TPU-based FID evaluation and GPU-based FID evaluation (ADM\'s TensorFlow evaluation suite [19]9). To ensure consistency with the basline DiT, we sample all of our models on GPU and obtain FID scores using the ADM evaluation suite.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline  & S & B & L & XL \\\\ \\hline DiT & 20.0 & 19.8 & 9.3 & 6.4 \\\\ \\hline SiT & 19.7 & 20.8 & 9.3 & 6.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **Training speed (iters/sec) across all model sizes.** All training speeds are measured on a TPU v4-64 pod. _We note that the training speed is largely influenced by the hardware state._\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline _Interpolant_ & _Model_ & _Objective_ & \\multicolumn{2}{c}{Heun} & \\multicolumn{2}{c}{Euler-Maruyama} \\\\  & & & \\(t_{0}\\) & \\(t_{N}\\) & \\(t_{0}\\) & \\(t_{N}\\) \\\\ \\hline SBDM-VP & velocity & \\(\\mathcal{L}_{\\text{v}}\\) & 1 & 1e-5 & 1 & 4e-2 \\\\  & score & \\(\\mathcal{L}_{\\text{s}_{\\lambda}}\\) & 1 & 1e-5 & 1 & 4e-2 \\\\ \\hline GVP & velocity & \\(\\mathcal{L}_{\\text{v}}\\) & 1 & 0 & 1 & 4e-2 \\\\  & score & \\(\\mathcal{L}_{\\text{s}}\\) & 1 - 1e-5 & 0 & 1 - 1e-3 & 4e-2 \\\\ \\hline Linear & velocity & \\(\\mathcal{L}_{\\text{v}}\\) & 1 & 0 & 1 & 4e-2 \\\\  & score & \\(\\mathcal{L}_{\\text{s}}\\) & 1 - 1e-5 & 0 & 1 - 1e-3 & 4e-2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 삼플러 구성표 3: 삼플러 구성표 3:\n' +
      '\n' +
      '표 4.0클래스 l abe l = "그림 4.0클래스 l abe l = (512\\tim e adtur틀"(3) 편집에서 그림 5:** (512\\im es512\\)SiT- XLT-in XLT-out ples** 및 그룹 r-freeififrtim es512\\) SiT- XLT-XX**에서 <표 4.0클래스 l.0클래스 S.0클래스 agg-XXS-XS-XS-XXmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\n' +
      '\n' +
      'Figure 6: **Uncurated \\(512\\times 512\\) SIT-XL samples**. Classifier-free guidance scale = 4.0 Class label = ”red panda”(387) Figure 7: **Uncurated \\(512\\times 512\\) SIT-XL samples**. Classifier-free guidance scale = 4.0 Class label = ”geyser”(974)\n' +
      '\n' +
      '그림 8: ** 폴리(256\\, 256\\) SIT-XL 샘플***. 분류자가 없는 지도 척도 = 4.0클래스 라벨 = "마카와" = (88) 그림 9: ** 미공개(256\\·256\\) SIT-XL 샘플***. 분류기 없는 지도 척도 = 4.0클래스 레이블 = ‘골든 리트리버’(207)이다.\n' +
      '\n' +
      '그림 10: ** 정제되지 않은\\(256\\) SIT-XL 샘플. 분류자가 없는 지도 척도 = 4.0클래스 라벨 = "이스 크림" = (928)*** 그림 11: ** 미공개 (256\\ 55\\) SIT-XL 샘플이다. 고전기 없는 지도 척도 = 분류기 없는 지도 척도 = 4.0클래스 라벨 = 4.0클래스 레이블=(972)***** <클립> = 4.0클래스 라벨이다.\n' +
      '\n' +
      '그림 12: ** 정제되지 않은\\(256\\) SIT-XL 샘플. 분류자가 없는 지도 척도 = 4.0클래스 라벨 = "후스키"(250)** 그림 13: ** 미공개(256\\·256\\) SIT-XL 샘플이다. 분류기 없는 지도 척도 = 분류기 없는 지도 척도 = 4.0클래스 라벨 = "발리" = (979)****=" 4.0클래스 라벨 = "발리"이다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>