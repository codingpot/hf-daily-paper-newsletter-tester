<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '훈련(Yao et al., 2021) 동안 광범위하게 불안정하며, 그리고/또는 운동 학습을 킥팅하기 위해 전처리된 모델에 의존한다.\n' +
      '\n' +
      '이 작업에서 우리는 _SPARse Fine-곡물 콘트라스트 정렬(SPARC)_을 제안하는데, 이는 거친-곡물/세계 및 미세-곡물/지역 정보를 모두 인코딩하는 표현을 학습하는 다중 모드 사전 실습에 대한 새로운 목표이다. 우리는 자막에서 개별 단어에 대응하는 이미지 패치(비지도 방식)를 집합하도록 학습함으로써 _언어 그룹 비전 임베딩s_를 구축할 것을 제안하며, 이는 보통 여러 이미지 패치가 자막에서 한 단어에 해당한다는 관찰에 의해 동기화된다. 첫 번째 단계로, SPARC는 개별 이미지-텍스트 쌍의 패치 및 토큰 임베딩 사이의 유사성을 계산하고 결과 유사도 매트릭스에서 유출을 시행한다. 이 유출은 가장 관련성이 높은 이미지 패치만 개별 토큰에 기인할 수 있게 한다. 다음으로 그림 1에 도시된 바와 같이, 모든 텍스트 토큰에 대해 해당 언어 그룹 비전 임베딩을 패치 임베딩의 정렬 가중 합으로 계산하며, 여기서 정렬 가중치는 유출된 유사성 매트릭스로부터 계산된다. 결과 언어 그룹 비전 임베딩은 개별 토큰과 해당 언어 그룹 비전 임베딩 및 다른 모든 언어 그룹 비전 임베딩에 대한 비유사성을 최적화하여 동일한 이미지 텍스트 쌍으로부터의 토큰 임베딩과 대조된다. SPARC는 생성된 미세 변경/지역 대조 손실과 이미지 및 텍스트 임베딩 간의 글로벌 대비 손실을 결합하여 학습된 표현에서 글로벌 및 로컬 정보를 동시에 인코딩할 수 있다.\n' +
      '\n' +
      'SPARC는 설계 선택을 통해 보다 미세화된 정보로 이미지 표현을 학습하기 위한 기존 방법의 몇 가지 단점을 다룬다. 첫째, 이러한 방법들 중 몇 가지(황 등 알, 2021, 묵호키 등, 2023, 야오 등 2021)는 모든 이미지 패치 임베딩과 모든 텍스트 토큰 임베딩 사이의 유사성을 배치로 계산하는 미세한 손실이 있는 표현을 배우게 된다. 이 접근법은 계산적으로나 기억 집약적이며 큰 배치 크기(Jia et al, 2021; Radford et al., 2021; Zhai et al., 2023b)로 스케일링되지 않는다. 한편, SPARC는 개별 이미지-텍스트 쌍 수준에서 패치 및 토큰 임베딩을 대조하고 배치로부터의 다른 예를 사용하여 유사성 매트릭스를 계산하지 않으며 이는 더 유리한 계산 및 메모리 발자국을 계산하고 더 쉽게 큰 배치 크기로 스케일링한다. 둘째, 이미지 패치와 텍스트 토큰 간의 소프트 대응성을 학습하기 위해 사전 작업(황 등, 2021, 묵호티 등, 2023, 왕 등 2022)은 일반적으로 패치 및 토큰 임베딩 유사성에 대한 소프트맥스로 계산된 가중치가 있는 교차 모달 가중치 표현을 구축하는 데 의존한다. 소프트맥스(Elfadel and Wyatt Jr, 1993; 페더슨 및 Soderberg, 1989)의 승자-호우-상동 역학은 학습을 일대일로 위해 강하게 편향시킨다.\n' +
      '\n' +
      '그림 1: 모든 텍스트 토큰에서 SPARC는 해당 토큰과 가장 유사한 패치의 정렬 가중 조합으로 해당 언어 그룹 비전 임베딩을 학습한다. 우리는 개별 이미지-텍스트 쌍(왼쪽)의 토큰과 패치 사이의 희소 유사도 메트릭을 계산하고 이를 사용하여 생성된 정렬 가중치(중간)를 계산한다. 우리는 언어 그룹화된 비전 임베딩과 미세 구성 대조 서열별 손실(오른쪽)에서 토큰 임베딩을 대조한다.\n' +
      '\n' +
      '개별 텍스트 토큰과 이미지 패치 사이에 종종 기초 데이터에 해당하지 않는다. 예를 들어, 강아지의 이미지에서 "개"에 대한 토큰 임베딩은 단순히 1/a가 아닌 이미지 내의 개에 해당하는 _all_ 패치 임베딩과 일치해야 한다. 더욱이, 소프트맥스는 경사 흐름 관점(Hoffmann et al., 2023; 선전 et al., 2023; Zhai et al., 2023)에서 문제가 될 수 있으며, 이는 소프트맥스 _saturates_ 및 따라서 Jacobian 소실(Hoffmann et al., 2023a)이 낮은 엔트로피 분포로 이어지는 경향이 있기 때문이다. 보다 자세한 설명을 위해 부록 A를 참조하세요. 플립 측에서는 SPARC가 개별 토큰과 해당 이미지 패치 사이의 유연한 일대일 매칭을 학습할 수 있고 소프트맥스의 승자-투-모든 역학을 피하기 위해 정렬 가중치를 계산하기 위해 소프트맥스를 사용하지 않는다. 셋째, 이러한 접근법 중 일부는 대조적으로 사전 훈련된 비전 언어 모델(Mukhoti et al., 2023), 또는 사전 훈련된 언어 모델(Huang et al, 2021; 왕 et al., 2022)에서 시작된다. 더욱이 기존 미세곡물 목표는 다양한 커뮤니티(즉, 의료(황 등 2021년, 왕 등 2022년) 대에서 개발되었다. 일반 비전(Mukhoti et al., 2023; Yao et al., 2021))은 데이터 세트, 아키텍처 및 전처리 세트의 다양한 유형과 크기를 활용합니다. 이를 통해 다양한 접근 방식을 비교하고 개별 미세 편성 목표를 사용하는 이점을 평가하기가 어렵다.\n' +
      '\n' +
      '요약하자면, 우리의 주요 기여금은 다음과 같습니다.\n' +
      '\n' +
      '* 우리는 거친 글과 미세한 글씨 정보를 모두 배우는 대규모 시끄러운 이미지 텍스트 데이터에 대한 멀티모달 모델을 사전 학습하기 위한 새로운 방법인 SPARC를 제안한다.\n' +
      '* 광범위한 실험 평가를 통해 SPARC가 경쟁 방법에 비해 미세 재배열 및 거친 재배열된 다운스트림 작업 모두에 대한 성능을 크게 향상시킨다는 것을 보여준다.\n' +
      '*는 문헌에서 처음으로 복합 모델의 대규모 사전 방제에 대한 다양한 미세 구성 목표의 이점에 대한 철저한 유사 비교를 수행한다.\n' +
      '\n' +
      '2 Sparse Fine으로 구성되어 있습니다.\n' +
      '\n' +
      'MS(\\mathbf{x}_{{x}_{{x}_{{x},\\mathbf{x}_{{nu},\\mathbf{x}_{{nu},\\mathbf{f{u},\\mathbf{f{x},^{f{nu},\\math{{x}_{{nu},^{f{x}_{{nu},^{{x}_{{x}},^{{x}}},^{{f}}},^{{f{f{f{u}}},^{{f{f{u}},^{f{f{u}},^{f{f{u}},^{f{x}},^{f{u}},^{f{x}},^{f{x},^{f{x},^{f{x},^{f{x},^{f{x},^{f{x},^{x} \\(f_{\\nu}(\\cdot)\\)는 이미지 인코더, \\(f_{t}(\\cdot)\\) 텍스트 인코더 및 \\(g_{\\nu}(\\cdot)\\) 및 \\(g_{t}(\\cdot) 선형 어댑터이다. (\\mathb}},2},\\mathb{i})\\(\\mathb{i}},\\mathb{i},\\math{nu},\\math{nu},\\b{i},\\math{nu},\\math{nu},\\b{i},\\b{i},\\b{i},\\b{i},\\nu},\\b{i},\\nu},\\b{i},\\b{i},\\nu},\\b{i},\\nu}},\\b{i},\\nu}},\\b{i}},\\nu}},\\b{i}},\\nu}},\\nu}},\\nu}},\\nu}},\\nu},\\nu},\\nu},\\nu}},\\nu}}. 우리는 글로벌 비전 임베딩을 \\(h_{\\mathbf{v}}_{\\nu})로 계산합니다(h_{\\nu}(\\text{avg\\_pool})(\\{f_{\\nu}(\\{f_{\\nu}(\\{f_{\\nu}_{\\nu},\\{f_{\\nu},\\{f_{\\nu},\\{f_{\\nu}:\\{v\\_{\\nu}(\\{v\\_{v\\_{v\\_{v\\_{\\nu}(\\{v\\_{\\nu},\\{f_{\\nu},\\{f_{\\nu},\\{f_{\\nu},\\{f_{\\nu}(\\{f_{\\nu}(\\{f_{\\nu},\\nu}(\\{f_{\\nu},\\nu}(\\{f_{\\nu}(\\{\\nu} (\\mathbf{x}_{{i}_{{\\tau}) 상응하는 텍스트 \\(\\mathbf{x}_{i,1}^{\\tau},\\mathbf{\\tau},\\mathbf{\\tau},\\mathbf{f{TP},1},^{\\tau},\\mathbf{\\tau},1},^{\\tau})의 경우, 우리는 토큰을 \\(\\bf{x}_{i}_{{i}_{{\\tau},^{\\-1},^{\\tau},^{TP},^{\\-1},^{\\-1},^{\\-1},^{\\o},^{\\-1},^{\\-1},^{\\-1},^{\\-1},^{\\-1},^{\\tau},\\)로 표시하며,\\) 샘플 \\(C)에 대한 토큰의 \\(<^{ (\\overline{t}} <\\mathb}}}) 평균 풀링(\\math{t}}} <\\math{t}} <{t}.{i,\\{t})(\\math{t}}{i,\\b{t}})을 사용하여 계산된다.\n' +
      '\n' +
      '글로벌 정렬: 글로벌 정보를 학습하기 위해 SPARC는 글로벌 이미지(\\(\\overline{\\mathbf{\\nu}}\\)) 및 글로벌 텍스트 임베딩(\\(\\overline{\\mathbf{t}}\\) 수준에서 작동하는 글로벌 대비 손실(Jia et al., 2021; Radford et al., 2021)을 사용한다. 구체적으로, 우리는 해당 텍스트 및 이미지 임베딩과의 유사성을 최대화하여 이미지 및 텍스트 임베딩을 학습하되, 배치에서의 다른 텍스트 및 이미지 임베딩과의 유사성을 최소화하면서, 즉 최적화를 통해 이미지 및 텍스트 임베딩을 학습한다.\n' +
      '\n' +
      '>\\파이팅(\\peline{f}}\\math{t})}}\\math{v} <\\math{t} <\\math{f} <\\math{{i})\n' +
      '\n' +
      '{\\|e{\\mathbf{v}}_{i}}{\\mathbf{v}}}{\\|_{i}}\\cdot\\mathbf{v}}}\\cdot\\frac{\\mathbf{v}}}}}\\cdot\\mathbf{v}}}{dot\\mathbf{v}}}}{dot\\cot{\\mathbf{v}}}{dot{mathbf{v}}}{dot{f{f{v}}}{dot{\\mathbf{f{t}}}{mathbf{mathbf{t}}{dot{f{f{t}}}}{dot{\\mathbf{mathbf{t}}{f{t}}}{f{t}}{f{t}}}}}{dot{\\mathbf{t}}{mathbf{t}}}{f{t}}}}\n' +
      '\n' +
      '수정된 정렬: 보통 여러 이미지 패치가 캡션에서 하나의 단어에 해당한다는 관찰에 의해 정렬되었으며, 우리는 개별 텍스트 토큰에 해당하는 패치들의 그룹화를 학습할 것을 제안한다. 구체적으로, 모든 토큰 임베딩에 대해, 우리는 시각적 도메인에서 해당 토큰을 인코딩하는 패치의 정렬 가중 조합으로서 대응하는 _언어 그룹 비전 임베딩_을 학습한다. 우리는 해당 이미지-텍스트 쌍의 토큰과 패치 임베딩 간의 유사도에 기초하여 정렬 가중치를 계산할 것을 제안한다. 텍스트 토큰이 제공된 적절한 패치 임베딩의 그룹화를 용이하게 하기 위해 유사성 매트릭스를 유출하고 min-max를 정규화하여 정렬 가중치를 계산한다. 언어 그룹화된 비전 임베딩을 배우기 위해 개별 토큰 임베딩과 주어진 이미지 텍스트 쌍 내에서 해당 언어 그룹화된 비전 임베딩 사이의 정렬을 최적화하는 미세 작성된 지역 손실을 제안한다. 구체적으로, 우리는 SPARC 내에서 미세 변경 정렬을 최적화하기 위해 서열별 대조 손실을 제안한다. 이 손실(이상의 글로벌 대비 손실 외에도)을 최적화하는 것은 글로벌 대비 손실을 최소화하기에 충분한 글로벌 정보 대신 이미지(캡션으로 기술된 것)에 대한 자세한 정보를 보존하기 위해 학습된 표현을 편향시킨다.\n' +
      '\n' +
      '(s_{i,lp}=\\mathbf{t}_{il}\\cot\\mathbf{t}}{ip}\\)의 경우,\\(s_{i,\\cf{\\b}{ip}\\)는 텍스트 토큰 임베딩 \\(s_{i,\\bf{\\bf{\\b}}.{i,\\i,\\bf{\\b}}{i,\\b}{i,\\b}{i,\\b}{i,\\b}{i,\\b}{i,\\b}{i,\\b}{i,\\b}{i,\\b}{i,\\b}.{i,\\b}.{i,\\b}.{i,\\b}.{i,\\b}.{i,\\i,\\b}.{i,\\i,\\i,\\i,\\i,\\i)과 \\)의 유사성을 나타내고 이미지 패치 임베딩 \\(s_{i,\\i,\\i,\\i,\\i,\\i,\\)과 \\ 우리는 단순성을 위해 예제 지수(i\\)를 떨어뜨린다. 정렬 가중치를 얻기 위해 각 토큰 \\(j\\)에 대해 먼저 컬럼(즉, 패치)에 걸쳐 min-max 정규화를 사용하여 \\(s_{lp}\\)를 \\([0,1]\\)로 정규화한다.\n' +
      '\n' +
      '\\[\\hat{s}_{lp}=\\frac{s_{lp}-\\min_{k}s_{lk}}{\\max_{k}s_{lk}-\\min_{k}s_{lk}} \\tag{2}\\]\n' +
      '\n' +
      '유사성 매트릭스 \\(S=(\\hat{s}_{jk})_{1\\leq j\\leq L,1\\leq k\\leq p}\\)를 융합하여 학습을 촉진하고 각 토큰이 몇 개의 패치, 즉 패치에 정렬되도록 장려한다.\n' +
      '\n' +
      '}.{jk}}\\tag{.\n' +
      '\n' +
      '그림 2: SPARC의 모든 아키텍처. 글로벌 정렬 손실은 글로벌 비전과 글로벌 텍스트 임베딩 간의 유사성을 극대화하는 동시에 배치의 다른 글로벌 임베딩과의 유사성을 최소화한다. 미세한 정렬을 얻기 위해 패치 임베딩과 토큰 임베딩 사이의 유사성을 계산한 다음 생성된 유사성 매트릭스를 유출하고 정규화하여 정렬 가중치를 얻는다. 이러한 정렬 가중치는 패치 임베딩을 그룹화하는 데 사용된다. 그 다음, 결과 언어 그룹화된 비전 임베딩들은 시퀀스-대략 미세 정렬 손실에서 토큰 에베딩들과 대조된다.\n' +
      '\n' +
      '\\(P\\)와 함께 이미지의 패치 임베딩 횟수와 \\(\\sigma\\) 스파어티 임계치이다. 우리는 정렬 가중치를 계산한다.\n' +
      '\n' +
      '\\[a_{jk}=\\frac{\\tilde{s}_{jk}}{\\sum_{r=1}^{R}\\tilde{s}_{jr}} \\tag{4}\\]\n' +
      '\n' +
      'Hi(a_{jk}\\)가 토큰 \\(j\\)에 대응하는 언어 그룹 비전 임베딩을 계산하기 위한 패치 \\(k\\)의 가중치를 나타낸다. 이 접근법은 시각적 영역에서 토큰을 인코딩하는 토큰과 임의로 많은 패치 임베딩 사이의 유연한 매핑을 가능하게 하고, 예를 들어 "개"에 해당하는 모든 이미지 패치는 "개" 인코딩 토큰에 매칭될 수 있다. 모든 토큰(t_{l}\\)에 대해 해당 언어 그룹 비전 임베딩 \\(\\mathbf{c}_{l}\\)를 계산합니다.\n' +
      '\n' +
      '\\[\\mathbf{c}_{l}=\\sum_{r=1}^{R}a_{lt}\\mathbf{\\nu}_{r} \\tag{5}\\]\n' +
      '\n' +
      'I\\(R\\)와 패치 임베딩의 정렬 가중 조합에서 0이 아닌 정렬 중량을 갖는 패치 수가 표시된다.\n' +
      '\n' +
      '미세 학습 정보를 배우기 위해 토큰 임베딩과 해당 언어 그룹 비전 임베딩 사이의 정렬을 최적화할 것을 제안한다. 구체적으로 우리는 각 이미지-텍스트 쌍 수준에서 토큰과 패치의 서열에 걸쳐 작동하고 다른 이미지-텍스트 쌍으로부터의 음성들을 필요로 하지 않는 미세-곡물 대비 손실을 제안한다. 이것은 미세한 손실을 계산하기 위해 전체 배치에서 샘플을 필요로 하는 이전 방법(황 등 알, 2021, 요오 등 2021)에 대한 계산 및 기억 비용을 상당히 감소시켰다. SPARC는 다음 미세 배열 정렬 대조 손실 손실을 최적화한다.\n' +
      '\n' +
      '(\\mathbf}_{i},\\math{t}}{f}}{f}}{f}}{ff}{t}{i}}\\math{t}/\\math{t}}{f}}{d{t}} <\\math{t}}{f}}{t}}{t}}{t}{t}}{t}{t}}{t}{t}}{t}{t}{t}{t}{t}{t}{t}{t}{t}{t}{t}{t}{t}{t}{t}}{t}{t}{t}}{t}{t}}{t}{t}{t}}{t}{t}{t}}{t}{t}{t}}{t}{t}}{t}{t}}{t}{t}}{t}{t}{t}{t}}{t}{t}}{t}}{t}}{t}}{t}\n' +
      '\n' +
      '모든 토큰 임베딩의 유사성을 해당 언어 그룹 비전 임베딩으로 최대화하고 시퀀스에서 다른 언어 그룹 비전 임베딩과의 유사성을 최소화하려고 노력하며 그 반대의 경우도 마찬가지이다.\n' +
      '\n' +
      '전체적인 목적: 전체 SPARC 목표는 글로벌 대비 손실 및 미세화 정렬 수축 손실의 가중 합이다.\n' +
      '\n' +
      '\\[L_{\\text{SPARC}}=\\lambda_{g}L_{g}+\\lambda_{f}L_{f} \\tag{7}\\]\n' +
      '\n' +
      'HH(\\lambda_{g}\\)와 \\(\\lambda_{f}\\)는 하이퍼파라미터이다. 부록 C에서 SPARC에 대한 의사코드를 제공합니다.\n' +
      '\n' +
      '유사 임계값 \\(P\\)의 이미지 패치 수를 갖는 \\(1/P\\)와 동일하도록 sparsity 임계값(\\sigma\\)을 선택한다. 이 선택은 모든 텍스트 토큰이 적어도 하나의 이미지 패치에 참석해야 한다는 고려에 의해 동기부여된다. 미니-맥스 정규화를 사용하기 때문에 모든 패치가 패치의 수와 동등하게 유사할 때 \\(1/P\\)의 가장 작은 유사성이 달성된다. 이 임계값은 자연스럽게 하나의 토큰에 해당하는 패치 수가 이미지뿐만 아니라 이미지 전체에 걸쳐 토큰 간에 상당히 다양하도록 허용하며, 이는 동일한 종류의 객체(예: "개")가 이미지 내 및 이미지 전반에 걸쳐 다양한 인스턴스에 걸쳐 크기, 스케일 및 형상의 차이에 관계없이 적절하게 표현될 수 있게 한다. 임계치는 또한 유사 행렬의 다른 행에서 상이한 수의 0개의 엔트리를 허용하므로 개별 패치의 유사성을 서로 다른 토큰으로 디커플링할 수 있으며, 따라서 패치가 토큰과 얼마나 유사한지 여부 및 단일 단어가 더 상세한 캡션(예: 큰 갈색 개) 및/또는 여러 토큰으로 표시되는 상황에서 다른 토큰과 얼마나 유사한지를 보유하지 않는다.\n' +
      '\n' +
      '3개의 관련 작업.\n' +
      '\n' +
      '콘텐츠 이미지-텍스트 사전 훈련CLIP(라드포드 등 2021)와 ALIGN(Jia et al., 2021)은 인터넷에서 폐기된 시끄러운 대규모 데이터로부터 텍스트 감독을 활용함으로써 대중화된 학습 일반 시각 표현을 활용했다. 이러한 방법은 전체 이미지의 표현과 매칭된 이미지-텍스트 쌍의 전체 텍스트의 표현 사이의 유사성을 최대화하는 대조적 목적을 통해 표현을 학습하고 배치 내의 나머지 이미지-텍스트 쌍 간의 유사성을 최소화한다. 그러나 글로벌 이미지 및 텍스트 임베딩에 매칭을 통한 시각적 표현 학습을 통해 많은 미세 변경 세부 정보를 폐기하는 거친 시각적 표현(즉, 글로벌 텍스트 임베딩의 매칭을 배치 내의 다른 텍스트 임베딩과 구별하기 위해 필요하지 않은 모든 세부 사항)을 초래할 수 있다. 이 문제를 해결하기 위해 FILIP(Yao et al., 2021)는 대조적 목적을 통해 이미지와 텍스트 토큰 간의 토큰별 최대 유사성을 최적화하는 _cross-modal 후기 상호작용 메커니즘_을 제안한다. 이 접근법은 텍스트에서 이미지 패치와 단어 간의 더 미세한 정렬을 달성하지만 배치에서 모든 이미지 패치와 텍스트 토큰 사이의 토큰별 유사성은 큰 배치 크기에 대해 메모리 비효율적이어서 이 문제를 해결하기 위해 사전 훈련 동안 여러 트릭을 사용한다. 관련 접근 방식 PACL(Mukhoti et al., 2023)은 CLIP 전처리 비전 및 텍스트 인코더로부터 시작하여 냉동 표현 위에 있는 열차 및 열차에서 시작하여 더 나은 미세한 이해를 얻는다. 어댑터는 잔차 연결을 갖는 2층 MLP로, 개별 이미지 패치와 글로벌 텍스트 임베딩 간의 코사인 유사도를 이용하여 계산된 가중치와 글로벌 텍스트 임베딩을 비교한 대조적인 목적을 통해 학습된다.\n' +
      '\n' +
      '병렬 작업 스트림에서 의료 이미지-방사선학 리포트 쌍(최대 200k 데이터 포인트 설정, 2023년 다와우위즈 등, 2023년, 황 등은 2021년, 왕 등은 2022년)을 사용하여 시각적 표현을 배우기 위해 의료 문헌에서 몇 가지 방법이 제안되었다. GLoRIA(Huang et al., 2021)는 텍스트 토큰과 주의 가중 패치 임베딩을 대조하여 국부적인 시각적 표현을 구축하며, 여기서 주의 가중치는 패치와 토큰 임베딩 사이의 유사성 매트릭스에서 소프트맥스를 통해 계산된다. FILIP와 유사하게 GLoRIA의 로컬 목표는 계산 집약적이며 큰 배치 크기로 스케일링되지 않는 배치 내의 모든 패치 및 토큰 임베딩 사이의 유사성을 컴퓨팅해야 한다. 대안적으로, MGCA(왕 등은 2022년)는 영상 패치와 토큰 임베딩 간의 매칭을 학습하기 위해 양방향 다중-헤드 주의 전략을 사용하는 토큰별 미세-곡물 손실을 고려한다. 이는 계산하는데 더 효율적이지만 양방향 다중 헤드 교차 선택 전략을 통해 이러한 매칭을 배우는 것은 이중 인코더에 더 많은 파라미터를 추가하며, 여러 개의 추가 하이퍼파라미터를 튜닝하고 주의 가중치를 계산하기 위해 소프트맥스를 사용하는 것과 동일한 문제로 고통받는다. MGCA는 또한 대상 간 의미 대응성을 레버리지하기 위해 클러스터 할당 일관성을 구현하는 도메인 특이적 질병 수준 정렬 손실을 사용한다. 보다 최근의 방법(Dawidowicz et al., 2023)은 GLoRIA 및 MGCA에서 사용된 것과 유사한 미세 훈련된 손실뿐만 아니라 도메인 특이적 특징 및 이미지 뷰에 통합되는 것을 고려한다. 의료 문헌의 이러한 방법은 의료 텍스트(알센저 등, 2019)와 미리 훈련된 텍스트 인코더로부터 시작하는 반면, 이미지 및 텍스트 인코더를 처음부터 공동으로 사전 훈련하는 경우를 고려한다.\n' +
      '\n' +
      '시력-언어 모델의 미세 구성 능력 향상을 위한 시력-언어 모델 대체 접근법에서 일관성 있는 이해는 사전 훈련된 모듈, 특수화된 네트워크 및 인간 주석을 필요로 한다. 한 작업 라인은 이미지 영역 - 텍스트 설명 쌍이 인간 주석(Li et al, 2022)에서 획득되거나 지역 제안 네트워크(Ren et al, 2015), 다양한 텍스트 매칭 접근법(Varma et al, 2023; 중 등은 2022)을 사용하여 설정된 이미지 영역을 대조적 손실을 통해 텍스트 설명으로 매칭하는 이미지 영역을 제안한다. 이중 영상-텍스트 인코더(Li et al, 2022, 2022), 마스크 언어 모델링(Li et al, 2022, 양 et al, 2022), 이미지-텍스트 매칭(Li et al, 2021, 양 et al, 2022, Zeng et al, 2021) 및 바운딩 박스 예측 손실(Krishna et al, 2017, Kuznetsova et al, 2020, Shao et al., Shao et al., Shao et al)을 사용하여 캡션(Li et al. 보다 관련된 작품들은 부록 B를 볼 수 있다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '벌목화된 표상을 배우는 데 상당한 관심이 있었지만, 문헌에 사용된 훈련 집합의 폭은 서로 다른 미세 학습 목표를 비교하기 어렵게 만들었다. 구체적으로 맞춤형 데이터셋(Yao et al, 2021)과 사전 학습된 언어 및/또는 비전 모델(Huang et al., 2021; Mukhoti et al., 2023; Wang et al., 2022)을 사용하면 학습에서 개별 미세 편성 손실의 이점을 식별하기가 어려웠다. 이 작업에서 우리는 유사 비교를 가능하게 하고 SPARC의 영향과 경쟁적인 미세곡물 손실이 하류 성능에 미치는 영향을 이해하고자 한다. 이를 위해 CLIP(라드포드 et al., 2021), FILIP(Yao et al., 2021), PACL(Mukhoti et al., 2023), MGCA(왕 et al., 2022) 및 GLoRIA(Huang et al., 2021)와 같은 전처리 데이터 세트, 아키텍처 및 훈련 단계를 사용하여 랜덤하게 초기화된 네트워크를 전처리했다. 우리는 분류 및 검색과 같은 거친 내용 이미지 수준 작업에서부터 객체 검출 및 의미 세분화와 같은 미세 구성 작업까지 광범위한 작업 및 데이터 세트에 걸쳐 학습된 표현을 철저히 평가한다. SPARC는 조잡한 과제 성과를 줄이는 비용에서 미세곡물 이해를 향상시키는 일부 경쟁 방법과 달리 다양한 벤치마크에 걸쳐 거친 작업과 미세곡물 작업 모두에 대한 성능을 동시에 증가시킨다.\n' +
      '\n' +
      '### Experimental setup\n' +
      '\n' +
      '** 모델 아키텍처** 문헌을 중심으로 비전트랜스포머(ViT)(Dosovitskiy et al., 2020)를 이미지 인코더 및 트랜스포머(Vaswani et al, 2017)를 텍스트 인코더로 사용한다. 우리는 ViT-B/32, ViT-B/16 및 ViT-L/14를 실험하고 해당 언어 모델과 페어링한다. 부록 D에서 세부 정보를 확인하세요.\n' +
      '\n' +
      '***Datasets**GT는 대규모 데이터셋 ALIGN(Jia et al., 2021), JFT(Sun et al, 2017; Zhai et al, 2022), LTIP(장기 텍스트 & 이미지 박람회)를 사용하여 훈련한다. ALIGN은 시끄러운 제트-텍스트와 짝을 이루는 11억 개의 이미지를 가지고 있으며 JFT는 30k 레이블의 클래스로 반자동적으로 주석이 달린 40억 개의 이미지를 가지고 있으며 LTIP는 3억 8천만 개의 고급 이미지 - 더 풍부한 이미지 캡션을 가진 텍스트 쌍을 가지고 있다. 자세한 내용은 부록 D를 참조하세요.\n' +
      '\n' +
      '** 사전 훈련 세부사항**(224\\t 224\\) 해상도로 이미지를 재구성하고 32k 어휘 문장 토큰화기(K유도 및 리처드론, 2018)로 텍스트를 토큰화하는 동시에 각 캡션 별로 최대 55개의 토큰을 유지한다. 우리는 선형 평가 및 체중 붕괴 규칙화로 코사인 학습률 일정인 AdamW(Loshchilov and Hutter, 2017) 최적기를 사용하여 모든 모델을 훈련시킨다. 우리는 16348의 배치 크기를 사용하고 200k 단계(\\(\\ 엔트스\\) 3.2억 데이터 포인트)에 대해 ViT-B 모델과 250k 단계(\\(\\(\\touchx\\) 4.1억 데이터 포인트)에 대한 ViT-L 모델을 사전 균주화한다. 더 하이퍼파라미터 디테일을 위해 부록 D를 참조하세요.\n' +
      '\n' +
      '제로샷 영상 분류.\n' +
      '\n' +
      '우리는 먼저 제로 샷 이미지 분류의 거친 작성 작업에 대한 SPARC를 평가한다. 구체적으로 이러한 목적을 위해 이미지넷(Recht et al., 2015), 이미지넷(Recht et al., 2019), 이미지넷-R(Hendrycks et al), 이미지넷-R(Hendrycks et al, 2021), 이미지넷-C(Hendrycks and Dietterich, 2019), 이미지넷-A(Hendrycks et al., 2019), 이미지넷-Sketch(Wang et al.,Wang et al., 2019), 이미지넷-Sketch) 및 이미지넷-S케토치(Wang et al., 2019), 이미지넷-Sketch) 및 이미지넷-Sketch) 및 이미지넷-Sketch) 및 이미지넷-Sketch(Wang et al., 2019) 및 이미지넷-Sketch) 및 이미지넷-Sketch(Wang et al., 이미지넷-Sketch) 및 이미지넷-Sketch(Wang et al. 평가를 위해 (라드포드 등 2021)와 유사한 프로토콜을 따르고 표 1의 한 가지 프롬프트(즉, 그룹 라벨)에 대한 결과를 계산하고 표 2의 신속한 앙상블을 사용할 때 평가 프로토콜에 대한 자세한 내용은 부록 D를 참조하십시오. 표 1과 표 2 모두에서 SPARC가 모든 설정과 다른 ViT 아키텍처를 통해 경쟁 방법을 능가하거나 일치한다는 것을 알 수 있다. 구체적으로, SPARC는 ViT B/32, 특히 ImageNet-R, -C, -A 및 -Sketch에서 ViT B/32에 대한 기저소에 대한 상당한 개선으로 나타난 바와 같이 더 큰 패치로부터 인코딩되는 매우 효과적인 정보를 보여주며, 특히 섭동 및 적대적 예들에 대한 견고성을 보여준다. 더욱이, 신속한 앙상블링은 제로 샷 이미지 분류(문헌과 일치)에 대한 모든 방법의 성능을 향상시키지만 SPARC로부터의 성능 이득은 여전히 이 평가 설정에서 보존된다는 것을 알 수 있다.\n' +
      '\n' +
      'PACL(Mukhoti et al., 2023), GLoRIA(Huang et al., 2021), MGCA(Wang et al., 2022)는 전처리된 언어 및/또는 비전 인코더를 염두에 두고 개발된 반면, 여기에서는 스크래치 설정에서 테스트된다. 표 1 및 표 2에서 우리는 전처리 설정 PACL 및 GLoRIA가 CLIP를 과소수행하는 반면 MGCA는 CLIP에 대한 더 경쟁적인 성능을 나타낸다는 것을 알 수 있다. 반면 처음부터 벌목 목적으로 개발된 FILIP(Yao et al, 2021)는 성능이 저하되는 광범위한 학습률과 체중 저하 매개 변수를 가로질러 훈련하는 데 매우 불안정한 것으로 입증되었다. 이 훈련의 어려움은 원지(Yao et al., 2021), (부록의 cf._트레이닝은 매우 불안정하고 Nan 손실이 쉽게 발생한다. FILIP는 이미지 증강, 포획의 역번역 및 맞춤형 프롬프트 앙상블과 같은 표준 전처리 설정에서 존재하지 않는 많은 추가 트릭을 사용한다.\n' +
      '\n' +
      '### Image-Text retrieval\n' +
      '\n' +
      '다음으로 제로 샷 교차 모달 검색 작업, 즉 Flickr30k(Plummer et al., 2015) 및 MSCOCO(Lin et al, 2014)에서 이미지 대 텍스트 검색에 대한 SPARC를 평가한다. 표 3에서 SPARC가 모든 메트릭에 걸쳐 경쟁하는 모든 기저부를 능가한다는 것을 알 수 있다. 미세 변경 손실 PACL 및 GLoRIA를 사용하면 전 세계 대조 목적 CLIP가 상당히 과소 평가되지만 MGCA는 전처리 환경에서 CLIP에 대한 경쟁 성능을 보여준다. 불행히도 FILIP(Yao et al., 2021)는 모든 메트릭에 걸쳐 CLIP를 다시 과소형성한다. FILIP를 안정화하려는 시도에서 CLIP와 결합하여 CLIP에 대한 다른 벤치마크에서 경쟁하면서 ViT B/32에서 이미지 대 텍스트 Flikr30k의 개선을 관찰했다. 우리는 이러한 결과를 부록 D에서 제공합니다.\n' +
      '\n' +
      '### Evaluating faithfulness\n' +
      '\n' +
      '우리는 _faithability_를 통해 SPARC의 미세 변경 성능을 추가로 조사하지만 모델의 최고 점수 자막이 그라운드 진리 자막(들)(지 등, 2023)과 일치한다. 이는 정확한 매칭 검색을 측정하는 탑-1 검색(R@1)과 다르며 이미지 내의 요소를 충실히 설명하는 모델의 능력을 평가하지 않는다. 고신성은 LLM 문헌에 사용되어 환각 모델(Adlakha et al., 2023; Razumovskaia et al., 2023)의 성향을 추가 정보를 삽입하지 않고 더 정확한 근거 진리의 세부 사항을 포착하는 모델로 평가했다. 뮤직 오버랩 메트릭.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c c c} \\hline \\hline  & Objective & IN & IN-V2 Th & IN-V2 MF & IN-V2 TI & IN-R & IN-C & IN-A & IN-Sketch \\\\ \\hline \\hline \\multirow{4}{*}{\\begin{tabular}{} \\end{tabular} } & CLIP & 69.0 & 68.8 & 60.4 & 73.4 & 62.4 & 44.6 & 15.8 & 52.4 \\\\  & FILIP & 56.8 & 54.8 & 48.4 & 60.0 & 44.6 & 30.8 & 7.8 & 39.6 \\\\  & PACL & 61.2 & 59.5 & 51.9 & 65.2 & 52.9 & 36.4 & 9.3 & 45.2 \\\\  & GloRIA & 65.9 & 64.8 & 57.0 & 69.6 & 57.4 & 40.7 & 11.7 & 48.7 \\\\  & MGCA & 68.6 & 67.4 & 59.2 & 72.6 & 61.0 & 43.5 & 14.1 & 50.9 \\\\  & SPARC (ours) & **70.4** & **69.6** & **62.1** & **74.5** & **63.2** & **46.5** & **17.3** & **52.7** \\\\ \\hline \\hline \\multirow{4}{*}{\\begin{tabular}{} \\end{tabular} } & CLIP & 73.9 & 73.6 & 66.1 & 77.1 & 68.8 & 50.4 & 32.5 & 57.3 \\\\  & FILIP & 61.4 & 61.0 & 53.8 & 65.6 & 53.2 & 35.9 & 14.2 & 45.1 \\\\  & PACL & 63.3 & 61.7 & 54.4 & 66.8 & 54.1 & 37.3 & 12.9 & 45.4 \\\\  & GloRIA & 70.4 & 70.0 & 62.8 & 74.7 & 65.7 & 46.4 & 25.0 & 54.8 \\\\  & MGCA & 72.7 & 72.7 & 65.3 & 76.3 & 67.6 & 48.4 & 29.8 & 55.5 \\\\  & SPARC (ours) & **74.7** & **74.0** & **67.1** & **77.8** & **71.1** & **51.31** & **34.2** & **57.9** \\\\ \\hline \\hline \\multirow{4}{*}{\n' +
      '\\begin{tabular}{} \\end{tabular} } & CLIP & 79.2 & 78.5 & 71.8 & 81.6 & 78.5 & **61.3** & 51.5 & 65.1 \\\\  & MGCA & 78.0 & 77.4 & 70.5 & 80.6 & 75.2 & 57.9 & 45.5 & 63.1 \\\\  & SPARC (ours) & **79.7** & **78.9** & **72.6** & **81.9** & **79.8** & **61.3** & **53.4** & **65.9** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '이미지넷(IN) 및 그 변이체 이미지넷-V2 Threshold(IN-V2 Th), 이미지넷-V2 Matched Frequency(In-V2 MF), ImageNet-V2 Top Images(IN-V2 TI), 이미지넷-R(IN-R), 이미지넷-C(IN-C), 이미지넷-Sketch(IN-Sketch)에 신속한 앙상블을 사용한 제로 샷 분류(IN-V2 Th.\n' +
      '\n' +
      '지상 진리 토큰에 나타나는 선택된 상단 자막에서 토큰의 비율을 측정하는 \\(\\mathcal{K}\\)-정정은 인간 판단(Adlakha et al., 2023)과 잘 상관관계가 있는 것으로 나타났다. 표 4에서 우리는 모든 토큰(\\(\\mathcal{K}\\)-P에 대한 MSCOCO에 대한\\(\\mathcal{K}\\)-정밀과 \\(\\mathcal{K}\\)-정정은 명사 및 형용사에 국한되어 이미지 내에서 관찰된 객체를 더 잘 인코딩하기 때문에 보고한다. 우리는 두 아키텍처에 대한 모든 방법을 평가하고 SPARC가 물체의 환각(더 높은 \\(\\mathcal{K}\\)-Pna)을 감소시키면서 모든 토큰을 고려할 때(\\(\\mathcal{K}\\)-P로 측정할 때) 관련 방법에 대한 경쟁 성능을 나타낸다는 것을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & \\multicolumn{4}{c}{VIT-B/32} & \\multicolumn{2}{c}{VIT-B/16} \\\\ Method & \\(\\mathcal{K}\\)-Pna & \\(\\mathcal{K}\\)-P & \\(\\mathcal{K}\\)-Pna & \\(\\mathcal{K}\\)-P \\\\ \\hline \\hline CLIP & 76.03 & 77.82 & 77.56 & 78.99 \\\\ FILIP & 63.3 & 66.83 & 66.05 & 70.09 \\\\ PACL & 3.36 & 26.26 & 4.09 & 27.31 \\\\ GLoRIA & 71.63 & 73.54 & 73.85 & 75.3 \\\\ MGCA & 75.79 & 77.98 & 77.66 & **80.03** \\\\ SPARC (ours) & **76.46** & **78.44** & **78.72** & 79.77 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: MSCOCO의 명사와 형용사(\\(\\mathcal{K}\\)-P) 및 \\(\\mathcal{K}\\)-정정은 명사 및 형용사(\\(\\mathcal{K}\\)-Pna)에 국한되었다.\n' +
      '\n' +
      '### Fine-grained localization\n' +
      '\n' +
      '우리는 SPARC를 개방형 동물 객체 검출 및 제로 샷 시맨틱 분할과 같은 정확한 국소화가 필요한 미세 구성 작업에 대해 평가하여 추가로 조사한다. 이러한 평가를 위해 ViT-B/16 아키텍처를 사용한다.\n' +
      '\n' +
      '열린 동물 객체 검출은 먼저 SPARC로 학습된 개선된 미세곡물 이해가 미세곡물 국소화가 필요한 과제로 번역되는지 여부를 평가하기 위해 SPARC를 객체 검출을 위한 백본으로 사용한다. 구체적으로, 우리는 ViT-B/16 백본과 함께 OWL-ViT 개방형 동물 대상 검출기(매더러 등, 2022)를 사용했다. SPARC 사전 학습 후, 검출 헤드를 백본에 추가하고 오브제365(Shao et al., 2019) 및 비주얼 유전체(Krishna et al, 2017) 데이터 세트를 매더러 등(2022)에서 접근한 후 미세 조정한다. 이미지 수준 전처리에서 지식의 전달을 테스트하는 데 잘 적용되는 대형 동물 데이터세트 LVIS(Gupta et al., 2019)에 대한 결과 모델을 평가한다. LVIS에는 1203개의 범주의 객체가 포함되어 있으며, 그 중 307개의 "레" 범주가 사전 실습에서 제로 샷 전달을 측정하기 위해 훈련 데이터에서 제외된다. 또한 80개의 MSCOCO 클래스에서도 검출을 평가합니다. 우리는 검출 교육을 3회 운영하고 표 5의 평균 및 표준 편차를 보고하며 평균 정밀도로 측정한 LVIS 및 MSCOCO의 CLIP +0.9%, LVIS "라레" 그룹의 +3.1%보다 개선한다. LVIS "라레" 수업은 탐지 훈련 데이터 동안 볼 수 없기 때문에 모델은 이러한 수업에 대해 전처리된 표현으로부터 정보 전달에 의존해야 한다. LVIS \\(텍스트{AP}_{\\text{rare}}\\)의 기준선보다 SPARC의 큰 개선은 SPARC가 더 유익한 미세 구성 표현을 배웠음을 시사한다.\n' +
      '\n' +
      '관련 작업(Mukhoti et al, 2023) 후, 우리는 또한 제로 샷 분할 _given_ 텍스트 레이블, 즉 주어진 이미지의 패치 임베딩을 계산하고 모든 그라운드-트루트 클래스(Mukhoti et al, 2023; Ranasinghe et al, 2022)의 텍스트 임베딩으로 패치 임베딩의 코사인 유사성을 계산한다. 우리는 각 패치에 대한 매칭 클래스를 해당 패치의 최대 코사인 유사도에 해당하는 텍스트로 할당한다. 그런 다음 지상-진실 분절의 해상도와 일치하고 예측-지상-진실 분절 간의 각 계층 간 교차로 과연합(IoU)에 대한 계산을 위해 패치를 상향 조정하며, 지상-진실 이미지에 존재하는 계층에 대한 IoU 점수의 평균을 보고한다. 이 평가에 대한 자세한 내용은 부록 D. 표 6에서 SPARC가 PASCAL VOC(Everingham et al., 2015) 데이터 세트에서 +4.34 mIoU, PASCAL Context(Mottaghi et al., 2014) 데이터 세트에 +1.2 mIoU에 의해 다음 최고의 모델을 크게 능가하는 다른 기저부에 비해 강하게 개선된다는 것을 알 수 있다. 우리는 예측된 것을 시각화했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & Pascal VOC & Pascal Context \\\\ \\hline CLIP & 23.02 & 20.45 \\\\ FILIP & 19.32 & 9.31 \\\\ PACL & 1.23 & 1.61 \\\\ GLoRIA & 22.64 & 15.26 \\\\ MGCA & 21.91 & 11.50 \\\\ SPARC (ours) & \\(\\mathbf{27.36}\\) & \\(\\mathbf{21.65}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 아라비아 세그먼트화: Pascal VOC 및 PASCAL 콘텍스트 데이터셋에 대한 예측 및 접지-진리 분할의 mIoU 및 접지-진리 분할이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & \\multicolumn{2}{c}{LVIS} & \\multicolumn{2}{c}{MSCOCO} \\\\ Method & \\(\\text{AP}_{\\text{all}}\\) & \\(\\text{AP}_{\\text{rare}}\\) & \\(\\text{AP}_{\\text{all}}\\) \\\\ \\hline \\hline CLIP & \\(26.9\\pm 0.12\\) & \\(22.0\\pm 0.79\\) & \\(38.5\\pm 0.19\\) \\\\ SPARC (ours) & \\(\\mathbf{27.9\\pm 0.11}\\) & \\(\\mathbf{25.1\\pm 0.95}\\) & \\(\\mathbf{39.4\\pm 0.13}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: | Mean 평균 정밀도(평균 \\(\\pm\\) 표준 편차)는 LVIS 및 MSCOCO의 모든 클래스에 대한 모든 및 희귀 클래스에 대해 표시된다.\n' +
      '\n' +
      '그림 3의 PASCAL VOC 데이터셋의 세그먼트화 마스크. CLIP는 이미지의 여러 다른 부분에 존재하는 객체를 예측하지만 SPARC는 더 나은 객체 위치를 달성하고 그 형태를 보다 정확하게 예측한다.\n' +
      '\n' +
      '비전 언어 모델에는 SPARC 백본이 있습니다.\n' +
      '\n' +
      '이미지-텍스트 쌍을 이루는 데이터로부터 대조적으로 훈련된비전 백본은 종종 냉동되어 플라밍고(Alayrac et al., 2022)와 같은 발견된 비전-언어 모델(VLM)에 사용된다. SPARC에서 얻은 미세곡물 성능 개선이 VLM에서 더 나은 자막 성능으로 번역되는지 여부를 이해하기 위해 CLIP 백본 대 CLIP를 사용하여 비교하는 실험을 수행한다. 플라밍고 스타일의 건축(알레이크 등 2022년)의 SPARC 백본이다. 이를 위해 CLIP 및 SPARC로 훈련된 ViT-B/16 비전 모델을 동결하고 냉동 400M 매개변수(사전 훈련된) 언어 모델과 페어링한다. 냉동 비전 및 언어 백본 위에, 우리는 프리폼 텍스트를 출력으로 생성하기 위해 퓨처버 레탐러 교차 의도 계층(Alayrac et al, 2022)을 훈련시킨다. 학습 세트에 대한 자세한 내용은 부록 D에서 찾을 수 있다. 우리는 MSCOCO 및 Flickr30k 데이터셋의 캡션 작업에 대한 모델을 평가하고 결과를 표 7에 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & MSCOCO & Flickr30k \\\\ \\hline \\hline CLIP & 24.3 & 12.9 \\\\ SPARC (ours) & **25.3** & **13.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: Flamingo식(Alayrac et al, 2022) 모델에서 서로 다른 비전 백본의 캡션 성능을 평가하는 CIDEr 점수는 표 7이다.\n' +
      '\n' +
      '그림 3: 파스칼 VOC 데이터셋에서 제로샷 분할에 대한 정성적 결과는 그림 3이다. 우리는 원래 이미지, 픽셀 레벨 접지-진열 라벨 및 SPARC, GLoRIA 및 CLIP에서 얻은 패치 레벨 분할 마스크를 보여준다.\n' +
      '\n' +
      '### Ablations\n' +
      '\n' +
      'SPARC에서 다양한 구성요소의 성능을 평가하기 위해 유사성 매트릭스에 대한 유출을 제거하고 패치 임베딩을 그룹화하기 위한 정렬 가중치를 계산하는 대신 소프트맥스를 사용하는 다음 두 가지 제거를 수행한다. 표 8의 결과로부터 미세곡물(MSCOCO 검색) 및 거친-곡물(이미지넷 제로샷 분류) 작업에 대한 결과를 통해 두 구성 요소 모두 모델의 성능에 중요한 역할을 한다는 것을 알 수 있다. 특히 소프트맥스를 사용하면 성능이 가장 많이 감소하는 결과를 낳는다. 정렬 가중치를 계산하기 위해 소프트맥스를 사용하는 문제에 대한 자세한 논의를 위해 부록 A를 참조하십시오.\n' +
      '\n' +
      '메모리 소비 및 FLOPS.\n' +
      '\n' +
      '다른 방법의 계산 및 메모리 효율을 이해하기 위해, 우리는 또한 다른 배치 크기에 대한 하나의 업데이트 단계에 대한 FLOPS 및 피크 메모리 사용을 계산한다. 모든 방법은 256개의 TPU에서 훈련된다. 그림 4(a)에서 1회분식 크기에 사용된 TFLOPS(B=16384)를 사용하여 GLoRIA를 학습시킬 수 있었고, 장치 제약으로 인해 FILIP의 경우 B=8196과 B=16384 사이, CLIP의 경우 100% 이상 증가했음을 알 수 있었고, SPARC 및 MGCA의 경우 1회분식 크기(B)는 2048에서 16384로 다른 방법(Huang et al. 또한, B=16384의 경우, FILIP와 PACL은 모두 CLIP, SPARC 및 MGCA에 비해 2배 피크 메모리를 갖는다. 반면에 CLIP, SPARC 및 MGCA는 FLOPS와 메모리의 동일한 크기를 사용한다는 점에 유의한다. 이들의 차이를 더욱 부각시키기 위해 우리는 CLIP와 관련하여 그림 4(c)의 TFLOPS의 상대적 증가와 SPARC 및 MGCA의 그림 4(c)의 피크 기억의 상대적 증가를 도표팅한다. B=16384의 경우, 즉 실험에 사용하는 배치 크기에 대해 SPARC에 대한 TFLOPS 및 피크 메모리의 상대적 증가는 MGCA의 절반에 가깝다. 우리는 부록 D.6의 FLOPS(in TFLOPS) 및 피크 메모리(in MB)에 대한 자세한 번호를 제공한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{MSCOCO (i2t)} & MSCOCO (t2i) & ImageNet \\\\  & R@1 & R@5 & R@1 & R@5 & Top-1 acc. \\\\ \\hline \\hline SPARC & **57.6** & **81.2** & **43.0** & **68.6** & **72.6** \\\\ - no sparsity & 56.1 & 80.7 & 42.4 & 68.2 & 72.1 \\\\ - softmax & 55.2 & 79.8 & 41.6 & 67.5 & 70.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: MSCOCO 이미지 대 텍스트(i2t)에 대한 ViT-B/16 SPARC 모델에 대한 | A블로그와 이미지(t2i) 검색 및 이미지넷에 대한 제로샷 분류를 보여준다.\n' +
      '\n' +
      '## 5 Discussion\n' +
      '\n' +
      '이 작업에서 미세 제조 비전-언어 사전 실습을 위한 새로운 방법 Sparse Fine-곡물 콘트라스트 정렬(SPARC)을 제안했다. SPARC는 이미지 레벨 및 캡션 레벨 임베딩 및 토큰 및 패치 임베딩 모두에 대조하여 다양한 수준의 과립도에서 정보를 동시에 학습한다. SPARC는 토큰과 유사성을 기반으로 그룹 패치를 학습하고 결과적인 언어-그라운드 패치 임베딩과 토큰 임베딩을 대조한다. 이 비교는 이전 작업과 달리 개별 이미지-텍스트 쌍 내에서 수행되며 전체 배치 내에서 모든 패치 및 토큰의 계산적이고 메모리 비싼 비교를 필요로 하지 않는다. 광범위한 실험 평가를 통해 SPARC가 분류 및 검색과 같은 이미지 수준 작업과 국소화가 필요한 객체 검출 및 분할과 같은 더 미세한 작업 모두에서 성능을 향상시킨다는 것을 보여준다. 더욱이 SPARC는 모델 충실성과 모델 충실성을 향상시키고 SPARC는 모델 충실성을 향상시킵니다.\n' +
      '\n' +
      'SPARC에서 유사성 매트릭스의 단순한 유출은 이미 성능을 향상시키지만 유출 및 학습 패치 그룹화에 대한 다양한 접근법을 탐색하는 것이 더 많은 정보적 표현으로 이어질 수 있다고 믿는다. 더욱이, SPARC가 연관된 자막을 기반으로 패치 그룹을 학습한다는 점을 감안할 때, 기술력이 높은 캡션으로 전처리 데이터를 탐색하는 것은 미래 작업의 또 다른 흥미로운 라인이다. 또한, 바운딩 박스 및 분할 마스크(이미지-텍스트 쌍에 추가하여)를 레버링하는 것은 이러한 신호에 따라 유사성 행렬이 미리 정렬될 수 있기 때문에 패치 그룹을 학습하고 학습 효율을 향상시킬 수 있다. 또 다른 흥미로운 미래 작업의 단계는 SPARC 인코더가 Flamingo(Alayrac et al, 2022), BLIP(Li et al., 2022), PALI(Chen et al., 2022)와 같은 복합적 발견 모델의 일부로 어떻게 수행하는지를 더 탐구한다.\n' +
      '\n' +
      '그림 4: 모든 방법으로 사용하는 TFLOPS(a)와봉기억(b)이다. SPARC와 MGCA를 CLIP와 비교할 때 TFLOPS(c)와 피크 메모리(d)의 상대적 증가가 나타났다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* 아들락하 등 (2023) V. Adlakha, P. BehnamGhader, X. H. 루, N. N. 메이드, S. 빨간색. 질문 답변에 대한 지도-회복 모델의 정확성 및 충실도를 평가한 __ 질문 대답을 위한 지도-회복 모델의 정확성 및 충실도를 평가한다. arXiv 프리프린트 arXiv:2307.16877_, 2023.\n' +
      '* 알레이크 등 (2022) J-B. 알레이크, J. 돈아에, P. 루스, A. 미흐, I. 바, Y. 하손, K. enc, A. 엠쉬, K. 밀리칸, M. 리놀드, 에프라미놀즈, 프로알 펠링고: 소수의 샷 학습을 위한 시각적 언어 모델. 신경 정보 처리 시스템_, 2022년 35:23716-23736의 발전입니다.\n' +
      '* 알센저 등 (2019) E. 알센저, J. R. 머피, W. 바그, W. H. 풍, D. 진, T. 나만, M. McDermott. 공공적으로 이용 가능한 임상 Bert 임베딩 __ 공공적으로 이용 가능한 임상 Bert 임베딩. _이다. arXiv 프리프린트 arXiv:1904.03323_ 2019.\n' +
      '* 첸 et al.(2022) X. 텐, X. 와, S. 창피요, A. 피르고이보반니, P. 파들레세키, D. 잘츠, S. 살즈. 굿맨, A. Grycner, B. Mustafa, L. 베이어, 예를 들어 팔리: A 공동 측정 다국어 언어 이미지 모델. __ 공동 측정 다국어 언어 이미지 모델. arXiv 프리프린트 arXiv:2209.06794_, 2022.\n' +
      '* 다와이도위즈 등 (2023) 가와이도위즈, 에르위니아 허쉬, A. 탈이다. 임리트러: 의료 영상-텍스트 표현을 위한 로컬 정보를 레버링하는 것 : 의료 영상-텍스트 표현을 위한 로컬 정보를 운용한다. ArXiv_, abs/2303.11755, 2023. URL[https://apisemanticscholar.org/CorpusID:257636659](https://apisemanticsicscholar.org/CorpusID:25763636659)\n' +
      '2020년 (2020) A. 도소비츠키이, L. 베이어, A. 콜레니코프, D. 웨이세네르, X. 지하이, T. 유니터타이너, M. 데헤가니, M. 매더러, 지아르디아 히폴드, S. 젤리, 예를 들어, 이미지는 16x16 단어, 즉 스케일에서의 이미지 인식을 위한 트랜스포머의 가치가 있다. arXiv 프리프린트 arXiv:2010.11929_ 2020.\n' +
      '* 엘파델과 와이너트 주니어가. (1993) I. M. Elfadel과 J. L. 와이너트 Jr. "소프트맥스" 비선형성: 통계 역학을 이용한 안정화와 다중 말단 아날로그 회로 소자로서의 유용한 특성을 사용한다. __. 신경 정보 처리 시스템_, 6, 1993의 발전이다.\n' +
      '* 에버링엄 등 (2015) M. 에버링엄, S. M. A. 에슬라미, L. 반골, C. K. I 윌리엄스, J. Winn 및 A. Zisserman. 파스칼 시각적 객체 클래스 챌린지: A 후향적. __ 소급. 컴퓨터 비전_, 111(1):98-136, 1월 국제 저널. 2015년.\n' +
      '2023) S.(2023) S. 멍, 조 원, Y. 티안, Y. 텐과 Y. 장. Hiclip: 조영 언어-영상은 계층적 인식 주의력을 가진 척하는 __ 조영 언어-이미지이다. arXiv 프리프린트 arXiv:2303.02995_, 2023.\n' +
      '* Gupta 등은 (2019) A. Gupta, P. Dollar 및 R. 기리킥. Lvis: 큰 어휘 인스턴스 분할을 위한 데이터셋. 컴퓨터 비전 및 패턴 인식_ 페이지 5356-5364에 대한 IEEE/CVF 회의의 _발표에서 2019년 페이지 5356-5364.\n' +
      '* 펜드롭스 및 디에테르히(2019) D. 펜드롭스 및 T. 다이어트리치. 공동 부패 및 섭동에 대한 신경 네트워크 견고성 _벤치마크링 신경망 견고성은 공통 부패 및 섭동에 대해 강건하다. arXiv 프리프린트 arXiv:1903.12261_ 2019.\n' +
      '* 펜드롭스 등 (2019) D. 헨드롭스, K. 자오, S. 바하트, J. 스테인하르트, D. 송. 자연적인 적대적 예. (2019년) ____(2019년) __(2019년). __(2019년) arXiv 프리프린트 cs.LG/1907.07174_, 5(6) 2019.\n' +
      'Hendrycks et al.(2021) D. Hendrycks, S. S. Hendrycks,2021) D. Hendrycks 등. 바하트, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. 데사이, T. 주, S. 파라졸리, M. 구, 예를 들어 강건성의 많은 면 : 유통 외 일반화에 대한 비판적 분석이 있다. IEEE/CVF 국제 회의는 컴퓨터 비전_ 페이지 8340-8349에서 2021년입니다.\n' +
      '2023년(2023) D. T. 호프만, S. 호프만 등. Schrodi, N. 베어만, V. 피셔, T. 형. 변압기의 에우레카-모멘트: 멀티 단계 작업은 소프트맥스 유도 최적화 문제를 보여준다. arXiv 프리프린트 arXiv:2310.12956_, 2023.\n' +
      '\n' +
      '*황 등은 S.(2021) S. -C. 황, L. 선, M. P. 루쿤렌 및 S. 예웅아. 플로리아: 라벨 효율적인 의료 이미지 인식을 위한 복합 글로벌 로컬 대표 학습 프레임워크입니다. IEEE/CVF 국제 컴퓨터 비전_페이지의 _검토에서 2021년 기준 3942-3951쪽.\n' +
      '*지 등은 Z(2023). 지, 네. 이씨, R. 피시케, T. 유, D. 수, Y. Xu, E. Ishii, Y. J. 방, A. 마도토 및 P. Fung. 자연어 생성에서의 환각 조사 __ 자연어 생성에서의 환각 조사. __ ACM 컴퓨팅 수리비스_, 55(12):1-38, 2023.\n' +
      '* Jia 등은 (2021) C. Jia, Y. 양, Y. 샤, Y. T. 텐, Z. Parekh, H. Pham, Q. 어, Y. H. 성, Z. 리, T. 두에리히. 시끄러운 텍스트 감독으로 시각적, 시력-언어 표현 학습을 구현하세요. 기계학습_국제회의에서는 2021년 PMLR, 4904-4916페이지가 있다.\n' +
      '* 크리슈나 등 (2017) R. 케리샤나, Y. 오, 오. 그로스, J. 존슨, K. 하타, J. 크라비츠, S. 텐, Y. 칼란티디스, L. J. Li, D. Shamma, et al. Visual 게놈: 인파가 밀집된 이미지 주석을 사용하여 언어와 비전을 연결한다. 국제 컴퓨터 비전 저널_, 123:32-73, 2017.\n' +
      '* 크로저 등 (2022) B. 크로저, V. 아돌락하, V. 비네트, Y. 고갈, 에르위니아 폰티 및 S. 빨간색. 맥락 설명에서 이미지 검색( __ 이미지 검색)은 상황 설명에서 검색한다. arXiv 프리프린트 arXiv:2203.15867_, 2022.\n' +
      '* 쿠도, 리처드론(2018) T. 구도와 J. 리처드론. 센텐셜피스: 신경 텍스트 처리를 위한 단순하고 언어 독립적인 서브워드 토큰라이저 및 스플릿라이저: __신경 텍스트 처리를 위한 언어 독립 서브워드 토큰라이저. arXiv 프리프린트 arXiv:1808.06226_ 2018.\n' +
      '* 쿠즈네소바(2020) A. 쿠즈네소바, H. 로마, N. 알딜린, J. 의족, I. 크라스린, J. 폰트-투세트, S. S. 카말리, S. 팝코프, M. Malloci, A. Kolesnikov, et al. 개방형 이미지 데이터셋 v4: 통합 이미지 분류, 객체 검출 및 스케일에서의 시각적 관계 검출. 컴퓨터 비전_ 128(7):1956-1981 2020년 국제 저널.\n' +
      '* Li 등은 (2021) J Li, R. 셀바라주, A. 고트마어, S. 자티, C. 시온그 및 S. C. 호이. 퓨즈 전 정렬: 비전 및 언어 표현 학습과 운동량 증류를 통한 언어 표현 학습. __ 퓨즈 전 정렬. 뉴럴 정보 처리 시스템_, 2021년 34:9694-9705에서의 발전이다.\n' +
      '* Li 등은 (2022a) J. Li, D. Li, D. Li, C. 시온그 및 S. 하이. 블립: 통일된 비전-언어 이해와 생성을 위한 Bootstrapping 언어-이미지를 사전 훈련한다. 기계학습_국제회의에서는 12888-12900쪽, 2022a. PMLR.\n' +
      '* Li et al. (2022b) L. H. 리, P. 장, H. 장, J. 양, C. Li, Y. 중, L. 왕, L. 위안, L. 장, J-N. 황씨, 예를 들어 지상 언어 이미지 사전 훈련. 컴퓨터 비전 및 패턴 인식_ 페이지 10965-10975, 2022b에 대한 IEEE/CVF 회의의 _발표에서.\n' +
      '* Lin 등은 T. (2014) T. >야. 린, M. 마이어, S. Belongie, J. Hys, P. Perona, D. 라만, P. Dollar 및 C. L. Zitnick. 마이크로소프트 코모: 맥락상 공통 객체. 2014_컴퓨터 비전-ECCV 2014: 제13차 유럽 회의, 스위스 취리히, 2014년 9월 6-12일, 제작, 파트 V 13_, 페이지 740-755. 스프링거.\n' +
      '* 로쉬칠로프와 허터(2017) I. 로쉬칠로프와 F. 허터. 감소된 체중 붕괴 규칙화 __ 감소 체중 붕괴 규칙화. __ 감소 체중 붕괴 규칙화. arXiv 프리프린트 arXiv:1711.05101_ 2017.\n' +
      '* 민더러 등 (2022) M.M. 매더러, A. 그리세넨코, A. 스톤, M. 노이만, D. 웨이세네아, A. 도소비츠키이, A. 마렌젠란, A. 아르나브, M. 도하야, Z. Shen, et al. Simple 오픈 배변 물체 검출. 컴퓨터 비전_에 관한 _유럽 콘퍼런스에서는 2022년 728-755페이지. 스프링거.\n' +
      '* 모타그기 등 (2014) R. 모타게이, X. 텐, X. 리, N. G. 조, S. W. 이씨, S. 피들러, R. 의아선, 아유유. 야생에서 객체 검출 및 의미 세분화를 위한 맥락의 역할. 컴퓨터 비전 및 패턴 인식(CVPR)_ 2014년 _IEEE 콘퍼런스에서.\n' +
      '\n' +
      '* Mukhoti et al. [2023] J. Mukhoti, T.-Y. Lin, O. Poursaeed, R. Wang, A. Shah, P. H. Torr, and S.-N. Lim. Open vocabulary semantic segmentation with patch aligned contrastive learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19413-19423, 2023.\n' +
      '* Paiss et al. [2023] R. Paiss, A. Ephrat, O. Tov, S. Zada, I. Mosseri, M. Irani, and T. Dekel. Teaching clip to count to ten. _arXiv preprint arXiv:2302.12066_, 2023.\n' +
      '* Parcalabescu et al. [2021] L. Parcalabescu, M. Cafagna, L. Muradjan, A. Frank, I. Calixto, and A. Gatt. Valse: A task-independent benchmark for vision and language models centered on linguistic phenomena. _arXiv preprint arXiv:2112.07566_, 2021.\n' +
      '* 페더슨 및 Soderberg[1989] C. 페더슨 및 B. Soderberg. 최적화 문제를 뉴럴 네트워크에 매핑하기 위한 새로운 방법. __ 신경 네트워크에 매핑하는 새로운 방법. 1989년 신경계 국제 저널_, 01(01):3-22.\n' +
      '* Plummer et al. [2015] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier, and S. Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In _Proceedings of the IEEE international conference on computer vision_, pages 2641-2649, 2015.\n' +
      '* Radford et al. [2021] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* Ranasinghe et al. [2022] K. Ranasinghe, B. McKinzie, S. Ravi, Y. Yang, A. Toshev, and J. Shlens. Perceptual grouping in vision-language models. _arXiv preprint arXiv:2210.09996_, 2022.\n' +
      '* Ranasinghe et al. [2023] K. Ranasinghe, B. McKinzie, S. Ravi, Y. Yang, A. Toshev, and J. Shlens. Perceptual grouping in contrastive vision-language models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5571-5584, 2023.\n' +
      '* Razumovskaia et al. [2023] E. Razumovskaia, I. Vulic, P. Markovic, T. Cichy, Q. Zheng, T.-H. Wen, and P. Budzianowski. _Dial BeInfo for Faithfulness_: Improving factuality of information-seeking dialogue via behavioural fine-tuning, 2023.\n' +
      '* Recht et al. [2019] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to imagenet? In _International conference on machine learning_, pages 5389-5400. PMLR, 2019.\n' +
      '* Ren et al. [2015] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. _Advances in neural information processing systems_, 28, 2015.\n' +
      '* Russakovsky et al. [2015] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. _International journal of computer vision_, 115:211-252, 2015.\n' +
      '* Shao et al. [2019] S. Shao, Z. Li, T. Zhang, C. Peng, G. Yu, X. Zhang, J. Li, and J. Sun. Objects365: A large-scale, high-quality dataset for object detection. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 8430-8439, 2019.\n' +
      '* Shen et al. [2023] K. Shen, J. Guo, X. Tan, S. Tang, R. Wang, and J. Bian. A study on relu and softmax in transformer. _arXiv preprint arXiv:2302.06461_, 2023.\n' +
      '* Sun et al. [2017] C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In _Proceedings of the IEEE international conference on computer vision_, pages 843-852, 2017.\n' +
      '\n' +
      '2023년(2023) M.M. 바마, J-B. 딜브룩, S. 호퍼, A. 차우다리, C. 랑로츠. 빌라: 현실 세계 데이터로부터 파인-그레이드된 비전-언어 표현 학습을 합니다. 컴퓨터 비전_에 대한 IEEE/CVF 국제 회의의 _검토에서, 22225-22235 페이지는 2023년이다.\n' +
      '* 바소와이 등은 (2017) A. 바소와이, N. 세제, N. 파마, J. 우스즈코레이트, L. 존스, A. N. 고메스, L. 카이저, 나 폴로숙신. 필요한 것이 전부입니다. __ 주의가 필요합니다. __ 주목된다. 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* 왕 등 (2022) F. 왕, Y. 주, S. 왕, V. 바다나부티, L. 유. 일반화된 의료 시각 표현 학습을 위한 다중-동시 교차-모달 정렬 __ 일반화된 의료 시각적 표현 학습을 위한 다중-동시 교차-모달 정렬이다. 신경 정보 처리 시스템_, 2022년 35:33536-33549의 발전입니다.\n' +
      '(2019) H. 왕, S.S. 왕. 지, Z. 라이톤과 E. P. Xing. 지역 예측력을 처벌하여 강력한 글로벌 표현을 학습합니다. 신경 정보 처리 시스템_의 _Adances에서 2019년 페이지 10506-10518.\n' +
      '2022) J Xu, S.(2022) J Xu, S. De Mello, S. 우, W. 편, T. 브뤼셀, J. 커츠, X. 왕아. 그룹빗: 독보적인 세분화는 텍스트 감독으로부터 나타난다. 컴퓨터 비전 및 패턴 인식_ 페이지 18134-18144, 2022년에 IEEE/CVF 회의의 _발표에서.\n' +
      '* Xu 등은 (2023) J.Xu, J. Hou, Y. 장, R. 펑, Y. 야, 야. Qiao와 W. 사이. 자연어 감독으로부터 개방형 시맨틱 세분화 모델을 학습한다. 컴퓨터 비전 및 패턴 인식_ 페이지 2935-2944, 2023에 대한 IEEE/CVF 회의의 _발표에서.\n' +
      '양(2022) J. 양.(2022) J. 양, J. 두안, S. S. Duan, S. S. 양 등. 트란, Y. S. 차다, L. 텐, 벅, T. 칠림비와 J.황. 3중 대비 학습으로 시각적 언어 사전 학습을 합니다. 컴퓨터 비전 및 패턴 인식_ 페이지 15671-15680에 대한 IEEE/CVF 회의의 _발표에서 2022년 페이지.\n' +
      '* 야오 등은 (2021) L. 야오, R. 황, L. 후, G. 루, M. Niu, H. Xu, X. Z, Z. 리, X. 지앙, C. 주. 필립: Filip: Fine-표지된 대화형 언어-이미지 사전 훈련. __표현형 언어-이미지 사전 훈련. arXiv 프리프린트 arXiv:2111.07783_ 2021.\n' +
      '*유 등 (2022) J 유, Z. 왕, V. 바슈다반, L. 예웅, M. 세예다오세니, Y. 우. 코카: 콘트롤 캡션들은 이미지-텍스트 기반 모델이다. __Coca: 콘틱 캡션들은 이미지-텍스트 기반 모델이다. arXiv 프리프린트 arXiv:2205.01917_, 2022.\n' +
      '(2022) M.M.M. 예키옥소울, F. 비아니치, P.콜리, D. 주라프스키 및 J. 자우. 시력 언어 모델은 언제, 왜 그것이 백-어 모델처럼 행동하는지, 그리고 그것이 무엇을 해야 하는가? arXiv 프리프린트 arXiv:2210.01936_, 2022.\n' +
      '*생 등은 Y. (2021) Y. 멍, X. 장, H. 리. 다중 학습 비전 언어 사전 훈련: 시각적 개념을 가진 텍스트를 정렬한다. __멀티 학습 비전 언어 사전 훈련. arXiv 프리프린트 arXiv:2111.08276_ 2021.\n' +
      '* 자이 등은 S. (2023a) S. 지하이, T. 리케만넨코, E. 리트윈, D. 버스브리지, J. 라마푸람, Y. 장, J구, J시슨슨. 비주얼 엔트로피 붕괴를 방지하여 변압기 교육을 안정화하는 __ 주의력 엔트로피 붕괴를 방지하여 변압기 교육을 안정화한다. ICML_, 2023a.\n' +
      '* 자하이 등 (2022) X. 자하이, 아시네토박터 칼레니코프, N. 해울비, L. 바이어. 시력 변압기를 충전합니다. 컴퓨터 비전 및 패턴 인식_ 페이지 12104-12113에 대한 IEEE/CVF 회의의 _발표에서 2022년 페이지 12104-12113.\n' +
      '* 자이 등은 X.(2023b) 지하이, 보트리티스 무스타파, A. 케르니코프 및 L. 바이어. 언어 이미지 사전 학습을 위한 언어 이미지 사전 학습을 위한 Sigmoid 손실. __ 언어 이미지 사전 학습을 위한 Sigmoid 손실. 컴퓨터 비전_, 2023b 국제 컨퍼런스입니다.\n' +
      '*중 등은 Y. 중, 조양, P. 장, C. 리, N. 코델라, L. H. Li, L. L. 주, X. 다이, L. 야, Y. Li, et al. 자치구클립: 지역 기반 언어 이미지 전처리. 컴퓨터 비전 및 패턴 인식_ 페이지 16793-16803에 대한 IEEE/CVF 회의의 _발표에서 2022년 페이지.\n' +
      '* 저우 et al.(2022) C. 저우, C. C. 로이 및 B. 디이. 클립에서 자유 조밀한 라벨을 추출합니다. 컴퓨터 비전_에 관한 _유럽 콘퍼런스에서는 2022년 696-712페이지. 스프링거.\n' +
      '\n' +
      '정렬 가중치 획득을 위해 소프트맥스를 사용하는 문제\n' +
      '\n' +
      '\\(소프트맥스\\)는 예를 들어 주의/정렬 가중치의 경우와 같이 확률로 해석하거나 해석할 수 있는 활성도를 정상화하는 데 유비퀴틴적으로 사용된다. 이러한 선택의 한 가지 잠재적인 이유는 분류 과제에 대한 출력 활성화 함수로 \\(softmax\\)를 다항 출력에 대한 표준 링크 함수로서 사용하는 것을 지배하는 관행이다. 또 다른 매력적 재산은 서로 다른 생존 가능한 \\(max\\)-운영자로 작용하여 _seering_1등급을 여러 개에서 자연스럽게 해석할 수 있다는 것이다.\n' +
      '\n' +
      '그러나 \\(소프트맥스\\)는 구배 흐름 관점(Hoffmann et al., 2023; 선전 et al., 2023; Zhai et al., 2023a)에서 문제가 될 수 있으며 이 섹션에서는 이 관찰과 특정 사용 사례에 미칠 수 있는 시사점을 확장할 것이다. 또한 소프트펜(max\\) 작업자로서의 역할에서 직관적으로 소프트맥스는 \\(k\\) 중 하나를 선택하여 피크디 언모달 분포로 수렴하는 것을 선호하며 다중 모달 분포를 나타낼 가능성이 적다. 이는 구배가 활성화를 통해 어떻게 흐르면서 승자-다이크-올 역학(엘파델과 와이너트 주니어 1993, 페더슨과 Soderberg, 1989)으로 이어지며, 분포의 정점과 불임성을 보장했기 때문이다.\n' +
      '\n' +
      '일부\\(\\mathbf{h}\\in\\mathcal{R}^{k}\\)에 대해 파생물을 a\\(a(\\mathbf{h})로 가정하면 소프트맥스(\\mathbf{h})를 쓸 수 있다.\n' +
      '\n' +
      'Footnote 1: 표기법의 남용은\\(\\mathbf{a}}^{k}\\)를 사용하고\\(\\mathbf{a}=a(\\mathbf{h})를 사용하는\\(\\mathbf{k}\\)를 사용한다.\n' +
      '\n' +
      '<\\{\\mathbf{h}_{j}}=\\{\\mathbf{a}}{\\ff{a}}_{i}_{i}-\\mathbf{a}_{i}_{i}_{i}_{i}－\\mathbf{a}_{i}-\\mathbf{a}-\\mathbf{a}-\\mathbf{i}-\\mathbf{i}}-\\mathbf{i}}-\\mathbf{i}-\\mathbf{i}}-\\mathbf{i}-\\mathbf{i}-\\mathbf{i}-\\mathbf{i}-\\mathbf{i}}-\\mathbf{i}}-\\mathbf{a}-\\mathbf{a}}-\\mathbf{a}-\\mathbf{a}-\\mathbf{a}-\\mathb\n' +
      '\n' +
      '우리는\\(\\sum_{i}\\mathbf{a}_{i}\\mathbf{V}_{i}\\)의 함수, 즉 일부 값(\\mathbf{V}_{i}_{i}\\)인 손실(L\\)을 가지고 있다.\n' +
      '\n' +
      '초기화 시 소프트맥스 구배가 사라지고, 우리가 참석하고 싶은 패치나 토큰이 많아. 우리의 표기법에서 \\(k\\gg 0\\)는. 초기화 시 모든 사전 활성화 항목 \\(\\mathbf{h}_{i}\\)은 작은 수의 유사한 크기가 될 것이다. 주의 가중치는 \\(k\\) 패치 위에 균일하게 분포되어 \\(\\mathbf{a}_{i}\\touchx\\frac{1}{k}\\ll 1,\\forall i\\)로 이어진다. 가중치가 거의 균일하게 분포되어 있기 때문에 다른 관찰은 무작위로 다른 패치를 선택하게 될 것이다. 따라서 특정 토큰(i\\)에서 소프트맥스를 통한 구배를 예상하는 것은 \\(\\)가 성장함에 따라 \\(0\\)로 매우 빠르게 사라질 \\(\\frac{1}{k^{2}}\\)에 의해 확장될 것이다. 시스템이 \\(i\\)-th 요소를 클릭하는 드문 시나리오에서 구배가 \\(\\frac{1}{k}\\)가 되며, 이는 또한 \\(k\\)가 성장함에 따라 \\(0\\)로 사라진다. 매우 큰 \\(k\\)를 고려한다면, 이것은 우리가 탈출하기 어려울 수 있는 초기화에 고원이 있다는 것을 보장한다(또는 그렇게 하기 위해 많은 업데이트를 취할 수 있다). 비슷한 관찰을 위해 (Hoffmann et al., 2023)도 참조하십시오.\n' +
      '\n' +
      '소프트맥스는 _winner-takes-all_ 역학을 나타내며, 이는 일찍부터 바람직한 특성으로 이해되어 왔으며, 예를 들어(Peterson and Soderberg, 1989), (Elfadel and 와이너트 Jr, 1993)를 볼 수 있다. 이러한 행동을 직관적으로 정당화하는 한 가지 방법은 소프트맥스 동작 다중 시간(즉, 전환 기능이 소프트맥스일 뿐인 시스템의 역학을 연구하는 것)을 적용하는 효과를 생각하는 것이다. 그림 (Peterson and Soderberg, 1989)에 나와 있는 바와 같이. 단순성의 모서리인 5는 모든 초기 조건에서 시스템이 매우 빠르게 모서리 중 하나로 수렴하는 역동적 시스템의 유인 물질로 작용한다. 이것은 구배의 역학에 의해 발생한다. 특정 가중치가 상향 조정되면 정상화로 인해 다른 모든 가중치가 밀린다. 무게가 밀리는 양은 크기에 따라 다릅니다. 따라서 특정 무게가 더 크고 원하는 행동과 양의 상관관계가 있다면 긍정적인 상관관계가 있는 다른 가중치보다 비례적으로 더 많이 밀릴 것이다. 특정 형태의 함수(지수 포함)는 구배가 취하는 형태로 역할을 하며 지수 제거는 행동을 변화시킬 것이다. 이러한 유형의 역학은 소프트맥스에 의해 유도된 분포를 무모달로 이끄는 단면을 가지고 있다.\n' +
      '\n' +
      '즉, 소프트맥스는 활성화의 이름이 _max_ 연산자로서 다수의 동등하게 관련된 후보들이 아니라 \\(k\\) 중 하나를 클릭하는 행동을 배우는 것을 선호하기 때문에 작용할 것이다.\n' +
      '\n' +
      '소프트맥스는 \\(\\forall j,j\\neq i\\)가 \\(a_{i}\\gg_{j}\\)를 갖도록 확실성 \\(\\forume \\)에 비례한다. 이는 \\(1-a_{i}\\to 0\\) 및 \\(a_{j}<1-a_{i}\\)이 있음을 의미한다. 방정식 8에 따른\\(a_{i}(1-a_{i})의 구배는 \\(a_{i}(1-a_{i})이고 \\(a_{i}\\)만큼 선형적으로 0으로 갈 것이고, 다른 위치(j\\)의 구배는 \\(1-a_{i}\\)에서 위에서 묶인 대략 \\(a_{j}\\)와 동일한 속도로 0으로 갈 것이다. 소프트맥스의 지수화 및 정상화로 인해 \\(h\\)에 대한 크기 \\(\\Delta\\)의 단계는 \\(a_{i}\\to 1\\)를 일정한 변화(h\\)를 위해 기하급수적으로 빠르게 만들 것이다.\n' +
      '\n' +
      '관련 작업\n' +
      '\n' +
      '우리는 여기서 추가 손실 및 모듈을 통해 시력-언어 모델(VLM)에서 미세 학습 이해 달성에 대한 논의를 더욱 확장한다.\n' +
      '\n' +
      '제3절에서 설명한 접근법 외에도 또 다른 작업 라인은 이미지 영역의 계층적 그룹화로 이어지는 모듈을 구축하기 위해 기본 비전 변형기 아키텍처를 수정하는 것을 제안하는데, 예를 들어 그룹ViT(Xu et al, 2022), OV세그먼트터(Xu et al., 2023), HiCLIP(Geng et al., 2023)를 포함한다. 이러한 방법은 건축적 변화를 제안하지만, 훈련에 사용되는 목적은 여전히 글로벌 대비적 손실을 갖는 것을 포함한다. 반대로, 우리 작품에서는 표준 비전 변압기 아키텍처를 사용하고 대신 훈련 목표의 변화를 제안하여 미세화된 이해를 달성한다.\n' +
      '\n' +
      '더욱이, 이러한 접근 방법 중 몇 가지(Xu et al, 2023)와 이중 이미지-텍스트 인코더(Li et al, 2021, 양 et al, 2022) 위에 교차 모달 인코더를 추가하는 다른 방법에는 미리 훈련된 텍스트 인코더 및 비전 인코더로부터 캡션/태닝된 언어 모델링 손실의 훈련이 시작된다는 점에 주목한다.\n' +
      '\n' +
      '유사하게, (Ranasinghe et al, 2023)은 패치 임베딩 방식들을 평균 풀링에서 맥스 풀링으로 변경하고 미리 학습된 비전 및 언어 인코더들 둘 다로 트레이닝을 시작함으로써 대조적으로 훈련된 이중 인코더들의 의미 및 공간 정보를 향상시킨다. 우리의 작업에서 우리는 특히 처음부터 이중 인코더를 훈련하는 설정에 중점을 둔다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:21]\n' +
      '\n' +
      '내역 내역.\n' +
      '\n' +
      '### Model architectures\n' +
      '\n' +
      '이중 인코더의 경우 표준 비전 트랜스포머(ViT)(Dosovitskiy et al., 2020)를 이미지 인코더 및 트랜스포머(Vaswani et al, 2017)를 텍스트 인코더로 사용한다. 우리는 패치 크기(ViT-B/32 및 ViT-B/16)가 다른 ViT-B 모델과 패치 크기 14(ViT-L/14)가 있는 ViT-L 모델을 사용한 실험을 수행한다. 따라서 ViT-B 이미지 인코더의 경우 12층, 768폭 및 12 주의 헤드가 있는 모델을 사용하는 반면 ViT-L 이미지 인코더는 24층, 1024 폭 및 16 주의 헤드가 있는 모델을 사용한다. 언어 인코더의 경우 12개의 층, 768개의 폭 및 12개의 주의 헤드가 있는 아키텍처를 사용합니다. 선형 어댑터 \\(g_{v}(\\cdot)\\) 및 \\(g_{t}(\\cdot)\\)는 각각 차원 512의 공유 임베딩 공간에 비전 및 언어 임베딩을 투영한다.\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      '제4절에서 설명한 바와 같이, 우리는 ALIGN(Jia et al, 2021), JFT(Sun et al, 2017; Zhai et al, 2022), LTIP(장기 텍스트 & 이미지 박람회)를 사전 학습을 위해 다음 데이터 세트를 사용한다. 이미지가 30k 라벨의 클래스 계층으로 반자동적으로 주석이 달린 JFT의 경우 계층적 라벨 구조를 평탄화하고 할당된 모든 라벨을 사용하여 이미지를 설명한다. 우리는 3개의 큰 데이터 세트 각각에서 배치들을 대체하는 다단계 트레이닝 전략을 사용하고, 그 다음 기울기 업데이트들은 데이터 세트들 각각에서 하나의 배치에 대한 손실을 컴퓨팅하는 구배를 집계함으로써 수행된다.\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      '바젤린의 구현은 여기에서 윤곽이 몇 가지 작은 차이로 공개적으로 이용 가능한 코드(사용 가능한 2)를 따른다.\n' +
      '\n' +
      'Footnote 2: GLoRIA: [https://githubcom/marshuang80/gloria] (https://githubcom/marshuang80/gloria), MGCA: [https://githubcom/HKUMedAI/MGCA][https://githubcom/HKUMedAI/MGCA] (https://githubcom/HKUMedAI/MGCA] (https://github.com/HKUMedAI/HKUMedAI/MGCA-MedAI/HKUMedAI/HKUMedAI/MGCA-MedAI/HKUMedAI/HKUMedAI/MGCA.com/HKUMedAI/HKUMedAI/MGCA-MedAI/MGCA-MedAI/HKUMedAI/MGCA-MedAI/MGCAKUMedAI/MGCA-MedAI/MGCA]/HKUMedAI/MGCA-MedAI/MGCA]/HKUMedAI/MGCA-MedAI/MGCA] (https://github.com/MG\n' +
      '\n' +
      '원래의 MGCA 구현에서 토큰별 교차 모달 정렬(Eqn 참조)을 나타낸다. 원지 중 5개(5개)는 시각 토큰에서 [CLS] 토큰(복수의 헤드에 걸쳐 측정된)까지의 마지막 레이어 주의 가중치를 사용하여 상이한 시각적 토큰(언어 토큰의 경우 그 반대)에 대한 손실 항을 가중한다. 우리의 구현에서 [CLS] 토큰을 사용하지 않고 대신 평균 풀링을 사용하여 글로벌 언어/감독 임베딩을 얻기 때문에 이 가중 작업을 생략한다.\n' +
      '\n' +
      '원래 GLoRIA 구현에서 언어 토큰은 대조된 언어 임베딩이 완전한 단어(원지 3.2.1절 참조)를 지칭하도록 각 단어에 대해 집계되지만, 공정한 비교를 보장하기 위해 이러한 추가 응집 동작이 없으며 대신 지역 손실에서 언어 토큰을 직접 사용한다. 또한, 실험에서 쌍별 시력-언어 임베딩 유사성(Eqn 참조)을 정규화하는 것이 중요하다는 것을 발견했다. 원지 3은 \\(D\\)가 임베딩 크기인 \\(\\sqrt{D}\\)에 의한 것이다. 이 정상화가 없으면 GLoRIA로 훈련하는 것이 불안정하다는 것을 발견했다. 더욱이, GLoRIA가 모든 토큰 임베딩과 배치 내의 모든 패치 임베딩 사이의 컴퓨팅 유사성을 필요로 한다는 것을 회상한다. 이는 메모리 비용이 비싸고 배치 크기 16348에 대해 (장치 메모리 제약으로 인한) 불가능했으며, 그 결과, 우리는 겔리아에 대해 4096의 배치 크기를 사용하고 800k 단계(다른 기준선에서 볼 수 있는 예들의 수와 일치) 모델에 대해 훈련했다. FLOP의 세부 계산 및 GLoRIA의 메모리 사용에 대한 섹션 D.6의 논의를 참조한다.\n' +
      '\n' +
      'FILIP [50]의 경우 우리는 원래의 논문을 따르고 저자가 제안한 FILIP에 대한 토큰 떨어뜨림을 구현하여 방법의 큰 기억 소비를 감소시킨다. 원본 논문에서 저자들은 원지(부록_A.3)의 훈련 난이도에 대해 언급하며, 훈련은 매우 불안정하고 난 손실이 쉽게 발생한다. 우리는 광범위한 학습 속도와 체중 붕괴 매개변수에 걸쳐 설정에서 유사한 훈련 불안정성을 관찰했다. 이러한 훈련 불안정성은 CLIP에 비해 상당한 성능 저하로 이어진다. 우리는 FILIP가 이미지 증가, 포획의 역번역 및 맞춤형 신속한 앙상블과 같은 사용을 사용하는 비표준 추가 트릭이 잠재적으로 훈련 안정성을 향상시킬 수 있다고 가정하며, 방법에 걸친 공정한 비교를 보장하기 위해 이러한 트릭을 사용하지 않는다는 점에 주목한다. FILIP의 훈련 불안정성을 감안할 때 훈련 불안정성을 더 잘 이해하기 위해 CLIP와 FILIP를 결합한 많은 추가 실험을 수행했다. 표 9 및 10에 아래에서는 이러한 결과를 제시하며, 이 두 방법을 조합하면 일부 벤치마크에서 일부 개선되고 다른 벤치마크에서 일부 성능 저하가 발생하는 것으로 볼 수 있다.\n' +
      '\n' +
      '마지막으로, 우리 논문의 모든 방법은 실험 결과 모든 방법에 대해 성능이 상당히 향상되었음을 보여주었기 때문에 학습된 온도 매개변수(원본 MGCA 및 GLoRIA 구현에서 수행된 바와 같이 고정 온도의 설치)를 사용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c c c c c c c} \\hline \\hline  & & \\multicolumn{6}{c}{MSCOCO} & \\multicolumn{6}{c}{Flickr30k} \\\\  & & \\multicolumn{3}{c}{image-to-text} & \\multicolumn{3}{c}{text-to-image} & \\multicolumn{3}{c}{image-to-text} & \\multicolumn{3}{c}{text-to-image} \\\\ \\multicolumn{2}{c}{Objective} & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\\\ \\hline \\hline \\multirow{4}{*}{\\begin{tabular}{c} CLIP \\\\ FILIP \\\\ SPLIC \\\\ SPLIC \\\\ SPARC (ours) \\\\ \\end{tabular} } & CLIP & 53.5 & 78.2 & 86.7 & 38.4 & 64.8 & 74.9 & 79.2 & 95.1 & 97.2 & 66.5 & 88.0 & **93.1** \\\\  & FILIP & 35.6 & 61.0 & 73.1 & 26.2 & 51.0 & 62.4 & 62.6 & 86.9 & 92.9 & 50.5 & 77.7 & 84.9 \\\\  & CLIP + FILIP & 52.0 & 77.0 & 85.6 & 37.8 & 64.4 & 74.5 & 81.2 & 95.4 & 97.1 & 66.8 & 87.7 & 92.3 \\\\  & SPARC (ours) & **55.0** & **79.1** & **87.3** & **39.7** & **65.9** & **75.7** & **82.5** & **96.2** & **97.6** & **67.7** & **88.2** & 93.0 \\\\ \\hline \\hline \\multirow{4}{*}{\n' +
      '\\begin{tabular}{c} CLIP \\\\ FILIP \\\\ SPLIC \\\\ SPLIC \\\\ SPARC (ours) \\\\ \\end{tabular} } & CLIP & 56.2 & 80.6 & 88.2 & 42.4 & **68.6** & 78.3 & 84.0 & 96.1 & 98.2 & 71.6 & 90.3 & 94.1 \\\\  & FILIP & 40.2 & 66.0 & 76.3 & 29.5 & 55.3 & 66.3 & 69.0 & 89.8 & 94.0 & 55.8 & 81.5 & 87.9 \\\\  & CLIP + FILIP & 54.9 & 79.0 & 87.4 & 41.3 & 67.7 & 77.5 & 82.7 & 97.0 & 98.4 & 71.1 & 90.5 & 94.7 \\\\  & SPARC (ours) & **57.6** & **81.2** & **88.5** & **43.0** & **68.6** & **78.5** & **84.4** & **97.6** & **98.7** & **72.0** & **91.2** & **94.9** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: MSCOCO 및 Flickr30k 데이터셋에 대한 제로샷 이미지 대 텍스트 및 텍스트 대 이미지 검색에 대한 결과. R@i는 i의 Recall을 나타낸다. 모든 방법은 동일한 수의 훈련 단계에 대해 ALIGN, JFT, LITP에서 훈련되었다.\n' +
      '\n' +
      '### Hyperparameters details\n' +
      '\n' +
      '2500단계의 선형 평가로 코사인 학습률 일정인 AdamW(Loshchilov and Hutter, 2017) 최적기를 사용하여 모든 모델을 훈련합니다. 모든 방법에 대해 \\([7e-4,9e-4,1.1e-4]\\)의 학습률과 \\([0.1,0.2,0.3]\\)의 체중 붕괴라는 학습 속도와 가중치 붕괴 값을 다음 범위에서 쓸어냈다. 우리는 16348의 배치 크기(4096 배치 크기를 사용하는 GLoRIA를 제외)를 사용하고 200k 단계(\\(\\ 승인 3.2\\)에 대해 ViT-B 모델을 사전 균주화한다.\n' +
      '\n' +
      '다른 SPARC 초모수의 경우 전 세계 손실 중량 \\(듐람다_{g}=0.5\\)를 설정하고 \\(\\lambda_{f}\\in[0.5,1.0,10.0]\\)에서 국소 손실 가중치를 스윕했다. 또한, 학습된 온도 매개변수 \\(\\tau\\)를 사용합니다.\n' +
      '\n' +
      '기준선 특정 하이퍼 파라미터의 경우 공개적으로 이용 가능한 코드(사용 가능한 경우)와 원본 논문을 따른다. 본 논문에서 설명한 바와 같이 MGCA(왕 et al, 2022)의 경우, 우리는 서로 다른 손실(\\lambda_{1}=1\\), \\(\\lambda_{2}=1\\), \\(\\lambda_{3}=1\\), 교차 모달 임베딩을 128 임베딩 차원으로 계산하기 위한 관심 헤드의 수를 1로 설정하였다. MGCA의 크로스모달 프로토타입 정렬 손실을 위해 우리는 \\(\\epsilon=0.05\\)가 있는 500개의 프로토타입과 Sinkhorn-Knopp 클러스터링 알고리즘에 대한 3개의 반복을 사용한다.\n' +
      '\n' +
      'FILIP의 경우, 우리는 논문에 설명된 토큰 낙하 절차를 구현하고 실험에서 20% 토큰을 떨어뜨리는 것을 사용한다.\n' +
      '\n' +
      'PACL의 경우, 우리는 최대 한 가지 주목할만한 세부 사항의 구현 측면에서 원지를 밀접하게 따르고 있으며, 이는 성능을 크게 향상시키는 것으로 밝혀졌기 때문에 손실에서 학습 가능한 온도 파라미터를 포함한다.\n' +
      '\n' +
      '제로샷 분류.\n' +
      '\n' +
      '라드포드(2021) 및 야오 등(2021)에 이어 분류 과제에 대한 라벨을 증강하기 위해 신속한 템플릿을 사용한다. 우리는 야오 등(2021)의 프롬프트 템플릿 형식을 사용한다.\n' +
      '\n' +
      '\\{ 컨텐츠{ 클래스 라벨}[문자{suix}]\n' +
      '\n' +
      'I\\([\\text{prefix}]\\)의 경우, 우리는 Radford et al(2021)의 템플릿을 사용한다. 한편, \\([\\text{suffix}]\\)의 경우 야오 등(2021)의 템플릿을 사용하는데, 이는 \'좋아한다\'라는 프롬프트 끝에 기준어 \'it\'를 추가하는 것을 보여준다.\n' +
      '\n' +
      '다양한 방법에 대한 기억 소비와 FLOPS는 서로 다른 방법에 대한 것이다.\n' +
      '\n' +
      '우리는 표 11의 FLOPS(in TFLOPS)와 피크 메모리(in MB)에 대한 자세한 번호를 제공한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline  & \\multicolumn{4}{c}{FLOPS (TFLOPS)} & \\multicolumn{4}{c}{Peak memory (MB)} \\\\ Objective & B = 2048 & B = 4096 & B = 8192 & B = 16384 & B = 2048 & B = 4096 & B = 8192 & B = 16384 \\\\ \\hline \\hline CLIP & 1.15 & 2.29 & 4.57 & 9.14 & 4394 & 4452 & 5889 & 8578 \\\\ PACL & 1.2 & 2.46 & 5.24 & 12.8 & 4682 & 6267 & 9786 & 14785 \\\\ GLoRIA & 3.34 & 13.21 & – & – & 8013 & 13840 & – & – \\\\ MGCA & 1.16 & 2.31 & 4.62 & 9.23 & 4412 & 4462 & 5936 & 8681 \\\\ FILIP & 1.37 & 3.17 & 8.09 & 27.25 & 4394 & 5230 & 8657 & 15463 \\\\ SPARC (ours) & 1.15 & 2.3 & 4.6 & 9.19 & 4408 & 4450 & 5914 & 8620 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 11: TFLOPS 및 다양한 배치 크기에 대한 각 방법의 하나의 업데이트 단계에 대한 피크 메모리 사용이다.\n' +
      '\n' +
      '### Semantic segmentation\n' +
      '\n' +
      '0샷 시맨틱 분할을 위해, 우리는 패치 임베딩을 추가 조밀한 층과 어댑터를 통해 통과시켜 접지-진실 클래스에 대한 텍스트 임베딩과 코사인 유사성을 계산한다. (Mukhoti et al., 2023)와 유사하게 전경 수업에 대해서만 평균 교차로(mIoU)를 계산한다.\n' +
      '\n' +
      '비전 언어 모델에는 SPARC 백본이 있습니다.\n' +
      '\n' +
      '우리는 ALIGN(Jia et al., 2021), LTIP(긴 텍스트 & 이미지 박람회) 및 VTP(비디오 & 텍스트 프로그램 al, 2022) 데이터셋(Alayrac et al., 2022)에 플라이밍고(Alayrac et al.,Alayrac et al., 2022)의 퍼세버 레탐플러 부분을 훈련시킨다. VTP는 텍스트 설명과 짝을 이루는 2,200만 개의 짧은 비디오로 구성되며, 여기서 평균 22개가 되면 각 비디오가이다. 우리는 \\(1e-4\\)의 피크 학습률을 갖는 코사인 학습률 일정인 AdamW 최적화기, 5000개의 평가 단계 및 총 250k 훈련 단계를 사용하여 선형 평가기를 사용한다.\n' +
      '\n' +
      'SPARC 대 CLIP 교정예.\n' +
      '\n' +
      '이미지 내의 요소를 충실히 설명하는 SPARC 및 CLIP 모델의 능력을 더 이해하기 위해 몇 가지 질적 예를 제공한다. 따라서 MSCOCO의 경우 SPARC와 CLIP 모두에 대해 검색된 탑-1이 그라운드 진리 캡션의 일부가 아니라 SPARC가 더 높은 올토큰 \\(\\mathcal{K}\\)-정밀(그림 5) 및 더 높은 \\(\\mathcal{K}\\)-정정이 명사 및 형용사(6)로 제한되는 예시를 선택했다. 이 그림에서 CLIP 표현을 사용하여 검색된 캡션이 이미지 내에 존재하지 않는 객체(예: "바에 대한 출처 징후")를 설명하거나 오토바이 하나가 있을 때 객체 수를 잘못(예: "오토바이 2개") 획득한다는 것을 알 수 있다. 대안적으로, SPARC 표현을 사용하여 검색된 캡션은 이미지에 더 충실하지만, 또한 더 설명적인 세부 사항(예: "화이트 셔츠에 있는 젊은 소년", "장소 설정이 있는 네이너 테이블"을 제공한다.\n' +
      '\n' +
      'SPARC가 명사 및 형용사({}_{\\text{K}}\\)에 국한되어 있는 예를 들어 SPARC 대 CLIP 대 Ground Truth)는\\(\\(\\mathcal{K})가 더 높다.\n' +
      '\n' +
      '그림 5 \\(|\\) SPARC 대 CLIP 대 그라운드 트러트는 SPARC가 더 높은 올토크(\\mathcal{K}\\)-정밀(\\(\\mathcal{K}\\)을 가지고 있다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>