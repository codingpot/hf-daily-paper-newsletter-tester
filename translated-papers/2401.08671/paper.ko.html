<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'MII를 통한 LLM용 고처리량 성능\n' +
      '\n' +
      '컨네르 홀메스, 마스마로 다나카, 제프 라자바다리, 마세 라자단디, 헤양 진, 레자 야자단니 아민바리, 헤라시 바실라리, 아라시 바실라바리, 아라시 바실라바리, 아라시 히아실라바리, 아라시 하우라바리, 아라실라바리, 아라실라바리, 아라실라바리, 아라실라바리, 아라실라바리, 라히아실라바리, 라일라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라세, 라세, 아라세, 라세, 아라세, 라세, 아라세, 라세, 라세, 아라세, 라세, 아라세, 라세, 라\n' +
      '\n' +
      '마이크로소프트 딥스피드(www.데프속도.ai).\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대형 언어 모델(LLM)의 배치 및 스케일링은 높은 처리량과 낮은 용량 서빙 시스템을 요구하며 다양한 애플리케이션에 스며들면서 중요해졌다. 기존의 프레임워크는 이러한 요구 사항, 특히 긴 프롬프트가 있는 업무량에 대한 균형을 맞추기 위해 고군분투한다. 이 논문은 새로운 신속하고 생성 구성 전략인 Dynamic SplitFuse를 사용하는 시스템 딥스피드-Fast 유전자를 도입하여 vLLM과 같은 최첨단 시스템에 비해 최대 2.3배 더 높은 유효 처리량, 평균 2배 낮은 잠복기, 최대 3.7배 낮은(토큰 수준) 꼬리 잠복기를 제공합니다. LLM을 위한 효율적이고 사용하기 쉬운 서빙 시스템을 제공하기 위해 딥스피드-MII와 딥스피드-인딩의 상승적 조합을 레버리니다. 딥스피드-패스트젠의 고급 구현은 다양한 모델을 지원하고 상호 작용 세션에서 장기 실행 애플리케이션까지 다양한 사용자 시나리오에 대한 비지속적이고 지속적인 배치 옵션, 케이터링 모두를 제공한다. 자세한 벤치마킹 방법론을 제시하고 레이턴시 처리량 곡선을 통한 성능을 분석하고 부하 균형을 통해 확장성을 조사한다. 우리의 평가는 다양한 모델 및 하드웨어 구성 전반에 걸쳐 처리량과 지연 시간의 실질적인 개선을 보여준다. 더 넓은 모델 지원과 새로운 하드웨어 백드래그를 포함하여 향후 개선을 위한 로드맵에 대해 논의합니다. 딥스피드-패스트 유전자 코드는 커뮤니티 참여 및 기여에 쉽게 사용할 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'GPT-4 [1] 및 LLaMA[2]와 같은 대형 언어 모델(LLM)은 모든 수준에서 AI가 주입된 광범위한 응용 프로그램을 제공하는 데 있어 지배적 작업량으로 부상했다. 일반 채팅 모델부터 문서 요약, 그리고 자율주행부터 소프트웨어 스택 층마다 코필롯까지 이러한 모델을 스케일로 배치하고 서비스하는 수요가 급증했습니다. 딥스피드, PyTorch[3] 및 기타 여러 것과 같은 프레임워크는 LLM 훈련 동안 정기적으로 좋은 하드웨어 활용을 달성할 수 있지만, 이러한 애플리케이션의 상호 작용 특성과 개방형 텍스트 생성과 같은 작업의 열악한 산술 강도는 기존 시스템에서 추론 처리량을 위한 병목 현상이 되었다.\n' +
      '\n' +
      '이를 위해 오리카[5]와 같은 피지 관심 및 연구 시스템에 의해 구동되는 vLLM[4]와 같은 프레임워크는 LLM에 대한 추론 성능을 크게 향상시켰다. 그러나 이러한 시스템은 특히 더 긴 신속한 작업량에 대해 일관된 서비스 품질을 제공하기 위해 여전히 어려움을 겪고 있다. MPT-스토리워터[6]와 같은 보다 많은 모델과 딥스피드 Ulysses[7], 수만 개의 토큰으로 스트레칭되는 지원 상황 창과 같은 시스템이 점점 더 중요해지고 있다. 문제 공간을 더 잘 이해하기 위해 신속한 처리 및 생성이라는 두 가지 뚜렷한 단계에서 텍스트 생성이 LLM에 어떻게 작용하는지에 대한 자세한 예를 제공한다. 시스템이 이들을 뚜렷한 단계로 취급하면 서비스 수준 협정(SLAs)을 어길 위험이 있는 신속한 처리에 의해 세대가 선점될 것이다.\n' +
      '\n' +
      '오늘날 우리는 제안된 Dynamic SplitFuse 기술을 활용하여 이러한 한계를 극복하고 vLLM과 같은 최첨단 시스템에 비해 최대 2.3배 더 높은 유효 처리량, 평균 2배 낮은 잠복기 및 최대 3.7배 낮은(토큰 수준) 꼬리 잠복기를 제공하는 시스템 딥스피드-Fast 유전자를 소개하게 되어 기쁩니다. 딥스피드-패스트 유전자는 쉽게 사용할 수 있는 서빙 시스템을 제공하기 위해 딥스피드-MII와 딥스피드-인딩의 조합을 강조합니다.\n' +
      '\n' +
      '2개의 문학 기술 제외.\n' +
      '\n' +
      '단일 시퀀스에 대한 텍스트 생성 작업량은 2단계:1) 프롬프트 처리로 구성되며, 사용자가 제공하는 텍스트를 토큰 배치로 효율적으로 처리하여 키-값(KV) 캐시를 구축하며, 2) 토큰 생성을 통해 해당 토큰에 단일 토큰을 추가하고 새로운 토큰을 생성할 것이다. 텍스트의 시퀀스를 생성하는 과정에서 모델은 텍스트의 전체 시퀀스를 생성하기 위해 모델에 많은 전방 통화를 할 것이다. 문헌에서 두 가지 주요 기술이 제안되었으며 이러한 단계에서 발생할 수 있는 다양한 한계와 병목 현상을 해결하는 시스템에 배치되었다.\n' +
      '\n' +
      'KV 교정을 차단했습니다.\n' +
      '\n' +
      'vLLM은 큰 단태성 KV-쿠션으로 인한 메모리 단편화가 LLM 서빙 시스템의 동시화를 크게 감소시켰음을 확인하고 비연속적인 캐시를 가능하게 하고 전체 시스템 처리량을 증가시키기 위해 Poustic 별지[8]을 제안했다. 개별 가변 크기의 메모리 연속 청크를 할당하기보다는 KV 캐시 내의 기본 스토리지는 고정 크기 블록(페이지로 알려져 있다)이다. 차단된 KV-cache는 KV-cache 유도 메모리 단편화를 제거하여 잠재적 서열 동시화의 양을 증가시켜 시스템 처리량을 증가시킨다. 비연속적인 KV 캐시 구현들은 또한 HuggingFace TGI[9] 및 NVIDIA TensorRT-LLM[10]에 포함된다.\n' +
      '\n' +
      '### Continuous Batching\n' +
      '\n' +
      '과거에는 서버가 서로 단계적으로 처리할 다수의 요청을 기다리는 동적 배치가 GPU 활용도를 향상시키는 데 사용되었다. 그러나 이 접근법은 보통 동일한 길이에 대한 패딩 입력 또는 더 큰 배치를 구성하기 위해 시스템이 기다려야 하기 때문에 단점이 있다.\n' +
      '\n' +
      '최근 대형 언어 모델(LLM) 추론과 서빙의 고도화는 미세 과립성 스케줄링과 메모리 효율 최적화에 초점을 맞추고 있다. 예를 들어, Orca는 모델의 각 전방 패스에서 별개의 스케줄링 결정을 내리는 반복 수준 스케줄링(연속 배치로 알려져 있는)을 제안한다. 이를 통해 필요에 따라 배치에 가입/제거할 것을 요청하여 패딩 요청의 필요성을 제거하여 전체 처리량을 향상시킬 수 있다. 오르카 외에도 NVIDIA TRT-LLM, HuggingFace TGI 및 vLLM에서 연속 배치가 구현되었다.\n' +
      '\n' +
      '현재 시스템에서 연속 배치를 구현하기 위한 두 가지 주요 접근법이 있다. TGI 및 vLLM에서 생성 단계는 생성과 함께 지속되기 전에 신속한 처리(TGI의 감염)를 수행하기 위해 선점된다. 오르카에서는 이러한 단계가 구별되지 않으며 대신 오르카는 총 서열 수가 고정된 결합체에 도달하지 않는 한 실행 배치에 프롬프트를 추가할 것이다. 다양한 정도에 대한 이러한 접근 방식은 긴 프롬프트(3.2절 참조)를 처리하기 위해 생성을 서둘러야 한다.\n' +
      '\n' +
      '우리는 다음 섹션에서 길이로 논의된 새로운 신속하고 생성 구성 전략인 Dynamic SplitFuse를 제안한다.\n' +
      '\n' +
      'A Novel Prompt 및 세대 구성 전략\n' +
      '\n' +
      '딥스피드-패스트 유전자는 TRT-LLM, TGI 및 vLLM과 같은 기존 프레임워크와 유사하게 데이터 센터에서 LLM을 서빙하기 위한 증가된 점유 및 더 높은 반응성을 가능하게 하기 위해 연속 배치 및 비연속적인 KV 캐스를 레버리지하기 위해 구축된다. 새로운 수준의 성능을 달성하기 위해 딥스피드-패스트 유전자는 지속적인 배치 및 시스템 처리량을 더욱 개선하기 위해 동적 프롬프트 및 생성 분해 및 통일을 보장하는 SplitFuse를 도입한다.\n' +
      '\n' +
      '3대\n' +
      '\n' +
      'Dynamic SplitFuse를 설명하기 전에 함께 디자인을 동기부여하는 세 가지 주요 성능 질문에 답합니다.\n' +
      '\n' +
      '#### 3.1.1은 단일 LLM의 전진 패스에 어떤 영향을 미치는가?\n' +
      '\n' +
      '효과적인 일정을 잡기 위해서는 스케줄링 루프가 조절해야 할 관련 독립 변수가 무엇인지 이해할 필요가 있다. 우리는 전방 패스(시퀀스의 배치 크기)에서 서열의 구성이 전방 패스 내의 토큰의 원시 수에 비해 성능에 무시할 수 있는 영향을 미친다는 것을 아래에서 관찰한다. 이는 유효 스케줄러가 단일 신호, 전방 패스 내의 토큰의 개수 주위에 구축될 수 있음을 의미한다.\n' +
      '\n' +
      '그림 1: 토켄 레이턴시(ms)는 배치 크기보다는 전방 토큰의 수에 의해 주로 결정된다.\n' +
      '\n' +
      '#### 3.1.2 모델 처리량은 전방 패스에서의 토큰 수를 변경하는 데 어떻게 반응합니까?\n' +
      '\n' +
      'LLM은 상대적으로 급격한 전환을 갖는 두 개의 핵심 운영 영역을 가지고 있다. 소수의 토큰을 사용하여 GPU 병목은 토큰의 수로 메모리 및 그렇게 처리량 스케일에서 모델을 판독하는 반면, 많은 토큰이 있는 경우 모델은 압축에 의해 결합된 처리량이며 근거리 처리량을 보고 있다. 모델은 모든 전방 통과가 처리량-포화 영역에 있는 경우 매우 효율적으로 실행되어야 한다.\n' +
      '\n' +
      '#### 3.1.3은 복수의 전방 통과에 걸쳐 토큰 풀이 어떻게 예정되어 있어야 하는가?\n' +
      '\n' +
      '우리는 잘 정렬된 입력들에 대해 토큰-처리량 곡선이 오목하다는 것을 관찰하는데, 이는 두 번째 유도체가 0보다 작거나 같을 수밖에 없다는 것을 의미하고, 예를 들어 \\(x)\\은 주어진 모델에 대한 처리량에 대한 레이턴시의 오목한 함수임을 의미한다. 오목함수의 경우(f(x)\\는 다음 각 호와 같다.\n' +
      '\n' +
      '\\[0\\geq\\lim_{h\\to 0}\\frac{f(x+h)-2f(x)+f(x-h)}{h^{2}}\\]\n' +
      '\n' +
      '\\[0\\geq f(x+h)-2f(x)+f(x-h)\\]\n' +
      '\n' +
      '\\[2f(x)\\geq f(x+h)+f(x-h)\\]\n' +
      '\n' +
      '이것은 2x 토큰의 주어진 풀이 처리되도록 하기 위해 처리량을 최대화하는 방식이 두 배치 사이에 고르게 갈라지는 것이다. 보다 일반적으로 F 정면에 걸쳐 P 토큰을 소비하고 처리해야 하는 시스템에서 이상적인 분할 방식은 동일하게 구분할 것이다.\n' +
      '\n' +
      '그림 2: 순방향 패스의 토큰 수가 증가함에 따라 시스템은 피크 성능(처리량 포화 영역)에 도달한다. 그 외에도 거의 잘못된 처리량이 관찰된다.\n' +
      '\n' +
      '### Dynamic SplitFuse\n' +
      '\n' +
      '동적 스플라트퓨즈는 신속한 처리 및 토큰 생성을 위한 새로운 토큰 구성 전략이다. 딥스피드-패스트 유전자는 Dynamic SplitFuse를 사용하여 프롬프트에서 부분 토큰을 취하고 이를 생성으로 구성하는 능력을 활용하여 일관된 전방 크기로 실행한다. 사라티[11]에서 유사한 접근법이 제안되어 더 많은 토큰 생성과 신속한 처리를 결합하고 일관된 배치 크기로 전방 통과를 실행하도록 더 작은 청크들로 프롬프트된다. 특히, Dynamic SplitFuse는 두 가지 핵심 행동을 수행한다.\n' +
      '\n' +
      '1. 롱 프롬프트는 훨씬 작은 청크로 분해되고 모든 세대를 수행하는 최종 패스만 있는 여러 전방 패스(문)에 걸쳐 예정되어 있다.\n' +
      '2. 단기 프롬프트는 목표 토큰 예산을 정확히 채울 수 있도록 구성됩니다. 예산이 정확하게 충족되고 전방 크기가 잘 정렬되도록 짧은 프롬프트도 분해될 수 있다.\n' +
      '\n' +
      '함께, 이 두 기술은 모든 사용자 메트릭에 대한 구체적인 이점을 제공한다.\n' +
      '\n' +
      '1. 베터 책임: 긴 프롬프트는 더 이상 처리를 위해 극도로 긴 전방 통과를 필요로 하지 않기 때문에 모델은 더 낮은 클라이언트 레이턴시를 제공할 것이다. 더 많은 전방 패스는 동일한 시간 윈도우 내에서 수행된다.\n' +
      '2. 더 높은 효율성: 짧은 프롬프트의 추가 토큰 예산은 모델이 높은 처리량 체제에서 일관되게 작동할 수 있도록 한다.\n' +
      '3. 낮은 분산과 더 나은 일관성: 전방 통과는 일관된 크기이고 전방 패스 크기가 성능의 주요 결정 요인이기 때문에 인지된 생성 빈도와 마찬가지로 각 전진 패스의 지연은 경쟁 시스템보다 훨씬 더 일관적이다. 다른 이전 작업에서와 같이 지연 시간을 늘리기 위한 선점이나 장기 운영 프롬프트가 없습니다. 이것은 4절에서 알 수 있듯이 세대 내 최대 3.7배 P95 잠복기의 감소로 변환된다.\n' +
      '\n' +
      '그림 3: 연속 배치 전략의 일러스트레이션. 각 블록은 전방 패스의 실행을 보여준다. 화살표는 정방향 패스가 하나 이상의 토큰이 생성된 시퀀스를 가지고 있음을 나타낸다. vLLM은 전방 패스에서 토큰 세대 또는 신속한 처리를 수행하며 토큰 생성은 신속한 처리를 선점한다. 오리카는 세대와 함께 완전한 길이로 신속한 성능을 발휘합니다. 동적 스플라트퓨즈는 세대 및 프롬프트 토큰으로 구성된 고정 크기의 배치의 동적 구성을 수행한다.\n' +
      '\n' +
      '결과적으로 딥스피드-패스트 유전자는 시스템 활용을 증가시키는 시스템에 토큰을 추가하여 다른 최첨단 서빙 시스템에 비해 모든 고객에게 더 낮은 레이턴시 및 더 높은 처리량 스트리밍 생성을 제공하는 동시에 빠른 지속적인 생성을 허용하는 속도로 들어오는 프롬프트에서 토큰을 소비할 것이다.\n' +
      '\n' +
      '4개의 성과 평가.\n' +
      '\n' +
      '딥스피드-패스트 유전자는 차단된 KV 캐시 및 Dynamic SplitFuse 연속 배치를 활용하는 최첨단 LLM 서빙 성능을 제공합니다. 아래에 논의된 벤치마킹 방법론에 따라 다양한 모델 및 하드웨어 구성에서 vLLM [4]에 대한 딥스피드-패스트 유전자를 평가한다. 평가는 딥스피드-패스트 유전자가 vLLM과 같은 최첨단 시스템에 비해 최대 2.3배 더 높은 유효 처리량, 평균 2배 낮은 잠복기, 최대 3.7배 낮은(토큰 수준) 꼬리 잠복기를 달성한다는 것을 보여준다.\n' +
      '\n' +
      '### Benchmarking Methodology\n' +
      '\n' +
      '우리는 성능을 측정하기 위해 두 가지 주요 정량적 계획을 사용한다.\n' +
      '\n' +
      '물량-정밀성 큐브 4.1.1.1.1도 측정기#####\n' +
      '\n' +
      '생산 준비도에 대한 두 가지 주요 메트릭은 처리량(초당 요청으로 측정) 및 지연 시간(각 요청의 응답성)이다. 이를 측정하기 위해 여러 고객(1에서 32로 변경)을 동시에 인스턴스하고 요청(총512개)을 서버에 전송한다. 각 요청의 결과 지연 시간은 엔드포인트에서 측정되고 처리량은 실험을 완료하기 위해 엔드 투 엔드 시간으로 측정된다.\n' +
      '\n' +
      '그림 4.1.2.2의 효과적 수율 수율#####\n' +
      '\n' +
      '채팅 애플리케이션과 같은 상호 작용 애플리케이션은 엔드 투 엔드 레이턴시와 같은 최상위 메트릭에 의해 캡처될 수 있는 것보다 더 엄격하고 복잡한 요구 사항을 가질 수 있다. 특히 점점 더 인기 있는 채팅 사용자 시나리오에 초점을 맞추고 있습니다.\n' +
      '\n' +
      '1. A 사용자는 프롬프트를 보내 작업을 시작한다.\n' +
      '2. 시스템은 프롬프트를 처리하고 첫 번째 토큰을 반환한다.\n' +
      '3. 후속 토큰은 생산됨에 따라 사용자에게 스트리밍된다.\n' +
      '\n' +
      '이 과정에서 각 지점에서 시스템이 부정적인 사용자 경험을 제공할 수 있는 기회가 있으며, 예를 들어, 제1 토큰이 너무 천천히 도착하거나 세대가 한동안 멈추는 것으로 보이는 경우이다. 우리는 이 두 차원을 모두 고려하는 SLA 프레임워크를 제안한다.\n' +
      '\n' +
      '신속하고 생성된 텍스트의 길이가 크게 달라 계산 비용에 영향을 미치므로 처리량과 대기 시간에 대한 단단한 SLA 값을 설정하는 것은 불가능하다. 따라서 신속한 지연에 대한 SLA를 \\(신속한 경우\\frac{-\\text{tokens,}}{512}\\) 초(=512 토큰/s)로 정의한다. 또한 인간의 읽기 속도를 고려하여 지수 이동 평균(EMA)에서 생성 잠복기를 위한 SLA를 2, 4 또는 6 토큰/sec로 설정했다. 이러한 SLFA에 부착하는 요청은 성공적인 것으로 간주되며 이러한 성공적인 요청의 처리량은 ** 효율적인 처리량**라고 한다.\n' +
      '\n' +
      '우리는 NVIDIA A100, H100 및 A6000에서 Llama-2 7B, Llama-2 13B 및 Llama-2 70B [2] 모두에서 vLLM 및 딥스피드-Fast 유전자를 평가한다.\n' +
      '\n' +
      '### Throughput-Latency Analysis\n' +
      '\n' +
      '이 실험에서 딥스피드-패스트 유전자는 처리량과 잠복기 모두에서 vLLM을 능가하여 더 큰 처리량 이상의 반응성 잠복기와 동일한 처리량을 갖는 동등한 지연 시간을 제공한다. 4 A100x80GB가 있는 라마-2 70B에서 딥스피드-패스트 유전자는 동일한 지연 시간(9초) 또는 최대 50% 잠복기 감소(7초 대 0.67 rps)에서 최대 2배 더 높은 처리량(1.36 rps 대 0.67 rps)을 보여준다. 그림 2와 같이 동일한 처리량(1.2 rps)을 달성하면서 14초)를 이뤘는데, 이 경향은 그림 3과 같이 라마-2 13B를 평가할 때 유지된다.\n' +
      '\n' +
      '효과.\n' +
      '\n' +
      '첫 번째 토큰 레이턴시와 생성이 발생하는 비율을 모두 고려하는 효과적인 처리량 분석에서 딥스피드-패스트 유전자는 vLLM보다 최대 2.3배 더 높은 처리량을 제공한다. 그림 4는 딥스피드-패스트유전자 및 vLLM의 유효 처리량에 대한 비교 분석을 제시한다. 각 도표 포인트는 유효 트로를 나타낸다.\n' +
      '\n' +
      '그림 4: 라마 2 70B(4 A100-80GB GPU에 걸친 센서 병렬주의)를 사용한 텍스트 생성의 산출 및 대기 시간을 나타낸다. 평균이 각각 1200/2600 및 128/60이고 분산이 30%인 신속한 및 생성 길이에 정규 분포를 적용했다.\n' +
      '\n' +
      '그림 5: 라마 2 13B(A100-80GB GPU, 텐서 병렬주의 없음)를 이용한 텍스트 생성의 산출 및 대기 시간을 나타낸다. 평균이 각각 1200/2600 및 60/128이고 분산이 30%인 신속한 및 생성 길이에 정규 분포를 적용했다.\n' +
      '\n' +
      '고객 수를 참조하세요. 우리는 고객의 수를 확장했기 때문에 초기에 효과적인 처리량의 증가를 관찰했다. 그러나 클라이언트가 시스템의 능력에 접근함에 따라 잠복기도 크게 증가하여 SLA를 충족시키는 데 많은 요청이 실패한다. 결과적으로 효과적인 처리량은 어느 시점에서 포화되거나 감소할 것이다. 사용성 관점에서 볼 때, 최대 유효 처리량을 달성하기 위해 몇 명의 고객이 필요한지는 특별히 관련이 없으며, 선의 최대 포인트는 최적의 서빙 포인트이다.\n' +
      '\n' +
      '투넨세대에 대한 특성의 저하.\n' +
      '\n' +
      '그림 5는 생성 과정의 P50, P90 및 P95 래치들을 표시한다. vLLM과 딥스피드-패스트 유전자는 모두 유사한 P50 래치들을 나타내지만 vLLM은 P90 및 P95에 대해 상당히 더 높은 래치들을 보여주며 P95 래치들을 행사하며 딥스피드-패스트 유전자는 3.7배 감소를 달성했다.\n' +
      '\n' +
      '이러한 불일치는 새로운 프롬프트를 처리하기 위해 진행 중인 세대를 선점할 때 vLLM의 생성 레이턴시의 눈에 띄는 스파이크 때문이다. 대조적으로, 딥스피드-패스트 유전자는 보통 이전 요청에 대한 신속하고 생성을 동시에 처리하여 훨씬 더 일관된 생성 잠복기로 이어진다.\n' +
      '\n' +
      '로드 발유행을 이용한 수용성.\n' +
      '\n' +
      '딥스피드-패스트 유전자는 여러 서버에 걸쳐 요청을 고르게 분산시키는 복제 수준 부하 균형을 제공하여 애플리케이션을 쉽게 스케일링할 수 있습니다. 그림 6은 부하 밸런서와 최대 16개의 복제물을 사용할 때 딥스피드-패스트 유전자의 확장성을 보여준다. 우리는 4개의 A100 GPU를 사용하여 라마 2 70B 모델을 계산했습니다. 총 8개의 노드를 사용하여 16개의 복제물을 실행했다. 결과는 딥스피드-패스트 유전자와 거의 완벽한 확장성을 보여준다. 단일 복제물의 처리량이 1.46 쿼리/sec인 점을 감안할 때 16개의 복제물을 사용한 처리량은 23.7 쿼리/sec에 도달하여 단일 복제물에 비해 선형 16배 증가를 나타낸다.\n' +
      '\n' +
      '하나웨어 플랫폼 외에요.\n' +
      '\n' +
      'A100에 대한 심층 분석 외에도 H100 및 A6000에 대한 추가 벤치마킹 결과를 제공하고 A6000과 H100 모두에서 A100과 동일한 성능 경향이 관찰되었다.\n' +
      '\n' +
      '그림 6: 딥스피드-패스트유전자 및 vLLM의 효과적 처리량(4 A100-80GB GPU에 걸쳐 텐서 병렬성을 사용하여 라마 2 70B/A100-80GB)은 평균 2600 및 60의 프롬프트 및 생성 길이에 각각 정규 분포를 적용했다.\n' +
      '\n' +
      '그림 8: 부하 밸런싱 기능을 사용한 계산가능성. 평균 2600 및 60과 30% 분산을 갖는 신속한 및 생성 길이에 정규 분포를 적용했다.\n' +
      '\n' +
      '그림 7:Llama 2 70B/A100-80GB의 Per-Token 생성 빈곤도는 4개의 A100-80GB GPU에 걸쳐 텐서 병렬성을 사용하여 16명의 클라이언트이다. 평균 2600 및 128과 30% 분산을 갖는 신속 및 생성 길이에 정규 분포를 적용했다.\n' +
      '\n' +
      '그림 9: 최저 지수 곡선 및 8 H100 GPU를 사용하여 라마 2 70b의 효과적인 처리량을 나타낸다. 평균 2600 및 60과 30% 분산을 갖는 신속한 및 생성 길이에 정규 분포를 적용했다.\n' +
      '\n' +
      '이행 및 폐기물 5개의 딥스피드-패스트.\n' +
      '\n' +
      '딥스피드-패스트젠은 아래 그림과 같이 딥스피드-MII와 딥스피드-반도의 상승적 구성이다. 함께, 이 두 소프트웨어 패키지 모두 프런트 API, 호스트 및 디바이스 인프라를 포함한 시스템의 다양한 구성 요소를 제공하여 Dynamic SplitFuse, 최적화된 커널 구현 및 새로운 모델 구현들을 구성하기 위한 도구를 사용하여 배치들을 스케줄링한다.\n' +
      '\n' +
      '딥스피드-패스트젠의 알파 방출로 시작하는 가장 빠른 방법은 다음 명령을 실행합니다.\n' +
      '\n' +
      '``` pipinstalldeepseed-mii\n' +
      '\n' +
      '그림 11: 딥스피드-패스트포트 건축\n' +
      '\n' +
      '그림 10: A6000을 사용하여 라마 2 7b의 가뭄-지연 곡선 및 효과적인 처리량은 정상 분포를 각각 평균 2600 및 60의 신속한 및 생성 길이와 30% 분산에 적용했다.\n' +
      '\n' +
      '### Supported Models\n' +
      '\n' +
      '우리는 현재 딥스피드패스트젠의 알파 방출에서 다음 HuggingFace 모델 계열1을 지원한다.\n' +
      '\n' +
      '폐지 1: [국무신경부진 표면.코/모듈] (국무신경부진 표면.코/모듈)\n' +
      '\n' +
      'LLaMA-2* LLaMA 및 LLaMA-2* LLaMA 및 LLaMA-2* LLaMA 및 LLaMA-2* LLaMA 및 LLaMA-2* LLaMA 및 LLaMA-2* LLaMA 및 LLaMA-2* LLaMA 및 LLaMA 및 LLaMA-2* LLaMA 및 LLaMA-2* LLaMA 및 LLaMA 및 LLaMA-2* LLaMA 및 LLaMA-2* LLaMA 및 LLaMA 및 LLaMA-2* LLaMA 및 LLaMA 및 LLaMA-2* LLaMA 및 LLaMA 및 LLaMA-2* LLaMA 및 LLaMA 및 LLaMA 및 LLaMA-2* LLaMA 및 LLaMA 및 LLaMA-2* LLaMA 및 LLaMA 및 LLaMA-2* LLaMA 및 LLaMA 및 LLaMA 및 LLaMA 및 LLaMA-2* LLaMA 및 LLaMA 및 LLaMA 및 LLaMA-2* LLaMA 및 LLaMA 및 LLaMA 및 LLa\n' +
      '* Mistral\n' +
      '페이스북 OPT.\n' +
      '\n' +
      '모든 현재 모델은 모델 가중치 및 모델의 해당 토큰라이저를 모두 제공하기 위해 백엔드에서 HuggingFace API를 레버리니다. 초기 출시 후 몇 주, 몇 달 후에 추가 모델을 추가할 계획입니다. 지원하고 싶은 구체적인 모델 아키텍처가 있다면 문제를 제기하여 알려주시기 바랍니다.\n' +
      '\n' +
      '### Deployment Options\n' +
      '\n' +
      '아래의 모든 예들은 딥스피드 예들에 있어서 실행가능하다. 일단 설치하면 상호 작용적인 비지속적 파이프라인 또는 지속적인 서빙 배치의 두 가지 옵션이 있습니다.\n' +
      '\n' +
      '비지속적 Pipeline Pipeline#### 5.2.1.2.1 비지속적 Pipeline Pipeline Pipeline#### 5.2.1 비지속적 Pipeline Pipeline##############\n' +
      '\n' +
      '지속되지 않는 파이프라인 배치는 시작하기에 훌륭하고 빠른 방법이며 몇 개의 코드 라인만으로 수행할 수 있습니다. 지속되지 않은 모델은 달리고 있는 피스톤 스크립트의 기간에만 있지만 일시적인 상호 작용 세션에 유용하다.\n' +
      '\n' +
      '```\n' +
      '미모포트피라인 파이프=피라인\'("미스트랄라이/미스트랄-7B-v0.1") 출력=페이프\'([안녕하세요,my-tois"],],max_new_tokens=128) 인쇄(출력)\n' +
      '```\n' +
      '\n' +
      '지속적 고용 고용##### 5.2.2.\n' +
      '\n' +
      '지속적인 배치는 장기 운영 및 생산 애플리케이션과 함께 사용하기에 이상적입니다. 지속적 배치는 다음 2개의 라인을 이용하여 생성될 수 있는 경량 GRPC 서버를 사용한다.\n' +
      '\n' +
      '```\n' +
      'importmii mi.serve("mistralai/Mistral-7B-v0.1")\n' +
      '```\n' +
      '\n' +
      '위 서버는 딥스피드-MII의 내장 로드 밸런서 덕분에 여러 클라이언트가 한번에 조회할 수 있습니다. 클라이언트를 만드는 것 또한 2개의 코드를 취합니다.\n' +
      '\n' +
      '```\n' +
      '액세서리("딥스피드리스",max_new_tokens=128) 인쇄(출력)\n' +
      '```\n' +
      '\n' +
      '더 이상 필요하지 않을 때 지속적인 배치가 종료될 수 있습니다.\n' +
      '\n' +
      '``` client.terminate_server()\n' +
      '\n' +
      '안내 정보.\n' +
      '\n' +
      '사용 용이성과 이 공간에서 많은 프로젝트가 필요로 하는 긴 컴파일 시간의 상당한 감소를 위해 딥스피드-커날스라는 새로운 라이브러리를 통해 대부분의 맞춤형 낟알을 덮는 미리 내장된 파이썬 휠을 배포한다. 이 라이브러리는 계산 능력 8.0+(Ampere+), CUDA 11.6+ 및 Ubuntu 20+가 있는 NVIDIA GPU가 있는 환경에서 매우 휴대할 수 있음을 발견했다. 대부분의 경우 딥스피드-MII의 종속성이므로 이 라이브러리가 존재한다는 것을 알아야 하며 이를 설치할 것이다. 그러나 어떤 이유로든 낟알을 수동으로 구성해야 하는 경우 고급 설치 도크를 보실 수 있습니다.\n' +
      '\n' +
      '6 레이싱.\n' +
      '\n' +
      '이 딥스피드-패스트 유전자 알파 방출을 공유하게 되어 매우 기쁩니다. 시작하려면 딥스피드-MII용 GitHub 랜딩 페이지로 방문하십시오.\n' +
      '\n' +
      '딥스피드-패스트젠은 다단계 딥 러닝 시스템 및 모델링 기술을 포함하는 더 큰 딥스피드 생태계의 일부이다. 좀 더 배우려면.\n' +
      '\n' +
      '** 자세한 블로그 게시물, 튜토리얼 및 도움이 되는 문서를 위해 당사 웹사이트를 방문하십시오.\n' +
      '*는 딥스피드에 대한 최신 뉴스를 위해 영어 트위터, 일본 트위터, 중국 지후에서도 따라갈 수 있습니다.\n' +
      '\n' +
      '심플한 느낌을 받아 기여금을 환영했습니다! 문제를 보고하고 PR에 기여하고 딥스피드 GitHub 페이지에 대한 논의에 동참하도록 권장드립니다. 자세한 내용은 기여 가이드를 참조하십시오. 딥러닝 연구에 함께 일하는 대학, 연구실, 기업 등과 협력, 실제 AI 모델 및 응용 분야에 힘을 실어주기 위해 딥스피드를 적용하는 등의 협력을 할 수 있습니다. 이러한 요청(GitHub에 적합하지 않은 다른 요청)을 위해 딥러닝-info@m마이크로소프트.com으로 직접 이메일을 보내주십시오.\n' +
      '\n' +
      '### Roadmap\n' +
      '\n' +
      '다음의 항목은 로드맵에 있으며 GitHub 이슈와 PR을 통해 커뮤니티와 함께 할 계획입니다.\n' +
      '\n' +
      '개선\n' +
      '브더기 모델 지원 지원* 브로드머 모델 지원 지원\n' +
      '뉴 하드웨어는 파트너와의 협업을 통해 되돌아간다.\n' +
      '* 릴리스 성능 벤치마크(이 블로그에서 플롯을 생성하는 데 사용)\n' +
      '\n' +
      '"**" 스타" 저희 딥스피드 GitHub와 딥스피드MII GitHub 마우스**입니다!\n' +
      '\n' +
      '## Acknowledgment\n' +
      '\n' +
      '딥스피드-패스트젠 소프트웨어 개발, 디버깅, 테스트 및 출시에 기여한 딥스피드 팀 전체에 감사드립니다. 휴깅페이스, vLLM, 휴깅페이스 TGI 등 다양한 오픈소스 커뮤니티 프로젝트에 감사드린다. 알파 방출에서 모델과 토큰라이저를 지원하기 위해 레버리지된HF API를 보유하고 있으며 더 많은 모델을 계속 추가할 것입니다. 우리는 특히 플래시표시 개발자[12, 13]가 훌륭한 일에 대해 인정하고 감사를 표합니다. 우리는 적절한 파일 헤드에서 코드 레귤레이터에서 인정된 수정과 함께 시스템의 광범위하게 레버리지된 플래시 주의 커널을 가지고 있다. 마지막으로, 우리는 MoE 낟알(딥스피드-커날스 저장소의 일부로 환원)에 사용한 캐스터 트랜스포머[14] 커널 개발자에게 감사의 인사를 전합니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] OpenAI. Gpt-4. [https://openai.com/gpt-4](https://openai.com/gpt-4), 2023.\n' +
      '* [2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [3] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library, 2019.\n' +
      '* [4] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. _arXiv preprint arXiv:2309.06180_, 2023.\n' +
      '* [5] GI Yu, Jeong JSeong, GW Kim, S Kim, and BG Chun. Orca: A distributed serving system for {Transformer-Based} generative models. 2022.\n' +
      '* [6] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms. [https://www.mosaicml.com/blog/mpt-7b](https://www.mosaicml.com/blog/mpt-7b), 2023.\n' +
      '* [7] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models. _arXiv preprint arXiv:2309.14509_, 2023.\n' +
      '* [8] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. _arXiv preprint arXiv:2309.06180_, 2023.\n' +
      '* [9] HuggingFace. Text generation inference. [https://huggingface.co/text-generation-inference](https://huggingface.co/text-generation-inference).\n' +
      '* [10] NVIDIA. Nvidia tensort-llm: A tensortt toolbox for large language model. [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM).\n' +
      '* [11] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, and Ramachandran Ramjee. Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills. _arXiv preprint arXiv:2308.16369_, 2023.\n' +
      '* [12] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In _Advances in Neural Information Processing Systems_, 2022.\n' +
      '\n' +
      '* [13] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.\n' +
      '* [14] NVIDIA. Fastertransformer. [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>