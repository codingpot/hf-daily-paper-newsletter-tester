<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'MII를 통한 LLM용 고처리량 성능\n' +
      '\n' +
      '컨네르 홀메스, 마스마로 다나카, 제프 라자바다리, 마세 라자단디, 헤양 진, 레자 야자단니 아민바리, 헤라시 바실라리, 아라시 바실라바리, 아라시 바실라바리, 아라시 히아실라바리, 아라시 하우라바리, 아라실라바리, 아라실라바리, 아라실라바리, 아라실라바리, 아라실라바리, 라히아실라바리, 라일라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라실라바리, 라세, 라세, 아라세, 라세, 아라세, 라세, 아라세, 라세, 라세, 아라세, 라세, 아라세, 라세, 라\n' +
      '\n' +
      '마이크로소프트 딥스피드(www.데프속도.ai).\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대형 언어 모델(LLM)의 배치 및 스케일링은 높은 처리량과 낮은 용량 서빙 시스템을 요구하며 다양한 애플리케이션에 스며들면서 중요해졌다. 기존의 프레임워크는 이러한 요구 사항, 특히 긴 프롬프트가 있는 업무량에 대한 균형을 맞추기 위해 고군분투한다. 이 논문은 새로운 신속하고 생성 구성 전략인 Dynamic SplitFuse를 사용하는 시스템 딥스피드-Fast 유전자를 도입하여 vLLM과 같은 최첨단 시스템에 비해 최대 2.3배 더 높은 유효 처리량, 평균 2배 낮은 잠복기, 최대 3.7배 낮은(토큰 수준) 꼬리 잠복기를 제공합니다. LLM을 위한 효율적이고 사용하기 쉬운 서빙 시스템을 제공하기 위해 딥스피드-MII와 딥스피드-인딩의 상승적 조합을 레버리니다. 딥스피드-패스트젠의 고급 구현은 다양한 모델을 지원하고 상호 작용 세션에서 장기 실행 애플리케이션까지 다양한 사용자 시나리오에 대한 비지속적이고 지속적인 배치 옵션, 케이터링 모두를 제공한다. 자세한 벤치마킹 방법론을 제시하고 레이턴시 처리량 곡선을 통한 성능을 분석하고 부하 균형을 통해 확장성을 조사한다. 우리의 평가는 다양한 모델 및 하드웨어 구성 전반에 걸쳐 처리량과 지연 시간의 실질적인 개선을 보여준다. 더 넓은 모델 지원과 새로운 하드웨어 백드래그를 포함하여 향후 개선을 위한 로드맵에 대해 논의합니다. 딥스피드-패스트 유전자 코드는 커뮤니티 참여 및 기여에 쉽게 사용할 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Large language models (LLMs) like GPT-4 [1] and LLaMA [2] have emerged as a dominant workload in serving a wide range of applications infused with AI at every level. From general chat models to document summarization, and from autonomous driving to copilots at every layer of the software stack, the demand to deploy and serve these models at scale has skyrocketed. While frameworks like DeepSpeed, PyTorch [3], and several others can regularly achieve good hardware utilization during LLM training, the interactive nature of these applications and the poor arithmetic intensity of tasks like open-ended text generation have become the bottleneck for inference throughput in existing systems.\n' +
      '\n' +
      'To this end, frameworks like vLLM [4] powered by PagedAttention and research systems like Orca [5] have significantly improved the performance of inference for LLMs. However, these systems still struggle to provide consistent quality of service, particularly for workloads with longer prompts. These long prompt workloads are becoming increasingly important as more and more models, like MPT-StoryWriter [6], and systems, such as DeepSpeed Ulysses [7],support context windows stretching to tens of thousands of tokens. To better understand the problem space, we provide detailed examples of how text generation works for LLMs in two distinct phases called prompt processing and generation. When systems treat them as distinct phases, generation will be preempted by prompt processing that risks breaking the service level agreements (SLAs).\n' +
      '\n' +
      '오늘날 우리는 제안된 Dynamic SplitFuse 기술을 활용하여 이러한 한계를 극복하고 vLLM과 같은 최첨단 시스템에 비해 최대 2.3배 더 높은 유효 처리량, 평균 2배 낮은 잠복기 및 최대 3.7배 낮은(토큰 수준) 꼬리 잠복기를 제공하는 시스템 딥스피드-Fast 유전자를 소개하게 되어 기쁩니다. 딥스피드-패스트 유전자는 쉽게 사용할 수 있는 서빙 시스템을 제공하기 위해 딥스피드-MII와 딥스피드-인딩의 조합을 강조합니다.\n' +
      '\n' +
      '2개의 문학 기술 제외.\n' +
      '\n' +
      '단일 시퀀스에 대한 텍스트 생성 작업량은 2단계:1) 프롬프트 처리로 구성되며, 사용자가 제공하는 텍스트를 토큰 배치로 효율적으로 처리하여 키-값(KV) 캐시를 구축하며, 2) 토큰 생성을 통해 해당 토큰에 단일 토큰을 추가하고 새로운 토큰을 생성할 것이다. 텍스트의 시퀀스를 생성하는 과정에서 모델은 텍스트의 전체 시퀀스를 생성하기 위해 모델에 많은 전방 통화를 할 것이다. 문헌에서 두 가지 주요 기술이 제안되었으며 이러한 단계에서 발생할 수 있는 다양한 한계와 병목 현상을 해결하는 시스템에 배치되었다.\n' +
      '\n' +
      'KV 교정을 차단했습니다.\n' +
      '\n' +
      'vLLM은 큰 단태성 KV-쿠션으로 인한 메모리 단편화가 LLM 서빙 시스템의 동시화를 크게 감소시켰음을 확인하고 비연속적인 캐시를 가능하게 하고 전체 시스템 처리량을 증가시키기 위해 Poustic 별지[8]을 제안했다. 개별 가변 크기의 메모리 연속 청크를 할당하기보다는 KV 캐시 내의 기본 스토리지는 고정 크기 블록(페이지로 알려져 있다)이다. 차단된 KV-cache는 KV-cache 유도 메모리 단편화를 제거하여 잠재적 서열 동시화의 양을 증가시켜 시스템 처리량을 증가시킨다. 비연속적인 KV 캐시 구현들은 또한 HuggingFace TGI[9] 및 NVIDIA TensorRT-LLM[10]에 포함된다.\n' +
      '\n' +
      '### Continuous Batching\n' +
      '\n' +
      '과거에는 서버가 서로 단계적으로 처리할 다수의 요청을 기다리는 동적 배치가 GPU 활용도를 향상시키는 데 사용되었다. 그러나 이 접근법은 보통 동일한 길이에 대한 패딩 입력 또는 더 큰 배치를 구성하기 위해 시스템이 기다려야 하기 때문에 단점이 있다.\n' +
      '\n' +
      'Recent advancement in large language model (LLM) inference and serving has been focusing on fine granularity scheduling and optimizing memory efficiency. For instance, Orca proposes iteration-level scheduling (also known as continuous batching) which makes distinct scheduling decisions at each forward pass of the model. This allows requests to join/leave the batch as needed, eliminating the need for padding requests thus improving the overall throughput. In addition to Orca, continuous batching has been implemented in NVIDIA TRT-LLM, HuggingFace TGI, and vLLM.\n' +
      '\n' +
      'In current systems, there are two primary approaches to implement continuous batching. In TGI and vLLM, the generation phase is preempted to perform prompt processing (called infill in TGI) before continuing with generation. In Orca, these phases are not distinguished; instead, Orca will add a prompt into the running batch so long as the total number of sequencesdoesn\'t reach a fixed bound. These approaches to varying degrees need to stall generation to process long prompts (see Section 3.2).\n' +
      '\n' +
      '우리는 다음 섹션에서 길이로 논의된 새로운 신속하고 생성 구성 전략인 Dynamic SplitFuse를 제안한다.\n' +
      '\n' +
      'A Novel Prompt 및 세대 구성 전략\n' +
      '\n' +
      '딥스피드-패스트 유전자는 TRT-LLM, TGI 및 vLLM과 같은 기존 프레임워크와 유사하게 데이터 센터에서 LLM을 서빙하기 위한 증가된 점유 및 더 높은 반응성을 가능하게 하기 위해 연속 배치 및 비연속적인 KV 캐스를 레버리지하기 위해 구축된다. 새로운 수준의 성능을 달성하기 위해 딥스피드-패스트 유전자는 지속적인 배치 및 시스템 처리량을 더욱 개선하기 위해 동적 프롬프트 및 생성 분해 및 통일을 보장하는 SplitFuse를 도입한다.\n' +
      '\n' +
      '3대\n' +
      '\n' +
      'Dynamic SplitFuse를 설명하기 전에 함께 디자인을 동기부여하는 세 가지 주요 성능 질문에 답합니다.\n' +
      '\n' +
      '#### 3.1.1은 단일 LLM의 전진 패스에 어떤 영향을 미치는가?\n' +
      '\n' +
      '효과적인 일정을 잡기 위해서는 스케줄링 루프가 조절해야 할 관련 독립 변수가 무엇인지 이해할 필요가 있다. 우리는 전방 패스(시퀀스의 배치 크기)에서 서열의 구성이 전방 패스 내의 토큰의 원시 수에 비해 성능에 무시할 수 있는 영향을 미친다는 것을 아래에서 관찰한다. 이는 유효 스케줄러가 단일 신호, 전방 패스 내의 토큰의 개수 주위에 구축될 수 있음을 의미한다.\n' +
      '\n' +
      '그림 1: 토켄 레이턴시(ms)는 배치 크기보다는 전방 토큰의 수에 의해 주로 결정된다.\n' +
      '\n' +
      '#### 3.1.2 모델 처리량은 전방 패스에서의 토큰 수를 변경하는 데 어떻게 반응합니까?\n' +
      '\n' +
      'LLM은 상대적으로 급격한 전환을 갖는 두 개의 핵심 운영 영역을 가지고 있다. 소수의 토큰을 사용하여 GPU 병목은 토큰의 수로 메모리 및 그렇게 처리량 스케일에서 모델을 판독하는 반면, 많은 토큰이 있는 경우 모델은 압축에 의해 결합된 처리량이며 근거리 처리량을 보고 있다. 모델은 모든 전방 통과가 처리량-포화 영역에 있는 경우 매우 효율적으로 실행되어야 한다.\n' +
      '\n' +
      '#### 3.1.3은 복수의 전방 통과에 걸쳐 토큰 풀이 어떻게 예정되어 있어야 하는가?\n' +
      '\n' +
      '우리는 잘 정렬된 입력들에 대해 토큰-처리량 곡선이 오목하다는 것을 관찰하는데, 이는 두 번째 유도체가 0보다 작거나 같을 수밖에 없다는 것을 의미하고, 예를 들어 \\(x)\\은 주어진 모델에 대한 처리량에 대한 레이턴시의 오목한 함수임을 의미한다. 오목함수의 경우(f(x)\\는 다음 각 호와 같다.\n' +
      '\n' +
      '\\[0\\geq\\lim_{h\\to 0}\\frac{f(x+h)-2f(x)+f(x-h)}{h^{2}}\\]\n' +
      '\n' +
      '\\[0\\geq f(x+h)-2f(x)+f(x-h)\\]\n' +
      '\n' +
      '\\[2f(x)\\geq f(x+h)+f(x-h)\\]\n' +
      '\n' +
      '이것은 2x 토큰의 주어진 풀이 처리되도록 하기 위해 처리량을 최대화하는 방식이 두 배치 사이에 고르게 갈라지는 것이다. 보다 일반적으로 F 정면에 걸쳐 P 토큰을 소비하고 처리해야 하는 시스템에서 이상적인 분할 방식은 동일하게 구분할 것이다.\n' +
      '\n' +
      '그림 2: 순방향 패스의 토큰 수가 증가함에 따라 시스템은 피크 성능(처리량 포화 영역)에 도달한다. 그 외에도 거의 잘못된 처리량이 관찰된다.\n' +
      '\n' +
      '### Dynamic SplitFuse\n' +
      '\n' +
      '동적 스플라트퓨즈는 신속한 처리 및 토큰 생성을 위한 새로운 토큰 구성 전략이다. 딥스피드-패스트 유전자는 Dynamic SplitFuse를 사용하여 프롬프트에서 부분 토큰을 취하고 이를 생성으로 구성하는 능력을 활용하여 일관된 전방 크기로 실행한다. 사라티[11]에서 유사한 접근법이 제안되어 더 많은 토큰 생성과 신속한 처리를 결합하고 일관된 배치 크기로 전방 통과를 실행하도록 더 작은 청크들로 프롬프트된다. 특히, Dynamic SplitFuse는 두 가지 핵심 행동을 수행한다.\n' +
      '\n' +
      '1. Bett erRespo nsi 정맥:Ngerolo ngerre 쿼리에 있는 Spr ompt scel:\n' +
      '2. 단기 프롬프트는 목표 토큰 예산을 정확히 채울 수 있도록 구성됩니다. 예산이 정확하게 충족되고 전방 크기가 잘 정렬되도록 짧은 프롬프트도 분해될 수 있다.\n' +
      '\n' +
      '함께, 이 두 기술은 모든 사용자 메트릭에 대한 구체적인 이점을 제공한다.\n' +
      '\n' +
      '1. Better Responsiveness: Since long prompts no longer require extremely long forward passes to process, the model will provide lower client latency. More forward passes are performed within the same window of time.\n' +
      '2. 더 높은 효율성: 짧은 프롬프트의 추가 토큰 예산은 모델이 높은 처리량 체제에서 일관되게 작동할 수 있도록 한다.\n' +
      '3. Lower variance and better consistency: Since forward passes are of consistent size and forward pass size is the primary determinant of performance, the latency of each forward pass is much more consistent than competing systems as is the perceived generation frequency. There are no preemption or long-running prompts to increase the latency as in other prior work. This translates to a reduction of up to 3.7x P95 latency in generation as we show in Section 4.\n' +
      '\n' +
      '그림 3: 연속 배치 전략의 일러스트레이션. 각 블록은 전방 패스의 실행을 보여준다. 화살표는 정방향 패스가 하나 이상의 토큰이 생성된 시퀀스를 가지고 있음을 나타낸다. vLLM은 전방 패스에서 토큰 세대 또는 신속한 처리를 수행하며 토큰 생성은 신속한 처리를 선점한다. 오리카는 세대와 함께 완전한 길이로 신속한 성능을 발휘합니다. 동적 스플라트퓨즈는 세대 및 프롬프트 토큰으로 구성된 고정 크기의 배치의 동적 구성을 수행한다.\n' +
      '\n' +
      'Consequently, DeepSpeed-FastGen will consume tokens from incoming prompts at a rate that permits fast ongoing generation while adding tokens to the system that increase system utilization, providing lower latency and higher throughput streaming generation to all clients as compared to other state-of-the-art serving systems.\n' +
      '\n' +
      '4개의 성과 평가.\n' +
      '\n' +
      '딥스피드-패스트 유전자는 차단된 KV 캐시 및 Dynamic SplitFuse 연속 배치를 활용하는 최첨단 LLM 서빙 성능을 제공합니다. 아래에 논의된 벤치마킹 방법론에 따라 다양한 모델 및 하드웨어 구성에서 vLLM [4]에 대한 딥스피드-패스트 유전자를 평가한다. 평가는 딥스피드-패스트 유전자가 vLLM과 같은 최첨단 시스템에 비해 최대 2.3배 더 높은 유효 처리량, 평균 2배 낮은 잠복기, 최대 3.7배 낮은(토큰 수준) 꼬리 잠복기를 달성한다는 것을 보여준다.\n' +
      '\n' +
      '### Benchmarking Methodology\n' +
      '\n' +
      'We use two primary quantitative schemes for measuring performance.\n' +
      '\n' +
      '그림 4.1hL 아트로디쿠버베스####.\n' +
      '\n' +
      '생산 준비도에 대한 두 가지 주요 메트릭은 처리량(초당 요청으로 측정) 및 지연 시간(각 요청의 응답성)이다. 이를 측정하기 위해 여러 고객(1에서 32로 변경)을 동시에 인스턴스하고 요청(총512개)을 서버에 전송한다. 각 요청의 결과 지연 시간은 엔드포인트에서 측정되고 처리량은 실험을 완료하기 위해 엔드 투 엔드 시간으로 측정된다.\n' +
      '\n' +
      '그림 4.1.2.2의 효과적 수율 수율#####\n' +
      '\n' +
      '채팅 애플리케이션과 같은 상호 작용 애플리케이션은 엔드 투 엔드 레이턴시와 같은 최상위 메트릭에 의해 캡처될 수 있는 것보다 더 엄격하고 복잡한 요구 사항을 가질 수 있다. 특히 점점 더 인기 있는 채팅 사용자 시나리오에 초점을 맞추고 있습니다.\n' +
      '\n' +
      '1. A 사용자는 프롬프트를 보내 작업을 시작한다.\n' +
      '2. 시스템은 프롬프트를 처리하고 첫 번째 토큰을 반환한다.\n' +
      '3. 후속 토큰은 생산됨에 따라 사용자에게 스트리밍된다.\n' +
      '\n' +
      '이 과정에서 각 지점에서 시스템이 부정적인 사용자 경험을 제공할 수 있는 기회가 있으며, 예를 들어, 제1 토큰이 너무 천천히 도착하거나 세대가 한동안 멈추는 것으로 보이는 경우이다. 우리는 이 두 차원을 모두 고려하는 SLA 프레임워크를 제안한다.\n' +
      '\n' +
      '신속하고 생성된 텍스트의 길이가 크게 달라 계산 비용에 영향을 미치므로 처리량과 대기 시간에 대한 단단한 SLA 값을 설정하는 것은 불가능하다. 따라서 신속한 지연에 대한 SLA를 \\(신속한 경우\\frac{-\\text{tokens,}}{512}\\) 초(=512 토큰/s)로 정의한다. 또한 인간의 읽기 속도를 고려하여 지수 이동 평균(EMA)에서 생성 잠복기를 위한 SLA를 2, 4 또는 6 토큰/sec로 설정했다. 이러한 SLFA에 부착하는 요청은 성공적인 것으로 간주되며 이러한 성공적인 요청의 처리량은 ** 효율적인 처리량**라고 한다.\n' +
      '\n' +
      '우리는 NVIDIA A100, H100 및 A6000에서 Llama-2 7B, Llama-2 13B 및 Llama-2 70B [2] 모두에서 vLLM 및 딥스피드-Fast 유전자를 평가한다.\n' +
      '\n' +
      '### Throughput-Latency Analysis\n' +
      '\n' +
      '이 실험에서 딥스피드-패스트 유전자는 처리량과 잠복기 모두에서 vLLM을 능가하여 더 큰 처리량 이상의 반응성 잠복기와 동일한 처리량을 갖는 동등한 지연 시간을 제공한다. 4 A100x80GB가 있는 라마-2 70B에서 딥스피드-패스트 유전자는 동일한 지연 시간(9초) 또는 최대 50% 잠복기 감소(7초 대 0.67 rps)에서 최대 2배 더 높은 처리량(1.36 rps 대 0.67 rps)을 보여준다. 그림 2와 같이 동일한 처리량(1.2 rps)을 달성하면서 14초)를 이뤘는데, 이 경향은 그림 3과 같이 라마-2 13B를 평가할 때 유지된다.\n' +
      '\n' +
      '효과.\n' +
      '\n' +
      '첫 번째 yandt 헤르르스를 모두 고려한 효과적인 처리량 분석에서 D eep Spe ed-F는 tGenp 로보디페스 o2.3xhi Gherthroughputtha nvLLM으로 간주되며, F.Fi 구는 4일 시멘타 비교 에나 용해를 반복한다.\n' +
      '\n' +
      'Figure 4: Throughput and latency of text generation using Llama 2 70B (Tensor parallelism across 4 A100-80GB GPUs). A normal distribution was applied to prompt and generation lengths with averages of 1200/2600 and 128/60, respectively, and a 30% variance.\n' +
      '\n' +
      '그림 5: 라마 2 13B(A100-80GB GPU, 텐서 병렬주의 없음)를 이용한 텍스트 생성의 산출 및 대기 시간을 나타낸다. 평균이 각각 1200/2600 및 60/128이고 분산이 30%인 신속한 및 생성 길이에 정규 분포를 적용했다.\n' +
      '\n' +
      '고객 수를 참조하세요. 우리는 고객의 수를 확장했기 때문에 초기에 효과적인 처리량의 증가를 관찰했다. 그러나 클라이언트가 시스템의 능력에 접근함에 따라 잠복기도 크게 증가하여 SLA를 충족시키는 데 많은 요청이 실패한다. 결과적으로 효과적인 처리량은 어느 시점에서 포화되거나 감소할 것이다. 사용성 관점에서 볼 때, 최대 유효 처리량을 달성하기 위해 몇 명의 고객이 필요한지는 특별히 관련이 없으며, 선의 최대 포인트는 최적의 서빙 포인트이다.\n' +
      '\n' +
      '투넨세대에 대한 특성의 저하.\n' +
      '\n' +
      'Figure 5 displays the P50, P90, and P95 latencies of the generation processes. Both vLLM and DeepSpeed-FastGen exhibit similar P50 latencies, but vLLM demonstrates significantly higher latencies for P90 and P95. Regarding the P95 latencies, DeepSpeed-FastGen achieved a reduction of 3.7 times.\n' +
      '\n' +
      'v LL40031912 63329 81 308세대 및 헤니프레템 ptstheo ngoing 아쿠아티 온톨로케이션 s 새로운 촉진물 s.의 표층성 sp ike. 프로브 천식에서 딥러닝 편집-FastGent yply 처리는 촉진 및 아게라 티오 nforpr을 회피할 때 sco n전류 용해, 류시디ngtomuch모텍이 nt 유전자 쥐기에 미치는 영향을 의미한다.\n' +
      '\n' +
      '로드 발유행을 이용한 수용성.\n' +
      '\n' +
      '딥스피드-패스트 유전자는 여러 서버에 걸쳐 요청을 고르게 분산시키는 복제 수준 부하 균형을 제공하여 애플리케이션을 쉽게 스케일링할 수 있습니다. 그림 6은 부하 밸런서와 최대 16개의 복제물을 사용할 때 딥스피드-패스트 유전자의 확장성을 보여준다. 우리는 4개의 A100 GPU를 사용하여 라마 2 70B 모델을 계산했습니다. 총 8개의 노드를 사용하여 16개의 복제물을 실행했다. 결과는 딥스피드-패스트 유전자와 거의 완벽한 확장성을 보여준다. 단일 복제물의 처리량이 1.46 쿼리/sec인 점을 감안할 때 16개의 복제물을 사용한 처리량은 23.7 쿼리/sec에 도달하여 단일 복제물에 비해 선형 16배 증가를 나타낸다.\n' +
      '\n' +
      '### Other Hardware Platforms\n' +
      '\n' +
      'A100에 대한 심층 분석 외에도 H100 및 A6000에 대한 추가 벤치마킹 결과를 제공하고 A6000과 H100 모두에서 A100과 동일한 성능 경향이 관찰되었다.\n' +
      '\n' +
      '그림 6: 딥스피드-패스트유전자 및 vLLM의 효과적 처리량(4 A100-80GB GPU에 걸쳐 텐서 병렬성을 사용하여 라마 2 70B/A100-80GB)은 평균 2600 및 60의 프롬프트 및 생성 길이에 각각 정규 분포를 적용했다.\n' +
      '\n' +
      '그림 8: 살리티쿠 단일 헬 오드브 알링프 에어처. n 또는mald는 피플로우트 옵 로프타 ndg 엔탈레이클은 이타 베라파 f2 600a nd6 0, 침습적으로, 나다 3 0%v 아리에 관여한다.\n' +
      '\n' +
      'Figure 7: Per-Token generation Latency of Llama 2 70B/A100-80GB using tensor parallelism across 4 A100-80GB GPUs, 16 clients. A normal distribution was applied to prompt and generation lengths with averages of 2600 and 128, respectively, and a 30% variance.\n' +
      '\n' +
      '그림 9: 최저 지수 곡선 및 8 H100 GPU를 사용하여 라마 2 70b의 효과적인 처리량을 나타낸다. 평균 2600 및 60과 30% 분산을 갖는 신속한 및 생성 길이에 정규 분포를 적용했다.\n' +
      '\n' +
      '이행 및 폐기물 5개의 딥스피드-패스트.\n' +
      '\n' +
      '딥스피드-패스트젠은 아래 그림과 같이 딥스피드-MII와 딥스피드-반도의 상승적 구성이다. 함께, 이 두 소프트웨어 패키지 모두 프런트 API, 호스트 및 디바이스 인프라를 포함한 시스템의 다양한 구성 요소를 제공하여 Dynamic SplitFuse, 최적화된 커널 구현 및 새로운 모델 구현들을 구성하기 위한 도구를 사용하여 배치들을 스케줄링한다.\n' +
      '\n' +
      '시작하는 가장 빠른 방법은 초기 우라 lphar lphar eleaseo fD eepSpeed-FastGeni sb yr을 풀어낼 수 있다.\n' +
      '\n' +
      '``` pipinstalldeepseed-mii\n' +
      '\n' +
      'Figure 11: Architecture of DeepSpeed-FastGen\n' +
      '\n' +
      'Figure 10: Throughput-latency curve and effective throughput of Llama 2 7b using A6000. A normal distribution was applied to prompt and generation lengths with averages of 2600 and 60, respectively, and a 30% variance.\n' +
      '\n' +
      '### Supported Models\n' +
      '\n' +
      'We currently support the following HuggingFace model families1 in this alpha release of DeepSpeedFastGen:\n' +
      '\n' +
      'Footnote 1: [https://huggingface.co/models](https://huggingface.co/models)\n' +
      '\n' +
      '* LLaMA and LLaMA-2\n' +
      '* Mistral\n' +
      '* Facebook OPT\n' +
      '\n' +
      '모든 현재 모델은 모델 가중치 및 모델의 해당 토큰라이저를 모두 제공하기 위해 백엔드에서 HuggingFace API를 레버리니다. 초기 출시 후 몇 주, 몇 달 후에 추가 모델을 추가할 계획입니다. 지원하고 싶은 구체적인 모델 아키텍처가 있다면 문제를 제기하여 알려주시기 바랍니다.\n' +
      '\n' +
      '### Deployment Options\n' +
      '\n' +
      '아래의 모든 예들은 딥스피드 예들에 있어서 실행가능하다. 일단 설치하면 상호 작용적인 비지속적 파이프라인 또는 지속적인 서빙 배치의 두 가지 옵션이 있습니다.\n' +
      '\n' +
      '비지속적 Pipeline Pipeline#### 5.2.1.2.1 비지속적 Pipeline Pipeline Pipeline#### 5.2.1 비지속적 Pipeline Pipeline##############\n' +
      '\n' +
      '지속되지 않는 파이프라인 배치는 시작하기에 훌륭하고 빠른 방법이며 몇 개의 코드 라인만으로 수행할 수 있습니다. 지속되지 않은 모델은 달리고 있는 피스톤 스크립트의 기간에만 있지만 일시적인 상호 작용 세션에 유용하다.\n' +
      '\n' +
      '```\n' +
      'frommiimportpipeline pipe=pipeline("mistralai/Mistral-7B-v0.1") output=pipe(["Hello,my-name-is","DeepSpeed-is"],max_new_tokens=128) print(output)\n' +
      '```\n' +
      '\n' +
      '지속적 고용 고용##### 5.2.2.\n' +
      '\n' +
      'A persistent deployment is ideal for use with long-running and production applications. The persistent deployment uses a lightweight GRPC server that can be created using the following 2 lines:\n' +
      '\n' +
      '```\n' +
      'importmii mi.serve("mistralai/Mistral-7B-v0.1")\n' +
      '```\n' +
      '\n' +
      '위 서버는 딥스피드-MII의 내장 로드 밸런서 덕분에 여러 클라이언트가 한번에 조회할 수 있습니다. 클라이언트를 만드는 것 또한 2개의 코드를 취합니다.\n' +
      '\n' +
      '```\n' +
      'client=mi.client("mistralai/Mistral-7B-v0.1") output=client.generate("DeepSpeed-is",max_new_tokens=128) print(output)\n' +
      '```\n' +
      '\n' +
      'A persistent deployment can be terminated when it is no longer needed:\n' +
      '\n' +
      '``` client.terminate_server()\n' +
      '\n' +
      '안내 정보.\n' +
      '\n' +
      '사용 용이성과 이 공간에서 많은 프로젝트가 필요로 하는 긴 컴파일 시간의 상당한 감소를 위해 딥스피드-커날스라는 새로운 라이브러리를 통해 대부분의 맞춤형 낟알을 덮는 미리 내장된 파이썬 휠을 배포한다. 이 라이브러리는 계산 능력 8.0+(Ampere+), CUDA 11.6+ 및 Ubuntu 20+가 있는 NVIDIA GPU가 있는 환경에서 매우 휴대할 수 있음을 발견했다. 대부분의 경우 딥스피드-MII의 종속성이므로 이 라이브러리가 존재한다는 것을 알아야 하며 이를 설치할 것이다. 그러나 어떤 이유로든 낟알을 수동으로 구성해야 하는 경우 고급 설치 도크를 보실 수 있습니다.\n' +
      '\n' +
      '6 레이싱.\n' +
      '\n' +
      '이 딥스피드-패스트 유전자 알파 방출을 공유하게 되어 매우 기쁩니다. 시작하려면 딥스피드-MII용 GitHub 랜딩 페이지로 방문하십시오.\n' +
      '\n' +
      '딥스피드-패스트젠은 다단계 딥 러닝 시스템 및 모델링 기술을 포함하는 더 큰 딥스피드 생태계의 일부이다. 좀 더 배우려면.\n' +
      '\n' +
      '** 자세한 블로그 게시물, 튜토리얼 및 도움이 되는 문서를 위해 당사 웹사이트를 방문하십시오.\n' +
      '* You can also follow us on our English Twitter, Japanese Twitter, and Chinese Zhihu for the latest news on DeepSpeed.\n' +
      '\n' +
      '귀의 공동 검색, 리빙 드 드링크, 퍼 퍼펙트 알-월드AI 모디픽스, 코 리페드 스웨이트 델리프, 코 리베이트 리피어 리밍스, 코 리베이트 리베이트 트루(코 리베이트 rGubi tHub)에 대한 사전 기여도 462945964705)를 극복한다.\n' +
      '\n' +
      '### Roadmap\n' +
      '\n' +
      '다음의 항목은 로드맵에 있으며 GitHub 이슈와 PR을 통해 커뮤니티와 함께 할 계획입니다.\n' +
      '\n' +
      '개선\n' +
      '브더기 모델 지원 지원* 브로드머 모델 지원 지원\n' +
      '뉴 하드웨어는 파트너와의 협업을 통해 되돌아간다.\n' +
      '* 릴리스 성능 벤치마크(이 블로그에서 플롯을 생성하는 데 사용)\n' +
      '\n' +
      '"**" 스타" 저희 딥스피드 GitHub와 딥스피드MII GitHub 마우스**입니다!\n' +
      '\n' +
      '## Acknowledgment\n' +
      '\n' +
      '딥스피드-패스트젠 소프트웨어 개발, 디버깅, 테스트 및 출시에 기여한 딥스피드 팀 전체에 감사드립니다. 휴깅페이스, vLLM, 휴깅페이스 TGI 등 다양한 오픈소스 커뮤니티 프로젝트에 감사드린다. 알파 방출에서 모델과 토큰라이저를 지원하기 위해 레버리지된HF API를 보유하고 있으며 더 많은 모델을 계속 추가할 것입니다. 우리는 특히 플래시표시 개발자[12, 13]가 훌륭한 일에 대해 인정하고 감사를 표합니다. 우리는 적절한 파일 헤드에서 코드 레귤레이터에서 인정된 수정과 함께 시스템의 광범위하게 레버리지된 플래시 주의 커널을 가지고 있다. 마지막으로, 우리는 MoE 낟알(딥스피드-커날스 저장소의 일부로 환원)에 사용한 캐스터 트랜스포머[14] 커널 개발자에게 감사의 인사를 전합니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] OpenAI. Gpt-4. [https://openai.com/gpt-4](https://openai.com/gpt-4), 2023.\n' +
      '* [2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [3] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library, 2019.\n' +
      '* [4] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. _arXiv preprint arXiv:2309.06180_, 2023.\n' +
      '* [5] GI Yu, Jeong JSeong, GW Kim, S Kim, and BG Chun. Orca: A distributed serving system for {Transformer-Based} generative models. 2022.\n' +
      '* [6] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms. [https://www.mosaicml.com/blog/mpt-7b](https://www.mosaicml.com/blog/mpt-7b), 2023.\n' +
      '* [7] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models. _arXiv preprint arXiv:2309.14509_, 2023.\n' +
      '* [8] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. _arXiv preprint arXiv:2309.06180_, 2023.\n' +
      '* [9] HuggingFace. Text generation inference. [https://huggingface.co/text-generation-inference](https://huggingface.co/text-generation-inference).\n' +
      '* [10] NVIDIA. Nvidia tensort-llm: A tensortt toolbox for large language model. [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM).\n' +
      '* [11] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, and Ramachandran Ramjee. Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills. _arXiv preprint arXiv:2308.16369_, 2023.\n' +
      '* [12] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In _Advances in Neural Information Processing Systems_, 2022.\n' +
      '\n' +
      '* [13] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.\n' +
      '* [14] NVIDIA. Fastertransformer. [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>