<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#sliceGPT: 행과 열을 삭제하여 대용량 언어 모델을 압축\n' +
      '\n' +
      'Saleh Ashkboos\n' +
      '\n' +
      'ETH Zurich\n' +
      '\n' +
      'saleh.ashkboos@inf.ethz.ch\n' +
      '\n' +
      '맥시밀리안 L. 크로시\n' +
      '\n' +
      'Microsoft Research\n' +
      '\n' +
      't-mcroci@microsoft.com\n' +
      '\n' +
      'Marcelo Gennari do Nascimento\n' +
      '\n' +
      'Microsoft\n' +
      '\n' +
      'marceloge@microsoft.com\n' +
      '\n' +
      '&Torsten Hoefler\n' +
      '\n' +
      'ETH Zurich\n' +
      '\n' +
      'torsten.hoefler@inf.ethz.ch\n' +
      '\n' +
      '&James Hensman\n' +
      '\n' +
      'Microsoft Research\n' +
      '\n' +
      'jameshensman@microsoft.com\n' +
      '\n' +
      '마이크로소프트에서 인턴으로 일하다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델은 자연어 처리의 초석이 되었지만, 그 사용은 컴퓨팅 및 메모리 리소스 측면에서 상당한 비용과 함께 제공된다. 희박화는 이러한 자원 제약을 완화하기 위한 솔루션을 제공하며, 최근 연구에서는 훈련된 모델이 사후적으로 희박화될 수 있음을 보여주었다. 기존의 희박화 기법은 추가적인 데이터 구조가 필요하고 현재 하드웨어로 제한된 속도를 제공하기 때문에 어려움에 직면해 있다. 본 논문에서는 각 가중치 행렬을 더 작은 (밀도) 행렬로 대체하여 네트워크의 임베딩 차원을 줄이는 새로운 훈련 후 희소화 기법인 SliceGPT를 제안한다. 광범위한 실험을 통해, SliceGPT는 Llama-2 70B, OPT 66B 및 Phi-2 모델에 대한 모델 파라미터(임베딩 포함)의 최대 25%를 제거하면서도 밀집 모델의 제로 샷 태스크 성능은 각각 99%, 99% 및 90%를 유지할 수 있음을 보인다. 슬라이스된 모델은 더 적은 수의 GPU에서 실행되고 추가 코드 최적화 없이 더 빠르게 실행된다: 24GB 소비자 GPU에서 Llama-2 70B에 대한 추론을 위해 전체 계산량을 밀도 모델의 64%로 줄인다; 40GB A100 GPU에서 66%로 줄인다. 우리는 SliceGPT를 가능하게 하는 트랜스포머 네트워크에서 새로운 통찰력, 계산 불변성을 제공하며, 사전 훈련된 모델에 대한 메모리 및 계산 요구를 줄일 수 있는 미래의 방법을 영감을 주고 가능하게 하기를 바란다. 코드는:[https://github.com/microsoft/TransformerCompression](https://github.com/microsoft/TransformerCompression)에서 이용 가능하다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대규모 언어 모델들(LLM)은 수조 개의 토큰들에 대해 트레이닝된 수십억 개의 파라미터들을 갖는 신경망들이다(Zhao et al., 2023). LLM을 훈련하는 데 드는 비용은 여러 작업에 대해 사전 훈련된 모델인 _foundation model_ 패러다임을 재사용하는 것으로 전환했다. LLM의 크기는 미리 훈련된 모델을 배치하는 것을 비용이 많이 드는 작업으로 만듭니다. 많은 모델들은 예측을 계산할 수 있도록 다수의 GPU들을 필요로 하고, 모델들이 자기회귀적이기 때문에, 텍스트 응답들을 생성하기 위해 신경망의 다수의 순방향 패스들이 필요하다. 따라서 일반적으로 _모델 압축_라고 하는 사후 훈련 기술을 통해 수행되는 이러한 모델의 계산 요구 사항을 줄이는 것이 널리 관심이 있다.\n' +
      '\n' +
      '대부분의 모델 압축 기술은 증류, 텐서 분해(저순위 인수분해 포함), 가지치기 및 양자화(Hoefler et al., 2021; Gholami et al., 2021; Zhu et al., 2023; Gupta and Agrawal, 2021)의 네 가지 범주 중 하나로 분류된다. 이 작업에서 우리는 가지치기에 초점을 맞추고 있지만, 우리의 방법론이 다른 영역에 대한 향후 작업에 영향을 미칠 수 있기를 바란다. 전치 방법은 오랜 기간 동안 존재했지만, 많은 접근 방식은 성능을 유지하기 위해 전치 후 복구 미세 조정(RFT)을 필요로 하므로 전체 프로세스가 비용이 많이 들고 규모가 어려운 작업이다. SliceGPT를 사용하여 단일 GPU를 사용하여 몇 시간 만에 대형 모델을 압축하고 RFT가 없어도 생성 및 다운스트림 작업에서 경쟁력 있는 성능을 유지합니다.\n' +
      '\n' +
      '프루닝 방법은 LLM에서 가중치 행렬의 일부 요소를 0으로 설정하고 (선택적으로) 행렬의 주변 요소를 업데이트하여 보상함으로써 작동한다. 결과는 희소 패턴이며, 이는 신경망의 순방향 패스에서 요구되는 행렬 곱셈에서 일부 부동 소수점 연산을 스킵할 수 있음을 의미한다. 연산의 상대적 속도는 희소성 수준과 희소성 패턴에 따라 달라지며, 더 구조화된 희소성은 더 많은 계산 이득과 연관된다. 다른 가지치기 방법과 달리 SliceGPT는 가중치 행렬의 전체 행 또는 열을 잘라낸다. 슬라이싱 전에 예측은 불변이지만 슬라이싱은 작은 효과만 가질 수 있는 네트워크의 단일 변환을 수행한다.\n' +
      '\n' +
      '그 결과 가중치 행렬이 더 작고, 신경망의 블록 사이에 전달되는 신호도 더 작으며, 신경망의 _embedding dimension_를 줄인다.\n' +
      '\n' +
      '그림 1은 우리의 접근 방식을 기존의 희소성 방법과 비교한다. 우리의 기여는 다음과 같습니다.\n' +
      '\n' +
      '1. _computational invariance_의 개념을 소개한다: 우리는 모델을 변경하지 않고 변환기에서 각 가중치 행렬에 직교 행렬 변환을 적용할 수 있음을 보여준다.\n' +
      '2. 트랜스포머 아키텍처에서 각 블록을 편집하기 위해 이를 사용하여 블록 간의 신호 행렬1을 자체 주성분에 투영한다. 우리는 모델 크기를 줄이기 위해 변환된 가중치 행렬의 열 또는 행을 제거한다. 우리는 무게의 변환과 제거를 SliceGPT라고 부른다. 각주 1: 신호 행렬은 때때로 활성화 행렬로 지칭된다.\n' +
      '3. OPT(Zhang et al., 2022), Llama-2(Touvron et al., 2023) 및 LLMs에 대해 다중 실험을 수행하여, SliceGPT가 최신 2:4 방식에 비해 우수한 복잡성으로 이들 모델을 최대 30%까지 압축할 수 있음을 입증한다. 다운스트림 작업에서 Phi-2를 추가로 실험하고 모든 모델을 고밀도 성능의 >90%를 유지하면서 최대 30%까지 슬라이스할 수 있음을 보여준다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '이 섹션에서는 먼저 변압기 아키텍처에 대한 몇 가지 필요한 배경을 설명하며, 이를 통해 주요 결과를 증명하는 데 사용할 표기법을 도입할 수 있다. 그런 다음 이러한 아키텍처를 압축하기 위한 희박화에 대한 관련 작업을 설명한다.\n' +
      '\n' +
      '그림 1: 다른 형태의 희소성 하에서 신호\\(\\mathbf{X}\\)와 가중치 행렬\\(\\mathbf{W}\\)의 행렬 곱셈. Left**: Unstructured sparsity, 여기서 \\(\\mathbf{W}\\)의 일부 원소는 0이고 \\(\\mathbf{X}\\)은 조밀하다. **Middle**: 2:4 구조 희소성으로, 여기서 네 개의 가중치 매트릭스 엔트리들의 각각의 블록은 두 개의 제로들을 포함하고, \\(\\mathbf{X}\\)는 조밀하다. *Right**: SliceGPT, 여기서 변환\\(\\mathbf{Q}\\)을 도입한 후 희소성은 모두 \\(\\mathbf{W}\\)의 하단 행에 배열되고 \\(\\mathbf{X}\\)의 해당 열은 제거된다.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '### Transformer Networks\n' +
      '\n' +
      '트랜스포머 네트워크(Vaswani et al., 2017)는 언어 모델링을 포함한 광범위한 작업에서 효과적인 것으로 밝혀진 신경망의 클래스이다. 변압기 구조는 일련의 계층으로 구성되며, 각 계층들은 다중 헤드 자기 주의 블록 다음에 피드 포워드 네트워크 블록으로 구성된다. 각 블록 사이에는 LayerNorm(Ba et al., 2016)(또는 RMSNorm(Zhang and Sennrich, 2019)) 블록이 있다. 도 2는 변압기 네트워크의 일부를 예시한다: FFN 블록에 레이어노름 블록을 통해 연결된 어텐션 블록, 잔여 연결들을 포함한다. 다음은 각 구성요소(훈련 후 적용되지 않는 드롭아웃 무시)의 동작을 설명한다.\n' +
      '\n' +
      'Embeddings \\(D\\)는 변압기의 임베딩 차원이고, \\(N\\)은 시퀀스 길이이다. 변압기 모델은 토큰 ID와 위치 ID의 시퀀스를 입력으로 하여 임베딩 행렬을 인덱싱하여 초기 신호\\(\\mathbf{X}\\)를 형상\\(N\\times D\\)으로 생성한다. 다음에서 우리는 일반성의 손실 없이 입력 시퀀스\\(\\mathbf{s}\\)에 의해 색인된 단일 임베딩 행렬\\(\\mathbf{W}_{\\text{embd}\\)을 고려한다.\n' +
      '\n' +
      'LayerNorm을 임베딩한 후, 신호 행렬은 LayerNorm 연산을 통과하는데, LayerNorm 연산은 행렬의 각 행에서 평균을 빼고, 행을 표준 편차로 나누고, (열 단위로) 재스케일하고, 오프셋을 더한다. 우리는 LayerNorm 블록을 다음과 같이 쓴다.\n' +
      '\n' +
      '\\text{LayerNorm}(\\mathbf{X})=\\text{RMSNorm}(\\mathbf{X}\\mathbf{M})\\text{diag}(\\mathbf{\\alpha}\\sqrt{D}+\\mathbf{1}_{N}\\mathbf{\\beta}^{\\top}\\tag{1}\\mathbf{X})\n' +
      '\n' +
      '여기서 \\(\\text{RMSNorm}(\\mathbf{X})\\)는 \\(\\mathbf{x}\\leftarrow\\mathbf{x}/\\|\\mathbf{x}\\|\\)의 각 행에 2\\(\\mathbf{x}\\leftarrow\\mathbf{x}/\\|\\mathbf{x}\\|\\을 적용한다. 벡터 파라미터\\(\\mathbf{\\alpha}\\)와 오프셋(vector) 파라미터\\(\\mathbf{\\beta}\\)은 각 LayerNorm 인스턴스에서 독립적으로 학습된다. 상수 행렬 \\(\\mathbf{M}=\\mathbf{I}-\\frac{1}{D}\\mathbf{1}\\mathbf{1}^{\\top}\\)은 \\(D\\times D\\) 행렬로서 \\(\\mathbf{X}\\)의 각 행에서 평균을 뺀다.\n' +
      '\n' +
      '각주 2: 일부 구현들에서, RMSNorm 블록은 스케일 파라미터들을 포함할 수 있다. 우리는 이것을 LayerNorm의 특별한 경우로 간주하고 그에 따라 처리합니다.\n' +
      '\n' +
      '어텐션 블록은 차원이 각각 \\(D\\times D\\)인 \\(\\mathbf{W}_{k},\\mathbf{W}_{q},\\mathbf{W}_{v}\\)과 \\(\\mathbf{W}_{o}\\)의 네 가지 행렬을 갖는다. 블록에 도착하는 입력 신호는 Key(\\(\\mathbf{X}\\mathbf{W}_{k}\\)), Query(\\(\\mathbf{X}\\mathbf{W}_{q}\\)), Value(\\(\\mathbf{X}\\mathbf{W}_{v}\\)) 행렬에 투영되어 여러 개의 _heads_로 분할된다. 각 헤드에서 비선형 연산은 신호를 조합하고 출력 가중치 행렬 \\(\\mathbf{W}_{o}\\)을 곱하기 전에 적용된다. 처음 세 개의 가중치 행렬이 입력에 별도로 적용되기 때문에, 우리는 그것들을 연결하고 단일 행렬 곱셈을 수행할 수 있다(그림 2의 이러한 행렬 주위에 흰색 상자로 표시). 우리는 이 행렬들의 연접을 \\(\\mathbf{W}_{\\text{in}\\)을 나타내는 단일 선형 층으로 간주할 수 있다. 또한 출력행렬을 \\(\\mathbf{W}_{\\text{out}\\)이라 한다. 우리는 주의 블록을 \\(\\sigma(\\mathbf{X}\\mathbf{W}_{\\text{in}}+\\mathbf{b}_{\\text{in}})\\mathbf{W}_{\\text{out}+\\mathbf{b}_{\\text{out}}3으로 취급하며, 여기서 \\(\\sigma\\)는 다중 헤드 주의 동작을 나타낸다.\n' +
      '\n' +
      '각주 3: 여기에서 그리고 이 논문 전반에 걸쳐 표기법의 용이성을 위해 표기법을 약간 남용하고 시퀀스 길이 차원에 걸친 편향 용어의 방송을 생략한다. 어텐션 블록의 동작을 위한 완전한 표기법은 \\(\\sigma(\\mathbf{X}\\mathbf{W}_{\\text{in}+\\mathbf{1}_{N}\\mathbf{b}_{\\text{in}^{\\top})\\mathbf{W}_{\\text{out}+\\mathbf{b}_{\\text{out}\\)이다.\n' +
      '\n' +
      'FFN 블록 트랜스포머 아키텍처에 나타나는 다른 유형의 블록은 FFN(feed Forward Network) 블록입니다. 다층 퍼셉트론(Multi-layer Perceptron, MLP)은 선형층\\(\\mathbf{W}_{1}\\), 요소연산\\(\\sigma\\), 두 번째 선형층\\(\\sigma(\\mathbf{X}\\mathbf{W}_{1}+\\mathbf{b}_{1})\\mathbf{W}_{2}+\\mathbf{b}_{2}\\). 일부 아키텍처는 게이트형식을 채택하여 추가행렬을 사용하였으며, 연산은 \\(\\big{(}\\sigma(\\mathbf{X}\\mathbf{W}_{1}+\\mathbf{b}_{1})\\circ(\\mathbff{X}\\mathbf{W}_{2})\\big{)}\\mathbf{W}_{3}\\(\\circ\\) 원소별 곱이다. 어텐션 모듈에서 처음 세 개의 선형 레이어와 마찬가지로 \\(\\mathbf{W}_{1}\\)과 \\(\\mathbf{W}_{2}\\)의 연접을 하나의 선형 연산으로 간주하고 이를 \\(\\mathbf{W}_{text{in}\\)으로 나타낼 수 있다. 따라서 우리는 MLP 또는 Gated FFN 층의 동작을 \\(\\sigma(\\mathbf{X}\\mathbf{W}_{\\text{in})\\mathbf{W}_{\\text{out}\\)으로 나타낼 수 있으며, 여기서 \\(\\sigma\\)는 주의에서와는 다른 의미를 갖는다.\n' +
      '\n' +
      '본 논문에서 SliceGPT를 적용한 LM(Language Modeling) HeadAll의 트랜스포머 네트워크는 다음과 같은 디코더 전용 구조를 가진다(Radford et al., 2018). 헤드 연산은 \\(\\mathbf{X}\\mathbf{W}_{\\text{head}+\\mathbf{b}_{\\text{head}\\)이며, 여기서 \\(\\mathbf{X}\\)는 마지막 트랜스포머 블록의 출력이다.\n' +
      '\n' +
      '모델이 훈련되고 모든 파라미터가 설정되면 예측을 생성하기 위해 변압기 네트워크에서 필요한 계산은 헤드 노드에 도달할 때까지 신호 행렬을 한 블록에서 다음 블록으로 전달하는 것을 포함한다. 우리는 FFN과 주의블록 모두를 \\(\\sigma(\\mathbf{X}\\mathbf{W}_{\\text{in}}+\\mathbf{b}_{\\text{in})\\mathbf{W}_{\\text{out}}+\\mathbf{b}_{\\text{out}}의 형태로 정의할 수 있기 때문에, \\(\\sigma\\)는 점-방향 또는 다중-헤드-어텐션 비선형성을 나타냄을 알 수 있으므로, 알고리즘 1을 사용하여 순방향 패스를 설명할 수 있다.\n' +
      '\n' +
      '### Related work\n' +
      '\n' +
      '가장 간단한 설정에서, 모델에서 가장 작은 가중치를 0으로 설정하는 것과 관련된 크기 기반 희소화를 사용할 수 있다(Han et al., 2016; Zhu and Gupta, 2017; Gale et al., 2019). 규모 희소화는 확장 가능하지만 LLM에 적용하면 성능이 너무 크게 저하된다(Frantar and Alistarh, 2023). 보다 정교한 방법인 최적 뇌외과의(Optimal Brain Surgeon, OBS) (Hassibi et al., 1993; LeCun et al., 1989)는 손실함수에 가장 영향이 적은 가중치를 체계적으로 제거한다. 이 방법은 헤시안 행렬의 역행렬을 이용하여 프루닝되지 않은 가중치를 갱신함으로써 가중치 제거에 의해 도입된 오차를 보상한다. 불행히도 OBS는 헤시안 행렬의 역수를 계산하고 저장할 필요성으로 인해 몇 백만 매개 변수를 가진 모델에는 비실용적이다. OBS에 의해 제기된 계산 한계를 해결하기 위해, 최근 연구는 우드피셔(Singh and Alistarh, 2020)와 같은 헤시안 행렬의 역행렬을 근사화하거나 레이어별 가지치기(layer-wise pruning)로 알려진 최적 뇌 압축(Optimal Brain Compression, OBC, Frantar and Alistarh, 2022)과 같은 각 레이어에 별도로 적용하는 두 가지 접근법을 탐구했다. 이러한 기법들은 중간 크기의 네트워크들에 대해 효과적인 것으로 입증되었지만, 개별 계층 가중치 행렬들이 일반적으로 \\(10^{8}\\) 이상의 파라미터들을 포함하는 큰 언어 모델들에 대해서는 실용적이지 않다.\n' +
      '\n' +
      'GPTQ(Frantar et al., 2022)는 컬럼-바이-컬럼 스킴을 사용하여 LLM들의 가중치 매트릭스를 양자화(더 낮은 정밀도를 사용하여 파라미터를 나타냄)하고 다음 컬럼들에서 아직 양자화되지 않은 모든 가중치들을 업데이트함으로써 이 문제를 해결하였다. SparseGPT(Frantar and Alistarh, 2023)는 프루닝에 대해 동일한 아이디어를 적용하고, 비구조화 및 반구조화 프루닝을 사용하여 LLM을 희소화하고, Sun et al. (2023)은 헤시안 대각선만을 사용하여 아이디어를 단순화하였다. 비구조 프루닝을 통해 종단 간 속도 향상을 달성하는 것은 까다로운 작업이기 때문에 2:4, 4:8과 같이 반구조화된 패턴으로 희소성을 유도하는 유사한 기법을 시도하기도 하였다(Mishra et al., 2021). 그러나, 이러한 구조를 구현하는 것은 모델의 정확도를 유지하지 못한다.\n' +
      '\n' +
      '그림 2: 변압기 네트워크의 단일 계층. 네트워크의 이전 블록에서 발생하는 신호(입력)는 주의, LayerNorm 및 FFN을 통과하기 전에 그림의 하단에 도달한다. 주제와 FFN 블록은 모두 입력 및 출력 선형 연산(\\(\\mathbf{W}_{\\text{in},\\mathbf{W}_{\\text{out})으로 텍스트에서 나타내는\\(\\mathbf{blue}\\))을 갖는다. LayerNorm\\(\\mathbf{\\bar{M}\\)과 \\(\\mathbf{diag}(\\boldsymbol{\\alpha})의 선형 연산이 강조된다. 이 수치와 후속 수치들은 편견을 보여주지 않는다.\n' +
      '\n' +
      '압축에 대한 또 다른 접근법은 낮은 순위 근사치이며, 여기서 각각의 가중치 행렬은 더 작은 내부 차원을 갖는 두 행렬의 곱으로 대체되고, 보통 미세 조정 단계가 뒤따른다(Hu et al., 2021; Mahabadi et al., 2021; Noach and Goldberg, 2020; Tukan et al., 2020). 압축을 수행하려면 내부 치수가 원래 치수의 절반보다 작아야 합니다. 대조적으로, 본 방법은 각 가중치 매트릭스를 하나의 더 작은 매트릭스로 대체하여 미세 조정이 필요 없이 임베딩 차원을 줄인다.\n' +
      '\n' +
      '우리는 convnet 문헌에서 필터와 채널의 가지치기(pruning)와 유사한 가중치 행렬의 행과 열을 삭제할 것을 제안한다. 그곳에서, 희소성-유도 정규화는 배치-규범 인자(Liu et al., 2017) 또는 네트워크 구조(Huang and Wang, 2018)에 추가되고, 네트워크는 훈련되거나 미세 조정되어, 네트워크의 채널들 또는 부분들의 가지치기를 초래한다. 아마도 우리의 방법과 가장 유사한 방법은 ThiNet(Luo et al., 2017; He et al., 2017)일 수 있으며, 이는 계층들 사이에 선형 연산들을 적용하는 것과 마찬가지로, 정규화와 함께 더 미세 조정으로 인터리빙된다. 이 문헌에서 모델 크기는 일반적으로 LLM보다 수십 배 작은데, 예를 들어 VGG16 네트워크는 138M 매개변수를 가지고 있으며, 이는 우리가 고려하는 매우 작은 OPT 모델과 비슷하다. LLM의 거대한 크기는 특히 정규화 매개변수를 선택하기 위해 외부 루프가 필요할 때 광범위한 미세 조정을 포함하는 방법을 매력적이지 않게 만든다.\n' +
      '\n' +
      '최근에는 LLM에 구조화된 가지치기를 적용한 후 손실된 성능을 회복하기 위해 지속적인 훈련(또는 미세 조정)을 수행하는 작업이 제안되었다. 예를 들어, LLM-pruner(Ma et al., 2023)는 추가 훈련 전에 LLM으로부터 연결된 구조들을 제거한다. 우리의 작업과 동시에, LLM Surgeon(van der Ouderaa et al., 2023)은 가지치기와 함께 복구 미세 조정을 상호 엮는다. 슬라이스GPT에 대한 결과를 단일 샷 방법과 슬라이스 후 복구 미세 조정으로 제공한다.\n' +
      '\n' +
      '## 3 SliceGPT\n' +
      '\n' +
      '우리의 SliceGPT 방법은 변압기 구조에 내재된 계산 불변성에 의존한다. 이를 통해, 우리는 한 컴포넌트의 출력에 직교 변환을 적용하는 것이 가능하다는 것을 의미하는데, 이는 다음에 실행되지 않는 한 가능하다. 우리의 핵심 통찰력은 네트워크의 블록 간에 수행되는 RMSNorm 연산이 변환에 영향을 미치지 않는다는 것이다: 연산은 출퇴근한다. 이 섹션에서는 먼저 RMSNorm 연결 변압기 네트워크에서 불변성이 발생하는 방법을 설명하고 LayerNorm 연결로 훈련된 네트워크가 RMSNorm으로 변환될 수 있는 방법에 주목한다. 다음으로 주성분 분석(Principal Component Analysis, PCA)을 이용하여 각 계층에서의 변환을 계산하여 블록 간 신호가 주성분에 투영되도록 하는 방법을 설명한다. 마지막으로, 부주성분을 삭제하는 것이 수정된 네트워크의 행이나 열을 잘라내는 것과 어떻게 대응하는지 설명한다.\n' +
      '\n' +
      '###변압기 네트워크에서의 계산불변\n' +
      '\n' +
      '\\(\\mathbf{Q}\\)는 직교행렬을 나타낸다. \\(\\mathbf{Q}^{\\top}\\mathbf{Q}=\\mathbf{Q}\\mathbf{Q}^{\\top}=\\mathbf{I}\\. 벡터\\(\\mathbf{x}\\)에 \\(\\mathbf{Q}\\)을 곱하는 것은 벡터의 규범을 바꾸지 않는다. 왜냐하면 \\(\\|\\mathbff{x}\\|=\\sqrt{\\mathbf{x}^{\\top}\\mathbff{Q}^{\\top}\\mathbff{Q}\\mathbf{x}=\\sqrt{\\mathbf{x}^{\\top}\\mathbf{x}}=\\|\\mathbf{x}\\|\\\\mathbf{x}\\\\|\\| 이 연구에서 \\(\\mathbf{Q}\\)의 치수는 항상 변압기의 임베딩 치수와 일치할 것이다.\n' +
      '\n' +
      '변압기 한 블럭의 출력이 \\(\\mathbf{X}_{\\ell}\\)이고, 이를 RMSNorm으로 처리한 후 후속 블럭에 \\(\\text{RMSNorm}(\\mathbf{X}_{\\ell}\\)로 입력한다고 가정하자. 직교행렬을 \\(\\text{RMSNorm}\\) 이전에, \\(\\mathbf{Q}\\) 다음에 \\(\\mathbf{Q}^{\\top}\\)으로 선형층을 삽입하면, 신호행렬의 각 행에 \\(\\mathbf{Q}\\)을 곱하고 정규화하고 \\(\\mathbf{Q}^{\\top}\\)을 곱하기 때문에 네트워크는 변하지 않는다. 우리에겐\n' +
      '\n' +
      '\\[\\text{RMSNorm}(\\mathbf{X}_{\\ell}\\mathbf{Q})\\mathbf{Q}^{\\top}=\\text{RMSNorm}(\\mathbf{X}_{\\ell}\\,.\\tag{2}\\)\n' +
      '\n' +
      '이 관계에 대한 증명은 부록 A.1에 나타나 있다. 이제 네트워크의 각 주의 또는 FFN 블록은 입력과 출력 모두에 선형 연산이 있기 때문에 블록의 선형 계층에 추가 연산 \\(\\mathbf{Q}\\)을 흡수할 수 있다. 네트워크는 잔여 연결들을 포함하고 있기 때문에, 모든 이전 레이어들의 출력(매립으로 돌아가는 길) 및 모든 후속 레이어들(LM Head까지 가는 길)에도 \\(\\mathbf{Q}\\)을 적용해야 한다.\n' +
      '\n' +
      '_불변_ 함수는 입력에 대한 변환이 출력에 대한 변경을 초래하지 않는 함수이다. 이 경우, 변환기의 가중치에 임의의 직교변환 \\(\\mathbf{Q}\\)을 적용할 수 있으며, 따라서 임의의 변환상태에서 _computation_를 수행할 수 있다. 우리는 이것을 _계산 불변치라고 부르며, 다음 정리에서 정의한다.\n' +
      '\n' +
      '1**. Theorem 1**.: _Let\\(\\mathbf{W}_{\\text{in}^{\\ell}\\) and \\(\\mathbf{W}_{\\text{out}^{\\ell}\\)은 RMSNorm-connected transformer network의 \\(\\ell\\)번째 블록의 선형층들의 가중치 행렬이고, \\(\\mathbf{b}_{\\text{in}^{\\ell},\\mathbf{b}_{\\text{out}^{\\ell}\\)은 해당 바이어스일 경우 해당 바이어스일 경우 임베딩 및 헤드 행렬이고, \\(\\mathbf{W}_{\\text{embd}\\) 및 \\(\\mathbf{W}_{\\text{head}\\)은 임베딩 및 헤드 행렬이다. \\(\\mathbf{Q}\\)를 차원\\(D\\)의 직교 행렬로 하자. 그런 다음 다음 네트워크는 원래 변압 네트워크와 동일합니다:_\n' +
      '\n' +
      '\\tilde{\\text{bf{W}}_{\\text{embd}}\\mathbf{Q}\\, \\tilde{\\mathbf{W}}_{\\text{in}}^{\\top}\\mathbf{W}_{\\text{in}}^{\\ell}\\,\\] (4}\\[\\tilde{\\mathbf{W}}_{\\text{head}}^{\\ell}\\, \\tag{5}\\}}}\\mathbf{Q}^{\\top}\\mathbf{W}}^{\\tell}\\,\\tilde{\\mathbf{W}}^{\\tell}\\,\\tag{5}\\.\n' +
      '\n' +
      '입력 및 헤드 바이어스는 복사된다: \\(\\tilde{\\mathbf{b}}_{\\text{in}^{\\ell}=\\mathbf{b}_{\\text{in}^{\\ell}\\), \\(\\tilde{\\mathbf{b}}_{\\text{head}}=\\mathbf{b}_{\\text{head}\\).\n' +
      '\n' +
      '[증명]: 알고리즘 1을 통해 변환된 네트워크가 원본과 동일한 결과를 계산한다는 것을 증명할 수 있다. 1번 라인에서, 원본 네트워크가 계산되었다고 가정하면, 수정된 네트워크는 수학식 3을 사용하여 \\(\\tilde{\\mathbf{X}}=\\mathbf{XQ}\\)을 계산한다. 2번 라인에서 RMSNorm을 적용하면, 두 네트워크의 동작이 일치한다는 것을 알 수 있다. 2번 라인에서 우리는 \\(\\text{RMSNorm}(\\tilde{\\mathbf{X}})=\\text{RMSNorm}(\\mathbf{X})\\mathbf{Q}\\text{RMSNorm}(\\mathbf{X})\\mathbf{Q}\\text{RMSNorm}(\\tilde{\\mathbf{X}})=\\text{RMSNorm}(\\mathbf{X})\\mathbf{Q}\\text{RMSNorm}(\\tilde{\\mathbf{X}})\\text{RMSNorm}(\\mathbf{X})\\mathbf{Q}\\text{RMSNorm 선 4에 비선형성을 적용하면, 식 4를 이용하여 \\(\\tilde{\\mathbf{X}}\\tilde{\\mathbf{W}}_{\\text{in}}^{\\ell}=\\mathbf{XW}_{\\text{in}}^{\\ell}}=\\mathbf{ZQ}\\(\\tilde{\\mathbf{Z}}=\\mathbf{ZQ}\\)임을 알 수 있다. 5번 라인에서 잔여연결은 \\((\\tilde{\\mathbf{X}+\\tilde{\\mathbf{Z}})=(\\mathbf{X}+\\mathbf{Z})\\mathbf{Q}\\)를 가지며, RMSNorm을 적용하면 \\(\\tilde{\\mathbf{X}=\\mathbf{XQ}\\)이 할당된다. 이것은 루프의 끝까지 이어진다. 마지막으로, 7행에서는 수학식 7을 이용하여 \\(\\mathbf{XW}_{\\text{head}=\\tilde{\\mathbf{X}}\\tilde{\\mathbf{W}_{\\text{head}\\)로 변환이 풀린다.\n' +
      '\n' +
      '### LayerNorm Transformers can converted to RMSNorm\n' +
      '\n' +
      '변압기 네트워크의 계산 불변성은 RMSNorm-connected 네트워크에만 적용된다. 레이어Norm으로 작업하기 전에, 우리는 레이어Norm의 선형 블록을 인접한 블록으로 흡수함으로써 네트워크를 RMSNorm으로 변환한다. 도 3은 변압기 네트워크 상에서의 이러한 변환을 도시한다(도 2 참조). 각 블록에서 출력 행렬 \\(\\mathbf{W}_{\\text{out}\\)에 후속 레이어Norm에서 발생할 평균 뺄셈을 설명하는 평균 뺄셈 행렬 \\(\\mathbf{M}\\)을 곱한다. 입력 행렬 \\(\\mathbf{W}_{\\text{in}\\)은 앞선 LayerNorm 블록의 스케일에 의해 미리 곱해진다. 임베딩 행렬 \\(\\mathbf{W}_{\\text{embd}\\)은 평균-감산되어야 하고, \\(\\mathbf{W}_{\\text{head}\\)은 마지막 LayerNorm 스케일에 의해 재스케일링되어야 한다. 이는 동작 순서의 직접적인 변화이며 네트워크 출력에 영향을 미치지 않는다.\n' +
      '\n' +
      '### 한 블록당 변환\n' +
      '\n' +
      '이제 변압기의 모든 LayerNorm이 RMSNorm으로 변환되었으므로, 우리는 모델을 수정하기 위해 \\(\\mathbf{Q}\\)을 선택할 수 있다. 우리의 초기 계획은 모델에서 신호를 수집하고, 이 신호를 사용하여 직교 행렬을 구성하고 네트워크의 일부를 삭제하는 것이었다. 우리는 네트워크의 서로 다른 블록에서 신호가 정렬되지 않고, 각 블록에서 서로 다른 직교 행렬을 적용해야 한다는 것을 빠르게 알았다.\n' +
      '\n' +
      '각 블록에 사용된 직교 행렬이 서로 다른 경우 모델을 벗어나는 것으로 표시할 수 있음\n' +
      '\n' +
      '그림 3: LayerNorm에서 RMSNorm으로 변압기 네트워크를 변환하는 단계: 스케일 행렬\\(\\text{diag}(\\mathbf{\\alpha})\\)은 후속 행렬\\(\\overline{\\mathbf{W}_{\\text{in}}\\)에 흡수된다. 그림은 블록을 결합된 색상으로 보여줍니다. 간결함을 위해 \\((\\mathbf{\\alpha})\\)를 사용한다. 평균-감산 행렬\\(\\overline{\\mathbf{M}}\\)은 각 행렬\\(\\overline{\\mathbf{W}_{\\text{out}}\\)에 적용된다. LayerNorm은 상수 \\(\\sqrt{D}\\)까지 RMSNorm이 된다(도시되지 않음). 여기서, 스케일링 \\((\\mathbf{\\alpha}^{\\prime})\\)은 이전 블록으로부터 나온다.\n' +
      '\n' +
      '정리 1과 동일한 증명을 사용하여unchanged하고,\n' +
      '\n' +
      '알고리즘 1의 라인 5를 제외하고 여기서 잔차 연결과 블록의 출력이 동일한 회전을 가져야 한다는 것을 알 수 있다. 이를 해결하기 위해 선형 변환 \\(\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}\\)을 잔차에 적용하여 잔차 연결을 수정한다. 그림 4는 잔차 연결에서 추가적인 선형 연산으로 서로 다른 회전이 서로 다른 블록에 어떻게 적용될 수 있는지를 보여준다. 가중치 행렬의 수정과 달리 이러한 추가 연산은 미리 계산될 수 없으며 모델에 작은(\\(D\\times D\\)) 오버헤드를 추가한다. 그럼에도 불구하고, 그들은 모델을 슬라이싱하는 것을 허용하는데 필요하고(섹션 3.4), 우리는 전반적으로 실제 속도 향상을 본다(섹션 4).\n' +
      '\n' +
      '행렬을 계산하기 위해 PCA(PCA)를 사용한다. 트레이닝 세트에서 교정 데이터 세트를 선택하고 모델을 통해 실행(레이어노름 연산을 RMSNorm으로 변환한 후)하고 레이어의 직교 행렬을 추출한다. 우리는 변환된 네트워크의 출력을 사용하여 다음 레이어의 직교 행렬을 계산한다. 보다 정확하게는, 교정 데이터세트에서 \\(\\mathbf{X}_{\\ell,i}\\)이 \\(\\ell^{\\text{th}}\\) 시퀀스에 대한 \\(\\mathbf{X}_{\\ell,i}\\) RMSNorm 블록의 출력인 경우, 이를 계산한다.\n' +
      '\n' +
      '\\[\\mathbf{C}_{\\ell}=\\sum_{i}\\mathbf{X}_{\\ell,i}^{\\top}\\mathbf{X}_{\\ell,i} \\tag{8}\\]\n' +
      '\n' +
      '그리고 \\(\\mathbf{Q}_{\\ell}\\)을 \\(\\mathbf{C}_{\\ell}\\)의 고유벡터로 설정하고, 고유값을 감소시켜 정렬한다.\n' +
      '\n' +
      '### Slicing\n' +
      '\n' +
      '주성분 분석의 목표는 일반적으로 데이터 행렬\\(\\mathbf{X}\\)을 취하여 낮은 차원 표현\\(\\mathbf{Z}\\)과 근사 재구성\\(\\tilde{\\mathbf{X}\\)을 계산하는 것이다:\n' +
      '\n' +
      '\\mathbf{Q}\\mathbf{D}\\,\\qquad\\tilde{\\mathbf{X}=\\mathbf{Z}\\mathbf{Z}\\mathbf{D}^{\\top}\\mathbf{Q}^{\\top}\\,.\\tag{9}\\\n' +
      '\n' +
      '여기서 \\(\\mathbf{Q}\\)는 \\(\\mathbf{X}^{\\top}\\mathbf{X}\\)의 고유벡터이고, \\(\\mathbf{D}\\)는 \\(D\\times D_{\\text{small}}\\)의 삭제 행렬( \\(D\\times D\\times D\\)의 열들 포함)이며, 이 행렬의 열들 중 일부를 왼쪽으로 제거한다. 복원은 \\(L_{2}\\) 최적이며, \\(\\mathbf{Q}\\mathbf{D}\\)는 \\(\\\\mathbf{X}-\\tilde{\\mathbf{X}\\|^{2}\\)을 최소화하는 선형 매핑이다.\n' +
      '\n' +
      '블록간의 신호행렬 \\(\\mathbf{X}\\)에 PCA를 적용할 때, 우리는 \\(N\\times D\\)의 신호행렬을 결코 구체화하지 않지만, 이미 \\(\\mathbf{Q}\\)을 곱한 그 행렬의 작성과 선행 연산에 삭제행렬 \\(\\mathbf{D}\\)을 적용한다. 우리는 \\(\\mathbf{W}_{\\text{in}\\)의 행과 \\(\\mathbf{W}_{\\text{out}\\) 및 \\(\\mathbf{W}_{\\text{embd}\\)의 열을 삭제한다. 또한 잔차 연결에 삽입한 행렬의 행과 열은 모두 삭제한다.\n' +
      '\n' +
      '##4 실험 검증\n' +
      '\n' +
      'SetupWe는 PyTorch(Paszke et al., 2019)로 우리의 코드를 구현하기 위해 HuggingFace Transformers(Wolf et al., 2019)를 사용한다. Llama-2 70B 모델을 완성하는 데 약 3.5시간이 소요되며, 80GB의 메모리를 갖는 단일 H100 GPU에서 \\(\\mathbf{Q}\\)의 계산을 수행한다. PCA 계산 과정에서 공분산 행렬의 고유벡터를 계산하기 위해 이중 정밀도를 사용한다. 우리\n' +
      '\n' +
      '그림 4: 네트워크가 RMSNorm으로 변환된 상태에서(그림 3 참조), 우리는 계산 불변 개념을 적용한다. 입력 가중치 행렬\\(\\operatorname{\\overline{diag}(\\boldsymbol{\\alpha})\\mathbf{W}_{\\text{in}\\)을 \\(\\mathbf{Q}^{\\top}\\)으로 미리 곱한다. 출력행렬 \\(\\mathbf{W}_{\\text{out}\\mathbf{M}\\)은 \\(\\mathbf{Q}\\)으로 후곱해진다. 스킵-접속에서 새로운 선형층이 추가된다.\\(\\mathbf{Q}_{\\ell}^{\\top}\\mathbf{Q}_{\\ell\\in\\mathbbm{1}}\\. 이러한 수정 후에, 매트릭스들은 슬라이스(해칭된 영역들)될 수 있다.\n' +
      '\n' +
      'PyTorch에서 고유 벡터 계산에 단일 정밀도를 사용하면 부록 A.2에 자세히 설명된 대로 최종 정확도의 불일치가 발생한다는 것을 발견한다.\n' +
      '\n' +
      '우리는 WikiText-2 훈련 데이터세트(Merity et al., 2016)에서 1024개의 샘플과 Alpaca 훈련 데이터세트(Taori et al., 2023)에서 5000개의 샘플로 두 가지 다른 교정 세트를 실험한다. 시퀀스 길이는 각 언어 모델의 최대값으로 선택된다. 교정 세트 크기 및 서열 길이에 대한 절제 연구는 부록 A.3에 제시되어 있다.\n' +
      '\n' +
      '모델, 태스크 및 GPU 우리는 OPT(Zhang et al., 2022), Llama-2(Touvron et al., 2023) 모델 패밀리에 대한 모든 실험을 평가하고 Phi-2(우리의 제로 샷 태스크에서) 실험을 추가로 평가한다. OPT 175B는 더 작은 라마-2 모델보다 성능이 우수하기 때문에 제외한다. 그럼에도 불구하고, 우리는 더 큰 모델이 일반적으로 압축에 더 유망한 기회를 제공하기 때문에 이 더 큰 모델이 개선된 결과를 산출할 것으로 예상한다(섹션 4.1 참조). 우리는 우리의 방식을 언어 생성과 인기 있는 제로 샷 작업 모두에 대해 평가한다. SliceGPT가 달성한 포괄적인 속도 향상을 입증하기 위해 우리는 24GB의 메모리를 사용하는 쿼드로 RTX6000 GPU를 소비자 수준 GPU의 대표적인 예로 사용하고 40GB A100과 80GB H100을 사용하여 데이터 센터 수준 벤치마크를 제공한다.\n' +
      '\n' +
      '기준 설정 처음에 가장 작은 규범으로 열(또는 행)을 가지치기한 스킴과 결과를 비교할 계획이었지만 몇 개의 열만 가지치기한 후 모델의 당혹감이 1000대로 치솟는 등 이 기준선이 매우 열악하다는 것을 발견했다. 그 대신 SliceGPT와 SparseGPT(Frantar and Alistarh, 2023)를 2:4 희소성 비율을 사용하여 비교하였는데, 이는 스피드업을 달성하는 유일한 희소성 기법이기 때문이다(Mishra et al., 2021).\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '생성 작업 위키텍스트-2 데이터 세트를 사용하여 우리의 발견을 보여주는 것으로 시작한다. 이러한 맥락에서 우리는 이 데이터 세트에 적용할 때 서로 다른 크기에 걸쳐 OPT 및 Llama-2 모델 패밀리의 성능을 평가한다. 표 1은 슬라이싱 수준을 변경하여 얻은 복잡성을 보여준다. SliceGPT는 해당 모델의 스펙트럼 분석에서 우리의 직관과 일치하는 Llama-2 모델에 비해 OPT 모델에 적용할 때 우수한 성능을 나타낸다(토론은 부록 A.4 참조). SliceGPT의 성능은 모델 크기가 증가함에 따라 향상된다. SliceGPT와 SparseGPT를 비교하면, 모든 Llama-2 모델에서 SparseGPT 2:4가 \\(25\\%\\) 슬라이스를 사용한 SliceGPT보다 성능이 더 나쁘다는 것을 알 수 있다. OPT의 경우, 2.7B를 제외한 모든 모델 크기에 대해 \\(30\\%\\) 슬라이스 모델이 2:4 희소성을 능가함을 알 수 있다.\n' +
      '\n' +
      'Zero-shot TaskWe assess SliceGPT across 5개의 잘 알려진 Zero-shot task: PIQA (Bisk et al., 2020); WinoGrande (Sakaguchi et al., 2021); HellaSwag (Zellers et al., 2019); ARC-e and ARC-c (Clark et al., 2018). 유사한 작업(Frantar and Alistarh, 2023; Dettmers et al., 2022; Frantar et al., 2022; Dettmers et al., 2023)에 이어서, 우리는 평가들에서 디폴트 파라미터들과 함께 LM 평가 Harness(Gao et al., 2021)를 사용한다.\n' +
      '\n' +
      '그림 5는 이러한 작업에 걸쳐 슬라이스된 모델에 의해 달성된 평균 점수를 보여준다. 그림의 상단 행은 위키텍스트-2를 보정에 사용할 때의 평균 정확도를 나타내고 하단 행은 Alpaca를 보정에 사용할 때의 정확도를 나타낸다. 결과에서 생성 태스크와 유사한 패턴을 관찰한다: OPT 모델은 Llama-2 모델보다 압축에 더 적합하고 더 큰 모델에서는 정확도의 감소가 덜 두드러진다. 여기서 우리는 또한 Phi-2 모델을 포함한다:\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|} \\hline \\hline \\multirow{3}{*}{} & \\multirow{3}{*}{Sparsity} & \\multicolumn{8}{c|}{OPT} & \\multicolumn{2}{c|}{Llama-2} \\\\  & & 125M & 1.3B & 2.7B & 6.7B & 13B & 30B & 66B & 7B & 13B & 70B \\\\ \\hline Dense & \\(0\\%\\) & 27.65 & 14.63 & 12.47 & 10.86 & 10.13 & 9.56 & 9.34 & 5.11 & 4.57 & 3.11 \\\\ \\hline SparseGPT & 2:4 & 45.07 & 29.61 & 14.90 & 13.00 & 11.80 & 10.53 & 10.22 & 8.15 & 6.63 & 4.70 \\\\ \\hline \\multirow{3}{*}{SliceGPT} & \\(10\\%\\) & 29.44 & 15.08 & 12.80 & 11.00 & 10.26 & 9.63 & 9.41 & 5.47 & 4.88 & 3.46 \\\\  & \\(20\\%\\) & 34.02 & 16.37 & 13.76 & 11.54 & 10.64 & 9.86 & 9.57 & 6.16 & 5.46 & 4.00 \\\\  & \\(25\\%\\) & 37.94 & 17.43 & 14.60 & 11.97 & 10.94 & 10.05 & 9.70 & 6.70 & 5.90 & 4.35 \\\\  & \\(30\\%\\) & 44.17 & 19.05 & 15.91 & 11.36 & 11.36 & 10.29 & 9.87 & 7.51 & 6.52 & 4.77 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 위키텍스트2 상의 OPT 및 Llama-2 복잡성 결과. OPT의 경우 교정 세트 크기는 512이고 시퀀스 길이는 2048이다. Llama-2의 경우 교정 세트 크기는 1024이고 시퀀스 길이는 4096이다.\n' +
      '\n' +
      'Phi-2 모델의 슬라이스 버전은 Llama-2 7B 모델의 슬라이스 버전과 비슷하다는 것을 알 수 있다. 가장 큰 OPT 및 Llama-2 모델은 66B OPT 모델의 30%를 제거할 때 몇 퍼센트 포인트 손실로 매우 효과적으로 압축될 수 있다.\n' +
      '\n' +
      '우리는 여기서 회복 미세 조정(recovery fine-tuning, RFT)을 추가로 실험한다. 우리는 Ma et al.(2023a)의 아이디어에 따라 LoRA(Hu et al., 2021)를 사용하여 슬라이스된 Llama-2 및 Phi-2 모델에 소량의 RFT를 적용한다. 위키텍스트-2로 슬라이스된 모델의 경우 약 1k개의 시퀀스를 사용하고 알파카 데이터 세트로 슬라이스된 모델의 경우 5k를 사용한다. 모든 RFT에 대해 \\(lora\\_r=32\\), \\(lora\\_alpha=10\\) 및 시퀀스 길이 1024를 사용하고, HuggingFace PEFT 패키지(Mangrwalkar et al., 2022)에서 다른 모든 하이퍼파라미터에 대한 디폴트들을 사용한다.\n' +
      '\n' +
      '도 6은 그 결과를 나타낸다. 위키텍스트-2와 알파카 데이터 세트에서 RFT 간의 현저한 차이를 볼 수 있으며 알파카 데이터 세트는 훨씬 더 높은 성능 모델을 제공한다. 우리는 이 차이를 알파카와 벤치마크 작업 간의 유사성에 기인한다. 30%로 슬라이스된 가장 큰 Llama-2 70B 모델의 경우 알파카의 RFT를 사용하여 조밀한 모델의 76.6%에 비해 평균 74.3%의 정확도를 달성할 수 있다. 슬라이스된 모델은 약 51.6B 매개변수를 가지며 나중에 입증할 때 처리량이 상당히 향상된다.\n' +
      '\n' +
      '우리는 Phi-2가 위키텍스트-2 데이터 세트만을 사용하여 슬라이싱으로 인한 정확도 저하를 복구할 수 없지만 Alpaca를 사용하여 몇 퍼센트 포인트를 복구할 수 있음을 알 수 있다. 25% 슬라이싱 및 RFT를 사용한 Phi-2의 평균 정확도는 65.2%인 반면 조밀한 모델의 경우 72.2%이다. 슬라이스된 모델은 약 2.2B 파라미터를 가지며 2.8B 모델의 정확도의 90.3%를 유지한다. 이것은 작은 LMs도 훈련 후 가지치기의 혜택을 받을 수 있음을 보여준다. 각 작업에 대한 정확도의 표는 부록 A.5에 나와 있다.\n' +
      '\n' +
      'Benchmarking Throughput은 \\(\\mathbf{W}_{\\text{in}\\) 및 \\(\\mathbf{W}_{\\text{out}\\)에 희소성을 도입하는 기존의 희소성 방법과 달리, SliceGPT는 \\(\\mathbf{X}\\)에 (구조화된) 희소성을 도입한다: \\(\\mathbf{X}\\)의 전체 컬럼을 슬라이스하여 임베딩 차원을 감소시킨다. 이렇게 하면 압축 모델 내에서 계산 복잡도(플롭 내)와 데이터 이동이 모두 향상됩니다.\n' +
      '\n' +
      '25%와 50%로 슬라이스된 모델의 토큰 처리량은 80GB H100 GPU의 조밀한 모델과 비교된다. 시퀀스 길이를 128로 설정하고 GPU가 메모리가 부족하거나 처리량이 떨어질 때까지 배치 크기를 두 배로 늘려 최대 처리량을 찾는다. 25% 슬라이스 모델은 고밀도 모델보다 최대 1.55\\(\\times\\)의 처리량 향상을 달성했다. 50% 슬라이싱에서 가장 큰 모델은 2개의 GPU 대신 1개의 GPU만 필요로 하며, 처리량은 3.13\\(\\times\\)과 1.87\\(\\times\\)으로 크게 증가한다. 이는 고정된 수의 GPU에 대해 조밀한 모델의 6.26\\(\\times\\) 및 3.75\\(\\times\\)의 처리량을 달성함을 의미한다. 우리는 SliceGPT의 WikiText2 perplexity가 50%일 때 SparseGPT 2:4보다 더 나쁘지만, 처리량은 \\(\\mathbf{X}\\)을 슬라이스하지 않는 희소 방법으로 달성할 수 있는 것보다 훨씬 더 높다는 것을 주목한다. 크기 13B의 모델의 경우 배치 크기 증가에 따른 성능 증가가 덜 두드러진다.\n' +
      '\n' +
      '그림 5: 보정을 위해 위키텍스트-2(위) 및 알파카(아래) 데이터세트로 슬라이싱한 후 여러 작업에 걸쳐 OPT, Llama-2 및 Phi-2에 대한 평균 제로 샷 정확도.\n' +
      '\n' +
      '모델이 GPU 메모리를 거의 차지하지 않기 때문입니다. 소비자 등급 GPU(메모리가 적은)에서는 이러한 더 작은 모델에 대한 처리량이 향상될 가능성이 높다. 자세한 내용은 부록 A.6을 참조하십시오.\n' +
      '\n' +
      '추론 TimeNext는 SliceGPT로 압축된 모델의 end-to-end 런타임을 연구한다. 표 2는 쿼드로 RTX6000 및 A100 GPU에서 OPT 66B 및 Llama-2 70B 모델에서 단일 토큰을 생성하는 시간을 비교한다. 우리는 25% 슬라이싱을 사용할 때 RTX6000 GPU에서 16-17%, A100에서 11-13%의 속도를 관찰한다. 우리는 두 경우 모두에서 사용되는 GPU의 수를 줄여 밀집 모델의 배치에 비해 에너지 및 비용 절감을 제공한다. Llama-2 70B의 경우 RTX6000 GPU를 사용하여 계산량을 1764 GPU에서 1075 GPU4로 64% 감소시켰으며, 이러한 개선은 가중치 매트릭스를 더 작은 매트릭스로 대체하고 압축된 모델에서 조밀한 커널을 사용하는 접근법에 기인하며, 이는 다른 가지치기 방식과는 불가능하다.\n' +
      '\n' +
      '각주 4: 우리의 포옹 페이스 기반 테스트는 지속적인 배치나 모델 샤딩을 즐기지 않습니다. 이는 추론 시간 측면에서 GPU 측면에서 조밀 모델이 슬라이스 모델보다 더 향상될 수 있음을 의미한다. 그럼에도 불구하고, 우리의 측정 _do_는 그러한 배치에서 토큰당 에너지 사용을 반영한다.\n' +
      '\n' +
      '작성 시 기준선 SparseGPT 2:4에서는 종단 간 성능 향상이 실현 가능하지 않습니다. 대신 변압기 층에 관련된 각 작업의 상대적 타이밍을 비교하여 SliceGPT와 SparseGPT 2:4를 비교한다. SliceGPT(25%)는 SparseGPT(2:4)와 비교하여 대형 모델에 대한 속도 향상 및 복잡성 측면에서 경쟁력이 있음을 알 수 있다. 자세한 내용은 부록 A.7을 참조하십시오.\n' +
      '\n' +
      'Compute costAll Llama-2, OPT 및 Phi-2 모델은 1~3시간 내에 단일 GPU 상에서 슬라이스될 수 있다. 회복 미세 조정을 통해 표 3과 같이 모든 LM을 총 1~5시간 내에 압축한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|} \\hline \\hline \\multicolumn{1}{|c|}{GPU Type} & Slicing & \\multicolumn{2}{c|}{OPT 66B} & \\multicolumn{2}{c|}{Llama-2 70B} \\\\ \\hline \\multirow{3}{*}{A100 (40GB)} & Dense & 114ms on 4 GPUs & 456 GPUs & 125ms on 4 GPUs & 500 GPUs \\\\ \\cline{2-5}  & \\(25\\%\\) & 102ms on 3 GPUs & 306 GPUs & 110ms on 3 GPUs & 330 GPUs \\\\ \\hline \\multirow{3}{*}{Quadro RTX6000} & Dense & 237ms on 6 GPUs & 1422 GPUs & 252ms on 7 GPUs & 1764 GPUs \\\\  & (24GB) & \\(25\\%\\) & 204ms on 5 GPUs & 1020 GPUs & 215ms on 5 GPUs & 1075 GPUs \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 길이 128(배치 크기가 1인)의 시퀀스를 생성할 때 조밀한 모델에 걸쳐 SliceGPT의 토큰당 평균 추론 시간. 각각의 경우에, 우리는 ms로 소요된 시간, 필요한 GPU의 수 및 GPU에서의 총 계산을 보여준다.\n' +
      '\n' +
      '그림 6: 슬라이싱 및 복구 미세 조정(RFT) 후 여러 작업에 걸쳐 라마-2 및 Phi-2에 대한 평균 제로 샷 정확도. 왼쪽: 보정 및 RFT에 사용된 위키텍스트-2입니다. 오른쪽: 보정 및 RFT에 사용되는 알파카입니다. 광범위한 검색에도 불구하고 OPT 모델에서 향상된 성능을 가능하게 하는 RFT 매개변수를 찾을 수 없었다.\n' +
      '\n' +
      '##5 결론 및 향후 과제\n' +
      '\n' +
      '우리는 대규모 언어 모델에 대해 구조화된 가지치기를 허용하는 SliceGPT를 도입했다. 추가적인 코드 최적화 없이 40GB A100 GPU에서 Llama-2 70B의 추론 비용을 조밀한 모델의 66%로 줄여 SparseGPT 2:4보다 더 나은 유지 복잡성을 유지하면서 더 적은 GPU(4에서 3)를 필요로 하고 24GB RTX6000 GPU에서 추론 비용이 64%로 감소하여 2개의 GPU(7에서 5)를 필요로 한다. 제로 샷 다운스트림 작업에서 OPT 66B, Llama-2 70B 및 Phi-2를 25%로 슬라이싱하는 것은 조밀한 성능의 99%, 96% 및 87%를 유지한다. 회수 미세 조정으로 25% 슬라이스 Llama-2 70B 및 Phi-2는 각각 99% 및 90%로 증가했다.\n' +
      '\n' +
      '우리의 방법을 기반으로 할 기회가 남아 있다. 더 작지만 조밀한 LM은 13B 매개변수 또는 유사한 크기로 덜 가지치기된 LM보다 더 나은 성능을 발휘하지만 이것이 오랫동안 그대로 유지될 것으로 기대하지는 않는다. 우리의 가지치기 모델은 SparseGPT로 가지치기된 모델보다 더 많은 매개변수를 가지고 있지만, 우리의 방법은 GPU 메모리에 더 큰 배치 크기를 로드할 수 있으며 희소성 구조에 대한 오버헤드가 없다. 아마도 결합된 방법이 둘 중 최고를 얻을 수 있을 것이다. 다른 계산 방법(\\(\\mathbf{Q}\\)은 결과를 개선할 수 있다. 추론 시간 및 GPU 카운트를 더 감소시키기 위해, 양자화를 포함하는 상보적 방법들(Xiao et al., 2023; Dettmers et al., 2022; Ashkboos et al., 2023; Dettmers et al., 2023; Frantar et al., 2022) 및 구조적 프루닝(예를 들어, Ma et al., 2023b)이 사용될 수 있다.\n' +
      '\n' +
      '우리는 계산 불변성에 대한 우리의 관찰이 딥 러닝 모델의 효율성을 향상시키는 데 있어 향후 연구에 도움이 될 수 있고 아마도 새로운 이론적 통찰력을 영감을 줄 수 있기를 바란다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Ashkboos et al. (2023) Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. Generative large language model에 대한 end-to-end 4-bit 추론을 위한 _ arXiv preprint arXiv:2310.09259_, 2023.\n' +
      '* Ba et al. (2016) Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 레이어 정규화 ArXiv:1607.06450_, 2016.\n' +
      '* Bisk 등(2020) 요나탄 비스크, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 피카: 자연어로 물리적 상식에 대한 추론. 2020년 인공 지능에 관한 34번째 AAAI 회의에서.\n' +
      '* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 질문에 답하는 걸 해결했다고 생각해? try arc, the ai2 reasoning challenge. _ ArXiv_, abs/1803.05457, 2018. URL[https://api.semanticscholar.org/CorpusID:3922816](https://api.semanticscholar.org/CorpusID:3922816).\n' +
      '* Dettmers et al.(2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 엘름 int8(): 스케일에서 트랜스포머에 대한 8비트 행렬 곱셈; _ ArXiv:2208.07339_, 2022.\n' +
      '*Dettmers et al. (2023) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: 거의 손실 없는 LLM 가중치 압축을 위한 희소 양자화 표현 _ arXiv preprint arXiv:2306.03078_, 2023.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|c|} \\hline \\multirow{2}{*}{Model} & \\multicolumn{2}{c|}{SliceGPT 30\\%} & \\multicolumn{2}{c|}{Recovery fine-tuning} & \\multirow{2}{*}{Total} \\\\  & \\multicolumn{1}{c|}{Time} & \\multicolumn{1}{c|}{GPUs} & \\multicolumn{1}{c|}{Time} & \\multicolumn{1}{c|}{GPUs} \\\\ \\hline Llama-2 7B & 0h44m & 1xH100 80GB & 0h23m & 1xH100 80GB & 1h07m \\\\ Llama-2 13B & 1h08m & 1xH100 80GB & 0h44m & 1xH100 80GB & 1h52m \\\\ Llama-2 70B & 3h31m & 1xH100 80GB & 1h35m & 4xH100 80GB & 5h06m \\\\ \\hline Phi-2 & 0h49m & 1xV100 32GB & 1h59m & 1xV100 32GB & 2h48m \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: SliceGPT로 30% 슬라이스하고 Alpaca 데이터 세트를 사용하여 복구 미세 조정을 수행하는 비용 계산. 여기에서 Llama-2 모델의 경우 1024, Phi-2의 경우 2048의 보정 세트 크기를 사용한다.\n' +
      '\n' +
      '* Frantar and Alistarh (2022) Ellias Frantar and Dan Alistarh. 최적의 뇌 압축: 정확한 훈련 후 양자화 및 프루닝을 위한 프레임워크. _ 신경 정보 처리 시스템_, 35:4475-4488, 2022에서의 발전.\n' +
      '* Frantar and Alistarh (2023) Ellias Frantar and Dan Alistarh. SparseGPT: 대규모 언어 모델은 원샷으로 정확하게 가지치기할 수 있다. 2023년\n' +
      '* Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: 생성적 사전 훈련된 변압기에 대한 정확한 사후 훈련 양자화. _ ArXiv:2210.17323_, 2022.\n' +
      '* Gale et al. (2019) Trevor Gale, Erich Elsen, and Sara Hooker. 2019년 심층 신경망의 희소성 상태.\n' +
      '* Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A framework for the few-shot language model evaluation. _ Version v.0.0.1. Sept_, 2021.\n' +
      '* Gholami et al. (2021) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. 마호니, 커트 커처 효율적인 신경망 추론을 위한 양자화 방법에 대한 조사. _ CoRR_, abs/2103.13630, 2021. URL[https://arxiv.org/abs/2103.13630](https://arxiv.org/abs/2103.13630).\n' +
      '* Gupta & Agrawal (2021) Manish Gupta and Puneet Agrawal. 텍스트에 대한 딥러닝 모델의 압축: A survey, 2021.\n' +
      '* Han et al. (2016) Song Han, Huizi Mao, and William J. Dally. 심층 압축: 2016년, 가지치기, 훈련된 양자화 및 허프만 코딩으로 심층 신경망을 압축한다.\n' +
      '* Hassibi et al. (1993) Babak Hassibi, David G Stork, and Gregory J Wolff. 최적의 뇌 외과의와 일반 네트워크 가지치기. In _IEEE international conference on neural networks_, pp. 293-299. IEEE, 1993.\n' +
      '* He et al.(2017) Yihui He, Xiangyu Zhang, Jian Sun. 매우 깊은 신경망을 가속화하기 위한 채널 가지치기. In _Proceedings of the IEEE international conference on computer vision_, pp. 1389-1397, 2017.\n' +
      '* Hoefler et al. (2021) Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. 딥러닝에서의 희소성: 신경망에서의 효율적인 추론 및 학습을 위한 프루닝 및 성장_ CoRR_, abs/2102.00554, 2021. URL[https://arxiv.org/abs/2102.00554](https://arxiv.org/abs/2102.00554).\n' +
      '* Hu et al. (2021) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: 2021년, 대형 언어 모델의 낮은 순위 적응.\n' +
      '* Huang & Wang (2018) Zehao Huang and Naiyan Wang. 딥 뉴럴 네트워크를 위한 데이터 기반 희소 구조 선택. In _Proceedings of the European conference on computer vision (ECCV)_, pp. 304-320, 2018.\n' +
      '* LeCun et al. (1989) Yann LeCun, John Denker, and Sara Solla. 최적의 뇌 손상이야 신경 정보 처리 시스템_, 2, 1989의 발전.\n' +
      '* Liu et al.(2017) Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. 네트워크 슬림화를 통해 효율적인 컨볼루션 네트워크를 학습합니다. In _Proceedings of the IEEE international conference on computer vision_, pp. 2736-2744, 2017.\n' +
      '* Luo et al.(2017) Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: 심층 신경망 압축을 위한 필터 레벨 가지치기 방법. In _Proceedings of the IEEE international conference on computer vision_, pp. 5058-5066, 2017.\n' +
      '* Ma et al. (2023a) Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: 대규모 언어 모델의 구조적 pruning. _ arXiv preprint arXiv:2305.11627_, 2023a. URL[https://arxiv.org/pdf/2305.11627.pdf](https://arxiv.org/pdf/2305.11627.pdf)\n' +
      '* Ma et al. (2023b) Xinyin Ma, Gongfan Fang, and Xinchao Wang. LLM-pruner: 2023b에서 대형 언어 모델의 구조적 가지치기.\n' +
      '* Mahabadi et al. (2021) Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layer, 2021.\n' +
      '* Mangrulkar et al. (2022) Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. Peft: 최첨단 파라미터 효율적인 미세 조정 방법. [https://github.com/huggingface/peft] (https://github.com/huggingface/peft), 2022.\n' +
      '\n' +
      '* Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 포인터 센티넬 혼합 모델 ArXiv:1609.07843_, 2016.\n' +
      '* Mishra et al. (2021) Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. 희소 심층 신경망을 가속화합니다. _ arXiv preprint arXiv:2104.08378_, 2021.\n' +
      '* Noach & Goldberg (2020) Matan Ben Noach and Yoav Goldberg. 행렬 분해에 의해 미리 훈련된 언어 모델을 압축합니다. _Proceedings of the 1th Asian-Pacific chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing_, pp. 884-889, Suzhou, China, December 2020. Association for Computational Linguistics. URL[https://aclanthology.org/2020.aacl-main.88](https://aclanthology.org/2020.aacl-main.88)\n' +
      '* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Kimelshein, Luca Antiga, et al. PyTorch: An imperative style, high performance deep learning library. _ 신경 정보 처리 시스템_, 32, 2019의 발전.\n' +
      '* Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Generative Pre-training에 의한 언어 이해력 향상. 2018년\n' +
      '* Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: 적대적 winograd schema challenge at scale. _ ACM_, 64(9):99-106, 2021의 통신.\n' +
      '* Singh & Alistarh (2020) Sidak Pal Singh and Dan Alistarh. Woodfisher: 신경망 압축을 위한 효율적인 2차 근사치 _ 신경 정보 처리 시스템_, 33:18098-18109, 2020에서의 발전.\n' +
      '* Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 대용량 언어 모델에 대한 간단하고 효과적인 프루닝 접근법. _ arXiv preprint arXiv:2306.11695_, 2023.\n' +
      '* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 스탠포드 알파카: 지시를 따르는 라마 모델. [https://github.com/tatsu-lab/stanford_alpaca] (https://github.com/tatsu-lab/stanford_alpaca), 2023.\n' +
      '* Touvron et al. (2021) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Bhargava, Shruti Bhosale, Dan Bikel, Likas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Bucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Bynthia Kardas, Vedantaj Goswami, Naman Goyal, Punit Singhouman, Saghar Hosseini, Rui Hou, Diana Liskovich, Yuan Silva, Eric Michael Smith, Ranjan Subramanian Tan, Binh Tang, Ross Taylor, Adina Williams, Zian Zaran Zarov, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, S. 라마 2: 오픈 파운데이션 및 미세 조정된 채팅 모델, 2023.\n' +
      '*Tukan et al. (2020) Murad Tukan, Alaa Maalouf, Matan Weksler, and Dan Feldman. Compressed deep networks: Goodbye SVD, hello robust low-rank approximation. _ arXiv preprint arXiv:2009.05647_, 2020.\n' +
      '* van der Oudera et al. (2023) Tycho FA van der Oudera, Markus Nagel, Mart van Baalen, Yuki M Asano, and Tijmen Blankevoort. llm 외과 의사요 arXiv preprint arXiv:2312.17244_, 2023.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 주목해 주세요 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Huggingface의 트랜스포머: 최첨단 자연어 처리. _ ArXiv preprint arXiv:1910.03771_, 2019.\n' +
      '\n' +
      '* Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: 대형 언어 모델을 위한 정확하고 효율적인 훈련 후 양자화. In _International Conference on Machine Learning_, pp. 38087-38099. PMLR, 2023.\n' +
      '* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, 요나탄 Bisk, Ali Farhadi, and Yejin Choi. 헬라스바그: 기계가 정말로 당신의 문장을 끝낼 수 있을까요? _ ArXiv preprint arXiv:1905.07830_, 2019.\n' +
      '* 장앤센리치(2019) 바이오장앤리코센리치. Root mean square layer normalization. _ Neural Information Processing Systems_, 32, 2019에서의 발전\n' +
      '* Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _ arXiv preprint arXiv:2205.01068_, 2022.\n' +
      '* Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. _ arXiv preprint arXiv:2303.18223_, 2023.\n' +
      '* Zhu & Gupta (2017) Michael Zhu and Suyog Gupta. 가지치기 또는 가지치기하지 않기: 2017년 모델 압축에 대한 가지치기의 효능을 탐구한다.\n' +
      '* Zhu et al.(2023) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 대형 언어 모델의 모델 압축에 대한 조사. _ arXiv preprint arXiv:2308.07633_, 2023.\n' +
      '\n' +
      'Appendix\n' +
      '\n' +
      '### 수학식 2의 증명\n' +
      '\n' +
      '직교행렬(\\(\\mathbf{Q}\\)은 (\\mathbf{Q}^{\\top}\\mathbf{Q}=\\mathbf{Q}\\mathbf{Q}^{\\top}=\\mathbf{I}\\)의 관계를 만족하는 정방행렬이다. 벡터의 norm은 원소의 제곱합의 제곱근이다. \\(\\|\\mathbf{x}\\|=\\sqrt{\\sum_{i}\\mathbf{x}_{i}^{2}=\\sqrt{\\mathbf{x}^{\\top}\\mathbf{x}}\\. 벡터에 \\(\\mathbf{Q}\\mathbf{x}\\|=\\sqrt{\\mathbf{x}^{\\top}\\mathbf{Q}^{\\top}\\mathbf{Q}\\mathbf{x}=\\|\\mathbf{x}\\|\\\\mathbf{x}\\|\\\n' +
      '\n' +
      'RMSNorm 연산은 입력 행렬의 각 행을 norm으로 나눈다. 선형대수의 기본규칙으로 \\(\\mathbf{x}\\)이 \\(\\mathbf{X}\\)의 행이라면 \\(\\mathbf{Q}^{\\top}\\mathbf{x}\\)은 \\(\\mathbf{X}\\mathbf{Q}\\)의 해당 행이다. RMSNorm을 \\(\\mathbf{X}\\mathbf{Q}\\)에 적용하면, 상기 행은 이제 \\(\\frac{1}{\\|\\mathbf{x}\\|}\\mathbf{Q}^{\\top}\\mathbf{x}\\)과 동일할 것이다. RMSnorm 후, 우리는 \\(\\mathbf{Q}^{\\top}\\)을 곱할 수 있다. 우리의 행은 이제 \\(\\frac{1}{\\|\\mathbf{x}\\|}\\mathbf{Q}\\mathbf{Q}\\mathbf{x}=\\frac{1}{\\|\\mathbf{x}\\|}\\bm{x}\\x}와 같다. 따라서 우리는 관계를 가지고 있다.\n' +
      '\n' +
      '\\text{RMSNorm}(\\mathbf{X}\\mathbf{Q})\\mathbf{Q}^{\\top}=\\text{RMSNorm}(\\mathbf{X})\\,.\\tag{10}\\\n' +
      '\n' +
      '### 단일 정밀 고유값 계산\n' +
      '\n' +
      '섹션 4에서 이전에 언급한 바와 같이 PCA 알고리즘을 수행할 때 이중 정밀도를 사용한다. 이러한 선택은 SliceGPT에서 직교 행렬의 계산 동안 발생할 수 있는 잠재적인 수치 오차를 완화하기 위해 이루어진다. 그럼에도 불구하고 PCA 계산에 더 낮은 정밀도를 사용하는 것이 궁극적인 정확도에 미치는 영향을 조사하는 것은 흥미롭다.\n' +
      '\n' +
      '표 4는 FP32 PCA를 스킴에 적용할 때 모든 모델의 복잡성을 보여준다. 그것은 PCA 계산 단계에서 더 큰 모델의 정확도가 수치 오차에 의해 영향을 받을 수 있음을 보여준다. 우리는 고유 벡터와 고유값을 계산하기 위해 PyTorch(torch.linalg)를 사용한다는 점에 유의해야 한다.\n' +
      '\n' +
      '### 교정 세트 크기 및 시퀀스 길이에 대한 민감도\n' +
      '\n' +
      '우리는 위키텍스트-2 교정 세트의 역할을 조사하기 위한 절제 연구를 제시한다. OPT 6.7B 및 Llama-2 7B 모델을 사용하여 25% 희소성을 갖는 생성 작업에 중점을 둔다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|} \\hline \\hline  & \\multirow{2}{*}{Sparsity} & \\multicolumn{5}{c|}{OPT} & \\multicolumn{5}{c|}{Llama-2} \\\\  & & 125M & 1.3B & 2.7B & 6.7B & 13B & 30B & 66B & 7B & 13B & 70B \\\\ \\hline Dense & \\(0\\%\\) & 27.65 & 14.63 & 12.47 & 10.86 & 10.13 & 9.56 & 9.34 & 5.47 & 4.88 & 3.32 \\\\ \\hline SparseGPT & 2:4 & 45.07 & 29.61 & 14.90 & 13.00 & 11.80 & 10.53 & 10.22 & 8.69 & 7.07 & 4.98 \\\\ \\hline  & \\(10\\%\\) & 29.48 & 15.15 & 12.83 & 11.05 & 10.28 & 9.68 & 9.45 & 6.51 & 5.64 & 4.20 \\\\ SliceGPT & \\(20\\%\\) & 34.12 & 16.51 & 13.87 & 11.64 & 10.73 & 9.94 & 9.80 & 7.30 & 6.07 & 5.82 \\\\  & \\(25\\%\\) & 38.25 & 17.67 & 14.78 & 12.14 & 11.08 & 10.15 & 9.81 & 8.52 & 6.65 & 7.01 \\\\  & \\(30\\%\\) & 44.17 & 19.33 & 16.20 & 12.82 & 11.53 & 10.43 & 9.99 & 10.41 & 7.49 & 8.75 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: FP32 PCA 계산을 사용하는 WikiText2에 대한 OPT 및 Llama-2 당혹성 결과. 모든 경우의 시퀀스 길이는 2048이다.\n' +
      '\n' +
      '그림 7: 보정 세트 크기 및 시퀀스 길이가 위키텍스트2의 복잡성에 미치는 영향.\n' +
      '\n' +
      '도 7의 (왼쪽)은 당혹감에 대한 캘리브레이션 세트의 크기를 가변한 결과를 나타낸다. 그것은 적어도 128의 샘플 크기가 우리의 교정 세트에 대한 합리적인 선택을 제공한다는 것을 보여준다.\n' +
      '\n' +
      '다음으로 보정 집합에서 서로 다른 시퀀스 길이 \\(N\\)를 사용하는 효과를 탐구한다. 고정 수의 \\(B\\) 샘플이 주어졌을 때, PCA 입력 행렬은 \\(NB\\) 임베딩 벡터를 사용하여 계산되며, 더 큰 \\(B\\) 또는 더 큰 \\(N\\)을 갖는 것 사이의 트레이드오프를 이해하는 것은 흥미롭다. 그림 7(오른쪽)은 교정 세트에서 시퀀스 길이를 128에서 4096으로 변경한 결과를 보여준다: 우리는 더 큰 시퀀스 길이를 갖는 것이 더 나은 당혹감을 초래할 수 있다는 결론을 내린다.\n' +
      '\n' +
      '이러한 통찰력을 사용하여 주요 실험에서 OPT의 경우 512, Llama-2의 경우 1024 및 OPT의 경우 서열 길이 2048, Llama-2의 경우 4096의 보정 세트 크기를 사용한다(표 1). 아래 표 5에서 우리는 더 작은 보정 세트 크기와 더 짧은 보정 시퀀스 길이로 위키텍스트-2에서 OPT 및 Llama-2 모델의 복잡성을 평가하며, 이는 둘 다 감소하는 경향이 모든 모델과 크기에 걸쳐 복잡성을 저하시킨다는 것을 확인시켜준다.\n' +
      '\n' +
      '라마-2 모델과 OPT 모델의### 스펙트럼 분석\n' +
      '\n' +
      '아래 그림은 OPT 6.7B 및 Llama-2 7B 모델에 대한 고유값 분포를 보여준다. 두 모델 모두 비슷한 매개변수 수를 가지고 있지만 Llama-2 모델은 임베딩 스펙트럼에서 더 엄격하게 압축된 분포를 가지고 있다. 이 관찰은 훨씬 더 많은 정보를 가진 지배적인 주성분들이 없다는 것을 보여주며, 이러한 성분들의 가지치기를 더 어려운 과제로 만든다.\n' +
      '\n' +
      '슬라이싱 수준을 미리 지정하는 대신 각 PCA 계산 동안 폐기하도록 총 분산의 분율을 설정했으며, 이는 각 매트릭스에서 잘라낼 행과 열의 수를 설정한다. 각 모델에 대해 25%에 가까운 네트워크에서 총 감소를 얻기 위해 다양한 목표 분산으로 세 가지 실험을 실행한다.\n' +
      '\n' +
      '도 8: 64개의 샘플을 사용하여 MLP 입력(로그 스케일)의 정규화(최대) 스펙트럼. Llama-2 모델의 첫 번째 층을 제외하고 두 모델의 고유값 분포는 후기 층에 비해 초기 층에서 더 빠른 감쇠를 보여준다. 이것은 이러한 초기 계층에서 직교 변환 후에 더 많은 양의 슬라이싱이 적용될 수 있음을 시사한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|} \\hline \\multirow{2}{*}{Method} & \\multicolumn{8}{c|}{OPT} & \\multicolumn{3}{c|}{Llama-2} \\\\  & 125M & 1.3B & 2.7B & 6.7B & 13B & 30B & 66B & 7B & 13B & 70B \\\\ \\hline Dense & 27.65 & 14.63 & 12.47 & 10.86 & 10.13 & 9.56 & 9.34 & 5.47 & 4.88 & 3.32 \\\\ \\hline SparseGPT 2:4 & 45.07 & 29.61 & 14.90 & 13.00 & 11.80 & 10.53 & 10.22 & 8.69 & 7.07 & 4.98 \\\\ \\hline SliceGPT (\\(10\\%\\)) & 29.50 & 15.16 & 12.82 & 11.07 & 10.29 & 9.65 & 9.43 & 5.94 & 5.31 & 3.79 \\\\ SliceGPT (\\(20\\%\\)) & 34.10 & 16.51 & 13.89 & 11.60 & 10.71 & 9.92 & 9.60 & 6.84 & 6.06 & 4.48 \\\\ SliceGPT (\\(25\\%\\)) & 38.24 & 17.67 & 14.76 & 12.10 & 11.04 & 10.13 & 9.75 & 7.55 & 6.63 & 4.89 \\\\ SliceGPT (\\(30\\%\\)) & 44.23 & 19.31 & 16.13 & 12.73 & 11.49 & 10.39 & 9.94 & 8.59 & 7.44 & 5.44 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 위키Text2에 대한 OPT 및 Llama-2 복잡성 결과. 모든 모델에 대해 교정 세트 크기는 128이고 시퀀스 길이는 2048이다.\n' +
      '\n' +
      '그 결과를 하기 표 6에 나타내었다. 계층별 슬라이싱 수준을 변화시키면 OPT 모델에서는 WikiText-\\(2\\)의 복잡성이 향상되지만 Llama-\\(2\\) 모델에서는 반대의 효과가 있다.\n' +
      '\n' +
      '### 상세한 제로샷 결과\n' +
      '\n' +
      '본 절에서는 논문에서 제시한 제로샷 과제의 세부 결과를 제시한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l|l|l|l|} \\hline \\hline Model & WikiText-2 PPL at 25\\% & WikiText-2 ppl varying slicing & PPL improvement \\\\  & constant slicing & by layer & \\\\ \\hline OPT 6.7B & 12.10 & 11.94, 24.7\\% total slicing & 0.16 \\\\ OPT 13B & 11.04 & 10.76, 24.2\\% total slicing & 0.28 \\\\ OPT 30B & 10.13 & 9.95, 24.8\\% total slicing & 0.18 \\\\ OPT 66B & 9.75 & 9.63, 24.1\\% total slicing & 0.12 \\\\ \\hline Llama-\\(2\\) 7B & 6.84 & 7.63, 24.1\\% total slicing & -0.79 \\\\ Llama-\\(2\\) 13B & 6.00 & 6.17, 23.3\\% total slicing & -0.17 \\\\ Llama-\\(2\\) 70B & 4.44 & 4.63, 25.5\\% total slicing & -0.19 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 층별로 슬라이싱 레벨을 변화시키는 것의 효과를 평가하는 것.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\hline \\hline Model & Slicing & PIQA & WinoGrande & HellaSwag & ARC-e & ARC-c & Avg. Score \\\\ \\hline \\hline \\multirow{4}{*}{OPT-1.3B} & Dense & 72.42 & 59.27 & 53.72 & 50.97 & 29.52 & 53.18 \\\\  & \\(20\\%\\) & 65.34 & 54.85 & 45.39 & 46.04 & 26.96 & 47.72 \\\\  & \\(25\\%\\) & 62.30 & 53.83 & 42.91 & 45.45 & 27.22 & 46.34 \\\\  & \\(30\\%\\) & 60.77 & 54.70 & 39.81 & 43.90 & 25.77 & 44.99 \\\\ \\hline \\multirow{4}{*}{OPT-2.7B} & Dense & 74.81 & 61.01 & 60.58 & 54.42 & 31.14 & 56.39 \\\\  & \\(20\\%\\) & 68.23 & 57.93 & 51.38 & 51.81 & 28.50 & 51.57 \\\\  & \\(25\\%\\) & 65.29 & 57.22 & 47.85 & 49.79 & 27.99 & 49.63 \\\\  & \\(30\\%\\) & 62.35 & 57.22 & 44.18 & 46.72 & 27.05 & 47.50 \\\\ \\hline \\multirow{4}{*}{OPT-6.7B} & Dense & 76.39 & 65.19 & 67.16 & 60.14 & 34.64 & 60.70 \\\\  & \\(20\\%\\) & 72.74 & 61.09 & 61.04 & 55.89 & 30.80 & 56.31 \\\\  & \\(25\\%\\) & 70.35 & 60.62 & 58.15 & 52.78 & 29.52 & 54.28 \\\\  & \\(30\\%\\) & 68.61 & 60.69 & 54.56 & 52.15 & 29.01 & 53.00 \\\\ \\hline \\multirow{4}{*}{OPT-13B} & Dense & 76.82 & 64.80 & 69.81 & 61.87 & 35.67 & 61.79 \\\\  & \\(20\\%\\) & 74.48 & 64.96 & 65.42 & 60.90 & 35.24 & 60.20 \\\\  & \\(25\\%\\) & 73.67 & 64.25 & 63.28 & 60.52 & 34.64 & 59.27 \\\\  & \\(30\\%\\) & 71.82 & 62.90 & 60.66 & 58.80 & 32.94 & 57.42 \\\\ \\hline \\multirow{4}{*}{OPT-3OB} & Dense & 78.07 & 68.19 & 72.27 & 65.24 & 38.23 & 64.40 \\\\  & \\(20\\%\\) & 76.50 & 66.61 & 70.61 & 64.18 & 35.75 & 62.73 \\\\  & \\(25\\%\\) & 75.30 & 66.61 & 69.42 & 63.55 & 35.67 & 62.11 \\\\  & \\(30\\%\\) & 74.97 & 65.04 & 68.15 & 63.55 & 34.64 & 61.27 \\\\ \\hline \\multirow{4}{*}{OPT-66B} & Dense & 79.82 & 68.90 & 74.85 & 67.21 & 40.02 & 66.16 \\\\  & \\(20\\%\\) & 78.73 & 67.88 & 73.79 & 68.81 & 39.51 & 65.74 \\\\  & \\(25\\%\\) & 78.40 & 67.09 & 73.33 & 67.89 & 39.16 & 65.17 \\\\  & \\(30\\%\\) & 77.42 & 66.30 & 72.62 & 66.90 & 37.97 & 64.24 \\\\ \\hline \\multirow{4}{*}{Llama-2 TB} & Dense & 79.11 & 69.06 & 75.99 & 74.58 & 46.25 & 69.00 \\\\  & \\(20\\%\\) & 69.42 & 65.11 & 59.04 & 59.76 & 37.54 & 58.18 \\\\  & \\(25\\%\\) & 66.87 & 63.38 & 54.16 & 58.46 & 34.56 & 55.48 \\\\  & \\(30\\%\\) & 63.55 & 61.33 & 49.62 & 51.77 & 31.23 & 51.50 \\\\ \\hline \\multirow{4}{*}{Llama-2 13B} & Dense & 80.47 & 72.22 & 79.39 & 77.48 & 49.23 & 71.76 \\\\  & \\(20\\%\\) & 71.87 & 69.38 & 63.04 & 69.87 & 43.09 & 63.45 \\\\  & \\(25\\%\\) & 68.55 & 67.48 & 58.10 & 62.50 & 37.88 & 58.90 \\\\  & \\(30\\%\\) & 66.10 & 65.11 & 52.69 & 56.82 & 35.07 & 55.16 \\\\ \\hline \\multirow{4}{*}{Llama-2 7OB} & Dense & 82.70 & 77.98 & 83.84 & 80.98 & 57.34 & 76.57 \\\\  & \\(20\\%\\) & 76.61 & 76.40 & 72.98 & 80.51 & 55.20 & 72.34 \\\\  & \\(25\\%\\) & 74.92 & 75.37 & 68.84 & 77.90 & 51.71 & 69.75 \\\\  & \\(30\\%\\) & 72.31 & 73.56 & 63.69 & 73.40 & 47.61 & 66.11 \\\\ \\hline \\multirow{4}{*}{Phi-2} & Dense & 79.11 & 75.77 & 73.83 & 78.32 & 54.18 & 72.24 \\\\  & \\(20\\%\\) & 71.87 & 67.80 & 57.76 & 58.00 & 35.32 & 58.15 \\\\ \\cline{1-1}  & \\(25\\%\\) & 69.21 & 65.35 & 52.40 & 53.70 & 31.66 & 54.46 \\\\ \\cline{1-1}  & \\(30\\%\\) & 65.94 & 63.14 & 47.56 & 53.03 & 30.29 & 51.99 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: OPT, Llama-2 및 Phi-2 모델 상의 위키Text2 데이터세트를 사용하여 슬라이싱할 때의 다운스트림 태스크 성능.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\hline \\hline Model & Slicing & PIQA & WinoGrande & HellaSwag & ARC-e & ARC-c & Avg. Score \\\\ \\hline \\hline \\multirow{4}{*}{OPT-1.3B} & Dense & 72.42 & 59.27 & 53.72 & 50.97 & 29.52 & 53.18 \\\\  & \\(20\\%\\) & 69.91 & 55.49 & 47.88 & 49.66 & 27.05 & 50.00 \\\\  & \\(25\\%\\) & 69.37 & 55.72 & 45.82 & 48.70 & 26.62 & 49.25 \\\\  & \\(30\\%\\) & 68.55 & 55.33 & 43.92 & 47.26 & 26.45 & 48.30 \\\\ \\hline \\multirow{4}{*}{OPT-2.7B} & Dense & 74.81 & 61.01 & 60.58 & 54.42 & 31.14 & 56.39 \\\\  & \\(20\\%\\) & 71.87 & 58.09 & 54.98 & 54.04 & 29.44 & 53.68 \\\\  & \\(25\\%\\) & 70.95 & 58.09 & 52.62 & 53.03 & 29.61 & 52.86 \\\\  & \\(30\\%\\) & 69.64 & 56.43 & 49.45 & 51.81 & 28.33 & 51.13 \\\\ \\hline \\multirow{4}{*}{OPT-6.7B} & Dense & 76.39 & 65.19 & 67.16 & 60.14 & 34.64 & 60.70 \\\\  & \\(20\\%\\) & 74.54 & 62.67 & 62.84 & 59.18 & 33.36 & 58.52 \\\\  & \\(25\\%\\) & 73.78 & 62.59 & 60.99 & 59.01 & 33.70 & 58.01 \\\\  & \\(30\\%\\) & 73.34 & 61.80 & 58.93 & 58.33 & 32.85 & 57.05 \\\\ \\hline \\multirow{4}{*}{OPT-13B} & Dense & 76.82 & 64.80 & 69.81 & 61.87 & 35.67 & 61.79 \\\\  & \\(20\\%\\) & 76.01 & 65.19 & 66.15 & 61.57 & 34.73 & 60.73 \\\\  & \\(25\\%\\) & 74.65 & 64.64 & 65.02 & 60.65 & 35.07 & 60.00 \\\\  & \\(30\\%\\) & 74.86 & 63.46 & 63.16 & 61.36 & 34.56 & 59.48 \\\\ \\hline \\multirow{4}{*}{OPT-3OB} & Dense & 78.07 & 68.19 & 72.27 & 65.24 & 38.23 & 64.40 \\\\  & \\(20\\%\\) & 78.35 & 66.61 & 70.64 & 65.19 & 37.46 & 63.65 \\\\  & \\(25\\%\\) & 77.48 & 65.82 & 69.58 & 65.91 & 37.63 & 63.28 \\\\  & \\(30\\%\\) & 76.93 & 64.96 & 68.66 & 65.70 & 37.12 & 62.67 \\\\ \\hline \\multirow{4}{*}{OPT-66B} & Dense & 79.82 & 68.90 & 74.85 & 67.21 & 40.02 & 66.16 \\\\  & \\(20\\%\\) & 79.49 & 68.19 & 73.69 & 67.26 & 39.25 & 65.58 \\\\  & \\(25\\%\\) & 79.11 & 68.35 & 73.30 & 67.00 & 38.74 & 65.30 \\\\  & \\(30\\%\\) & 79.05 & 68.75 & 72.62 & 66.29 & 38.31 & 65.00 \\\\ \\hline \\multirow{4}{*}{LLama-2 TB} & Dense & 79.11 & 69.06 & 75.99 & 74.58 & 46.25 & 69.00 \\\\  & \\(20\\%\\) & 76.50 & 65.51 & 65.20 & 69.99 & 41.21 & 63.68 \\\\  & \\(25\\%\\) & 74.21 & 64.01 & 60.55 & 66.88 & 38.91 & 60.91 \\\\  & \\(30\\%\\) & 72.25 & 59.83 & 55.86 & 63.93 & 37.80 & 57.93 \\\\ \\hline \\multirow{4}{*}{LLama-2 13B} & Dense & 80.47 & 72.22 & 79.39 & 77.48 & 49.23 & 71.76 \\\\  & \\(20\\%\\) & 77.97 & 68.90 & 69.64 & 74.71 & 45.99 & 67.44 \\\\  & \\(25\\%\\) & 76.88 & 67.40 & 65.85 & 72.52 & 44.54 & 65.44 \\\\  & \\(30\\%\\) & 74.10 & 65.82 & 60.91 & 68.43 & 42.41 & 62.34 \\\\ \\hline \\multirow{4}{*}{LLama-2 7OB} & Dense & 82.70 & 77.98 & 83.84 & 80.98 & 57.34 & 76.57 \\\\  & \\(20\\%\\) & 81.99 & 76.87 & 78.93 & 80.26 & 54.10 & 74.43 \\\\  & \\(25\\%\\) & 80.69 & 77.98 & 76.97 & 79.67 & 52.65 & 73.59 \\\\  & \\(30\\%\\) & 79.33 & 77.27 & 73.11 & 77.44 & 51.19 & 71.67 \\\\ \\hline \\multirow{4}{*}{Phi-2} & Dense & 79.11 & 75.77 & 73.83 & 78.32 & 54.18 & 72.24 \\\\  & \\(20\\%\\) & 76.17 & 68.75 & 61.95 & 72.18 & 45.48 & 64.90 \\\\ \\cline{1-1}  & \\(25\\%\\) & 75.68 & 64.88 & 58.19 & 70.41 & 43.43 & 62.52 \\\\ \\cline{1-1}  & \\(30\\%\\) & 74.05 & 62.12 & 53.31 & 67.26 & 39.42 & 63.47 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: OPT, Llama-2 및 Phi-2 모델 상의 Alpaca 데이터세트를 사용하여 슬라이싱할 때의 다운스트림 태스크 성능.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\hline \\hline Model & Slicing & PIQA & WinoGrande & HellaSwag & ARC-e & ARC-c & Avg. Score \\\\ \\hline \\multirow{4}{*}{Llama-2 TB} & Dense & 79.11 & 69.06 & 75.99 & 74.58 & 46.25 & 69.00 \\\\  & \\(20\\%\\) & 76.55 & 65.59 & 68.26 & 71.84 & 45.05 & 65.46 \\\\  & \\(25\\%\\) & 75.79 & 63.22 & 65.12 & 68.22 & 42.83 & 63.04 \\\\  & \\(30\\%\\) & 74.59 & 61.64 & 63.06 & 66.54 & 40.87 & 61.34 \\\\ \\hline \\multirow{4}{*}{Llama-2 13B} & Dense & 80.47 & 72.22 & 79.39 & 77.48 & 49.23 & 71.76 \\\\  & \\(20\\%\\) & 79.27 & 68.27 & 73.21 & 74.37 & 49.83 & 68.99 \\\\  & \\(25\\%\\) & 78.84 & 67.64 & 71.21 & 73.57 & 49.66 & 68.18 \\\\  & \\(30\\%\\) & 76.11 & 68.03 & 68.58 & 71.42 & 47.10 & 66.35 \\\\ \\hline \\multirow{4}{*}{Llama-2 70B} & Dense & 82.70 & 77.98 & 83.84 & 80.98 & 57.34 & 76.57 \\\\  & \\(20\\%\\) & 81.94 & 77.74 & 79.39 & 81.57 & 58.45 & 75.82 \\\\  & \\(25\\%\\) & 81.88 & 77.11 & 79.04 & 81.36 & 58.70 & 75.62 \\\\  & \\(30\\%\\) & 80.30 & 75.85 & 77.13 & 80.05 & 58.19 & 74.30 \\\\ \\hline \\multirow{4}{*}{Phi-2} & Dense & 79.11 & 75.77 & 73.83 & 78.32 & 54.18 & 72.24 \\\\  & \\(20\\%\\) & 77.42 & 72.14 & 65.33 & 74.20 & 49.91 & 67.80 \\\\  & \\(25\\%\\) & 76.17 & 68.75 & 63.39 & 70.45 & 47.44 & 65.24 \\\\  & \\(30\\%\\) & 75.24 & 65.59 & 60.10 & 70.16 & 46.25 & 63.47 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 라마-2 및 Phi-2 모델에 대한 알파카 데이터 세트를 사용하여 슬라이싱 및 복구 미세 조정 시 다운스트림 태스크 성능.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\hline \\hline Model & Slicing & PIQA & WinoGrande & HellaSwag & ARC-e & ARC-c & Avg. Score \\\\ \\hline \\multirow{4}{*}{Llama-2 TB} & Dense & 79.11 & 69.06 & 75.99 & 74.58 & 46.25 & 69.00 \\\\  & \\(20\\%\\) & 76.55 & 65.59 & 68.26 & 71.84 & 45.05 & 65.46 \\\\  & \\(25\\%\\) & 75.79 & 63.22 & 65.12 & 68.22 & 42.83 & 63.04 \\\\  & \\(30\\%\\) & 74.59 & 61.64 & 63.06 & 66.54 & 40.87 & 61.34 \\\\ \\hline \\multirow{4}{*}{Llama-2 TB} & Dense & 80.47 & 72.22 & 79.39 & 77.48 & 49.23 & 71.76 \\\\  & \\(20\\%\\) & 79.27 & 68.27 & 73.21 & 74.37 & 49.83 & 68.99 \\\\  & \\(25\\%\\) & 78.84 & 67.64 & 71.21 & 73.57 & 49.66 & 68.18 \\\\  & \\(30\\%\\) & 76.11 & 68.03 & 68.58 & 71.42 & 47.10 & 66.35 \\\\ \\hline \\multirow{4}{*}{Llama-2 TBB} & Dense & 82.70 & 77.98 & 83.84 & 80.98 & 57.34 & 76.57 \\\\  & \\(20\\%\\) & 77.08 & 68.75 & 63.39 & 70.45 & 47.44 & 65.24 \\\\ \\cline{1-1}  & \\(30\\%\\) & 75.24 & 65.59 & 60.10 & 70.16 & 46.25 & 63.47 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 라마-2 및 Phi-2 모델에 대한 알파카 데이터 세트를 사용하여 슬라이싱 및 복구 미세 조정 시 다운스트림 태스크 성능.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:21]\n' +
      '\n' +
      'SparseGPT에 대한 SliceGPT의 벤치마킹 추론 시간\n' +
      '\n' +
      '우리는 CuSparseLT 0.5 라이브러리를 사용하여 80GB A100 GPU에서 희소 행렬 곱셈을 실행하여 세 가지 다른 크기의 라마-2 모델에서 행렬 행렬 곱셈의 크기를 복제한다. 우리는 PyTorch를 사용하여 조밀한 등가물과 SliceGPT(간단한 조밀한 매트물이지만 더 작음)에 대해 유사한 행렬 곱셈을 실행했다. 2048의 시퀀스 길이를 선택하고 허깅페이스 구성 파일에서 매트릭스 크기를 취했다. 우리는 \\(10^{3}\\)의 시도에서 런타임의 중앙값을 취했다.\n' +
      '\n' +
      '각각의 Llama-2 층은 하나의 상향 투영, 하나의 하향 투영 및 게이트 투영을 갖는 게이트 FFN을 필요로 한다. 주의할 점은, 모델의 아키텍처는 질의 행렬 곱셈이 키 및 값 행렬 곱셈에 대한 상이한 크기임을 의미한다. 다음 표는 모델의 각 행렬 곱셈을 실행하는 데 걸리는 시간과 "총" 시간 및 상대 속도를 보여준다.\n' +
      '\n' +
      '우리는 또한 같은 방식으로 OPT 아키텍처를 벤치마킹했다. 이 경우, Key, Value, Query 및 Out과 연관된 행렬 곱셈은 모두 동일한 크기이며, MLP 섹션(FC1 및 FC2)에는 두 개의 행렬 곱셈만이 존재한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l|l|l|l|l|l|l|l|} \\hline \\hline \\multirow{2}{*}{Model} & \\multirow{2}{*}{Method} & \\multirow{2}{*}{PPL} & \\multicolumn{4}{c|}{Operation (ms)} & \\multicolumn{2}{c|}{Total in ms} \\\\  & & & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{FC2} & \\multicolumn{1}{c|}{FC1} & \\multicolumn{1}{c|}{K,V,Q,Out} & \\multicolumn{1}{c|}{(speedup)} \\\\ \\hline \\multirow{4}{*}{OPT 13B} & Dense & 10.13 & 1.89 & 1.89 & 0.52 & 7.75 \\\\  & SparseGPT 2:4 & 11.80 & 1.18 & 1.50 & 0.31 & 5.42 (1.43\\(\\times\\)) \\\\  & SliceGPT (25\\%) & 10.94 & 1.50 & 1.45 & 0.38 & 5.92 (1.31\\(\\times\\)) \\\\  & SliceGPT (50\\%) & 15.39 & 0.96 & 0.99 & 0.26 & 3.98 (1.95\\(\\times\\)) \\\\ \\hline \\multirow{4}{*}{OPT 30B} & Dense & 9.56 & 10.29 & 1.28 & 0.52 & 5.93 \\\\  & SparseGPT 2:4 & 10.53 & 0.81 & 0.95 & 0.31 & 3.95 (1.50\\(\\times\\)) \\\\  & SliceGPT (25\\%) & 10.05 & 1.03 & 0.98 & 0.39 & 4.55 (1.30\\(\\times\\)) \\\\  & SliceGPT (50\\%) & 12.47 & 0.68 & 0.67 & 0.26 & 3.06 (1.94\\(\\times\\)) \\\\ \\hline \\multirow{4}{*}{OPT 66B} & Dense & 9.34 & 4.63 & 4.27 & 0.21 & 14.01 \\\\  & SparseGPT 2:4 & 10.22 & 2.87 & 3.69 & 0.14 & 10.81 (1.30\\(\\times\\)) \\\\ \\cline{1-1}  & SliceGPT (25\\%) & 9.70 & 3.40 & 3.26 & 0.16 & 10.56 (1.33\\(\\times\\)) \\\\ \\cline{1-1}  & SliceGPT (50\\%) & 11.39 & 2.28 & 2.34 & 0.15 & 7.56 (1.85\\(\\times\\)) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 13: OPT 모델의 각 계층에 관련된 행렬 곱셈의 타이밍 결과. 더 큰 모델의 경우 SliceGPT(25%)가 SparseGPT 2:4보다 약간 더 나은 속도를 제공하고 더 나은 당혹감을 제공한다. 더 작은 모델의 경우 SparseGPT 2:4는 더 난해하지만 더 나은 속도를 제공한다. 50%의 슬라이싱은 더 큰 속도 향상을 위해 당혹감을 거래한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l|l|l|l|l|l|l|l|l|} \\hline \\hline \\multirow{2}{*}{Model} & \\multirow{2}{*}{Method} & \\multirow{2}{*}{PPL} & \\multicolumn{4}{c|}{Operation (ms)} & \\multicolumn{2}{c|}{Total in ms} \\\\  & & & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} \\\\  & & & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} \\\\  & & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} \\\\ \\hline \\multirow{4}{*}{Llama-2 7B} & Dense & 5.11 & 0.89 & 0.87 & 0.34 & 0.34 & 0.34 & 3.99 \\\\  & SparseGPT 2:4 & 8.15 & 0.56 & 0.61 & 0.23 & 0.23 & 0.23 & 2.70 (1.48\\(\\times\\)) \\\\  & SliceGPT (25\\%) & 6.70 & 0.67 & 0.64 & 0.26 & 0.25 & 0.27 & 2.99 (1.33\\(\\times\\)) \\\\  & SliceGPT (50\\%) & 17.17 & 0.46 & 0.44 & 0.18 & 0.18 & 0.18 & 2.06 (1.94\\(\\times\\)) \\\\ \\hline \\multirow{4}{*}{Llama-2 13B} & Dense & 4.58 & 1.29 & 1.28 & 0.52 & 0.52 & 0.52 & 5.93 \\\\  & SparseGPT 2:4 & 6.63 & 0.81 & 0.95 & 0.31 & 0.31 & 0.31 & 3.95 (1.50\\(\\times\\)) \\\\  & SliceGPT (25\\%) & 5.90 & 1.03 & 0.98 & 0.39 & 0.39 & 0.41 & 4.57 (1.30\\(\\times\\)) \\\\  & SliceGPT (50\\%) & 13.71 & 0.68 & 0.67 & 0.26 & 0.27 & 0.30 & 3.11 (1.91\\(\\times\\)) \\\\ \\hline \\multirow{4}{*}{Llama-2 70B} & Dense & 3.12 & 4.63 & 4.27 & 0.21 & 1.27 & 1.27 & 16.13 \\\\  & SparseGPT 2:4 & 4.70 & 2.87 & 3.69 & 0.14 & 0.84 & 0.83 & 12.20 (1.32\\(\\times\\)) \\\\ \\cline{1-1}  & SliceGPT (25\\%) & 4.35 & 3.4 & 3.26 & 0.16 & 0.96 & 1.00 & 12.20 (1.32\\(\\times\\)) \\\\ \\cline{1-1}  & SliceGPT (50\\%) & 8.86 & 2.28 & 2.34 & 0.15 & 0.69 & 0.68 & 8.63 (1.87\\(\\times\\)) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 12: Llama-2 모델의 각 계층에 관련된 행렬 곱셈의 타이밍 결과. 더 큰 모델의 경우 SliceGPT(25%)는 SparseGPT 2:4와 동일한 속도를 제공하지만 더 나은 당혹감을 제공한다. 더 작은 모델의 경우 SparseGPT 2:4는 더 난해하지만 더 나은 속도를 제공한다. 50%의 슬라이싱은 더 큰 속도 향상을 위해 당혹감을 거래한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>