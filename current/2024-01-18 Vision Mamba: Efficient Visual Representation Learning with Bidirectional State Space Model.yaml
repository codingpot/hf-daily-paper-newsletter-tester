date: "2024-01-18"
author: Lianghui Zhu
title: 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3CR9QVEIjbOr4I8mOvDy1.png
link: https://huggingface.co/papers/2401.09417
summary: This paper proposes a new vision backbone called Vim that uses bidirectional state space models to learn efficient visual representations for image classification, object detection, and semantic segmentation tasks. Vim achieves higher performance compared to vision transformers like DeiT, while also being faster and more memory-efficient. The results demonstrate that Vim has the potential to become a next-generation backbone for vision foundation models. Code is available at <https://github.com/...
opinion: placeholder
tags:
    - Computer Vision
    - Deep Learning
