date: "2024-01-18"
author: Lianghui Zhu
title: 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3CR9QVEIjbOr4I8mOvDy1.png
link: https://huggingface.co/papers/2401.09417
summary: This paper proposes a new vision backbone called Vision Mamba (Vim) that uses bidirectional state space models to efficiently represent visual data. Vim outperforms existing vision transformers like DeiT on image classification, object detection, and semantic segmentation tasks, while being more computationally and memory-efficient. It overcomes the limitations of SSMs in representing visual data and has the potential to become the next-generation backbone for vision foundation models. The code ...
opinion: placeholder
tags:
    - Computer Vision
    - Deep Learning
    - Optimization and Learning Algorithms
