date: "2024-01-18"
author: Lianghui Zhu
title: 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3CR9QVEIjbOr4I8mOvDy1.png
link: https://huggingface.co/papers/2401.09417
summary: This paper proposes a new generic vision backbone, called Vision Mamba (Vim), which is based on bidirectional state space models and is capable of efficient visual representation learning. Vim uses position embeddings and bidirectional state space models to compress the visual representation and perform Transformer-style understanding on high-resolution images, outperforming well-established vision transformers like DeiT in terms of both performance and efficiency....
opinion: placeholder
tags:
    - Computer Vision
    - Deep Learning
