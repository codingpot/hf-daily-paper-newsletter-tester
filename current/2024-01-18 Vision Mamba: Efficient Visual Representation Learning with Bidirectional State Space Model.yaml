date: "2024-01-18"
author: Lianghui Zhu
title: 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3CR9QVEIjbOr4I8mOvDy1.png
link: https://huggingface.co/papers/2401.09417
summary: This paper proposes a new vision representation learning method called Vision Mamba (Vim), which uses bidirectional state space models to efficiently compress visual representations of images while maintaining high performance on tasks such as image classification, object detection, and semantic segmentation. Vim is faster and more memory-efficient than existing vision transformers, and has the potential to become the next-generation backbone for vision foundation models....
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Computer Vision
