date: "2024-01-18"
author: Lianghui Zhu
title: 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3CR9QVEIjbOr4I8mOvDy1.png
link: https://huggingface.co/papers/2401.09417
summary: This paper proposes a new vision backbone called Vim that uses bidirectional Mamba blocks for efficient visual representation learning, compared to existing transformers. Vim outperforms DeiT on ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks while being faster and more memory-efficient. It addresses the position-sensitivity of visual data and the need for global context for visual understanding....
opinion: placeholder
tags:
    - Computer Vision
    - Deep Learning
