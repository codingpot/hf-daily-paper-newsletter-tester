date: "2024-01-18"
author: Lianghui Zhu
title: 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3CR9QVEIjbOr4I8mOvDy1.png
link: https://huggingface.co/papers/2401.09417
summary: The paper presents Vision Mamba (Vim), a new vision backbone with bidirectional Mamba blocks for efficient visual representation learning. Unlike vision transformers, Vim uses state space models to handle position-sensitive visual data and compress visual representation using bidirectional models. Vim outperforms DeiT on ImageNet, COCO object detection, and ADE20k semantic segmentation tasks, and is more computationally and memory efficient. Vim has the potential to become a next-generation back...
opinion: placeholder
tags:
    - Deep Learning
    - Computer Vision
