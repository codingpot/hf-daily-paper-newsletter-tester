date: "2024-01-18"
author: Lianghui Zhu
title: 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3CR9QVEIjbOr4I8mOvDy1.png
link: https://huggingface.co/papers/2401.09417
summary: This paper proposes a new vision backbone called Vision Mamba (Vim) that learns visual representations using bidirectional state space models. Vim is efficient and performs better than DeiT on ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks while saving computation and memory. Vim can overcome the constraints of Transformer-style understanding for high-resolution images and has the potential to become the next-generation vision backbone....
opinion: placeholder
tags:
    - Computer Vision
