date: "2024-01-18"
author: Lianghui Zhu
title: 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3CR9QVEIjbOr4I8mOvDy1.png
link: https://huggingface.co/papers/2401.09417
summary: The paper proposes a new vision backbone called Vision Mamba or Vim, which uses bidirectional state space models for efficient and generic visual representation learning. Vim outperforms existing vision transformers like DeiT on ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks while being more computationally and memory efficient. It has the potential to become the next-generation backbone for vision foundation models....
opinion: placeholder
tags:
    - Computer Vision
