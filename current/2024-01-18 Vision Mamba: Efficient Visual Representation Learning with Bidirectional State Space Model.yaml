author: Lianghui Zhu
date: '2024-01-18'
link: https://huggingface.co/papers/2401.09417
opinion: placeholder
summary: The paper proposes a new vision backbone called Vision Mamba or Vim, which
  uses bidirectional state space models for efficient and generic visual representation
  learning. Vim outperforms existing vision transformers like DeiT on ImageNet classification,
  COCO object detection, and ADE20k semantic segmentation tasks while being more computationally
  and memory efficient. It has the potential to become the next-generation backbone
  for vision foundation models....
tags:
- Computer Vision
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3CR9QVEIjbOr4I8mOvDy1.png
title: 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional
  State Space Model'
translated_path:
  ko: ../translated-papers/2401.09417/paper.ko.html
