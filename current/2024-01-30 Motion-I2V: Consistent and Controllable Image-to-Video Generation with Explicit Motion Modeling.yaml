author: Xiaoyu Shi
date: '2024-01-30'
link: https://huggingface.co/papers/2401.15977
opinion: placeholder
summary: 'The paper proposes Motion-I2V, a framework for generating consistent and
  controllable videos by splitting the process into two stages: one for predicting
  motion, and another for enhancing video with this motion information. It offers
  more control over video generation than previous methods and can perform zero-shot
  video-to-video translation. ...'
tags:
- Computer Vision
- Deep Learning
- Explainable AI and Interpretability
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/DFR3GpMI4021qU9o36Vhk.png
title: 'Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit
  Motion Modeling'
translated_paths:
  INT: https://raw.githack.com/codingpot/hf-daily-paper-newsletter-tester/main/translated-papers/2401.15977/paper.en.html
  KR: https://raw.githack.com/codingpot/hf-daily-paper-newsletter-tester/main/translated-papers/2401.15977/paper.ko.html
