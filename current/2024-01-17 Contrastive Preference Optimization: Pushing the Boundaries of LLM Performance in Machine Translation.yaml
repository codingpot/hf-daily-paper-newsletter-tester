date: "2024-01-17"
author: Haoran Xu
title: 'Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/8z2LsPN_L9OdoevoTOZiO.png
link: https://huggingface.co/papers/2401.08417
summary: This paper proposes Contrastive Preference Optimization (CPO), a novel training approach for machine translation (MT) that trains models to avoid generating adequate but not perfect translations instead of mimicking human-generated translations from reference data, which have quality issues. The study applies CPO to ALMA models with limited data and achieves significant improvements, producing the ALMA-R model that equals or surpasses the performance of the WMT competition winners and GPT-4 on v...
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Natural Language Processing
    - Machine Translation
