author: Bin Lin
date: '2024-01-30'
link: https://huggingface.co/papers/2401.15947
opinion: placeholder
summary: The paper proposes a technique called MoE-tuning (Mix of Experts) that can
  create a large, sparse model that only uses a few parameters at a time, which reduces
  training and inference costs. They also present MoE-LLaVA, a framework based on
  this technique that can better understand images and reduce errors in predictions
  compared to other methods....
tags:
- Supervised Learning
- Deep Learning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3ta3Rnuv4Syt6mKIu_8BE.png
title: 'MoE-LLaVA: Mixture of Experts for Large Vision-Language Models'
translated_paths:
  INT: https://raw.githack.com/codingpot/hf-daily-paper-newsletter-tester/main/translated-papers/2401.15947/paper.en.html
  KR: https://raw.githack.com/codingpot/hf-daily-paper-newsletter-tester/main/translated-papers/2401.15947/paper.ko.html
