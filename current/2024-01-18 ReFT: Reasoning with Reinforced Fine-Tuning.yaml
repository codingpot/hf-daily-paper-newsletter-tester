date: "2024-01-18"
author: Trung Quoc Luong
title: 'ReFT: Reasoning with Reinforced Fine-Tuning'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/bPp1x6IrywnkFJUhY7QLH.png
link: https://huggingface.co/papers/2401.08967
summary: One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple ...
opinion: placeholder
tags:
    - Supervised Learning
    - Reinforcement Learning
    - Natural Language Processing
