date: "2024-01-18"
author: Trung Quoc Luong
title: 'ReFT: Reasoning with Reinforced Fine-Tuning'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/bPp1x6IrywnkFJUhY7QLH.png
link: https://huggingface.co/papers/2401.08967
summary: ReFT is a method that uses reinforcement learning to fine-tune large language models (LLMs) for reasoning tasks. It employs an online PPO algorithm to fine-tune the model further after supervised fine-tuning with chain-of-thought (CoT) annotations. ReFT generates a large number of reasoning paths automatically and uses ground-truth answers as rewards. Experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms supervised fine-tuning and can further improve performan...
opinion: placeholder
tags:
    - Supervised Learning
    - Unsupervised Learning
    - Reinforcement Learning
    - Deep Learning
    - Natural Language Processing
