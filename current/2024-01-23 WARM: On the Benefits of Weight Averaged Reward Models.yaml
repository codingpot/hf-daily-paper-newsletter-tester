author: "Alexandre Ram\xE9"
date: '2024-01-23'
link: https://huggingface.co/papers/2401.12187
opinion: placeholder
summary: The paper proposes a solution called Weight Averaged Reward Models (WARM)
  to mitigate reward hacking in large language models by averaging multiple reward
  models' weights. WARM improves the quality and alignment of language model predictions
  and is more efficient compared to traditional ensembling methods....
tags:
- Supervised Learning
- Reinforcement Learning
- Optimization and Learning Algorithms
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/T4eMvoGUt2g-EoLiQRDqP.png
title: 'WARM: On the Benefits of Weight Averaged Reward Models'
translated_paths:
  INT: https://raw.githack.com/codingpot/hf-daily-paper-newsletter-tester/main/translated-papers/2401.12187/paper.en.html
  KR: https://raw.githack.com/codingpot/hf-daily-paper-newsletter-tester/main/translated-papers/2401.12187/paper.ko.html
