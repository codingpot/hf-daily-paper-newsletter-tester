author: Ling Yang
date: '2024-01-23'
link: https://huggingface.co/papers/2401.11708
opinion: placeholder
summary: This paper proposes a training-free text-to-image generation/edition framework
  called RPG that uses multimodal LLMs to improve text-to-image diffusion models for
  handling complex text prompts. It decomposes the image generation process into simpler
  tasks, enables compositional generation, and integrates text-guided image generation
  and editing. Experiments show RPG outperforms state-of-the-art models, particularly
  in multi-category object composition and semantic alignment....
tags:
- Supervised Learning
- Deep Learning
- Natural Language Processing
- Computer Vision
- Emerging Applications of Machine Learning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/x8Du5h62UaslJ3-VwzFK7.png
title: 'Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating
  with Multimodal LLMs'
translated_paths:
  INT: https://raw.githack.com/codingpot/hf-daily-paper-newsletter-tester/main/translated-papers/2401.11708/paper.en.html
  KR: https://raw.githack.com/codingpot/hf-daily-paper-newsletter-tester/main/translated-papers/2401.11708/paper.ko.html
