date: "2024-01-12"
author: Hui Wu
title: Efficient LLM inference solution on Intel GPU
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/6gmOA3lUAt4669tHBzvKy.png
link: https://huggingface.co/papers/2401.05391
summary: This paper proposes an efficient LLM inference solution on Intel GPU with low latency and high throughput by simplifying the LLM decoder layer and proposing a segment KV cache policy. The customized Scaled-Dot-Product-Attention kernel is designed to match the fusion policy based on the segment KV cache solution. The solution achieves up to 7x lower token latency and 27x higher throughput for some popular LLMs on Intel GPU compared to the standard HuggingFace implementation. The paper is publicly...
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
