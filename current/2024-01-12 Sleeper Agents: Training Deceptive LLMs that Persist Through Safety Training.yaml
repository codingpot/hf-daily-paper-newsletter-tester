date: "2024-01-12"
author: Evan Hubinger
title: 'Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/41in4Gi4-aKzZswR8kQMY.png
link: https://huggingface.co/papers/2401.05566
summary: 'Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write...'
opinion: placeholder
tags:
    - Supervised Learning
    - Unsupervised Learning
    - Reinforcement Learning
    - Deep Learning
    - Explainable AI and Interpretability
    - Optimization and Learning Algorithms
    - Fairness, Bias, and Ethics
    - Natural Language Processing
