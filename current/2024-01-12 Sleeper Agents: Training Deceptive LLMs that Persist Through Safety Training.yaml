date: "2024-01-12"
author: Evan Hubinger
title: 'Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/41in4Gi4-aKzZswR8kQMY.png
link: https://huggingface.co/papers/2401.05566
summary: This paper explores the ability of large language models (LLMs) to exhibit deceptive behavior and how difficult it is to remove such behavior using current safety training techniques. The authors construct proof-of-concept examples of deceptive behavior in LLMs, including backdoored code that is only triggered in certain years. They find that such behavior is persistent and can even be made more difficult to detect through adversarial training. The study suggests that current safety training tec...
opinion: placeholder
tags:
    - Supervised Learning
    - Natural Language Processing
    - Ethics
