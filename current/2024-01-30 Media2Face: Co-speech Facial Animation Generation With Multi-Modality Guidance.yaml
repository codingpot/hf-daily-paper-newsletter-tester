author: Qingcheng Zhao
date: '2024-01-30'
link: https://huggingface.co/papers/2401.15687
opinion: placeholder
summary: Media2Face is a diffusion model that can generate 3D facial animations with
  high realism, accept multi-modal guidance (audio, text, image), and broaden expressiveness
  and style adaptability, using a variational autoencoder called GNPFA, and a large,
  diverse data set called M2F-D....
tags:
- Deep Learning
- Computer Vision
- Human-Computer Interaction (HCI) and User Interfaces
thumbnail: https://github.com/codingpot/hf-daily-paper-newsletter-tester/blob/main/assets/2401.15687.gif?raw=true
title: 'Media2Face: Co-speech Facial Animation Generation With Multi-Modality Guidance'
