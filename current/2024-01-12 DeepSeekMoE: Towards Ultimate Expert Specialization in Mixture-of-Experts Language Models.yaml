date: "2024-01-12"
author: Damai Dai
title: 'DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/1HZxDdOSYIMaIsR1eyKBP.png
link: https://huggingface.co/papers/2401.06066
summary: This paper proposes a new architecture, DeepSeekMoE, for Mixture-of-Experts language models toward ultimate expert specialization. The proposed architecture involves segmenting experts into a larger number of finer experts and activating a more flexible combination of them. It also isolates shared experts for capturing common knowledge and isolating redundancy in routed experts. DeepSeekMoE achieves comparable performance with fewer parameters and computations compared to the conventional GShard...
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Natural Language Processing
