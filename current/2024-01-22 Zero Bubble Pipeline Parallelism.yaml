date: "2024-01-22"
author: Penghui Qi
title: Zero Bubble Pipeline Parallelism
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/gITPhCb7DEvKRiVeZUuI0.png
link: https://huggingface.co/papers/2401.10241
summary: This paper introduces a scheduling strategy for large-scale distributed training that achieves zero pipeline bubbles under synchronous training semantics. The key idea is to split the backward computation into two parts and handcraft novel pipeline schedules. An algorithm is also developed to automatically find an optimal schedule based on specific model configuration and memory limit. Experimental evaluations show that this method outperforms the 1F1B schedule up to 31% in throughput under rela...
opinion: placeholder
tags:
    - Optimization and Learning Algorithms
