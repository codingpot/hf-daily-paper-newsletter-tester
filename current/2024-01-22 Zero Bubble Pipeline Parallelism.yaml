author: Penghui Qi
date: '2024-01-22'
link: https://huggingface.co/papers/2401.10241
opinion: placeholder
summary: This paper introduces a scheduling strategy for large-scale distributed training
  that achieves zero pipeline bubbles under synchronous training semantics. The key
  idea is to split the backward computation into two parts and handcraft novel pipeline
  schedules. An algorithm is also developed to automatically find an optimal schedule
  based on specific model configuration and memory limit. Experimental evaluations
  show that this method outperforms the 1F1B schedule up to 31% in throughput under
  rela...
tags:
- Optimization and Learning Algorithms
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/gITPhCb7DEvKRiVeZUuI0.png
title: Zero Bubble Pipeline Parallelism
translated_paths:
  KR: https://raw.githack.com/codingpot/hf-daily-paper-newsletter-tester/main/translated-papers/2401.10241/paper.ko.html
