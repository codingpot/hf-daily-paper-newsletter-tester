date: "2024-01-12"
author: Binghai Wang
title: 'Secrets of RLHF in Large Language Models Part II: Reward Modeling'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/t6RdI5ayxVYgL-AlJCFyv.png
link: https://huggingface.co/papers/2401.06080
summary: The paper discusses challenges faced by Reward Models in Reinforcement Learning from Human Feedback (RLHF) and proposes solutions to address them. The first solution is a method to measure the strength of preferences in the dataset using a voting mechanism of multiple reward models. Experimental results show that data with varying preference strengths have different impacts on reward model performance. The paper introduces novel methods to mitigate the influence of incorrect and ambiguous prefer...
opinion: placeholder
tags:
    - Reinforcement Learning
