date: "2024-01-18"
author: Nanye Ma
title: 'SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/hdPDnPoXo91-ivR-5d52c.png
link: https://huggingface.co/papers/2401.08740
summary: Scalable Interpolant Transformers (SiT) is a family of generative models built on the backbone of Diffusion Transformers (DiT). SiT allows for more flexible modular design choices which include continuous or discrete time learning, objective function, interpolant connecting the distributions, and deterministic or stochastic sampler. SiT was found to perform better than DiT uniformly across model sizes on the conditional ImageNet 256x256 benchmark with an FID 50K score of 2.06....
opinion: placeholder
tags:
    - Generative Models
    - Bayesian Learning
    - Deep Learning
    - Computer Vision
