date: "2024-01-18"
author: Bo Liu
title: Asynchronous Local-SGD Training for Language Modeling
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wV15FTUGHbZ62WNxqwYgF.png
link: https://huggingface.co/papers/2401.09135
summary: This paper presents an empirical study of asynchronous Local-SGD for training language models. The study investigates the impact of worker hardware heterogeneity, model size, number of workers, and optimizer on learning performance. The results show that with naive implementations, asynchronous Local-SGD takes more iterations to converge than synchronous Local-SGD. The authors identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge and propose a...
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Optimization and Learning Algorithms
    - Natural Language Processing
