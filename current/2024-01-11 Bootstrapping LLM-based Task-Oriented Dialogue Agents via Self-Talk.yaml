date: 2024-01-11 00:00
author: Dennis Ulmer
title: Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/krSRbXOs5mGCADXGa_929.png
link: https://huggingface.co/papers/2401.05033
summary: Large language models (LLMs) are powerful dialogue agents, but specializing them towards fulfilling a specific function can be challenging. Instructing tuning, i.e. tuning models on instruction and sample responses generated by humans (Ouyang et al., 2022), has proven as an effective method to do so, yet requires a number of data samples that a) might not be available or b) costly to generate. Furthermore, this cost increases when the goal is to make the LLM follow a specific workflow within a d...
opinion: placeholder
tags:
    - Supervised Learning
    - Natural Language Processing
