date: "2024-01-18"
author: Bo Liu
title: Asynchronous Local-SGD Training for Language Modeling
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wV15FTUGHbZ62WNxqwYgF.png
link: https://huggingface.co/papers/2401.09135
summary: This paper presents an empirical study of asynchronous Local-SGD for training language models. They found that the approach takes more iterations to converge than its synchronous counterpart due to the challenge of dealing with stale worker gradients. They propose a novel method using delayed Nesterov momentum update and adjusting worker training steps based on computation speed, which matches the performance of synchronous Local-SGD in terms of perplexity per update step and reduces wall clock ...
opinion: placeholder
tags:
    - Supervised Learning
    - Natural Language Processing
    - Optimization and Learning Algorithms
