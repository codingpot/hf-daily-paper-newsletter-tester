date: "2024-01-18"
author: Lianghui Zhu
title: 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3CR9QVEIjbOr4I8mOvDy1.png
link: https://huggingface.co/papers/2401.09417
summary: This paper proposes a new generic vision backbone called Vim, which uses bidirectional state space models (SSMs) for efficient visual representation learning. Unlike existing vision transformers, Vim relies on position embeddings and bidirectional SSMs to compress visual representations. It achieves higher performance on ImageNet, COCO, and ADE20k tasks while improving computation and memory efficiency. Vim outperforms DeiT by 2.8 times in speed and saves 86.8% GPU memory. The proposed backbone ...
opinion: placeholder
tags:
    - Computer Vision
    - Deep Learning
