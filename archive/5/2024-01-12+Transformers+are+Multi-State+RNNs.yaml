date: "2024-01-12"
author: Matanel Oren
title: Transformers are Multi-State RNNs
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/1E499u2_kDGHv5MrCOB2k.png
link: https://huggingface.co/papers/2401.06104
summary: This paper shows that transformers, a new type of NLP model, can be reinterpreted as a type of RNN called infinite multi-state RNN. The paper also introduces a new policy called TOVA that reduces the cache memory size of transformers while maintaining performance. The results suggest that transformers often behave like RNNs and this finding could help mitigate one of their biggest challenges - cache memory size....
opinion: placeholder
tags:
    - Natural Language Processing
    - Deep Learning
    - Explainable AI and Interpretability
    - Optimization and Learning Algorithms
    - Emerging Applications of Machine Learning
