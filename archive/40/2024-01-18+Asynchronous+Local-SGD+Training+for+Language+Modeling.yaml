date: "2024-01-18"
author: Bo Liu
title: Asynchronous Local-SGD Training for Language Modeling
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wV15FTUGHbZ62WNxqwYgF.png
link: https://huggingface.co/papers/2401.09135
summary: This paper presents an empirical study of asynchronous Local-SGD for training language models. It explores the impact of worker hardware heterogeneity, model size, number of workers, and optimizer on learning performance. The paper identifies momentum acceleration on stale worker gradients as a key challenge. A novel method, which utilizes a delayed Nesterov momentum update and adjusts workers' local training steps based on computation speed, is proposed. This approach matches the performance of...
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Optimization and Learning Algorithms
    - Natural Language Processing
