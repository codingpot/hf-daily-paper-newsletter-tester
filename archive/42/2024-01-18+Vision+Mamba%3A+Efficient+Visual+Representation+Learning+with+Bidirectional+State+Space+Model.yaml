date: "2024-01-18"
author: Lianghui Zhu
title: 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3CR9QVEIjbOr4I8mOvDy1.png
link: https://huggingface.co/papers/2401.09417
summary: The paper proposes a new generic vision backbone called Vision Mamba (Vim) for efficient visual representation learning using bidirectional state space models. Vim represents visual data using position embeddings and bidirectional state space models to achieve higher performance and efficiency compared to well-established vision transformers like DeiT. Vim has the potential to become the next-generation backbone for vision foundation models and can be accessed at <https://github.com/hustvl/Vim>....
opinion: placeholder
tags:
    - Computer Vision
    - Deep Learning
