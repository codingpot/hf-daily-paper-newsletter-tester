date: "2024-01-12"
author: Hui Wu
title: Efficient LLM inference solution on Intel GPU
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/6gmOA3lUAt4669tHBzvKy.png
link: https://huggingface.co/papers/2401.05391
summary: This paper proposes an efficient inference solution for large language models (LLMs) using Intel GPU. The solution reduces memory access frequency and lower system latency by simplifying the decoder layer and using segment KV cache policy. It improves throughput and reduces token latency by up to 27x and 7x, respectively, compared to the standard HuggingFace implementation....
opinion: placeholder
tags:
    - Deep Learning
    - Optimization and Learning Algorithms
    - Natural Language Processing
