date: "2024-01-18"
author: Lianghui Zhu
title: 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3CR9QVEIjbOr4I8mOvDy1.png
link: https://huggingface.co/papers/2401.09417
summary: This paper proposes a new generic vision backbone, Vision Mamba (Vim), using bidirectional state space models for efficient and generic visual representation learning. Vim uses position embeddings and bidirectional state space models to compress visual representation. Vim outperforms DeiT, a well-established vision transformer, while achieving higher computation and memory efficiency. Vim has the potential to become the next-generation backbone for vision foundation models....
opinion: placeholder
tags:
    - Computer Vision
    - Deep Learning
