date: "2024-01-18"
author: Bo Liu
title: Asynchronous Local-SGD Training for Language Modeling
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wV15FTUGHbZ62WNxqwYgF.png
link: https://huggingface.co/papers/2401.09135
summary: This paper presents an empirical study of asynchronous Local-SGD for training language models. It examines the impact of hardware heterogeneity, model size, number of workers, and optimizer on learning performance. The authors find that naive implementations of asynchronous Local-SGD take more iterations to converge than synchronous Local-SGD. They propose a novel method that utilizes a delayed Nesterov momentum update and adjusts local training steps based on computation speed, which can match ...
opinion: placeholder
tags:
    - Supervised Learning
    - Optimization and Learning Algorithms
    - Natural Language Processing
