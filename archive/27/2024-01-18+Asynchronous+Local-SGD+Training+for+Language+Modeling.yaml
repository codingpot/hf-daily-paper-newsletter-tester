date: "2024-01-18"
author: Bo Liu
title: Asynchronous Local-SGD Training for Language Modeling
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wV15FTUGHbZ62WNxqwYgF.png
link: https://huggingface.co/papers/2401.09135
summary: This paper presents an empirical study of asynchronous Local-SGD for training language models and identifies momentum acceleration on stale worker gradients as a key challenge. It proposes a novel method that utilizes a delayed Nesterov momentum update and adjusts workers' local training steps based on their computation speed, which improves the performance in terms of wall clock time and matches the performance of synchronous Local-SGD in terms of perplexity per update step....
opinion: placeholder
tags:
    - Supervised Learning
    - Optimization and Learning Algorithms
    - Deep Learning
    - Natural Language Processing
