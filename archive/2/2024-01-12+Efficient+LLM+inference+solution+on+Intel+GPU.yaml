date: "2024-01-12"
author: Hui Wu
title: Efficient LLM inference solution on Intel GPU
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/6gmOA3lUAt4669tHBzvKy.png
link: https://huggingface.co/papers/2401.05391
summary: Transformer based Large Language Models (LLMs) have been widely used in many fields, and the efficiency of LLM inference becomes hot topic in real applications. However, LLMs are usually complicatedly designed in model structure with massive operations and perform inference in the auto-regressive mode, making it a challenging task to design a system with high efficiency.   In this paper, we propose an efficient LLM inference solution with low latency and high throughput. Firstly, we simplify the...
opinion: placeholder
tags:
    - Deep Learning
    - Optimization and Learning Algorithms
    - Computer Vision
