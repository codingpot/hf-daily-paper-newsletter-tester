author: Trung Quoc Luong
date: '2024-01-18'
link: https://huggingface.co/papers/2401.08967
opinion: placeholder
summary: One way to enhance the reasoning capability of Large Language Models (LLMs)
  is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations.
  This approach does not show sufficiently strong generalization ability, however,
  because the training only relies on the given CoT data. In math problem-solving,
  for example, there is usually only one annotated reasoning path for each question
  in the training data. Intuitively, it would be better for the algorithm to learn
  from multiple ...
tags:
- Supervised Learning
- Reinforcement Learning
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/bPp1x6IrywnkFJUhY7QLH.png
title: 'ReFT: Reasoning with Reinforced Fine-Tuning'
translated_path:
  ko: ../translated-papers/2401.08967/paper.ko.html
