date: "2024-01-18"
author: Bo Liu
title: Asynchronous Local-SGD Training for Language Modeling
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wV15FTUGHbZ62WNxqwYgF.png
link: https://huggingface.co/papers/2401.09135
summary: This paper presents an empirical study of asynchronous Local-SGD for training language models and examines the impact of factors like worker hardware heterogeneity, model size, number of workers, and optimizer on learning performance. The study identifies momentum acceleration on global parameters when worker gradients are stale as a key challenge and proposes a novel method using a delayed Nesterov momentum update to match the performance of synchronous Local-SGD in terms of perplexity per upda...
opinion: placeholder
tags:
    - Optimization and Learning Algorithms
    - Natural Language Processing
