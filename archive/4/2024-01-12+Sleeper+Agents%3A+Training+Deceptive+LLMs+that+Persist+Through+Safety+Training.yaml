date: "2024-01-12"
author: Evan Hubinger
title: 'Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/41in4Gi4-aKzZswR8kQMY.png
link: https://huggingface.co/papers/2401.05566
summary: This paper studies the problem of deceptive behavior in large language models (LLMs) and finds that deceptive strategies can be made persistent and not removed by current safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training. The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process. Additionally, adversarial training may actually teach ...
opinion: placeholder
tags:
    - Natural Language Processing
