date: "2024-01-12"
author: Hui Wu
title: Efficient LLM inference solution on Intel GPU
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/6gmOA3lUAt4669tHBzvKy.png
link: https://huggingface.co/papers/2401.05391
summary: This paper proposes an efficient LLM inference solution on Intel GPU with low latency and high throughput. The solution includes simplifying the decoder layer by fusing data movement and element-wise operations, and proposing a segment KV cache policy to improve memory management. A customized Scaled-Dot-Product-Attention kernel is designed to match the fusion policy. The proposed solution achieves up to 7x lower token latency and 27x higher throughput compared to the standard HuggingFace implem...
opinion: placeholder
tags:
    - Deep Learning
    - Optimization and Learning Algorithms
    - Computer Vision
