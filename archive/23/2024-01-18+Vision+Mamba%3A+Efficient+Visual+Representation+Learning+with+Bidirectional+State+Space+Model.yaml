date: "2024-01-18"
author: Lianghui Zhu
title: 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3CR9QVEIjbOr4I8mOvDy1.png
link: https://huggingface.co/papers/2401.09417
summary: This paper proposes a new framework for visual representation learning called Vision Mamba (Vim) that uses bidirectional state space models instead of self-attention. Vim achieves better performance and efficiency than existing vision transformers like DeiT on ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks. Vim is 2.8 times faster and saves 86.8% GPU memory compared to DeiT for batch inference on high-resolution images, making it a promising candidate for ...
opinion: placeholder
tags:
    - Deep Learning
    - Computer Vision
