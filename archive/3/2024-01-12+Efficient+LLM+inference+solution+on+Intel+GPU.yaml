date: "2024-01-12"
author: Hui Wu
title: Efficient LLM inference solution on Intel GPU
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/6gmOA3lUAt4669tHBzvKy.png
link: https://huggingface.co/papers/2401.05391
summary: This paper proposes an efficient inference solution for large language models (LLMs) on Intel GPU, focused on reducing latency and improving throughput. The solution involves simplifying the LLM decoder layer, using segment KV cache policy for memory management, and designing a custom Scaled-Dot-Product-Attention kernel. The solution achieves up to 7x lower token latency and 27x higher throughput for some popular LLMs on Intel GPU compared to the standard HuggingFace implementation....
opinion: placeholder
tags:
    - Optimization and Learning Algorithms
    - Deep Learning
    - Natural Language Processing
